{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'EleutherAI/gpt-neo-1.3B'\n",
    "    TOKENIZER_PATH = 'EleutherAI/gpt-neo-1.3B'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     DEVICE = \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'gpt-neo-1_3B'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929301e1-626d-4ba5-9f32-d361769f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenizer.vocab.txt', 'w') as f:\n",
    "    for k, v in tokenizer.vocab.items():\n",
    "        f.write(f'{k}: {v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b946f5-a6f0-4415-911c-e5a7f628f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '______'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "        assert tokenizer.pad_token == pad_token\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69ec8b1-1d38-46f9-af3b-4a34e0c8dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(cfg.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9b003d-a13f-43c9-830e-edecafdec275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 50257)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daf9ae26-87b2-4e81-b360-dc7cb379e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25947"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7,\n",
    "            \"pad_token_id\": 25947\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, past_key_values=None, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.transformer.wte.weight torch.Size([50257, 2048])\n",
      "1 transformer_model.transformer.wpe.weight torch.Size([2048, 2048])\n",
      "2 transformer_model.transformer.h.0.ln_1.weight torch.Size([2048])\n",
      "3 transformer_model.transformer.h.0.ln_1.bias torch.Size([2048])\n",
      "4 transformer_model.transformer.h.0.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "5 transformer_model.transformer.h.0.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "6 transformer_model.transformer.h.0.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "7 transformer_model.transformer.h.0.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "8 transformer_model.transformer.h.0.attn.attention.out_proj.bias torch.Size([2048])\n",
      "9 transformer_model.transformer.h.0.ln_2.weight torch.Size([2048])\n",
      "10 transformer_model.transformer.h.0.ln_2.bias torch.Size([2048])\n",
      "11 transformer_model.transformer.h.0.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "12 transformer_model.transformer.h.0.mlp.c_fc.bias torch.Size([8192])\n",
      "13 transformer_model.transformer.h.0.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "14 transformer_model.transformer.h.0.mlp.c_proj.bias torch.Size([2048])\n",
      "15 transformer_model.transformer.h.1.ln_1.weight torch.Size([2048])\n",
      "16 transformer_model.transformer.h.1.ln_1.bias torch.Size([2048])\n",
      "17 transformer_model.transformer.h.1.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "18 transformer_model.transformer.h.1.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "19 transformer_model.transformer.h.1.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "20 transformer_model.transformer.h.1.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "21 transformer_model.transformer.h.1.attn.attention.out_proj.bias torch.Size([2048])\n",
      "22 transformer_model.transformer.h.1.ln_2.weight torch.Size([2048])\n",
      "23 transformer_model.transformer.h.1.ln_2.bias torch.Size([2048])\n",
      "24 transformer_model.transformer.h.1.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "25 transformer_model.transformer.h.1.mlp.c_fc.bias torch.Size([8192])\n",
      "26 transformer_model.transformer.h.1.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "27 transformer_model.transformer.h.1.mlp.c_proj.bias torch.Size([2048])\n",
      "28 transformer_model.transformer.h.2.ln_1.weight torch.Size([2048])\n",
      "29 transformer_model.transformer.h.2.ln_1.bias torch.Size([2048])\n",
      "30 transformer_model.transformer.h.2.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "31 transformer_model.transformer.h.2.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "32 transformer_model.transformer.h.2.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "33 transformer_model.transformer.h.2.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "34 transformer_model.transformer.h.2.attn.attention.out_proj.bias torch.Size([2048])\n",
      "35 transformer_model.transformer.h.2.ln_2.weight torch.Size([2048])\n",
      "36 transformer_model.transformer.h.2.ln_2.bias torch.Size([2048])\n",
      "37 transformer_model.transformer.h.2.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "38 transformer_model.transformer.h.2.mlp.c_fc.bias torch.Size([8192])\n",
      "39 transformer_model.transformer.h.2.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "40 transformer_model.transformer.h.2.mlp.c_proj.bias torch.Size([2048])\n",
      "41 transformer_model.transformer.h.3.ln_1.weight torch.Size([2048])\n",
      "42 transformer_model.transformer.h.3.ln_1.bias torch.Size([2048])\n",
      "43 transformer_model.transformer.h.3.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "44 transformer_model.transformer.h.3.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "45 transformer_model.transformer.h.3.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "46 transformer_model.transformer.h.3.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "47 transformer_model.transformer.h.3.attn.attention.out_proj.bias torch.Size([2048])\n",
      "48 transformer_model.transformer.h.3.ln_2.weight torch.Size([2048])\n",
      "49 transformer_model.transformer.h.3.ln_2.bias torch.Size([2048])\n",
      "50 transformer_model.transformer.h.3.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "51 transformer_model.transformer.h.3.mlp.c_fc.bias torch.Size([8192])\n",
      "52 transformer_model.transformer.h.3.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "53 transformer_model.transformer.h.3.mlp.c_proj.bias torch.Size([2048])\n",
      "54 transformer_model.transformer.h.4.ln_1.weight torch.Size([2048])\n",
      "55 transformer_model.transformer.h.4.ln_1.bias torch.Size([2048])\n",
      "56 transformer_model.transformer.h.4.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "57 transformer_model.transformer.h.4.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "58 transformer_model.transformer.h.4.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "59 transformer_model.transformer.h.4.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "60 transformer_model.transformer.h.4.attn.attention.out_proj.bias torch.Size([2048])\n",
      "61 transformer_model.transformer.h.4.ln_2.weight torch.Size([2048])\n",
      "62 transformer_model.transformer.h.4.ln_2.bias torch.Size([2048])\n",
      "63 transformer_model.transformer.h.4.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "64 transformer_model.transformer.h.4.mlp.c_fc.bias torch.Size([8192])\n",
      "65 transformer_model.transformer.h.4.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "66 transformer_model.transformer.h.4.mlp.c_proj.bias torch.Size([2048])\n",
      "67 transformer_model.transformer.h.5.ln_1.weight torch.Size([2048])\n",
      "68 transformer_model.transformer.h.5.ln_1.bias torch.Size([2048])\n",
      "69 transformer_model.transformer.h.5.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "70 transformer_model.transformer.h.5.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "71 transformer_model.transformer.h.5.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "72 transformer_model.transformer.h.5.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "73 transformer_model.transformer.h.5.attn.attention.out_proj.bias torch.Size([2048])\n",
      "74 transformer_model.transformer.h.5.ln_2.weight torch.Size([2048])\n",
      "75 transformer_model.transformer.h.5.ln_2.bias torch.Size([2048])\n",
      "76 transformer_model.transformer.h.5.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "77 transformer_model.transformer.h.5.mlp.c_fc.bias torch.Size([8192])\n",
      "78 transformer_model.transformer.h.5.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "79 transformer_model.transformer.h.5.mlp.c_proj.bias torch.Size([2048])\n",
      "80 transformer_model.transformer.h.6.ln_1.weight torch.Size([2048])\n",
      "81 transformer_model.transformer.h.6.ln_1.bias torch.Size([2048])\n",
      "82 transformer_model.transformer.h.6.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "83 transformer_model.transformer.h.6.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "84 transformer_model.transformer.h.6.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "85 transformer_model.transformer.h.6.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "86 transformer_model.transformer.h.6.attn.attention.out_proj.bias torch.Size([2048])\n",
      "87 transformer_model.transformer.h.6.ln_2.weight torch.Size([2048])\n",
      "88 transformer_model.transformer.h.6.ln_2.bias torch.Size([2048])\n",
      "89 transformer_model.transformer.h.6.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "90 transformer_model.transformer.h.6.mlp.c_fc.bias torch.Size([8192])\n",
      "91 transformer_model.transformer.h.6.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "92 transformer_model.transformer.h.6.mlp.c_proj.bias torch.Size([2048])\n",
      "93 transformer_model.transformer.h.7.ln_1.weight torch.Size([2048])\n",
      "94 transformer_model.transformer.h.7.ln_1.bias torch.Size([2048])\n",
      "95 transformer_model.transformer.h.7.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "96 transformer_model.transformer.h.7.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "97 transformer_model.transformer.h.7.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "98 transformer_model.transformer.h.7.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "99 transformer_model.transformer.h.7.attn.attention.out_proj.bias torch.Size([2048])\n",
      "100 transformer_model.transformer.h.7.ln_2.weight torch.Size([2048])\n",
      "101 transformer_model.transformer.h.7.ln_2.bias torch.Size([2048])\n",
      "102 transformer_model.transformer.h.7.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "103 transformer_model.transformer.h.7.mlp.c_fc.bias torch.Size([8192])\n",
      "104 transformer_model.transformer.h.7.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "105 transformer_model.transformer.h.7.mlp.c_proj.bias torch.Size([2048])\n",
      "106 transformer_model.transformer.h.8.ln_1.weight torch.Size([2048])\n",
      "107 transformer_model.transformer.h.8.ln_1.bias torch.Size([2048])\n",
      "108 transformer_model.transformer.h.8.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "109 transformer_model.transformer.h.8.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "110 transformer_model.transformer.h.8.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "111 transformer_model.transformer.h.8.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "112 transformer_model.transformer.h.8.attn.attention.out_proj.bias torch.Size([2048])\n",
      "113 transformer_model.transformer.h.8.ln_2.weight torch.Size([2048])\n",
      "114 transformer_model.transformer.h.8.ln_2.bias torch.Size([2048])\n",
      "115 transformer_model.transformer.h.8.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "116 transformer_model.transformer.h.8.mlp.c_fc.bias torch.Size([8192])\n",
      "117 transformer_model.transformer.h.8.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "118 transformer_model.transformer.h.8.mlp.c_proj.bias torch.Size([2048])\n",
      "119 transformer_model.transformer.h.9.ln_1.weight torch.Size([2048])\n",
      "120 transformer_model.transformer.h.9.ln_1.bias torch.Size([2048])\n",
      "121 transformer_model.transformer.h.9.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "122 transformer_model.transformer.h.9.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "123 transformer_model.transformer.h.9.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "124 transformer_model.transformer.h.9.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "125 transformer_model.transformer.h.9.attn.attention.out_proj.bias torch.Size([2048])\n",
      "126 transformer_model.transformer.h.9.ln_2.weight torch.Size([2048])\n",
      "127 transformer_model.transformer.h.9.ln_2.bias torch.Size([2048])\n",
      "128 transformer_model.transformer.h.9.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "129 transformer_model.transformer.h.9.mlp.c_fc.bias torch.Size([8192])\n",
      "130 transformer_model.transformer.h.9.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "131 transformer_model.transformer.h.9.mlp.c_proj.bias torch.Size([2048])\n",
      "132 transformer_model.transformer.h.10.ln_1.weight torch.Size([2048])\n",
      "133 transformer_model.transformer.h.10.ln_1.bias torch.Size([2048])\n",
      "134 transformer_model.transformer.h.10.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "135 transformer_model.transformer.h.10.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "136 transformer_model.transformer.h.10.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "137 transformer_model.transformer.h.10.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "138 transformer_model.transformer.h.10.attn.attention.out_proj.bias torch.Size([2048])\n",
      "139 transformer_model.transformer.h.10.ln_2.weight torch.Size([2048])\n",
      "140 transformer_model.transformer.h.10.ln_2.bias torch.Size([2048])\n",
      "141 transformer_model.transformer.h.10.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "142 transformer_model.transformer.h.10.mlp.c_fc.bias torch.Size([8192])\n",
      "143 transformer_model.transformer.h.10.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "144 transformer_model.transformer.h.10.mlp.c_proj.bias torch.Size([2048])\n",
      "145 transformer_model.transformer.h.11.ln_1.weight torch.Size([2048])\n",
      "146 transformer_model.transformer.h.11.ln_1.bias torch.Size([2048])\n",
      "147 transformer_model.transformer.h.11.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "148 transformer_model.transformer.h.11.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "149 transformer_model.transformer.h.11.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "150 transformer_model.transformer.h.11.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "151 transformer_model.transformer.h.11.attn.attention.out_proj.bias torch.Size([2048])\n",
      "152 transformer_model.transformer.h.11.ln_2.weight torch.Size([2048])\n",
      "153 transformer_model.transformer.h.11.ln_2.bias torch.Size([2048])\n",
      "154 transformer_model.transformer.h.11.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "155 transformer_model.transformer.h.11.mlp.c_fc.bias torch.Size([8192])\n",
      "156 transformer_model.transformer.h.11.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "157 transformer_model.transformer.h.11.mlp.c_proj.bias torch.Size([2048])\n",
      "158 transformer_model.transformer.h.12.ln_1.weight torch.Size([2048])\n",
      "159 transformer_model.transformer.h.12.ln_1.bias torch.Size([2048])\n",
      "160 transformer_model.transformer.h.12.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "161 transformer_model.transformer.h.12.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "162 transformer_model.transformer.h.12.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "163 transformer_model.transformer.h.12.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "164 transformer_model.transformer.h.12.attn.attention.out_proj.bias torch.Size([2048])\n",
      "165 transformer_model.transformer.h.12.ln_2.weight torch.Size([2048])\n",
      "166 transformer_model.transformer.h.12.ln_2.bias torch.Size([2048])\n",
      "167 transformer_model.transformer.h.12.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "168 transformer_model.transformer.h.12.mlp.c_fc.bias torch.Size([8192])\n",
      "169 transformer_model.transformer.h.12.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "170 transformer_model.transformer.h.12.mlp.c_proj.bias torch.Size([2048])\n",
      "171 transformer_model.transformer.h.13.ln_1.weight torch.Size([2048])\n",
      "172 transformer_model.transformer.h.13.ln_1.bias torch.Size([2048])\n",
      "173 transformer_model.transformer.h.13.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "174 transformer_model.transformer.h.13.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "175 transformer_model.transformer.h.13.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "176 transformer_model.transformer.h.13.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "177 transformer_model.transformer.h.13.attn.attention.out_proj.bias torch.Size([2048])\n",
      "178 transformer_model.transformer.h.13.ln_2.weight torch.Size([2048])\n",
      "179 transformer_model.transformer.h.13.ln_2.bias torch.Size([2048])\n",
      "180 transformer_model.transformer.h.13.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "181 transformer_model.transformer.h.13.mlp.c_fc.bias torch.Size([8192])\n",
      "182 transformer_model.transformer.h.13.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "183 transformer_model.transformer.h.13.mlp.c_proj.bias torch.Size([2048])\n",
      "184 transformer_model.transformer.h.14.ln_1.weight torch.Size([2048])\n",
      "185 transformer_model.transformer.h.14.ln_1.bias torch.Size([2048])\n",
      "186 transformer_model.transformer.h.14.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "187 transformer_model.transformer.h.14.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "188 transformer_model.transformer.h.14.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "189 transformer_model.transformer.h.14.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "190 transformer_model.transformer.h.14.attn.attention.out_proj.bias torch.Size([2048])\n",
      "191 transformer_model.transformer.h.14.ln_2.weight torch.Size([2048])\n",
      "192 transformer_model.transformer.h.14.ln_2.bias torch.Size([2048])\n",
      "193 transformer_model.transformer.h.14.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "194 transformer_model.transformer.h.14.mlp.c_fc.bias torch.Size([8192])\n",
      "195 transformer_model.transformer.h.14.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "196 transformer_model.transformer.h.14.mlp.c_proj.bias torch.Size([2048])\n",
      "197 transformer_model.transformer.h.15.ln_1.weight torch.Size([2048])\n",
      "198 transformer_model.transformer.h.15.ln_1.bias torch.Size([2048])\n",
      "199 transformer_model.transformer.h.15.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "200 transformer_model.transformer.h.15.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "201 transformer_model.transformer.h.15.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "202 transformer_model.transformer.h.15.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "203 transformer_model.transformer.h.15.attn.attention.out_proj.bias torch.Size([2048])\n",
      "204 transformer_model.transformer.h.15.ln_2.weight torch.Size([2048])\n",
      "205 transformer_model.transformer.h.15.ln_2.bias torch.Size([2048])\n",
      "206 transformer_model.transformer.h.15.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "207 transformer_model.transformer.h.15.mlp.c_fc.bias torch.Size([8192])\n",
      "208 transformer_model.transformer.h.15.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "209 transformer_model.transformer.h.15.mlp.c_proj.bias torch.Size([2048])\n",
      "210 transformer_model.transformer.h.16.ln_1.weight torch.Size([2048])\n",
      "211 transformer_model.transformer.h.16.ln_1.bias torch.Size([2048])\n",
      "212 transformer_model.transformer.h.16.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "213 transformer_model.transformer.h.16.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "214 transformer_model.transformer.h.16.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "215 transformer_model.transformer.h.16.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "216 transformer_model.transformer.h.16.attn.attention.out_proj.bias torch.Size([2048])\n",
      "217 transformer_model.transformer.h.16.ln_2.weight torch.Size([2048])\n",
      "218 transformer_model.transformer.h.16.ln_2.bias torch.Size([2048])\n",
      "219 transformer_model.transformer.h.16.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "220 transformer_model.transformer.h.16.mlp.c_fc.bias torch.Size([8192])\n",
      "221 transformer_model.transformer.h.16.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "222 transformer_model.transformer.h.16.mlp.c_proj.bias torch.Size([2048])\n",
      "223 transformer_model.transformer.h.17.ln_1.weight torch.Size([2048])\n",
      "224 transformer_model.transformer.h.17.ln_1.bias torch.Size([2048])\n",
      "225 transformer_model.transformer.h.17.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "226 transformer_model.transformer.h.17.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "227 transformer_model.transformer.h.17.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "228 transformer_model.transformer.h.17.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "229 transformer_model.transformer.h.17.attn.attention.out_proj.bias torch.Size([2048])\n",
      "230 transformer_model.transformer.h.17.ln_2.weight torch.Size([2048])\n",
      "231 transformer_model.transformer.h.17.ln_2.bias torch.Size([2048])\n",
      "232 transformer_model.transformer.h.17.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "233 transformer_model.transformer.h.17.mlp.c_fc.bias torch.Size([8192])\n",
      "234 transformer_model.transformer.h.17.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "235 transformer_model.transformer.h.17.mlp.c_proj.bias torch.Size([2048])\n",
      "236 transformer_model.transformer.h.18.ln_1.weight torch.Size([2048])\n",
      "237 transformer_model.transformer.h.18.ln_1.bias torch.Size([2048])\n",
      "238 transformer_model.transformer.h.18.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "239 transformer_model.transformer.h.18.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "240 transformer_model.transformer.h.18.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "241 transformer_model.transformer.h.18.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "242 transformer_model.transformer.h.18.attn.attention.out_proj.bias torch.Size([2048])\n",
      "243 transformer_model.transformer.h.18.ln_2.weight torch.Size([2048])\n",
      "244 transformer_model.transformer.h.18.ln_2.bias torch.Size([2048])\n",
      "245 transformer_model.transformer.h.18.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "246 transformer_model.transformer.h.18.mlp.c_fc.bias torch.Size([8192])\n",
      "247 transformer_model.transformer.h.18.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "248 transformer_model.transformer.h.18.mlp.c_proj.bias torch.Size([2048])\n",
      "249 transformer_model.transformer.h.19.ln_1.weight torch.Size([2048])\n",
      "250 transformer_model.transformer.h.19.ln_1.bias torch.Size([2048])\n",
      "251 transformer_model.transformer.h.19.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "252 transformer_model.transformer.h.19.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "253 transformer_model.transformer.h.19.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "254 transformer_model.transformer.h.19.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "255 transformer_model.transformer.h.19.attn.attention.out_proj.bias torch.Size([2048])\n",
      "256 transformer_model.transformer.h.19.ln_2.weight torch.Size([2048])\n",
      "257 transformer_model.transformer.h.19.ln_2.bias torch.Size([2048])\n",
      "258 transformer_model.transformer.h.19.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "259 transformer_model.transformer.h.19.mlp.c_fc.bias torch.Size([8192])\n",
      "260 transformer_model.transformer.h.19.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "261 transformer_model.transformer.h.19.mlp.c_proj.bias torch.Size([2048])\n",
      "262 transformer_model.transformer.h.20.ln_1.weight torch.Size([2048])\n",
      "263 transformer_model.transformer.h.20.ln_1.bias torch.Size([2048])\n",
      "264 transformer_model.transformer.h.20.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "265 transformer_model.transformer.h.20.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "266 transformer_model.transformer.h.20.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "267 transformer_model.transformer.h.20.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "268 transformer_model.transformer.h.20.attn.attention.out_proj.bias torch.Size([2048])\n",
      "269 transformer_model.transformer.h.20.ln_2.weight torch.Size([2048])\n",
      "270 transformer_model.transformer.h.20.ln_2.bias torch.Size([2048])\n",
      "271 transformer_model.transformer.h.20.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "272 transformer_model.transformer.h.20.mlp.c_fc.bias torch.Size([8192])\n",
      "273 transformer_model.transformer.h.20.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "274 transformer_model.transformer.h.20.mlp.c_proj.bias torch.Size([2048])\n",
      "275 transformer_model.transformer.h.21.ln_1.weight torch.Size([2048])\n",
      "276 transformer_model.transformer.h.21.ln_1.bias torch.Size([2048])\n",
      "277 transformer_model.transformer.h.21.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "278 transformer_model.transformer.h.21.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "279 transformer_model.transformer.h.21.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "280 transformer_model.transformer.h.21.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "281 transformer_model.transformer.h.21.attn.attention.out_proj.bias torch.Size([2048])\n",
      "282 transformer_model.transformer.h.21.ln_2.weight torch.Size([2048])\n",
      "283 transformer_model.transformer.h.21.ln_2.bias torch.Size([2048])\n",
      "284 transformer_model.transformer.h.21.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "285 transformer_model.transformer.h.21.mlp.c_fc.bias torch.Size([8192])\n",
      "286 transformer_model.transformer.h.21.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "287 transformer_model.transformer.h.21.mlp.c_proj.bias torch.Size([2048])\n",
      "288 transformer_model.transformer.h.22.ln_1.weight torch.Size([2048])\n",
      "289 transformer_model.transformer.h.22.ln_1.bias torch.Size([2048])\n",
      "290 transformer_model.transformer.h.22.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "291 transformer_model.transformer.h.22.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "292 transformer_model.transformer.h.22.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "293 transformer_model.transformer.h.22.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "294 transformer_model.transformer.h.22.attn.attention.out_proj.bias torch.Size([2048])\n",
      "295 transformer_model.transformer.h.22.ln_2.weight torch.Size([2048])\n",
      "296 transformer_model.transformer.h.22.ln_2.bias torch.Size([2048])\n",
      "297 transformer_model.transformer.h.22.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "298 transformer_model.transformer.h.22.mlp.c_fc.bias torch.Size([8192])\n",
      "299 transformer_model.transformer.h.22.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "300 transformer_model.transformer.h.22.mlp.c_proj.bias torch.Size([2048])\n",
      "301 transformer_model.transformer.h.23.ln_1.weight torch.Size([2048])\n",
      "302 transformer_model.transformer.h.23.ln_1.bias torch.Size([2048])\n",
      "303 transformer_model.transformer.h.23.attn.attention.k_proj.weight torch.Size([2048, 2048])\n",
      "304 transformer_model.transformer.h.23.attn.attention.v_proj.weight torch.Size([2048, 2048])\n",
      "305 transformer_model.transformer.h.23.attn.attention.q_proj.weight torch.Size([2048, 2048])\n",
      "306 transformer_model.transformer.h.23.attn.attention.out_proj.weight torch.Size([2048, 2048])\n",
      "307 transformer_model.transformer.h.23.attn.attention.out_proj.bias torch.Size([2048])\n",
      "308 transformer_model.transformer.h.23.ln_2.weight torch.Size([2048])\n",
      "309 transformer_model.transformer.h.23.ln_2.bias torch.Size([2048])\n",
      "310 transformer_model.transformer.h.23.mlp.c_fc.weight torch.Size([8192, 2048])\n",
      "311 transformer_model.transformer.h.23.mlp.c_fc.bias torch.Size([8192])\n",
      "312 transformer_model.transformer.h.23.mlp.c_proj.weight torch.Size([2048, 8192])\n",
      "313 transformer_model.transformer.h.23.mlp.c_proj.bias torch.Size([2048])\n",
      "314 transformer_model.transformer.ln_f.weight torch.Size([2048])\n",
      "315 transformer_model.transformer.ln_f.bias torch.Size([2048])\n",
      "316 transformer_model.score.weight torch.Size([2, 2048])\n",
      "317 attention.hidden_layer.weight torch.Size([512, 2048])\n",
      "318 attention.hidden_layer.bias torch.Size([512])\n",
      "319 attention.final_layer.weight torch.Size([1, 512])\n",
      "320 attention.final_layer.bias torch.Size([1])\n",
      "321 regressor.weight torch.Size([1, 2048])\n",
      "322 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_input_ids = torch.randint(0, 1000, [2, 248])\n",
    "# sample_attention_mask = torch.randint(0, 1000, [2, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca35f5f4-51d1-4000-ad76-ed912daa8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_records = [sample_ds[i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "709e45b4-ac40-4d67-8fd3-45c0f40d8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'target'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_records[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3389bd94-785b-47b5-bc32-1e4e58dd9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.stack([r['input_ids'] for r in sample_records])\n",
    "sample_attention_mask = torch.stack([r['attention_mask'] for r in sample_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04781a7b-218a-41cc-b81f-d2d248e2c7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 248]), torch.Size([2, 248]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape, sample_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66a3b2b4-920e-4dff-bf9f-210d12b9c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2215,   262,  1862,   661,  4504,   284,   262,  2613,  3823,    11,\n",
       "           340,  5545,   257, 32343,  3421,  5585,    13,  5455,   286,   281,\n",
       "         11087,  3715,    11,   340,   373,   257,  7374, 10747,    13,   198,\n",
       "           464,  4314,   373,  5017,   351,  6729,    12, 11186, 21978,    11,\n",
       "           407,  8104,   319, 21461,    11,   475,  7440, 10137,   625, 34029,\n",
       "           290, 12788,  3320,    11,   588,   257,  1103,  6729,  2214,    13,\n",
       "           383,  6409, 39513,   290,  1683,    70,  5681,   326,   550, 24789,\n",
       "           262,  2119,    11,   547, 41642,   351, 10601,   290,  1937,   675,\n",
       "           351, 12777, 35594,   286, 15985,    11,   588,  6729,    13,  4418,\n",
       "         15291,  8977,   550,   587, 15376, 49692,   319,   606,    11,   290,\n",
       "         31133,   278, 15121, 14158,  2983,  9174,   422,   262, 13737,    13,\n",
       "           198,  2953,  1123,   886,   286,   262,  2119,    11,   319,   262,\n",
       "          3355,    11,  9174,   257,  4950,  6842,    12, 20407, 14477,    13,\n",
       "           198,  4711,   374, 10339,   547,   329, 21740,    11,   530,   329,\n",
       "           262,  4813,   290,   530,   329,   262,  6510,    13,   843,   428,\n",
       "           373,   262,   983,    13,   198,   464,  4813,   547,  9272,   379,\n",
       "           530,   886,   286,   262,  2119,   290,   262,  6510,   379,   262,\n",
       "           584,    11,   290,   530,   886,   373,  1444,   262,  2258, 29385,\n",
       "            11,   290,   262,   584,   262,  2520, 29385,    13,  5501,  2137,\n",
       "           373,  1813,   257,  1402,  6056,   543,   484,   547,   284,  4618,\n",
       "           319,  8978,   262, 29385,    13,   198,  1212,   561,   423,   587,\n",
       "           281,  2562,  2300,    11,   475,  1123, 49130,   373, 23278,   284,\n",
       "          5806,  6729,  1477,  3028,    13, 25947, 25947, 25947, 25947, 25947,\n",
       "         25947, 25947, 25947, 25947, 25947, 25947, 25947, 25947, 25947, 25947,\n",
       "         25947, 25947, 25947, 25947, 25947, 25947, 25947, 25947],\n",
       "        [ 3237,   832,  8073,   640,    11,  9074,    13, 35136,   260,   373,\n",
       "          6454, 10574,    11,   607,  2951, 19186,   319,   360,  5098,   351,\n",
       "           257,   266,   396,   913,    11,  8627,  5408,    13,  1375,  2227,\n",
       "           284,  1577,   262,  1200,   262,  9476,   673, 15671,  1079,    11,\n",
       "           475,   673,   550,  1327,   670,   284,  2222,  5223,   284,   262,\n",
       "           966,   286, 36438,   607,   898, 21954,    13,   198,  2953,   938,\n",
       "            11,  2158,    11,   618,   262,  9799,   373,  3016,   625,    11,\n",
       "           673, 13541,   379,   607,  1310,  4957,    11,   290,   531,    11,\n",
       "           366,  3237,   826,    11,   360,  5098,    11,   345,   743,   467,\n",
       "           526,   198,     1,  5812,    11,  2802,  2474,   360,  5098, 16896,\n",
       "            11, 20974,   351,  4802, 10974,    13,   366, 26392,    30,   198,\n",
       "          5812,    11,   314,   716,   523,  9675,     0,  4231,   345,  1654,\n",
       "           345,   821,  4684,  1701,   198,     1,    40,  1053, 25562,  3589,\n",
       "           284,   307,  4684,    11,  1028,   616,   481,   553,  4504,  9074,\n",
       "            13, 35136,   260,    11, 38411,  1146,    13,   366,    40, 22127,\n",
       "           314,   655,  5465,   284,   423,   345,   467,    11,   475,   314,\n",
       "           460,   470,  6842,   284, 41613,   345,   286,   262,  9476,  5296,\n",
       "            13,   843,    11,   355,   345,   910,    11,   340,   561,   635,\n",
       "          1394, 22875,   774,   379,  1363,    11,   290,   523,    11, 13318,\n",
       "            11,   314,   892,   314,  2236,   423,   284,  1577,   287,   526,\n",
       "           198,     1,  5812,    11,   345, 18304,  2802,     0,   921, 18259,\n",
       "         10846,     0,  1374,   922,   345,   389,  2474,   843,   360,  5098,\n",
       "         13112,  1088,   262,  3084,   290,  2921,   607,  2802,   257, 16225,\n",
       "           326,  3016,  3027, 10533,   607,    13, 25947, 25947, 25947, 25947,\n",
       "         25947, 25947, 25947, 25947, 25947, 25947, 25947, 25947]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25dd4b70-f9d7-4c3a-81d6-da03dd8cf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_out = sample_model.transformer_model(sample_input_ids, None, attention_mask=sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a770a11a-485d-49b5-ae8a-9d5dccc90a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "841fcfeb-6e75-40e5-8f82-265ed8da72d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), 25, torch.Size([2, 248, 2048]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.logits.shape, len(internal_out.hidden_states), internal_out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res = sample_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea66f03e-eac6-478c-ab27-042d97ec1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 2048]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res[0].shape, sample_res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.4560, -35.3667, -38.8465,  ...,   8.2450,   5.2565,  31.6071],\n",
       "        [ 12.8714,   8.7000,  54.9812,  ...,  20.5634, -21.9594,  27.0649],\n",
       "        [-22.8936, -18.5518, -13.0813,  ...,  23.5728, -22.3478, -13.3507],\n",
       "        ...,\n",
       "        [-12.5346,  21.4551,  -7.3699,  ...,  -3.1873, -14.2434,  -7.1931],\n",
       "        [-12.6141,  19.8685, -16.0629,  ...,  18.2870,   2.3613,  -1.1960],\n",
       "        [ 36.0326, -30.1134,  -2.8769,  ...,  37.0739,  10.1244,  -3.6945]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    attention_param_start = 317\n",
    "    regressor_param_start = 321\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 132\n",
    "    layer_middle_threshold = 262\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "#                 mse.backward()\n",
    "#                 self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f3b704f-b4e5-4b33-a33b-159ba8b5685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: \n",
    "# Fold 1: \n",
    "# Fold 2: \n",
    "# Fold 3: \n",
    "# Fold 4: \n",
    "# Fold 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 3e-5, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48210ee3-d9ea-4bab-b852-627f6f75ce0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-21 14:53:38,282]\u001b[0m A new study created in memory with name: no-name-76761356-b4bb-447b-90a0-245c8455e2e5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.0003014381824608722 last_lr 0.0018134836081300776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59f68694e7d43cab9e7085252f432c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.807 New best_val_rmse: 0.807\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7811 New best_val_rmse: 0.7811\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.099 Still best_val_rmse: 0.7811 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7453 New best_val_rmse: 0.7453\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.647 New best_val_rmse: 0.647\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6786 Still best_val_rmse: 0.647 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9379 Still best_val_rmse: 0.647 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6085 New best_val_rmse: 0.6085\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7113 Still best_val_rmse: 0.6085 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-21 14:57:53,751]\u001b[0m Trial 0 finished with value: 0.6085230708122253 and parameters: {'base_lr': 0.0003014381824608722, 'last_lr': 0.0018134836081300776}. Best is trial 0 with value: 0.6085230708122253.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00021767030758472564 last_lr 0.00032747770359245335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c635ef571f34797adc24c271c783d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7778 New best_val_rmse: 0.7778\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.678 New best_val_rmse: 0.678\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8691 Still best_val_rmse: 0.678 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6786 Still best_val_rmse: 0.678 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5953 New best_val_rmse: 0.5953\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7076 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7133 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6157 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6972 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6798 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6047 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5954 Still best_val_rmse: 0.5953 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5729 New best_val_rmse: 0.5729\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5416 New best_val_rmse: 0.5416\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5644 Still best_val_rmse: 0.5416 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5644 Still best_val_rmse: 0.5416 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5606 Still best_val_rmse: 0.5416 (from epoch 1)\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5663 Still best_val_rmse: 0.5416 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5528 Still best_val_rmse: 0.5416 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5399 New best_val_rmse: 0.5399\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.538 New best_val_rmse: 0.538\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5411 Still best_val_rmse: 0.538 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5394 Still best_val_rmse: 0.538 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5368 New best_val_rmse: 0.5368\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5393 Still best_val_rmse: 0.5368 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5395 Still best_val_rmse: 0.5368 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-21 15:09:35,187]\u001b[0m Trial 1 finished with value: 0.5368483662605286 and parameters: {'base_lr': 0.00021767030758472564, 'last_lr': 0.00032747770359245335}. Best is trial 1 with value: 0.5368483662605286.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00021140096507165233 last_lr 0.0005085108486456999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d37cac7b3654fe588093e385fa8ae2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.754 New best_val_rmse: 0.754\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6761 New best_val_rmse: 0.6761\n",
      "\n",
      "16 steps took 14.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.256 Still best_val_rmse: 0.6761 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7428 Still best_val_rmse: 0.6761 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6523 New best_val_rmse: 0.6523\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6946 Still best_val_rmse: 0.6523 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5672 New best_val_rmse: 0.5672\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6184 Still best_val_rmse: 0.5672 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7107 Still best_val_rmse: 0.5672 (from epoch 0)\n",
      "\n",
      "16 steps took 14.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.614 Still best_val_rmse: 0.5672 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5798 Still best_val_rmse: 0.5672 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5654 New best_val_rmse: 0.5654\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.56 New best_val_rmse: 0.56\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5916 Still best_val_rmse: 0.56 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5553 New best_val_rmse: 0.5553\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5683 Still best_val_rmse: 0.5553 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5701 Still best_val_rmse: 0.5553 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5418 New best_val_rmse: 0.5418\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5457 Still best_val_rmse: 0.5418 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5414 New best_val_rmse: 0.5414\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5353 New best_val_rmse: 0.5353\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5337 New best_val_rmse: 0.5337\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5386 Still best_val_rmse: 0.5337 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5383 Still best_val_rmse: 0.5337 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5384 Still best_val_rmse: 0.5337 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5388 Still best_val_rmse: 0.5337 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5395 Still best_val_rmse: 0.5337 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-21 15:21:16,194]\u001b[0m Trial 2 finished with value: 0.5337430238723755 and parameters: {'base_lr': 0.00021140096507165233, 'last_lr': 0.0005085108486456999}. Best is trial 2 with value: 0.5337430238723755.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00028996753449343744 last_lr 0.0018321273486045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1852246429d545b09704e150b597b8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.078 New best_val_rmse: 1.078\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7002 New best_val_rmse: 0.7002\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7033 Still best_val_rmse: 0.7002 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5946 New best_val_rmse: 0.5946\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6746 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6811 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9612 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7316 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6224 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6499 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6044 Still best_val_rmse: 0.5946 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5484 New best_val_rmse: 0.5484\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6386 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5558 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5875 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5729 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5704 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.538 New best_val_rmse: 0.538\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5582 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5643 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.542 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5446 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5426 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5485 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5424 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5422 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5438 Still best_val_rmse: 0.538 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-21 15:32:58,465]\u001b[0m Trial 3 finished with value: 0.5379840135574341 and parameters: {'base_lr': 0.00028996753449343744, 'last_lr': 0.0018321273486045}. Best is trial 2 with value: 0.5337430238723755.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00018153266032198194 last_lr 0.00013125673717990084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09449130bb74ad08345132c252f2a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.721 New best_val_rmse: 0.721\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7477 Still best_val_rmse: 0.721 (from epoch 0)\n",
      "\n",
      "16 steps took 14.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.909 Still best_val_rmse: 0.721 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8296 Still best_val_rmse: 0.721 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6158 New best_val_rmse: 0.6158\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.643 Still best_val_rmse: 0.6158 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7114 Still best_val_rmse: 0.6158 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6074 New best_val_rmse: 0.6074\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6601 Still best_val_rmse: 0.6074 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-21 15:37:12,032]\u001b[0m Trial 4 finished with value: 0.6073958873748779 and parameters: {'base_lr': 0.00018153266032198194, 'last_lr': 0.00013125673717990084}. Best is trial 2 with value: 0.5337430238723755.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.0003997643384995778 last_lr 0.003941388810030477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f29169decf4ca3a7ba8fc1c19c0302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.206 New best_val_rmse: 1.206\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8909 New best_val_rmse: 0.8909\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7518 New best_val_rmse: 0.7518\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6399 New best_val_rmse: 0.6399\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.701 Still best_val_rmse: 0.6399 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6721 Still best_val_rmse: 0.6399 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9307 Still best_val_rmse: 0.6399 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.646 Still best_val_rmse: 0.6399 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5795 New best_val_rmse: 0.5795\n",
      "\n",
      "16 steps took 14.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6094 Still best_val_rmse: 0.5795 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7157 Still best_val_rmse: 0.5795 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.574 New best_val_rmse: 0.574\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6205 Still best_val_rmse: 0.574 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6009 Still best_val_rmse: 0.574 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5648 New best_val_rmse: 0.5648\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5612 New best_val_rmse: 0.5612\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.57 Still best_val_rmse: 0.5612 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.556 New best_val_rmse: 0.556\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5704 Still best_val_rmse: 0.556 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5639 Still best_val_rmse: 0.556 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5428 New best_val_rmse: 0.5428\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.548 Still best_val_rmse: 0.5428 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5545 Still best_val_rmse: 0.5428 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5511 Still best_val_rmse: 0.5428 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5486 Still best_val_rmse: 0.5428 (from epoch 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(3, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360e24c-6ca3-4486-b2ba-27d94cb53913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 3):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
