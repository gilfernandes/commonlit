{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = 'roberta-large-mnli'\n",
    "    TOKENIZER_PATH = 'roberta-large-mnli'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'roberta-large-mnli'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "1 transformer_model.embeddings.position_embeddings.weight torch.Size([514, 1024])\n",
      "2 transformer_model.embeddings.token_type_embeddings.weight torch.Size([1, 1024])\n",
      "3 transformer_model.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "4 transformer_model.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "5 transformer_model.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "6 transformer_model.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "7 transformer_model.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "9 transformer_model.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "11 transformer_model.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "12 transformer_model.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "13 transformer_model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "14 transformer_model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "15 transformer_model.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "17 transformer_model.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "18 transformer_model.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "19 transformer_model.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "20 transformer_model.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "21 transformer_model.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "22 transformer_model.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "23 transformer_model.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "25 transformer_model.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "27 transformer_model.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "28 transformer_model.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "29 transformer_model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "30 transformer_model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "31 transformer_model.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "32 transformer_model.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "33 transformer_model.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "34 transformer_model.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "35 transformer_model.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "36 transformer_model.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "37 transformer_model.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "38 transformer_model.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "39 transformer_model.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "41 transformer_model.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "43 transformer_model.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "44 transformer_model.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "45 transformer_model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "46 transformer_model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "47 transformer_model.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "48 transformer_model.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "49 transformer_model.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "50 transformer_model.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "51 transformer_model.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "52 transformer_model.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "53 transformer_model.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "54 transformer_model.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "55 transformer_model.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "57 transformer_model.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "59 transformer_model.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "60 transformer_model.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "61 transformer_model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "62 transformer_model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "63 transformer_model.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "64 transformer_model.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "65 transformer_model.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "66 transformer_model.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "67 transformer_model.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "68 transformer_model.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "69 transformer_model.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "70 transformer_model.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "71 transformer_model.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "73 transformer_model.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "75 transformer_model.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "76 transformer_model.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "77 transformer_model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "78 transformer_model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "79 transformer_model.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "80 transformer_model.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "81 transformer_model.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "82 transformer_model.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "83 transformer_model.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "84 transformer_model.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "85 transformer_model.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "86 transformer_model.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "87 transformer_model.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "89 transformer_model.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "91 transformer_model.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "92 transformer_model.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "93 transformer_model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "94 transformer_model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "95 transformer_model.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "96 transformer_model.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "97 transformer_model.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "98 transformer_model.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "99 transformer_model.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "100 transformer_model.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "101 transformer_model.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "102 transformer_model.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "103 transformer_model.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "105 transformer_model.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "107 transformer_model.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "108 transformer_model.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "109 transformer_model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "110 transformer_model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "111 transformer_model.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "112 transformer_model.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "113 transformer_model.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "114 transformer_model.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "115 transformer_model.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "116 transformer_model.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "117 transformer_model.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "118 transformer_model.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "119 transformer_model.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "121 transformer_model.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "123 transformer_model.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "124 transformer_model.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "125 transformer_model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "126 transformer_model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "127 transformer_model.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "128 transformer_model.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "129 transformer_model.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "130 transformer_model.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "131 transformer_model.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "132 transformer_model.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "133 transformer_model.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "134 transformer_model.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "135 transformer_model.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "137 transformer_model.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "139 transformer_model.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "140 transformer_model.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "141 transformer_model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "142 transformer_model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "143 transformer_model.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "144 transformer_model.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "145 transformer_model.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "146 transformer_model.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "147 transformer_model.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "148 transformer_model.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "149 transformer_model.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "150 transformer_model.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "151 transformer_model.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "153 transformer_model.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "155 transformer_model.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "156 transformer_model.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "157 transformer_model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "158 transformer_model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "159 transformer_model.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "160 transformer_model.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "161 transformer_model.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "162 transformer_model.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "163 transformer_model.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "164 transformer_model.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "165 transformer_model.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "166 transformer_model.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "167 transformer_model.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "169 transformer_model.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "171 transformer_model.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "172 transformer_model.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "173 transformer_model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "174 transformer_model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "175 transformer_model.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "176 transformer_model.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "177 transformer_model.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "178 transformer_model.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "179 transformer_model.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "180 transformer_model.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "181 transformer_model.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "182 transformer_model.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "183 transformer_model.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "185 transformer_model.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "187 transformer_model.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "188 transformer_model.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "189 transformer_model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "190 transformer_model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "191 transformer_model.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "192 transformer_model.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "193 transformer_model.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "194 transformer_model.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "195 transformer_model.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "196 transformer_model.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "197 transformer_model.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "198 transformer_model.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "199 transformer_model.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "201 transformer_model.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "203 transformer_model.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "204 transformer_model.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "205 transformer_model.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "206 transformer_model.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "207 transformer_model.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "208 transformer_model.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "209 transformer_model.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "210 transformer_model.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "211 transformer_model.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "212 transformer_model.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "213 transformer_model.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "214 transformer_model.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "215 transformer_model.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "217 transformer_model.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "219 transformer_model.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "220 transformer_model.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "221 transformer_model.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "222 transformer_model.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "223 transformer_model.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "224 transformer_model.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "225 transformer_model.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "226 transformer_model.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "227 transformer_model.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "228 transformer_model.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "229 transformer_model.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "230 transformer_model.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "231 transformer_model.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "233 transformer_model.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "235 transformer_model.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "236 transformer_model.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "237 transformer_model.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "238 transformer_model.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "239 transformer_model.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "240 transformer_model.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "241 transformer_model.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "242 transformer_model.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "243 transformer_model.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "244 transformer_model.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "245 transformer_model.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "246 transformer_model.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "247 transformer_model.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "249 transformer_model.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "251 transformer_model.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "252 transformer_model.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "253 transformer_model.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "254 transformer_model.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "255 transformer_model.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "256 transformer_model.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "257 transformer_model.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "258 transformer_model.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "259 transformer_model.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "260 transformer_model.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "261 transformer_model.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "262 transformer_model.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "263 transformer_model.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "265 transformer_model.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "267 transformer_model.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "268 transformer_model.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "269 transformer_model.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "270 transformer_model.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "271 transformer_model.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "272 transformer_model.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "273 transformer_model.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "274 transformer_model.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "275 transformer_model.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "276 transformer_model.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "277 transformer_model.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "278 transformer_model.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "279 transformer_model.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "281 transformer_model.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "283 transformer_model.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "284 transformer_model.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "285 transformer_model.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "286 transformer_model.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "287 transformer_model.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "288 transformer_model.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "289 transformer_model.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "290 transformer_model.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "291 transformer_model.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "292 transformer_model.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "293 transformer_model.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "294 transformer_model.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "295 transformer_model.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "297 transformer_model.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "299 transformer_model.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "300 transformer_model.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "301 transformer_model.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "302 transformer_model.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "303 transformer_model.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "304 transformer_model.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "305 transformer_model.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "306 transformer_model.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "307 transformer_model.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "308 transformer_model.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "309 transformer_model.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "310 transformer_model.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "311 transformer_model.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "313 transformer_model.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "315 transformer_model.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "316 transformer_model.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "317 transformer_model.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "318 transformer_model.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "319 transformer_model.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "320 transformer_model.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "321 transformer_model.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "322 transformer_model.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "323 transformer_model.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "324 transformer_model.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "325 transformer_model.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "326 transformer_model.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "327 transformer_model.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "329 transformer_model.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "331 transformer_model.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "332 transformer_model.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "333 transformer_model.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "334 transformer_model.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "335 transformer_model.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "336 transformer_model.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "337 transformer_model.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "338 transformer_model.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "339 transformer_model.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "340 transformer_model.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "341 transformer_model.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "342 transformer_model.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "343 transformer_model.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "345 transformer_model.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "347 transformer_model.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "348 transformer_model.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "349 transformer_model.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "350 transformer_model.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "351 transformer_model.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "352 transformer_model.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "353 transformer_model.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "354 transformer_model.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "355 transformer_model.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "356 transformer_model.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "357 transformer_model.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "358 transformer_model.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "359 transformer_model.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "361 transformer_model.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "363 transformer_model.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "364 transformer_model.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "365 transformer_model.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "366 transformer_model.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "367 transformer_model.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "368 transformer_model.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "369 transformer_model.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "370 transformer_model.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "371 transformer_model.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "372 transformer_model.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "373 transformer_model.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "374 transformer_model.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "375 transformer_model.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "377 transformer_model.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "379 transformer_model.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "380 transformer_model.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "381 transformer_model.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "382 transformer_model.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "383 transformer_model.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "384 transformer_model.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "385 transformer_model.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "386 transformer_model.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "387 transformer_model.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "388 transformer_model.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "389 transformer_model.pooler.dense.weight torch.Size([1024, 1024])\n",
      "390 transformer_model.pooler.dense.bias torch.Size([1024])\n",
      "391 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "392 attention.hidden_layer.bias torch.Size([512])\n",
      "393 attention.final_layer.weight torch.Size([1, 512])\n",
      "394 attention.final_layer.bias torch.Size([1])\n",
      "395 regressor.weight torch.Size([1, 1024])\n",
      "396 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.7320,   2.2587,  30.8258,  ...,  17.4633, -30.4209,  34.1521],\n",
       "        [ 36.0794,  49.1316, -36.8480,  ..., -20.6265, -36.4150,  -0.8295],\n",
       "        [ 16.8225,  21.9673,   8.4700,  ..., -12.0893,  -0.0784,   8.2608],\n",
       "        ...,\n",
       "        [-10.8944, -20.0534, -18.3019,  ...,  36.6813, -10.7048,  11.8113],\n",
       "        [ 28.1540, -33.0799, -10.2496,  ..., -17.4998, -34.1061,   5.0624],\n",
       "        [-14.8967,  33.1763,   0.9350,  ...,  35.0615,   0.2091,   7.9617]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 395\n",
    "    roberta_parameters = named_parameters[:389]\n",
    "    attention_parameters = named_parameters[391:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= 260:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= 132:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                \n",
    "                mse.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6:\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 0 best: {'base_lr': 5.399287252438555e-05, 'last_lr': 0.00031407679884352875} 0.4808\n",
    "# fold 0 best: {'base_lr': 8.008719222051192e-05, 'last_lr': 9.176800039281186e-05}. Best is trial 13 with value: 0.48905229568481445.\n",
    "# fold 1 best: {'base_lr': 6.499521436817742e-05, 'last_lr': 0.003793058089074889}. Best is trial 16 with value: 0.4678534269332886.\n",
    "# fold 2 best: {'base_lr': 8.906201454928846e-05, 'last_lr': 0.00012184015294923997}. Best is trial 2 with value: 0.47420474886894226.\n",
    "# fold 3 best: {'base_lr': 3.226406820160473e-05, 'last_lr': 0.00238164178709544}. Best trial:  0.48740705847740173\n",
    "# fold 4 best: {'base_lr': 4.25982124671479e-05, 'last_lr':  0.00046783085975998843}. Best trial:  0.49172893166542053\n",
    "# fold 5 best: {'base_lr': 2.517671262528407e-05, 'last_lr': 0.00406777602451577}. Best is trial 0 with value: 0.4810352027416229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 8e-6, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    trainer = Trainer(model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 07:59:01,593]\u001b[0m A new study created in memory with name: no-name-4db231ff-e2d5-4192-ba8c-b26885c3282e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6a13f4f8c84a2ebc1b6e0af729b22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.109 New best_val_rmse: 1.109\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.18 Still best_val_rmse: 1.109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7689 New best_val_rmse: 0.7689\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8934 Still best_val_rmse: 0.7689 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7836 Still best_val_rmse: 0.7689 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5792 New best_val_rmse: 0.5792\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5853 Still best_val_rmse: 0.5792 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7048 Still best_val_rmse: 0.5792 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5701 New best_val_rmse: 0.5701\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5429 New best_val_rmse: 0.5429\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5538 Still best_val_rmse: 0.5429 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.536 New best_val_rmse: 0.536\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5095 New best_val_rmse: 0.5095\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5123 Still best_val_rmse: 0.5095 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5289 Still best_val_rmse: 0.5095 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5018 New best_val_rmse: 0.5018\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5014 New best_val_rmse: 0.5014\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4942 New best_val_rmse: 0.4942\n",
      "\n",
      "8 steps took 6.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5296 Still best_val_rmse: 0.4942 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5177 Still best_val_rmse: 0.4942 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5013 Still best_val_rmse: 0.4942 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4964 Still best_val_rmse: 0.4942 (from epoch 1)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.499 Still best_val_rmse: 0.4934 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5074 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4875 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4877 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4882 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4892 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.49 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4896 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4892 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4889 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4888 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4887 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4887 Still best_val_rmse: 0.4874 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:10:47,041]\u001b[0m Trial 0 finished with value: 0.48740705847740173 and parameters: {'base_lr': 3.226406820160473e-05, 'last_lr': 0.00238164178709544}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31659f5deed7493a996f73b3c751a67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.036 New best_val_rmse: 1.036\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9773 New best_val_rmse: 0.9773\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7996 New best_val_rmse: 0.7996\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6484 New best_val_rmse: 0.6484\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7931 Still best_val_rmse: 0.6484 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6312 New best_val_rmse: 0.6312\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6435 Still best_val_rmse: 0.6312 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7457 Still best_val_rmse: 0.6312 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.716 Still best_val_rmse: 0.6312 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:14:19,526]\u001b[0m Trial 1 finished with value: 0.6312096118927002 and parameters: {'base_lr': 5.0650380156407335e-05, 'last_lr': 0.0019169971339642756}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c533de5a1c40d4ae14dfc2c98a47cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.115 New best_val_rmse: 1.115\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.024 New best_val_rmse: 1.024\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9418 New best_val_rmse: 0.9418\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7 New best_val_rmse: 0.7\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6755 New best_val_rmse: 0.6755\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5956 New best_val_rmse: 0.5956\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5757 New best_val_rmse: 0.5757\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7491 Still best_val_rmse: 0.5757 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6896 Still best_val_rmse: 0.5757 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5557 New best_val_rmse: 0.5557\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5252 New best_val_rmse: 0.5252\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5356 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.548 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5182 New best_val_rmse: 0.5182\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5227 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5107 New best_val_rmse: 0.5107\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5055 New best_val_rmse: 0.5055\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.514 Still best_val_rmse: 0.5055 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5082 Still best_val_rmse: 0.5055 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5119 Still best_val_rmse: 0.5055 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4986 New best_val_rmse: 0.4986\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5082 Still best_val_rmse: 0.4986 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5124 Still best_val_rmse: 0.4986 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4967 New best_val_rmse: 0.4967\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4959 New best_val_rmse: 0.4959\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4956 New best_val_rmse: 0.4956\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.496 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4959 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4958 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4958 Still best_val_rmse: 0.4956 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:25:12,253]\u001b[0m Trial 2 finished with value: 0.4955609440803528 and parameters: {'base_lr': 1.623896394027085e-05, 'last_lr': 0.0001593112483093467}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21681498cad4c5ebe22b9531995c668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.161 New best_val_rmse: 1.161\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.324 Still best_val_rmse: 1.161 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.142 New best_val_rmse: 1.142\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.047 New best_val_rmse: 1.047\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.057 Still best_val_rmse: 1.047 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.044 New best_val_rmse: 1.044\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.067 Still best_val_rmse: 1.044 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.064 Still best_val_rmse: 1.044 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.045 Still best_val_rmse: 1.044 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:28:42,807]\u001b[0m Trial 3 finished with value: 1.0437140464782715 and parameters: {'base_lr': 7.707379473012052e-05, 'last_lr': 0.0018886627125349312}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d67deb35a149f88d652035796734bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.097 New best_val_rmse: 1.097\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.01 New best_val_rmse: 1.01\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9927 New best_val_rmse: 0.9927\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.069 Still best_val_rmse: 0.9927 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.088 Still best_val_rmse: 0.9927 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.042 Still best_val_rmse: 0.9927 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.22 Still best_val_rmse: 0.9927 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.08 Still best_val_rmse: 0.9927 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.045 Still best_val_rmse: 0.9927 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:32:09,435]\u001b[0m Trial 4 finished with value: 0.9926589727401733 and parameters: {'base_lr': 0.00030608464251860324, 'last_lr': 0.004258430458938841}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a15271bdcfb4218a33bb018a6930867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.03 New best_val_rmse: 1.03\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.156 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.042 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.042 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.054 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.04 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.37 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.506 Still best_val_rmse: 1.03 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.221 Still best_val_rmse: 1.03 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:35:29,509]\u001b[0m Trial 5 finished with value: 1.0304316282272339 and parameters: {'base_lr': 0.0003219582161600415, 'last_lr': 0.00014373934795551644}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2ad3d9db40407aaafacfe28edec202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.068 New best_val_rmse: 1.068\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9885 New best_val_rmse: 0.9885\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.046 Still best_val_rmse: 0.9885 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7773 New best_val_rmse: 0.7773\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7118 New best_val_rmse: 0.7118\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6614 New best_val_rmse: 0.6614\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.637 New best_val_rmse: 0.637\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6309 New best_val_rmse: 0.6309\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6243 New best_val_rmse: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:39:11,429]\u001b[0m Trial 6 finished with value: 0.6242756843566895 and parameters: {'base_lr': 1.08227367059183e-05, 'last_lr': 9.576502655421232e-05}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78645e7c65ea483cbe14dfc06ea25588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.05 New best_val_rmse: 1.05\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.001 New best_val_rmse: 1.001\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8875 New best_val_rmse: 0.8875\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7849 New best_val_rmse: 0.7849\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6712 New best_val_rmse: 0.6712\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6308 New best_val_rmse: 0.6308\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5946 New best_val_rmse: 0.5946\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5941 New best_val_rmse: 0.5941\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5884 New best_val_rmse: 0.5884\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5751 New best_val_rmse: 0.5751\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5426 New best_val_rmse: 0.5426\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5452 Still best_val_rmse: 0.5426 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5616 Still best_val_rmse: 0.5426 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.535 New best_val_rmse: 0.535\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5326 New best_val_rmse: 0.5326\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5361 Still best_val_rmse: 0.5326 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5426 Still best_val_rmse: 0.5326 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5275 New best_val_rmse: 0.5275\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5267 New best_val_rmse: 0.5267\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5189 New best_val_rmse: 0.5189\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5307 Still best_val_rmse: 0.5189 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5229 Still best_val_rmse: 0.5189 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5218 Still best_val_rmse: 0.5189 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5201 Still best_val_rmse: 0.5189 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5185 New best_val_rmse: 0.5185\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5187 Still best_val_rmse: 0.5185 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5186 Still best_val_rmse: 0.5185 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:49:33,953]\u001b[0m Trial 7 finished with value: 0.5185472965240479 and parameters: {'base_lr': 9.478088635051655e-06, 'last_lr': 8.52154544554797e-05}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccb9c4504e640d3975420ef7f80bff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.043 New best_val_rmse: 1.043\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8402 New best_val_rmse: 0.8402\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9243 Still best_val_rmse: 0.8402 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8568 Still best_val_rmse: 0.8402 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.072 Still best_val_rmse: 0.8402 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.043 Still best_val_rmse: 0.8402 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.053 Still best_val_rmse: 0.8402 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.065 Still best_val_rmse: 0.8402 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.043 Still best_val_rmse: 0.8402 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:52:57,840]\u001b[0m Trial 8 finished with value: 0.8401888012886047 and parameters: {'base_lr': 6.498141366625214e-05, 'last_lr': 0.00026174059012865964}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e645879aefb4e689207a46c41dd6f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.076 New best_val_rmse: 1.076\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9979 New best_val_rmse: 0.9979\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7987 New best_val_rmse: 0.7987\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8887 Still best_val_rmse: 0.7987 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.811 Still best_val_rmse: 0.7987 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8357 Still best_val_rmse: 0.7987 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6789 New best_val_rmse: 0.6789\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8297 Still best_val_rmse: 0.6789 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7766 Still best_val_rmse: 0.6789 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 08:56:26,927]\u001b[0m Trial 9 finished with value: 0.6789323091506958 and parameters: {'base_lr': 1.57188580049819e-05, 'last_lr': 0.0007715723221240116}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7bfbc8b7614099889f6b941affda8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.111 New best_val_rmse: 1.111\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.024 New best_val_rmse: 1.024\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8389 New best_val_rmse: 0.8389\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7056 New best_val_rmse: 0.7056\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6996 New best_val_rmse: 0.6996\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7152 Still best_val_rmse: 0.6996 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6173 New best_val_rmse: 0.6173\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.9434 Still best_val_rmse: 0.6173 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6715 Still best_val_rmse: 0.6173 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:00:02,829]\u001b[0m Trial 10 finished with value: 0.6172806024551392 and parameters: {'base_lr': 3.5645110422610476e-05, 'last_lr': 0.004134932864629202}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e65150db3046b7979a62a86a17a407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.033 New best_val_rmse: 1.033\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8281 New best_val_rmse: 0.8281\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8355 Still best_val_rmse: 0.8281 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7785 New best_val_rmse: 0.7785\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7484 New best_val_rmse: 0.7484\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7097 New best_val_rmse: 0.7097\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6257 New best_val_rmse: 0.6257\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5785 New best_val_rmse: 0.5785\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5826 Still best_val_rmse: 0.5785 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5524 New best_val_rmse: 0.5524\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5553 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5586 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5599 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5433 New best_val_rmse: 0.5433\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5503 Still best_val_rmse: 0.5433 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.555 Still best_val_rmse: 0.5433 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5434 Still best_val_rmse: 0.5433 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5448 Still best_val_rmse: 0.5433 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5243 New best_val_rmse: 0.5243\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.529 Still best_val_rmse: 0.5243 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5298 Still best_val_rmse: 0.5243 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5247 Still best_val_rmse: 0.5243 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5228 New best_val_rmse: 0.5228\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5199 New best_val_rmse: 0.5199\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5177 New best_val_rmse: 0.5177\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5186 Still best_val_rmse: 0.5177 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5183 Still best_val_rmse: 0.5177 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:10:07,809]\u001b[0m Trial 11 finished with value: 0.5177271366119385 and parameters: {'base_lr': 2.4570003964377455e-05, 'last_lr': 0.0004139454666719998}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bd0c76aa494442a2214b7602d97fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9728 New best_val_rmse: 0.9728\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.242 Still best_val_rmse: 0.9728 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.146 Still best_val_rmse: 0.9728 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8581 New best_val_rmse: 0.8581\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.258 Still best_val_rmse: 0.8581 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.059 Still best_val_rmse: 0.8581 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.07 Still best_val_rmse: 0.8581 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.09 Still best_val_rmse: 0.8581 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.041 Still best_val_rmse: 0.8581 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:13:32,750]\u001b[0m Trial 12 finished with value: 0.8580694794654846 and parameters: {'base_lr': 0.00012798657998965728, 'last_lr': 0.0014394313833508198}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1959f2f26ce4709b30b0e430c9f8504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.026 New best_val_rmse: 1.026\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9683 New best_val_rmse: 0.9683\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8661 New best_val_rmse: 0.8661\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8524 New best_val_rmse: 0.8524\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7854 New best_val_rmse: 0.7854\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6047 New best_val_rmse: 0.6047\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5764 New best_val_rmse: 0.5764\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7898 Still best_val_rmse: 0.5764 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.638 Still best_val_rmse: 0.5764 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5466 New best_val_rmse: 0.5466\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5211 New best_val_rmse: 0.5211\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5349 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5562 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5415 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5021 New best_val_rmse: 0.5021\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5082 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.513 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5107 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5108 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5145 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.494 Still best_val_rmse: 0.4937 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5046 Still best_val_rmse: 0.4929 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.492 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4923 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4924 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4924 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4923 Still best_val_rmse: 0.4917 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:24:22,976]\u001b[0m Trial 13 finished with value: 0.4916834533214569 and parameters: {'base_lr': 2.295995320560339e-05, 'last_lr': 0.0002251081411140287}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cd3d4486ee43e1a1166ce71b3ee3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.068 New best_val_rmse: 1.068\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.084 Still best_val_rmse: 1.068 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7638 New best_val_rmse: 0.7638\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.757 New best_val_rmse: 0.757\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6969 New best_val_rmse: 0.6969\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5682 New best_val_rmse: 0.5682\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5828 Still best_val_rmse: 0.5682 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.9831 Still best_val_rmse: 0.5682 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7164 Still best_val_rmse: 0.5682 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6088 Still best_val_rmse: 0.5682 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5691 Still best_val_rmse: 0.5682 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5397 New best_val_rmse: 0.5397\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5331 New best_val_rmse: 0.5331\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5347 Still best_val_rmse: 0.5331 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5275 New best_val_rmse: 0.5275\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5316 Still best_val_rmse: 0.5275 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5134 New best_val_rmse: 0.5134\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5154 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5144 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5144 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5026 New best_val_rmse: 0.5026\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5 New best_val_rmse: 0.5\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5259 Still best_val_rmse: 0.5 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.5005 Still best_val_rmse: 0.5 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4972 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4972 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4971 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.497 Still best_val_rmse: 0.4969 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:34:52,290]\u001b[0m Trial 14 finished with value: 0.4968758821487427 and parameters: {'base_lr': 2.776364796099378e-05, 'last_lr': 0.000769372229016373}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c585baa8a20f4a9195e8080f56d63601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.096 New best_val_rmse: 1.096\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9935 New best_val_rmse: 0.9935\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7512 New best_val_rmse: 0.7512\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8505 Still best_val_rmse: 0.7512 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6723 New best_val_rmse: 0.6723\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6027 New best_val_rmse: 0.6027\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6637 Still best_val_rmse: 0.6027 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5659 New best_val_rmse: 0.5659\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5796 Still best_val_rmse: 0.5659 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5524 New best_val_rmse: 0.5524\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.54 New best_val_rmse: 0.54\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5315 New best_val_rmse: 0.5315\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5637 Still best_val_rmse: 0.5315 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5466 Still best_val_rmse: 0.5315 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5234 New best_val_rmse: 0.5234\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5377 Still best_val_rmse: 0.5234 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.513 New best_val_rmse: 0.513\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5178 Still best_val_rmse: 0.513 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5093 New best_val_rmse: 0.5093\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5044 New best_val_rmse: 0.5044\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5147 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5028 New best_val_rmse: 0.5028\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5007 New best_val_rmse: 0.5007\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5006 New best_val_rmse: 0.5006\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4989 New best_val_rmse: 0.4989\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4986 New best_val_rmse: 0.4986\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.499 Still best_val_rmse: 0.4986 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4989 Still best_val_rmse: 0.4986 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4989 Still best_val_rmse: 0.4986 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4988 Still best_val_rmse: 0.4986 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:45:38,190]\u001b[0m Trial 15 finished with value: 0.4986198842525482 and parameters: {'base_lr': 1.719833259096913e-05, 'last_lr': 0.0002900707863882122}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d3f4948f34c8393750891e74de6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.005 New best_val_rmse: 1.005\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8029 New best_val_rmse: 0.8029\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8281 Still best_val_rmse: 0.8029 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.05 Still best_val_rmse: 0.8029 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.779 New best_val_rmse: 0.779\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.053 Still best_val_rmse: 0.779 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.117 Still best_val_rmse: 0.779 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.042 Still best_val_rmse: 0.779 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.042 Still best_val_rmse: 0.779 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 09:49:03,772]\u001b[0m Trial 16 finished with value: 0.7789701223373413 and parameters: {'base_lr': 0.00011464411208561552, 'last_lr': 0.0004463445687831131}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b75e0c75b7f4b99b27451dead32d0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.124 New best_val_rmse: 1.124\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9539 New best_val_rmse: 0.9539\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6714 New best_val_rmse: 0.6714\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8184 Still best_val_rmse: 0.6714 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8913 Still best_val_rmse: 0.6714 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6674 New best_val_rmse: 0.6674\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5903 New best_val_rmse: 0.5903\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7801 Still best_val_rmse: 0.5903 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6928 Still best_val_rmse: 0.5903 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5463 New best_val_rmse: 0.5463\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5637 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5352 New best_val_rmse: 0.5352\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5257 New best_val_rmse: 0.5257\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5575 Still best_val_rmse: 0.5257 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5353 Still best_val_rmse: 0.5257 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5033 New best_val_rmse: 0.5033\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.504 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5114 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5286 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5063 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5014 New best_val_rmse: 0.5014\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4956 New best_val_rmse: 0.4956\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4994 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5045 Still best_val_rmse: 0.491 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4939 Still best_val_rmse: 0.491 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4906 New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4897 New best_val_rmse: 0.4897\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4897 New best_val_rmse: 0.4897\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.49 Still best_val_rmse: 0.4897 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.49 Still best_val_rmse: 0.4897 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4899 Still best_val_rmse: 0.4897 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4898 Still best_val_rmse: 0.4897 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4897 Still best_val_rmse: 0.4897 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4896 New best_val_rmse: 0.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:00:34,006]\u001b[0m Trial 17 finished with value: 0.4896315336227417 and parameters: {'base_lr': 4.259134139400092e-05, 'last_lr': 0.0013604223988754553}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c79cb2938b4f5bb47d92e5d646e55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.192 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.213 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.098 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.041 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.053 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.064 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.052 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.047 Still best_val_rmse: 1.04 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:03:54,678]\u001b[0m Trial 18 finished with value: 1.0395355224609375 and parameters: {'base_lr': 4.28125697100149e-05, 'last_lr': 0.0029387836901945542}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22937b3358e4deaab1987cf65188fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.025 New best_val_rmse: 1.025\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9857 New best_val_rmse: 0.9857\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9392 New best_val_rmse: 0.9392\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7839 New best_val_rmse: 0.7839\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.009 Still best_val_rmse: 0.7839 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7913 Still best_val_rmse: 0.7839 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.917 Still best_val_rmse: 0.7839 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.046 Still best_val_rmse: 0.7839 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.061 Still best_val_rmse: 0.7839 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:07:23,234]\u001b[0m Trial 19 finished with value: 0.7838825583457947 and parameters: {'base_lr': 0.00018275096677978996, 'last_lr': 0.0011428449261499623}. Best is trial 0 with value: 0.48740705847740173.\u001b[0m\n",
      "\u001b[32m[I 2021-07-11 10:07:23,236]\u001b[0m A new study created in memory with name: no-name-f9c2b496-99c1-4f50-bd23-62528ffe97a9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.48740705847740173\n",
      " Best params: \n",
      "    base_lr: 3.226406820160473e-05\n",
      "    last_lr: 0.00238164178709544\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c66892570e84ff0814507d8af086782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8239 New best_val_rmse: 0.8239\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9845 Still best_val_rmse: 0.8239 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9298 Still best_val_rmse: 0.8239 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7159 New best_val_rmse: 0.7159\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6701 New best_val_rmse: 0.6701\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6099 New best_val_rmse: 0.6099\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5945 New best_val_rmse: 0.5945\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6063 Still best_val_rmse: 0.5945 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5626 New best_val_rmse: 0.5626\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5837 Still best_val_rmse: 0.5626 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5549 New best_val_rmse: 0.5549\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5239 New best_val_rmse: 0.5239\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5965 Still best_val_rmse: 0.5239 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5158 New best_val_rmse: 0.5158\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6287 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5349 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5152 New best_val_rmse: 0.5152\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5625 Still best_val_rmse: 0.5152 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5037 New best_val_rmse: 0.5037\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5039 Still best_val_rmse: 0.5037 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.496 New best_val_rmse: 0.496\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4947 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4974 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4967 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4953 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4927 New best_val_rmse: 0.4927\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4927 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4929 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4929 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.493 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.493 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4931 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4931 Still best_val_rmse: 0.4927 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:18:26,749]\u001b[0m Trial 0 finished with value: 0.49270474910736084 and parameters: {'base_lr': 5.500307030273427e-05, 'last_lr': 0.0006403732314030953}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb0f0565a5147e4856af04d3a2f8230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.079 New best_val_rmse: 1.079\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.081 Still best_val_rmse: 1.079 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.244 Still best_val_rmse: 1.079 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.044 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.052 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.042 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.078 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.075 Still best_val_rmse: 1.04 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:21:49,816]\u001b[0m Trial 1 finished with value: 1.0399631261825562 and parameters: {'base_lr': 0.00037545113828094123, 'last_lr': 0.0017743638378241453}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8135410898a542369c40bf42db2275d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8186 New best_val_rmse: 0.8186\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.114 Still best_val_rmse: 0.8186 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8966 Still best_val_rmse: 0.8186 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7289 New best_val_rmse: 0.7289\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6352 New best_val_rmse: 0.6352\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6055 New best_val_rmse: 0.6055\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5705 New best_val_rmse: 0.5705\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6351 Still best_val_rmse: 0.5705 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6146 Still best_val_rmse: 0.5705 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5917 Still best_val_rmse: 0.5705 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5611 New best_val_rmse: 0.5611\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5518 New best_val_rmse: 0.5518\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5586 Still best_val_rmse: 0.5518 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5232 New best_val_rmse: 0.5232\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6076 Still best_val_rmse: 0.5232 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5457 Still best_val_rmse: 0.5232 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5416 Still best_val_rmse: 0.5232 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5697 Still best_val_rmse: 0.5232 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5148 New best_val_rmse: 0.5148\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5184 Still best_val_rmse: 0.5148 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.511 New best_val_rmse: 0.511\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5041 New best_val_rmse: 0.5041\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5015 New best_val_rmse: 0.5015\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5015 Still best_val_rmse: 0.501 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5016 Still best_val_rmse: 0.501 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5012 Still best_val_rmse: 0.501 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:31:52,420]\u001b[0m Trial 2 finished with value: 0.5010343194007874 and parameters: {'base_lr': 5.761810365833942e-05, 'last_lr': 0.00012441583144915167}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71aa1fffab8a41baa12d4d24b54c1538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.068 New best_val_rmse: 1.068\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9494 New best_val_rmse: 0.9494\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7069 New best_val_rmse: 0.7069\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6596 New best_val_rmse: 0.6596\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6102 New best_val_rmse: 0.6102\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6099 New best_val_rmse: 0.6099\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6124 Still best_val_rmse: 0.6099 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5936 New best_val_rmse: 0.5936\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6484 Still best_val_rmse: 0.5936 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5443 New best_val_rmse: 0.5443\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.541 New best_val_rmse: 0.541\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.538 New best_val_rmse: 0.538\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.523 New best_val_rmse: 0.523\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5344 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6423 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5464 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5276 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5616 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.52 New best_val_rmse: 0.52\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.522 Still best_val_rmse: 0.52 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5172 New best_val_rmse: 0.5172\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5159 New best_val_rmse: 0.5159\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5152 New best_val_rmse: 0.5152\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5116 New best_val_rmse: 0.5116\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.512 Still best_val_rmse: 0.5116 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5125 Still best_val_rmse: 0.5116 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5116 New best_val_rmse: 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:42:11,637]\u001b[0m Trial 3 finished with value: 0.511554479598999 and parameters: {'base_lr': 1.2842135751689883e-05, 'last_lr': 0.0004185856077827944}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3c0bbfb902450590bb5b3f3f9f1c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.096 New best_val_rmse: 1.096\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8626 New best_val_rmse: 0.8626\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7915 New best_val_rmse: 0.7915\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7702 New best_val_rmse: 0.7702\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8292 Still best_val_rmse: 0.7702 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6682 New best_val_rmse: 0.6682\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6425 New best_val_rmse: 0.6425\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.584 New best_val_rmse: 0.584\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5571 New best_val_rmse: 0.5571\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5461 New best_val_rmse: 0.5461\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.577 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5382 New best_val_rmse: 0.5382\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5305 New best_val_rmse: 0.5305\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5388 Still best_val_rmse: 0.5305 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6416 Still best_val_rmse: 0.5305 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.527 New best_val_rmse: 0.527\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5287 Still best_val_rmse: 0.527 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5694 Still best_val_rmse: 0.527 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5213 New best_val_rmse: 0.5213\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5247 Still best_val_rmse: 0.5213 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5215 Still best_val_rmse: 0.5213 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5123 New best_val_rmse: 0.5123\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5106 New best_val_rmse: 0.5106\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5101 New best_val_rmse: 0.5101\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5102 Still best_val_rmse: 0.5101 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5109 Still best_val_rmse: 0.5101 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5103 Still best_val_rmse: 0.5101 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:52:25,476]\u001b[0m Trial 4 finished with value: 0.5101152062416077 and parameters: {'base_lr': 1.6820031392249838e-05, 'last_lr': 0.0016550548055579862}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1abe50fea44075be546c767761f4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9308 New best_val_rmse: 0.9308\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.094 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.112 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.089 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.043 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.045 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.133 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.044 Still best_val_rmse: 0.9308 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.083 Still best_val_rmse: 0.9308 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:55:44,655]\u001b[0m Trial 5 finished with value: 0.930773913860321 and parameters: {'base_lr': 0.00034292286321233083, 'last_lr': 0.00017722907591614917}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d882941ab9949f0b9b8e7eb392c7ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8296 New best_val_rmse: 0.8296\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.157 Still best_val_rmse: 0.8296 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9546 Still best_val_rmse: 0.8296 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7351 New best_val_rmse: 0.7351\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6773 New best_val_rmse: 0.6773\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6497 New best_val_rmse: 0.6497\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6521 Still best_val_rmse: 0.6497 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6495 New best_val_rmse: 0.6495\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6547 Still best_val_rmse: 0.6495 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 10:59:16,769]\u001b[0m Trial 6 finished with value: 0.6494649648666382 and parameters: {'base_lr': 7.973970970226153e-05, 'last_lr': 0.002755802330682124}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b020310543e410e9ac61ed9e1944a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.12 New best_val_rmse: 1.12\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.24 Still best_val_rmse: 1.12 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.114 New best_val_rmse: 1.114\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.167 Still best_val_rmse: 1.114 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.186 Still best_val_rmse: 1.114 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.057 New best_val_rmse: 1.057\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.203 Still best_val_rmse: 1.057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.107 Still best_val_rmse: 1.057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.055 New best_val_rmse: 1.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:02:45,812]\u001b[0m Trial 7 finished with value: 1.0554085969924927 and parameters: {'base_lr': 0.0004995055247502465, 'last_lr': 0.0002850608072624741}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8613bfc27c451d92e5ccf3a1feba23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.063 New best_val_rmse: 1.063\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.949 New best_val_rmse: 0.949\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7095 New best_val_rmse: 0.7095\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6593 New best_val_rmse: 0.6593\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6113 New best_val_rmse: 0.6113\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6032 New best_val_rmse: 0.6032\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5948 New best_val_rmse: 0.5948\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5788 New best_val_rmse: 0.5788\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6355 Still best_val_rmse: 0.5788 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5397 New best_val_rmse: 0.5397\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5406 Still best_val_rmse: 0.5397 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5309 New best_val_rmse: 0.5309\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5192 New best_val_rmse: 0.5192\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.539 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6438 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.547 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5202 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.573 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5138 New best_val_rmse: 0.5138\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.516 Still best_val_rmse: 0.5138 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5109 New best_val_rmse: 0.5109\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5099 New best_val_rmse: 0.5099\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5074 New best_val_rmse: 0.5074\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5056 New best_val_rmse: 0.5056\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5056 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5064 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5057 Still best_val_rmse: 0.5056 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:13:00,098]\u001b[0m Trial 8 finished with value: 0.5056313276290894 and parameters: {'base_lr': 1.3787414141031954e-05, 'last_lr': 0.00024373871452270445}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4c7c9f8a154527b1a5d67b37d15cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.843 New best_val_rmse: 0.843\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.059 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.336 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.018 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.103 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.046 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.04 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.049 Still best_val_rmse: 0.843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.082 Still best_val_rmse: 0.843 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:16:22,233]\u001b[0m Trial 9 finished with value: 0.8430390954017639 and parameters: {'base_lr': 8.85205212608486e-05, 'last_lr': 0.0022779066126276696}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0c996b009c4fe29358ede64d5a8d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9478 New best_val_rmse: 0.9478\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6872 New best_val_rmse: 0.6872\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.005 Still best_val_rmse: 0.6872 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8061 Still best_val_rmse: 0.6872 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.745 Still best_val_rmse: 0.6872 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6326 New best_val_rmse: 0.6326\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5962 New best_val_rmse: 0.5962\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6004 Still best_val_rmse: 0.5962 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5491 New best_val_rmse: 0.5491\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5477 New best_val_rmse: 0.5477\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5656 Still best_val_rmse: 0.5477 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5328 New best_val_rmse: 0.5328\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5123 New best_val_rmse: 0.5123\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.53 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6335 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5288 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5192 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5451 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5054 New best_val_rmse: 0.5054\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5063 Still best_val_rmse: 0.5054 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5066 Still best_val_rmse: 0.5054 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5048 New best_val_rmse: 0.5048\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5011 New best_val_rmse: 0.5011\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5007 New best_val_rmse: 0.5007\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5012 Still best_val_rmse: 0.5007 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5007 Still best_val_rmse: 0.5007 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5003 New best_val_rmse: 0.5003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:26:26,851]\u001b[0m Trial 10 finished with value: 0.5003262162208557 and parameters: {'base_lr': 3.311575696025064e-05, 'last_lr': 0.0008638794575571843}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d368dc66e5504ff6905f45da87ef587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9465 New best_val_rmse: 0.9465\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6969 New best_val_rmse: 0.6969\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9743 Still best_val_rmse: 0.6969 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7749 Still best_val_rmse: 0.6969 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7621 Still best_val_rmse: 0.6969 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6622 New best_val_rmse: 0.6622\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5886 New best_val_rmse: 0.5886\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5707 New best_val_rmse: 0.5707\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6114 Still best_val_rmse: 0.5707 (from epoch 0)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6214 Still best_val_rmse: 0.5707 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5545 New best_val_rmse: 0.5545\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5643 Still best_val_rmse: 0.5545 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6171 Still best_val_rmse: 0.5545 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5317 New best_val_rmse: 0.5317\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6224 Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5408 Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5367 Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5801 Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.517 New best_val_rmse: 0.517\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5198 Still best_val_rmse: 0.517 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5075 New best_val_rmse: 0.5075\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5075 Still best_val_rmse: 0.5075 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5038 New best_val_rmse: 0.5038\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.504 Still best_val_rmse: 0.5038 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.505 Still best_val_rmse: 0.5038 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.504 Still best_val_rmse: 0.5038 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:36:24,394]\u001b[0m Trial 11 finished with value: 0.5037858486175537 and parameters: {'base_lr': 3.3539706120753366e-05, 'last_lr': 0.0008654608420454564}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309f07a47195460a8ccc6c1c98c1712b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.951 New best_val_rmse: 0.951\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6782 New best_val_rmse: 0.6782\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.035 Still best_val_rmse: 0.6782 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7676 Still best_val_rmse: 0.6782 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7518 Still best_val_rmse: 0.6782 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6737 New best_val_rmse: 0.6737\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5781 New best_val_rmse: 0.5781\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6054 Still best_val_rmse: 0.5781 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5479 New best_val_rmse: 0.5479\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.557 Still best_val_rmse: 0.5479 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5633 Still best_val_rmse: 0.5479 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5353 New best_val_rmse: 0.5353\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5229 New best_val_rmse: 0.5229\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5214 New best_val_rmse: 0.5214\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.641 Still best_val_rmse: 0.5214 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5325 Still best_val_rmse: 0.5214 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5162 New best_val_rmse: 0.5162\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5549 Still best_val_rmse: 0.5162 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5036 New best_val_rmse: 0.5036\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5053 Still best_val_rmse: 0.5036 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5041 Still best_val_rmse: 0.5036 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5019 New best_val_rmse: 0.5019\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5011 New best_val_rmse: 0.5011\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5008 New best_val_rmse: 0.5008\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5005 New best_val_rmse: 0.5005\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5013 Still best_val_rmse: 0.5005 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5004 New best_val_rmse: 0.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:46:36,475]\u001b[0m Trial 12 finished with value: 0.5003801584243774 and parameters: {'base_lr': 3.2523515760001636e-05, 'last_lr': 0.0008465770027184623}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21aad7ec5e34b1ba2532cb80c128fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.346 New best_val_rmse: 1.346\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7417 New best_val_rmse: 0.7417\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9716 Still best_val_rmse: 0.7417 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9958 Still best_val_rmse: 0.7417 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7809 Still best_val_rmse: 0.7417 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8567 Still best_val_rmse: 0.7417 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.042 Still best_val_rmse: 0.7417 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.103 Still best_val_rmse: 0.7417 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.069 Still best_val_rmse: 0.7417 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 11:50:02,623]\u001b[0m Trial 13 finished with value: 0.7416815757751465 and parameters: {'base_lr': 0.00014929685467720974, 'last_lr': 0.0005998880572453282}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27cdd0314c44841b402c57b89dba711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9692 New best_val_rmse: 0.9692\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7107 New best_val_rmse: 0.7107\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.043 Still best_val_rmse: 0.7107 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7533 Still best_val_rmse: 0.7107 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6496 New best_val_rmse: 0.6496\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6896 Still best_val_rmse: 0.6496 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5855 New best_val_rmse: 0.5855\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5758 New best_val_rmse: 0.5758\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5971 Still best_val_rmse: 0.5758 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5539 New best_val_rmse: 0.5539\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5575 Still best_val_rmse: 0.5539 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5524 New best_val_rmse: 0.5524\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5591 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5376 New best_val_rmse: 0.5376\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6817 Still best_val_rmse: 0.5376 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5316 New best_val_rmse: 0.5316\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.523 New best_val_rmse: 0.523\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5454 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5119 New best_val_rmse: 0.5119\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5116 New best_val_rmse: 0.5116\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5129 Still best_val_rmse: 0.5116 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5061 New best_val_rmse: 0.5061\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5036 New best_val_rmse: 0.5036\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5029 New best_val_rmse: 0.5029\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5031 Still best_val_rmse: 0.5029 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5037 Still best_val_rmse: 0.5029 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5027 New best_val_rmse: 0.5027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:00:15,356]\u001b[0m Trial 14 finished with value: 0.5027363300323486 and parameters: {'base_lr': 2.7217394418855534e-05, 'last_lr': 0.0010528696023137842}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4d7f81a1ba446bb5c4c59747018d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8647 New best_val_rmse: 0.8647\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9205 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8855 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.041 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.038 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.039 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.04 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.072 Still best_val_rmse: 0.8647 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.081 Still best_val_rmse: 0.8647 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:03:36,127]\u001b[0m Trial 15 finished with value: 0.8646877408027649 and parameters: {'base_lr': 0.00017542193606878774, 'last_lr': 0.004947415715299062}. Best is trial 0 with value: 0.49270474910736084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce78ea3b31c4a2782476d68d3b93115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9465 New best_val_rmse: 0.9465\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.186 Still best_val_rmse: 0.9465 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8792 New best_val_rmse: 0.8792\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7415 New best_val_rmse: 0.7415\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6902 New best_val_rmse: 0.6902\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6502 New best_val_rmse: 0.6502\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6027 New best_val_rmse: 0.6027\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6291 Still best_val_rmse: 0.6027 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5905 New best_val_rmse: 0.5905\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.541 New best_val_rmse: 0.541\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5565 Still best_val_rmse: 0.541 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5308 New best_val_rmse: 0.5308\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5391 Still best_val_rmse: 0.5308 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5209 New best_val_rmse: 0.5209\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6101 Still best_val_rmse: 0.5209 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5155 New best_val_rmse: 0.5155\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5121 New best_val_rmse: 0.5121\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5451 Still best_val_rmse: 0.5121 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4998 New best_val_rmse: 0.4998\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4995 New best_val_rmse: 0.4995\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5034 Still best_val_rmse: 0.4961 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4929 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4978 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4921 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4931 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4928 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4935 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4931 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4927 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4926 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4925 Still best_val_rmse: 0.4917 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:15:00,773]\u001b[0m Trial 16 finished with value: 0.49172893166542053 and parameters: {'base_lr': 4.25982124671479e-05, 'last_lr': 0.00046783085975998843}. Best is trial 16 with value: 0.49172893166542053.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222950e2dbf84045ba95147c3fca6efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.017 New best_val_rmse: 1.017\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.036 Still best_val_rmse: 1.017 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8625 New best_val_rmse: 0.8625\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6924 New best_val_rmse: 0.6924\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.669 New best_val_rmse: 0.669\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6589 New best_val_rmse: 0.6589\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6222 New best_val_rmse: 0.6222\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6085 New best_val_rmse: 0.6085\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5715 New best_val_rmse: 0.5715\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5769 Still best_val_rmse: 0.5715 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5679 New best_val_rmse: 0.5679\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5782 Still best_val_rmse: 0.5679 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5661 New best_val_rmse: 0.5661\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.554 New best_val_rmse: 0.554\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.619 Still best_val_rmse: 0.554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5863 Still best_val_rmse: 0.554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5577 Still best_val_rmse: 0.554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5671 Still best_val_rmse: 0.554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5414 New best_val_rmse: 0.5414\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.552 Still best_val_rmse: 0.5414 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5406 New best_val_rmse: 0.5406\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.542 Still best_val_rmse: 0.5406 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5399 New best_val_rmse: 0.5399\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5358 New best_val_rmse: 0.5358\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.536 Still best_val_rmse: 0.5358 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5369 Still best_val_rmse: 0.5358 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5364 Still best_val_rmse: 0.5358 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:25:13,529]\u001b[0m Trial 17 finished with value: 0.535816490650177 and parameters: {'base_lr': 8.090440111990233e-06, 'last_lr': 8.280069297700935e-05}. Best is trial 16 with value: 0.49172893166542053.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a6100e12a24f8d8a514ee1e77dc10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8258 New best_val_rmse: 0.8258\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8926 Still best_val_rmse: 0.8258 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.792 New best_val_rmse: 0.792\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8385 Still best_val_rmse: 0.792 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7655 New best_val_rmse: 0.7655\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6902 New best_val_rmse: 0.6902\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6116 New best_val_rmse: 0.6116\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6033 New best_val_rmse: 0.6033\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6036 Still best_val_rmse: 0.6033 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:28:51,289]\u001b[0m Trial 18 finished with value: 0.6033336520195007 and parameters: {'base_lr': 6.195357929866518e-05, 'last_lr': 0.00045241704354138495}. Best is trial 16 with value: 0.49172893166542053.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0f3d520fa14897aaececed838ee20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.535 New best_val_rmse: 1.535\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8699 New best_val_rmse: 0.8699\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7074 New best_val_rmse: 0.7074\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.797 Still best_val_rmse: 0.7074 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6375 New best_val_rmse: 0.6375\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.61 New best_val_rmse: 0.61\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6601 Still best_val_rmse: 0.61 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6411 Still best_val_rmse: 0.61 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6286 Still best_val_rmse: 0.61 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:32:24,289]\u001b[0m Trial 19 finished with value: 0.6099985241889954 and parameters: {'base_lr': 0.0001337250646422305, 'last_lr': 0.0003298751720339185}. Best is trial 16 with value: 0.49172893166542053.\u001b[0m\n",
      "\u001b[32m[I 2021-07-11 12:32:24,291]\u001b[0m A new study created in memory with name: no-name-86dd7c39-ba3d-44c0-a9a2-265eba2df1df\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.49172893166542053\n",
      " Best params: \n",
      "    base_lr: 4.25982124671479e-05\n",
      "    last_lr: 0.00046783085975998843\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06bd83988848bbb7c045924d6b496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9253 New best_val_rmse: 0.9253\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7431 New best_val_rmse: 0.7431\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.664 New best_val_rmse: 0.664\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7751 Still best_val_rmse: 0.664 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7152 Still best_val_rmse: 0.664 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6315 New best_val_rmse: 0.6315\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6378 Still best_val_rmse: 0.6315 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.585 New best_val_rmse: 0.585\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5612 New best_val_rmse: 0.5612\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5238 New best_val_rmse: 0.5238\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5131 New best_val_rmse: 0.5131\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6015 Still best_val_rmse: 0.5131 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6065 Still best_val_rmse: 0.5131 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5384 Still best_val_rmse: 0.5131 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5977 Still best_val_rmse: 0.5131 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4973 New best_val_rmse: 0.4973\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5163 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4931 New best_val_rmse: 0.4931\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4976 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "8 steps took 6.29 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4964 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4881 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4859 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4938 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4826 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4873 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4958 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4816 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4822 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4849 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4835 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4844 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4843 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4811 New best_val_rmse: 0.4811\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4822 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4964 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.486 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4814 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4812 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4823 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.485 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4855 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4852 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4838 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4831 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4829 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4825 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4823 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4822 Still best_val_rmse: 0.481 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:45:52,782]\u001b[0m Trial 0 finished with value: 0.4810352027416229 and parameters: {'base_lr': 2.517671262528407e-05, 'last_lr': 0.00406777602451577}. Best is trial 0 with value: 0.4810352027416229.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaf45ea024d4f4082131743bacf50c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9661 New best_val_rmse: 0.9661\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.75 New best_val_rmse: 0.75\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6913 New best_val_rmse: 0.6913\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6711 New best_val_rmse: 0.6711\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6686 New best_val_rmse: 0.6686\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5502 New best_val_rmse: 0.5502\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5968 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8112 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6082 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 1.043 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.02 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.042 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.02 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.037 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.036 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.031 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.023 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.029 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.02 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.021 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.021 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.019 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.021 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.021 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.021 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.021 Still best_val_rmse: 0.5502 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.02 Still best_val_rmse: 0.5502 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 12:55:31,967]\u001b[0m Trial 1 finished with value: 0.5501596927642822 and parameters: {'base_lr': 8.375302263422773e-05, 'last_lr': 0.0004311746355821461}. Best is trial 0 with value: 0.4810352027416229.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6c99e4326a43ee9a89e61ad07a8776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.194 New best_val_rmse: 1.194\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.085 New best_val_rmse: 1.085\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6777 New best_val_rmse: 0.6777\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7128 Still best_val_rmse: 0.6777 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5913 New best_val_rmse: 0.5913\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5536 New best_val_rmse: 0.5536\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7023 Still best_val_rmse: 0.5536 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5971 Still best_val_rmse: 0.5536 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.556 Still best_val_rmse: 0.5536 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5467 New best_val_rmse: 0.5467\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5426 New best_val_rmse: 0.5426\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5302 New best_val_rmse: 0.5302\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6532 Still best_val_rmse: 0.5302 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5121 New best_val_rmse: 0.5121\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.531 Still best_val_rmse: 0.5121 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5126 Still best_val_rmse: 0.5121 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5315 Still best_val_rmse: 0.5121 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4887 New best_val_rmse: 0.4887\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4934 Still best_val_rmse: 0.4887 (from epoch 1)\n",
      "\n",
      "8 steps took 6.28 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4893 Still best_val_rmse: 0.4887 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4909 Still best_val_rmse: 0.4887 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4892 Still best_val_rmse: 0.4887 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4867 New best_val_rmse: 0.4867\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4899 Still best_val_rmse: 0.4867 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4974 Still best_val_rmse: 0.4867 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5181 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4995 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4847 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4874 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4889 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.5011 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4834 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4863 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4863 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4858 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4836 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4817 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4818 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4818 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4817 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4817 Still best_val_rmse: 0.4816 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 13:08:16,576]\u001b[0m Trial 2 finished with value: 0.4815599322319031 and parameters: {'base_lr': 1.8493681713701894e-05, 'last_lr': 0.0030814367033004386}. Best is trial 0 with value: 0.4810352027416229.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1917bf07438947d28cc9634f0b97457d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9368 New best_val_rmse: 0.9368\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9012 New best_val_rmse: 0.9012\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.011 Still best_val_rmse: 0.9012 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7637 New best_val_rmse: 0.7637\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7309 New best_val_rmse: 0.7309\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6732 New best_val_rmse: 0.6732\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5866 New best_val_rmse: 0.5866\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6044 Still best_val_rmse: 0.5866 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5796 New best_val_rmse: 0.5796\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.599 Still best_val_rmse: 0.5796 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7283 Still best_val_rmse: 0.5796 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5723 New best_val_rmse: 0.5723\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6013 Still best_val_rmse: 0.5723 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5476 New best_val_rmse: 0.5476\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5601 Still best_val_rmse: 0.5476 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5169 New best_val_rmse: 0.5169\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5446 Still best_val_rmse: 0.5169 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5682 Still best_val_rmse: 0.5169 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5139 New best_val_rmse: 0.5139\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5099 New best_val_rmse: 0.5099\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5155 Still best_val_rmse: 0.5099 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5267 Still best_val_rmse: 0.5099 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5356 Still best_val_rmse: 0.5099 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5022 New best_val_rmse: 0.5022\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.5002 Still best_val_rmse: 0.4993 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4985 New best_val_rmse: 0.4985\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4984 New best_val_rmse: 0.4984\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4985 Still best_val_rmse: 0.4984 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 13:18:47,333]\u001b[0m Trial 3 finished with value: 0.498441219329834 and parameters: {'base_lr': 8.941843080851954e-05, 'last_lr': 0.001837188858801888}. Best is trial 0 with value: 0.4810352027416229.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b1d66621254d7c875aa795c2fd77b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.013 New best_val_rmse: 1.013\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9741 New best_val_rmse: 0.9741\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.105 Still best_val_rmse: 0.9741 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.025 Still best_val_rmse: 0.9741 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.038 Still best_val_rmse: 0.9741 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.029 Still best_val_rmse: 0.9741 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.02 Still best_val_rmse: 0.9741 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.079 Still best_val_rmse: 0.9741 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.061 Still best_val_rmse: 0.9741 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 13:22:10,424]\u001b[0m Trial 4 finished with value: 0.9740661978721619 and parameters: {'base_lr': 0.00017781014466275693, 'last_lr': 0.003533750914273647}. Best is trial 0 with value: 0.4810352027416229.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54e5df6273148689389aa56eb5361fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.022 New best_val_rmse: 1.022\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.768 New best_val_rmse: 0.768\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7392 New best_val_rmse: 0.7392\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6949 New best_val_rmse: 0.6949\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6451 New best_val_rmse: 0.6451\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5749 New best_val_rmse: 0.5749\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5767 Still best_val_rmse: 0.5749 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5662 New best_val_rmse: 0.5662\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6244 Still best_val_rmse: 0.5662 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5529 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5537 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5503 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5881 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5402 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4913 Still best_val_rmse: 0.4818 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.5389 Still best_val_rmse: 0.4818 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4856 Still best_val_rmse: 0.4818 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4837 Still best_val_rmse: 0.4818 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4842 Still best_val_rmse: 0.4818 (from epoch 1)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4843 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4926 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4834 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4836 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4961 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4947 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4972 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4762 New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4767 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4777 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4784 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4805 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4793 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4829 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4838 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4817 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4816 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4815 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4798 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4792 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4787 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4785 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4801 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4892 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4935 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4816 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4785 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.478 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4778 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4776 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4776 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4777 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4779 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.478 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.478 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.478 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4779 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4779 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4778 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4778 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4778 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4778 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4778 Still best_val_rmse: 0.4762 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 13:37:27,736]\u001b[0m Trial 5 finished with value: 0.47618797421455383 and parameters: {'base_lr': 3.1211012676571296e-05, 'last_lr': 0.003503681146690041}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1127b26bbfa44835b9d4182be95e99e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.203 New best_val_rmse: 1.203\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.13 New best_val_rmse: 1.13\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.688 New best_val_rmse: 0.688\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6974 Still best_val_rmse: 0.688 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6124 New best_val_rmse: 0.6124\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5664 New best_val_rmse: 0.5664\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5949 Still best_val_rmse: 0.5664 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5515 New best_val_rmse: 0.5515\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.59 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5198 New best_val_rmse: 0.5198\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5362 Still best_val_rmse: 0.5198 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5601 Still best_val_rmse: 0.5198 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6629 Still best_val_rmse: 0.5198 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5792 Still best_val_rmse: 0.5197 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4993 Still best_val_rmse: 0.4954 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5194 Still best_val_rmse: 0.4954 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4893 Still best_val_rmse: 0.4889 (from epoch 1)\n",
      "\n",
      "4 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4927 Still best_val_rmse: 0.4889 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.485 New best_val_rmse: 0.485\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4853 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4865 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4853 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5009 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4858 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4881 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4906 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4804 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.481 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4803 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4816 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4815 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4802 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.486 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4945 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4876 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4815 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4801 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4826 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4844 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4837 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.483 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4819 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4816 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4812 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4808 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4806 Still best_val_rmse: 0.4797 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 13:50:53,572]\u001b[0m Trial 6 finished with value: 0.47968539595603943 and parameters: {'base_lr': 1.819371369732658e-05, 'last_lr': 0.002281520488029198}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05c692e3bb64accb820f88416cc2c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.076 New best_val_rmse: 1.076\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.066 New best_val_rmse: 1.066\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7071 New best_val_rmse: 0.7071\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6459 New best_val_rmse: 0.6459\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6011 New best_val_rmse: 0.6011\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5793 New best_val_rmse: 0.5793\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.565 New best_val_rmse: 0.565\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6191 Still best_val_rmse: 0.565 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6155 Still best_val_rmse: 0.565 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5375 New best_val_rmse: 0.5375\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5364 New best_val_rmse: 0.5364\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5305 New best_val_rmse: 0.5305\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.595 Still best_val_rmse: 0.5305 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5642 Still best_val_rmse: 0.5305 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5248 New best_val_rmse: 0.5248\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5172 Still best_val_rmse: 0.4909 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5063 Still best_val_rmse: 0.4909 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4976 Still best_val_rmse: 0.4909 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4863 New best_val_rmse: 0.4863\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4912 Still best_val_rmse: 0.4863 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.494 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4842 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4837 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4854 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4828 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4884 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.489 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4834 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4826 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4966 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4829 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4904 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4821 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4848 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.486 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.487 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.487 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4859 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.484 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4835 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4831 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4829 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4827 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4825 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4824 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4823 Still best_val_rmse: 0.481 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 14:04:13,659]\u001b[0m Trial 7 finished with value: 0.4810379445552826 and parameters: {'base_lr': 2.2975827471191227e-05, 'last_lr': 0.0004391967966467416}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0ca7360f8f4cf986caf60e58c3ba83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.985 New best_val_rmse: 0.985\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7299 New best_val_rmse: 0.7299\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9189 Still best_val_rmse: 0.7299 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7402 Still best_val_rmse: 0.7299 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9065 Still best_val_rmse: 0.7299 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.672 New best_val_rmse: 0.672\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7443 Still best_val_rmse: 0.672 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8526 Still best_val_rmse: 0.672 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.03 Still best_val_rmse: 0.672 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 14:07:41,638]\u001b[0m Trial 8 finished with value: 0.672038197517395 and parameters: {'base_lr': 0.00016163527182352886, 'last_lr': 0.0011072832131516972}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04e0727157b4ac39477b7f5898030d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.011 New best_val_rmse: 1.011\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7488 New best_val_rmse: 0.7488\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6342 New best_val_rmse: 0.6342\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8114 Still best_val_rmse: 0.6342 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6133 New best_val_rmse: 0.6133\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6256 Still best_val_rmse: 0.6133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6311 Still best_val_rmse: 0.6133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6326 Still best_val_rmse: 0.6133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5668 New best_val_rmse: 0.5668\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5071 New best_val_rmse: 0.5071\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.511 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5591 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5234 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5253 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5361 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5009 New best_val_rmse: 0.5009\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.513 Still best_val_rmse: 0.5009 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4997 New best_val_rmse: 0.4997\n",
      "\n",
      "8 steps took 6.34 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4955 New best_val_rmse: 0.4955\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.493 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4864 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4884 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.5003 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4934 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4915 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4896 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.495 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4909 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4881 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4858 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.486 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4893 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4917 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4897 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4872 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4862 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4854 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4851 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4849 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4848 New best_val_rmse: 0.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 14:20:21,598]\u001b[0m Trial 9 finished with value: 0.48475295305252075 and parameters: {'base_lr': 4.988824996116573e-05, 'last_lr': 0.0001505909143424294}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf52e34a973b49dc93a92cfdd7e6b441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.097 New best_val_rmse: 1.097\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.108 Still best_val_rmse: 1.097 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7927 New best_val_rmse: 0.7927\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7666 New best_val_rmse: 0.7666\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6231 New best_val_rmse: 0.6231\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.588 New best_val_rmse: 0.588\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5742 New best_val_rmse: 0.5742\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5524 New best_val_rmse: 0.5524\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5451 New best_val_rmse: 0.5451\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5505 Still best_val_rmse: 0.5451 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5574 Still best_val_rmse: 0.5451 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5304 New best_val_rmse: 0.5304\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5142 New best_val_rmse: 0.5142\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5233 Still best_val_rmse: 0.5142 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5355 Still best_val_rmse: 0.5142 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5438 Still best_val_rmse: 0.5142 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.522 Still best_val_rmse: 0.5142 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5092 Still best_val_rmse: 0.5065 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4974 New best_val_rmse: 0.4974\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5016 Still best_val_rmse: 0.4974 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4963 New best_val_rmse: 0.4963\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5001 Still best_val_rmse: 0.4963 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4968 Still best_val_rmse: 0.4963 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4965 Still best_val_rmse: 0.4963 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4956 New best_val_rmse: 0.4956\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4971 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.5023 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.5023 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.5006 Still best_val_rmse: 0.4956 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 14:31:10,208]\u001b[0m Trial 10 finished with value: 0.49558311700820923 and parameters: {'base_lr': 8.017085602014504e-06, 'last_lr': 0.00013492093789299912}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c45ff966c7f4f48ab2e735247a693f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.199 New best_val_rmse: 1.199\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.21 Still best_val_rmse: 1.199 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9498 New best_val_rmse: 0.9498\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9283 New best_val_rmse: 0.9283\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6395 New best_val_rmse: 0.6395\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6178 New best_val_rmse: 0.6178\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5998 New best_val_rmse: 0.5998\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5853 New best_val_rmse: 0.5853\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5625 New best_val_rmse: 0.5625\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5902 Still best_val_rmse: 0.5625 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5781 Still best_val_rmse: 0.5625 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5603 New best_val_rmse: 0.5603\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5441 New best_val_rmse: 0.5441\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5415 New best_val_rmse: 0.5415\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5684 Still best_val_rmse: 0.5415 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.533 New best_val_rmse: 0.533\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5355 Still best_val_rmse: 0.533 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5167 New best_val_rmse: 0.5167\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5146 New best_val_rmse: 0.5146\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.515 Still best_val_rmse: 0.5146 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.516 Still best_val_rmse: 0.5146 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5155 Still best_val_rmse: 0.5146 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5169 Still best_val_rmse: 0.5146 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5127 New best_val_rmse: 0.5127\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5135 Still best_val_rmse: 0.5127 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5205 Still best_val_rmse: 0.5127 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5196 Still best_val_rmse: 0.5127 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 14:41:27,460]\u001b[0m Trial 11 finished with value: 0.51274573802948 and parameters: {'base_lr': 8.162314023752154e-06, 'last_lr': 0.0012648043538341749}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4d96fdf7ef4c8a9a1709b0f24078e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.148 New best_val_rmse: 1.148\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6693 New best_val_rmse: 0.6693\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.657 New best_val_rmse: 0.657\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7588 Still best_val_rmse: 0.657 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6051 New best_val_rmse: 0.6051\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6251 Still best_val_rmse: 0.6051 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7151 Still best_val_rmse: 0.6051 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6284 Still best_val_rmse: 0.6051 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5375 New best_val_rmse: 0.5375\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5398 Still best_val_rmse: 0.5375 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5138 New best_val_rmse: 0.5138\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5328 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5944 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.514 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5517 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4904 New best_val_rmse: 0.4904\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5368 Still best_val_rmse: 0.4904 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5083 Still best_val_rmse: 0.4904 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4916 Still best_val_rmse: 0.4904 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4892 New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4892 Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.49 Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4855 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4917 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4859 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4887 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4918 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4978 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4873 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4926 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4843 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4854 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4874 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4887 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4895 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4887 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4871 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4869 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4862 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4862 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4858 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4855 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4853 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4852 Still best_val_rmse: 0.483 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 14:54:30,854]\u001b[0m Trial 12 finished with value: 0.4829988181591034 and parameters: {'base_lr': 3.993251697368309e-05, 'last_lr': 0.004782203168092541}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50466163595045ab9b966029fea96cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9993 New best_val_rmse: 0.9993\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9077 New best_val_rmse: 0.9077\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.68 New best_val_rmse: 0.68\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6592 New best_val_rmse: 0.6592\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6086 New best_val_rmse: 0.6086\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5701 New best_val_rmse: 0.5701\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7384 Still best_val_rmse: 0.5701 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5761 Still best_val_rmse: 0.5701 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6004 Still best_val_rmse: 0.5701 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5511 New best_val_rmse: 0.5511\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5383 New best_val_rmse: 0.5383\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.551 Still best_val_rmse: 0.5383 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.568 Still best_val_rmse: 0.5383 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5266 New best_val_rmse: 0.5266\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5483 Still best_val_rmse: 0.5266 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5135 New best_val_rmse: 0.5135\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5409 Still best_val_rmse: 0.5135 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5069 New best_val_rmse: 0.5069\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.506 New best_val_rmse: 0.506\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5 New best_val_rmse: 0.5\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4984 New best_val_rmse: 0.4984\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5025 Still best_val_rmse: 0.4984 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5057 Still best_val_rmse: 0.4984 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4999 Still best_val_rmse: 0.4984 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.498 New best_val_rmse: 0.498\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5041 Still best_val_rmse: 0.498 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5033 Still best_val_rmse: 0.498 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5001 Still best_val_rmse: 0.498 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 15:05:08,092]\u001b[0m Trial 13 finished with value: 0.49801746010780334 and parameters: {'base_lr': 1.3118716479802712e-05, 'last_lr': 0.0019475564222367723}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e2fe7204eb4568901feb8890fe4bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.152 New best_val_rmse: 1.152\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.075 New best_val_rmse: 1.075\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.026 New best_val_rmse: 1.026\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.158 Still best_val_rmse: 1.026 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.081 Still best_val_rmse: 1.026 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.019 New best_val_rmse: 1.019\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.027 Still best_val_rmse: 1.019 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.065 Still best_val_rmse: 1.019 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.084 Still best_val_rmse: 1.019 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 15:08:38,374]\u001b[0m Trial 14 finished with value: 1.0189093351364136 and parameters: {'base_lr': 0.00048724104170082695, 'last_lr': 0.0006631778951914523}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b706c52675ea4c0d8d4ae329d584ec0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.07 New best_val_rmse: 1.07\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9305 New best_val_rmse: 0.9305\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.667 New best_val_rmse: 0.667\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.644 New best_val_rmse: 0.644\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5959 New best_val_rmse: 0.5959\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.563 New best_val_rmse: 0.563\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7647 Still best_val_rmse: 0.563 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6152 Still best_val_rmse: 0.563 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5632 Still best_val_rmse: 0.563 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5499 New best_val_rmse: 0.5499\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5515 Still best_val_rmse: 0.5499 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.546 New best_val_rmse: 0.546\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6068 Still best_val_rmse: 0.546 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5226 New best_val_rmse: 0.5226\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5372 Still best_val_rmse: 0.5226 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5229 Still best_val_rmse: 0.5226 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5309 Still best_val_rmse: 0.5226 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5051 New best_val_rmse: 0.5051\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5034 New best_val_rmse: 0.5034\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5189 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4997 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4974 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5066 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.497 Still best_val_rmse: 0.4969 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5021 Still best_val_rmse: 0.4954 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.501 Still best_val_rmse: 0.4954 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4978 Still best_val_rmse: 0.4954 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4975 Still best_val_rmse: 0.4954 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 15:19:17,839]\u001b[0m Trial 15 finished with value: 0.4953896999359131 and parameters: {'base_lr': 1.3497842269193575e-05, 'last_lr': 0.0022516911780975715}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58ee614a8f4e88999b7aaaeed747ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9399 New best_val_rmse: 0.9399\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8119 New best_val_rmse: 0.8119\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6663 New best_val_rmse: 0.6663\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7917 Still best_val_rmse: 0.6663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6626 New best_val_rmse: 0.6626\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5827 New best_val_rmse: 0.5827\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5677 New best_val_rmse: 0.5677\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5896 Still best_val_rmse: 0.5677 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.546 New best_val_rmse: 0.546\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5153 New best_val_rmse: 0.5153\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4931 New best_val_rmse: 0.4931\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.6123 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5941 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5496 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5081 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4987 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5116 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.52 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5031 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5014 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4925 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5028 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4861 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4858 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4855 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4852 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4905 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4921 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4866 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4873 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4857 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4864 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4874 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4869 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4831 New best_val_rmse: 0.4831\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4828 New best_val_rmse: 0.4828\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4827 Still best_val_rmse: 0.4827 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 15:32:07,875]\u001b[0m Trial 16 finished with value: 0.4826622009277344 and parameters: {'base_lr': 3.331068288166276e-05, 'last_lr': 0.0009179994715435172}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d34adc62f941e491b653db750c4b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.072 New best_val_rmse: 1.072\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9895 New best_val_rmse: 0.9895\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6842 New best_val_rmse: 0.6842\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6699 New best_val_rmse: 0.6699\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6094 New best_val_rmse: 0.6094\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.565 New best_val_rmse: 0.565\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5541 New best_val_rmse: 0.5541\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5872 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5379 New best_val_rmse: 0.5379\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.532 New best_val_rmse: 0.532\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5574 Still best_val_rmse: 0.532 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5094 New best_val_rmse: 0.5094\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5051 New best_val_rmse: 0.5051\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5046 New best_val_rmse: 0.5046\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.531 Still best_val_rmse: 0.5046 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4992 New best_val_rmse: 0.4992\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5048 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4937 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "8 steps took 6.36 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4978 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4956 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5093 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4985 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5043 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4898 New best_val_rmse: 0.4898\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4973 Still best_val_rmse: 0.4898 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4898 Still best_val_rmse: 0.4898 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4902 Still best_val_rmse: 0.4898 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.489 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4887 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4893 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.49 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4893 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.489 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4878 New best_val_rmse: 0.4878\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4873 Still best_val_rmse: 0.4873 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 15:45:18,122]\u001b[0m Trial 17 finished with value: 0.4872971773147583 and parameters: {'base_lr': 1.3001208754924894e-05, 'last_lr': 0.00024317357096544272}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c4dd14207e4dc69e334e777429c8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9715 New best_val_rmse: 0.9715\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7895 New best_val_rmse: 0.7895\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9471 Still best_val_rmse: 0.7895 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.075 Still best_val_rmse: 0.7895 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8776 Still best_val_rmse: 0.7895 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.9583 Still best_val_rmse: 0.7895 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8304 Still best_val_rmse: 0.7895 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6568 New best_val_rmse: 0.6568\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7231 Still best_val_rmse: 0.6568 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 15:48:48,033]\u001b[0m Trial 18 finished with value: 0.6567704081535339 and parameters: {'base_lr': 6.495609880726438e-05, 'last_lr': 0.0025172819620726955}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1d36103b844058bb8a250ef69a02b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.169 New best_val_rmse: 1.169\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7031 New best_val_rmse: 0.7031\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6681 New best_val_rmse: 0.6681\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8923 Still best_val_rmse: 0.6681 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6045 New best_val_rmse: 0.6045\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5919 New best_val_rmse: 0.5919\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6644 Still best_val_rmse: 0.5919 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5758 New best_val_rmse: 0.5758\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5583 New best_val_rmse: 0.5583\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5339 New best_val_rmse: 0.5339\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5474 Still best_val_rmse: 0.5339 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5553 Still best_val_rmse: 0.5339 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5516 Still best_val_rmse: 0.5339 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5624 Still best_val_rmse: 0.5339 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5564 Still best_val_rmse: 0.5339 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4903 New best_val_rmse: 0.4903\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5066 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4941 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4931 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4909 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4922 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4821 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4876 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4832 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4898 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4848 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.484 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4898 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.482 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4841 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4813 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4799 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4814 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4892 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4883 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4829 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4803 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4803 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4801 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4806 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4831 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4835 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4833 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4825 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4819 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4816 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4812 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.481 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.481 Still best_val_rmse: 0.4797 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-11 16:02:39,973]\u001b[0m Trial 19 finished with value: 0.4797256886959076 and parameters: {'base_lr': 3.2222410815486126e-05, 'last_lr': 0.004855094453925757}. Best is trial 5 with value: 0.47618797421455383.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.47618797421455383\n",
      " Best params: \n",
      "    base_lr: 3.1211012676571296e-05\n",
      "    last_lr: 0.003503681146690041\n",
      "CPU times: user 6h 4min 54s, sys: 1h 35min 34s, total: 7h 40min 29s\n",
      "Wall time: 8h 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(3, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "list_val_rmse = []\n",
    "use_lr_map = {0: False, 1: True, 2: False, 3: False, 4: False, 5: False}\n",
    "\n",
    "pbar = tqdm(enumerate(splits), total=cfg.NUM_FOLDS, position=0, leave=True)\n",
    "for fold, (train_indices, val_indices) in pbar:\n",
    "    pbar.set_description(f'Fold {fold}')\n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "        \n",
    "    optimizer = create_optimizer(model, use_lr=use_lr_map[fold])\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    trainer = Trainer(model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    list_val_rmse.append(trainer.train())\n",
    "    \n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    if cfg.DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print(\"\\nPerformance estimates:\")\n",
    "print(list_val_rmse)\n",
    "print(\"Mean:\", np.array(list_val_rmse).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
