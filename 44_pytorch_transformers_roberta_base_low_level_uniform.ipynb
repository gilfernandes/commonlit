{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = str(MODELS_PATH/'roberta-base_lm')\n",
    "    TOKENIZER_PATH = str(MODELS_PATH/'roberta-base-0')\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'roberta-base'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(8, 256, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((sample_model.attention(a) * a), dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 203\n",
    "    roberta_parameters = named_parameters[:197]\n",
    "    attention_parameters = named_parameters[199:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5\n",
    "        if layer_num >= 133:\n",
    "            lr = base_lr / 0.5\n",
    "        elif layer_num >= 69:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pred = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                \n",
    "                mse.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, self.model_path)\n",
    "                    start = time.time()\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = kfold.split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3905944ef4ad3b098bbca2b917814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e45357fbcff48d4990314b145a241b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 5.01 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9504 New best_val_rmse: 0.9504\n",
      "\n",
      "16 steps took 3.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7897 New best_val_rmse: 0.7897\n",
      "\n",
      "16 steps took 3.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7539 New best_val_rmse: 0.7539\n",
      "\n",
      "16 steps took 3.79 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6913 New best_val_rmse: 0.6913\n",
      "\n",
      "16 steps took 3.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5913 New best_val_rmse: 0.5913\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5479 New best_val_rmse: 0.5479\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5653 Still best_val_rmse: 0.5479 (from epoch 0)\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6806 Still best_val_rmse: 0.5479 (from epoch 0)\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5339 New best_val_rmse: 0.5339\n",
      "\n",
      "16 steps took 4.31 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5389 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5413 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4892 New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 1 batch_num: 48 val_rmse: 0.557 Still best_val_rmse: 0.4892 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.5198 Still best_val_rmse: 0.4892 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4927 Still best_val_rmse: 0.4892 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4922 Still best_val_rmse: 0.4892 (from epoch 1)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5039 Still best_val_rmse: 0.4892 (from epoch 1)\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4802 New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 0.963 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4833 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4816 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "4 steps took 0.964 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4924 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5022 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "16 steps took 4.23 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4823 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4906 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4928 Still best_val_rmse: 0.4802 (from epoch 1)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4785 New best_val_rmse: 0.4785\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.483 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "4 steps took 0.966 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4929 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4852 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4862 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4791 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.488 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.5081 Still best_val_rmse: 0.4785 (from epoch 2)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4805 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4846 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4792 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4766 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4751 New best_val_rmse: 0.4751\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4762 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4773 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4777 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4779 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4778 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4782 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4786 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4787 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4788 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4786 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4785 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4783 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4783 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4783 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4782 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.478 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4778 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4776 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4773 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4772 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.477 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4768 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4766 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4765 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4763 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4762 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4762 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4761 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4761 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4761 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4761 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c41d657834747068559667606290b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.67 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.047 New best_val_rmse: 1.047\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7213 New best_val_rmse: 0.7213\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.684 New best_val_rmse: 0.684\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6205 New best_val_rmse: 0.6205\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5899 New best_val_rmse: 0.5899\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6759 Still best_val_rmse: 0.5899 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6428 Still best_val_rmse: 0.5899 (from epoch 0)\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5893 New best_val_rmse: 0.5893\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5338 New best_val_rmse: 0.5338\n",
      "\n",
      "16 steps took 4.18 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5349 Still best_val_rmse: 0.5338 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5387 Still best_val_rmse: 0.5338 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5185 New best_val_rmse: 0.5185\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5255 Still best_val_rmse: 0.5185 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5175 New best_val_rmse: 0.5175\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5116 New best_val_rmse: 0.5116\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5146 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5007 New best_val_rmse: 0.5007\n",
      "\n",
      "16 steps took 4.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4962 New best_val_rmse: 0.4962\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4938 New best_val_rmse: 0.4938\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4968 Still best_val_rmse: 0.4938 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4965 Still best_val_rmse: 0.4938 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4953 Still best_val_rmse: 0.4938 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4969 Still best_val_rmse: 0.4921 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4931 Still best_val_rmse: 0.4921 (from epoch 2)\n",
      "\n",
      "8 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4943 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4921 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4928 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4924 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4921 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4921 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4921 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4921 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02540b91fa4424a89de32cb67dc9456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.64 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8523 New best_val_rmse: 0.8523\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9049 Still best_val_rmse: 0.8523 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7095 New best_val_rmse: 0.7095\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.686 New best_val_rmse: 0.686\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7221 Still best_val_rmse: 0.686 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6423 New best_val_rmse: 0.6423\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5901 New best_val_rmse: 0.5901\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5677 New best_val_rmse: 0.5677\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5743 Still best_val_rmse: 0.5677 (from epoch 0)\n",
      "\n",
      "16 steps took 4.24 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5626 New best_val_rmse: 0.5626\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6306 Still best_val_rmse: 0.5626 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5903 Still best_val_rmse: 0.5626 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.558 New best_val_rmse: 0.558\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5436 New best_val_rmse: 0.5436\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.524 New best_val_rmse: 0.524\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5341 Still best_val_rmse: 0.524 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5289 Still best_val_rmse: 0.524 (from epoch 1)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.516 New best_val_rmse: 0.516\n",
      "\n",
      "16 steps took 4.24 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5128 New best_val_rmse: 0.5128\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5177 Still best_val_rmse: 0.5128 (from epoch 2)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5126 New best_val_rmse: 0.5126\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5096 New best_val_rmse: 0.5096\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5067 New best_val_rmse: 0.5067\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5067 New best_val_rmse: 0.5067\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5062 New best_val_rmse: 0.5062\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5048 New best_val_rmse: 0.5048\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5047 New best_val_rmse: 0.5047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fc69296d1a44bda1ad035142aebdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.62 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.076 New best_val_rmse: 1.076\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7772 New best_val_rmse: 0.7772\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6755 New best_val_rmse: 0.6755\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7292 Still best_val_rmse: 0.6755 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6183 New best_val_rmse: 0.6183\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5755 New best_val_rmse: 0.5755\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5493 New best_val_rmse: 0.5493\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5339 New best_val_rmse: 0.5339\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6544 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 4.15 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5758 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5425 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5344 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5525 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5427 Still best_val_rmse: 0.5339 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5038 New best_val_rmse: 0.5038\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5251 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5063 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5096 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 4.18 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4916 New best_val_rmse: 0.4916\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4887 New best_val_rmse: 0.4887\n",
      "\n",
      "4 steps took 0.966 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4835 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.501 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.493 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4791 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4801 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4841 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4833 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4834 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 0.983 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4827 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4807 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4789 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 0.5 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4765 New best_val_rmse: 0.4765\n",
      "\n",
      "2 steps took 0.483 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4766 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4769 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.503 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4775 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4781 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4794 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4806 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.984 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4817 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4829 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4826 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4817 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.985 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4808 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4802 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.48 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4799 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4798 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4798 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4797 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4797 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4796 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4796 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4796 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4796 Still best_val_rmse: 0.4765 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e685331f13f443389a0ca0e9ce38620c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.54 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8104 New best_val_rmse: 0.8104\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.259 Still best_val_rmse: 0.8104 (from epoch 0)\n",
      "\n",
      "16 steps took 3.84 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7261 New best_val_rmse: 0.7261\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7352 Still best_val_rmse: 0.7261 (from epoch 0)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6073 New best_val_rmse: 0.6073\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6192 Still best_val_rmse: 0.6073 (from epoch 0)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5488 New best_val_rmse: 0.5488\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6277 Still best_val_rmse: 0.5488 (from epoch 0)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5508 Still best_val_rmse: 0.5488 (from epoch 0)\n",
      "\n",
      "16 steps took 4.29 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6197 Still best_val_rmse: 0.5488 (from epoch 0)\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5365 New best_val_rmse: 0.5365\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5138 New best_val_rmse: 0.5138\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5162 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.516 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5109 New best_val_rmse: 0.5109\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 1 batch_num: 110 val_rmse: 0.496 Still best_val_rmse: 0.4781 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.4844 Still best_val_rmse: 0.4781 (from epoch 1)\n",
      "\n",
      "4 steps took 0.966 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.4864 Still best_val_rmse: 0.4781 (from epoch 1)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.4861 Still best_val_rmse: 0.4781 (from epoch 1)\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.4903 Still best_val_rmse: 0.4781 (from epoch 1)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4792 Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4795 Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4741 New best_val_rmse: 0.4741\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4787 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4821 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4761 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4755 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4765 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4786 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4771 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4782 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4835 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4874 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5006 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4783 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4785 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4842 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4805 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.967 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4799 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4824 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4846 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.968 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4811 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4791 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4816 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4802 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4772 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4772 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4771 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.477 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.477 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4768 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4767 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4769 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4768 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4764 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4764 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4763 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4761 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4761 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4761 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4763 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4764 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4765 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4767 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4767 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4766 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4765 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4765 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4765 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4765 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "\n",
      "\n",
      "Performance estimates:\n",
      "[tensor(0.4751), tensor(0.4919), tensor(0.5047), tensor(0.4765), tensor(0.4741)]\n",
      "Mean: 0.48444372\n"
     ]
    }
   ],
   "source": [
    "list_val_rmse = []\n",
    "\n",
    "pbar = tqdm(enumerate(splits), total=cfg.NUM_FOLDS, position=0, leave=True)\n",
    "for fold, (train_indices, val_indices) in pbar:\n",
    "    pbar.set_description(f'Fold {fold}')\n",
    "    model_path = cfg.MODEL_FOLDER/f\"roberta-base_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, sampler=WeightedSampler(train_dataset), batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "        \n",
    "    optimizer = create_optimizer(model)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    trainer = Trainer(model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    list_val_rmse.append(trainer.train())\n",
    "    \n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    if cfg.DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print(\"\\nPerformance estimates:\")\n",
    "print(list_val_rmse)\n",
    "print(\"Mean:\", np.array(list_val_rmse).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.66 s, sys: 2.64 s, total: 10.3 s\n",
      "Wall time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a7fa4db2d54610a0367756fd5eeb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44511efb06c4e6f8c5648326be32263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.312142841484001\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.35453536052747725\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.32059224740411746\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3342262121475011\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.33193625064639976\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3588098979584189\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a4605c6e334f1781c716ce7172fed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79307df964849e3af1cca68a736ef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3314009215674823\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.33989511856480586\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3200691621809814\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.30714172510842125\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3297242850958922\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3351967107631785\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ecc8bc94754775a02cf96c44afdc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8630de5e26a43088a8c6675366459a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.31492303382301645\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3710494337479098\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3354141274363938\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3243960011506718\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3308837801548399\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3435653199727582\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28589ad12ab42358f943ca1240e0c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9705f9d6b4dd4ddf9d334d04a118fb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.33738823237280635\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3421355049648379\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.316694350146067\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3034172157657744\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.35079517899032514\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3670471707717054\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93849bbbe524e1eb98f47fe180cf3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5cd8a7e73f4505a9e186bc377318db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.345575946808507\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3622680371661247\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.34982519235186066\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3359857872447713\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3384502199006281\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.37340757962166077\n",
      "FINAL RMSE score 0.33729642819464456\n",
      "CPU times: user 2min 8s, sys: 2.92 s, total: 2min 11s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.004122795950432596],\n",
       " [-0.0008061889274363845],\n",
       " [0.0018296439835301104],\n",
       " [-0.0015377556223151255],\n",
       " [-0.0023664210936125934]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20028501, 0.20149104, 0.20008763, 0.20015567, 0.19798065])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.9767934966056343)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54467689, -0.46206632, -0.38874319, -2.42437832, -1.85041837,\n",
       "       -1.60337413,  0.43547454])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01722584639091085, 0.0034451692781821697)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.527451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.444840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.371517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.407152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.833193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.586148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.527451\n",
       "1  f0953f0a5 -0.444840\n",
       "2  0df072751 -0.371517\n",
       "3  04caf4e0c -2.407152\n",
       "4  0e63f8bea -1.833193\n",
       "5  12537fe78 -1.586148\n",
       "6  965e592c0  0.452700"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/roberta-base/best')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/roberta-base_1'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_2'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_3'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_4'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_5'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_6')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    best_model_file = f'{best_model}/model_{i + 1}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/tokenizer.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        config_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/config.json'))\n",
    "        assert config_json.exists()\n",
    "        copyfile(config_json, tokenizer_path/'config.json')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/best_models.zip'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip  dataset-metadata.json  lm.zip  roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/roberta-base.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-0\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-1\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-2\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-3\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-4\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-5\n",
      "2.9G\t/home/commonlit/models/roberta-base/best\n",
      "2.7G\t/home/commonlit/models/roberta-base/best_models.zip\n",
      "4.0K\t/home/commonlit/models/roberta-base/dataset-metadata.json\n",
      "476M\t/home/commonlit/models/roberta-base/lm\n",
      "442M\t/home/commonlit/models/roberta-base/lm.zip\n",
      "4.0K\t/home/commonlit/models/roberta-base/roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/lm.zip'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/roberta-base/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-roberta-base-light-balanced\",\n",
      "  \"id\": \"gilfernandes/commonlit-roberta-base-light-balanced\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light-balanced').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light-balanced')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.60G/2.60G [04:19<00:00, 10.8MB/s]\n",
      "Upload successful: best_models.zip (3GB)\n",
      "Starting upload for file roberta-base.yaml\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:07<00:00, 15.1B/s]\n",
      "Upload successful: roberta-base.yaml (114B)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442M/442M [00:53<00:00, 8.59MB/s]\n",
      "Upload successful: lm.zip (442MB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-roberta-base-light-balanced\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with normal distribution by bin and extra pre-training\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
