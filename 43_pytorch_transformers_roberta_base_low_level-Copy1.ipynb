{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = str(MODELS_PATH/'roberta-base_lm')\n",
    "    TOKENIZER_PATH = str(MODELS_PATH/'roberta-base-0')\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'roberta-base'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 203\n",
    "    roberta_parameters = named_parameters[:197]\n",
    "    attention_parameters = named_parameters[199:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5\n",
    "        if layer_num >= 133:\n",
    "            lr = base_lr / 0.5\n",
    "        elif layer_num >= 69:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pred = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                \n",
    "                mse.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, self.model_path)\n",
    "                    start = time.time()\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = kfold.split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c418a167dca4f0faae8a0ceb679fc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eaecb801fe4177bc06e0b78206fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 5.04 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9517 New best_val_rmse: 0.9517\n",
      "\n",
      "16 steps took 3.76 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.759 New best_val_rmse: 0.759\n",
      "\n",
      "16 steps took 3.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6664 New best_val_rmse: 0.6664\n",
      "\n",
      "16 steps took 3.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5823 New best_val_rmse: 0.5823\n",
      "\n",
      "16 steps took 3.83 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5702 New best_val_rmse: 0.5702\n",
      "\n",
      "16 steps took 3.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5391 New best_val_rmse: 0.5391\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6835 Still best_val_rmse: 0.5391 (from epoch 0)\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5267 New best_val_rmse: 0.5267\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8343 Still best_val_rmse: 0.5267 (from epoch 0)\n",
      "\n",
      "16 steps took 4.17 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5488 Still best_val_rmse: 0.5267 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5304 Still best_val_rmse: 0.501 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5174 Still best_val_rmse: 0.501 (from epoch 1)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4989 New best_val_rmse: 0.4989\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4933 New best_val_rmse: 0.4933\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5017 Still best_val_rmse: 0.4933 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5365 Still best_val_rmse: 0.4933 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5038 Still best_val_rmse: 0.4933 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4872 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 1.19 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4863 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4873 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4879 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 0.981 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4847 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5241 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4873 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.495 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4953 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4917 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.487 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.985 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4854 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.985 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4868 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4873 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4857 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4862 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.982 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4879 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.487 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.981 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4862 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4857 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4856 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4853 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.982 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4859 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.982 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4864 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4865 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4861 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4859 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4856 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4855 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4854 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4854 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c1f8cfe2c5472d88b8f14bd81dbbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.65 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.047 New best_val_rmse: 1.047\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6807 New best_val_rmse: 0.6807\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6995 Still best_val_rmse: 0.6807 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.604 New best_val_rmse: 0.604\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7154 Still best_val_rmse: 0.604 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5419 New best_val_rmse: 0.5419\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5395 New best_val_rmse: 0.5395\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6336 Still best_val_rmse: 0.5395 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6143 Still best_val_rmse: 0.5395 (from epoch 0)\n",
      "\n",
      "16 steps took 4.14 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5099 New best_val_rmse: 0.5099\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.504 New best_val_rmse: 0.504\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5016 New best_val_rmse: 0.5016\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5386 Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4719 New best_val_rmse: 0.4719\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 1 batch_num: 78 val_rmse: 0.483 Still best_val_rmse: 0.4719 (from epoch 1)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 1 batch_num: 82 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 83 val_rmse: 0.4667 Still best_val_rmse: 0.4658 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 85 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 86 val_rmse: 0.4726 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4902 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.4749 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "2 steps took 0.497 seconds\n",
      "Epoch: 1 batch_num: 98 val_rmse: 0.4737 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.481 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.4647 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 105 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 106 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 107 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 109 val_rmse: 0.4619 Still best_val_rmse: 0.4617 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 1 batch_num: 110 val_rmse: 0.4665 Still best_val_rmse: 0.4617 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 111 val_rmse: 0.4754 Still best_val_rmse: 0.4617 (from epoch 1)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 1 batch_num: 113 val_rmse: 0.4848 Still best_val_rmse: 0.4617 (from epoch 1)\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 1 batch_num: 117 val_rmse: 0.4611 New best_val_rmse: 0.4611\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.4572 New best_val_rmse: 0.4572\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 119 val_rmse: 0.4586 Still best_val_rmse: 0.4572 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4675 Still best_val_rmse: 0.4572 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 121 val_rmse: 0.474 Still best_val_rmse: 0.4572 (from epoch 1)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 1 batch_num: 123 val_rmse: 0.4639 Still best_val_rmse: 0.4572 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4615 Still best_val_rmse: 0.4572 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 125 val_rmse: 0.4568 New best_val_rmse: 0.4568\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.4555 New best_val_rmse: 0.4555\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 127 val_rmse: 0.4568 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4595 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 129 val_rmse: 0.4614 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.4619 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4611 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4596 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.4588 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4585 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 1 batch_num: 135 val_rmse: 0.4581 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.459 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.4602 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4608 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 1 batch_num: 139 val_rmse: 0.4614 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4617 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4639 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4674 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 143 val_rmse: 0.4755 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 1 batch_num: 145 val_rmse: 0.4813 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "4 steps took 1.26 seconds\n",
      "Epoch: 2 batch_num: 1 val_rmse: 0.4602 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4634 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 3 val_rmse: 0.465 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4631 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 5 val_rmse: 0.4593 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4572 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.4568 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4565 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 9 val_rmse: 0.4566 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4568 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 11 val_rmse: 0.4574 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4578 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 13 val_rmse: 0.4587 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4595 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4595 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4595 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4601 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4609 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4601 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4597 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4587 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4574 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4569 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4565 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4563 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4561 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4563 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4583 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4611 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4637 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4658 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4676 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4695 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4727 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4724 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4667 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4641 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4614 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4602 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4608 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4625 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4641 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4652 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4665 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4672 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4676 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4667 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4648 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4622 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.461 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4633 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4668 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.47 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4717 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4703 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4635 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4606 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4584 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.457 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4562 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4557 Still best_val_rmse: 0.4555 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4553 New best_val_rmse: 0.4553\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.455 New best_val_rmse: 0.455\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.455 New best_val_rmse: 0.455\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.455 New best_val_rmse: 0.455\n",
      "\n",
      "1 steps took 0.241 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4556 Still best_val_rmse: 0.455 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4562 Still best_val_rmse: 0.455 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4557 Still best_val_rmse: 0.455 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4551 Still best_val_rmse: 0.455 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4543 New best_val_rmse: 0.4543\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4538 New best_val_rmse: 0.4538\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4539 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4545 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4551 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4561 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4566 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4567 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4565 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4567 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4566 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4561 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4555 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.455 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4545 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4542 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4541 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4539 Still best_val_rmse: 0.4538 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4537 New best_val_rmse: 0.4537\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4536 New best_val_rmse: 0.4536\n",
      "\n",
      "1 steps took 0.242 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4535 New best_val_rmse: 0.4535\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4535 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4535 New best_val_rmse: 0.4535\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4535 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4535 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4536 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4536 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4537 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4539 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4541 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4542 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4542 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4542 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4542 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4542 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4541 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4541 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.454 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4541 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4541 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.454 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.454 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.454 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4539 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4539 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4537 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4537 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4537 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4537 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4537 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.254 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n",
      "1 steps took 0.139 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4538 Still best_val_rmse: 0.4535 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5fe761d6374e0fa4eb350871009a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.989 New best_val_rmse: 0.989\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7959 New best_val_rmse: 0.7959\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7085 New best_val_rmse: 0.7085\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6543 New best_val_rmse: 0.6543\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6789 Still best_val_rmse: 0.6543 (from epoch 0)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6121 New best_val_rmse: 0.6121\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6157 Still best_val_rmse: 0.6121 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5464 New best_val_rmse: 0.5464\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5759 Still best_val_rmse: 0.5464 (from epoch 0)\n",
      "\n",
      "16 steps took 4.18 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5881 Still best_val_rmse: 0.5464 (from epoch 0)\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5244 New best_val_rmse: 0.5244\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.536 Still best_val_rmse: 0.5244 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5406 Still best_val_rmse: 0.5244 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.518 New best_val_rmse: 0.518\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.511 New best_val_rmse: 0.511\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5165 Still best_val_rmse: 0.511 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5053 New best_val_rmse: 0.5053\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5032 New best_val_rmse: 0.5032\n",
      "\n",
      "16 steps took 4.17 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4956 New best_val_rmse: 0.4956\n",
      "\n",
      "8 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5012 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5072 Still best_val_rmse: 0.4956 (from epoch 2)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4954 Still best_val_rmse: 0.4934 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4937 Still best_val_rmse: 0.4934 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4935 Still best_val_rmse: 0.4934 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4959 Still best_val_rmse: 0.4934 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4925 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4927 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4926 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 1.98 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4923 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4923 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3a132be16e4031b90b6d013ccffde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.55 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.06 New best_val_rmse: 1.06\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7764 New best_val_rmse: 0.7764\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7042 New best_val_rmse: 0.7042\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6157 New best_val_rmse: 0.6157\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5837 New best_val_rmse: 0.5837\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5696 New best_val_rmse: 0.5696\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5706 Still best_val_rmse: 0.5696 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5473 New best_val_rmse: 0.5473\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5381 New best_val_rmse: 0.5381\n",
      "\n",
      "16 steps took 4.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5134 New best_val_rmse: 0.5134\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.512 New best_val_rmse: 0.512\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.513 Still best_val_rmse: 0.512 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5115 New best_val_rmse: 0.5115\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.523 Still best_val_rmse: 0.5115 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.504 New best_val_rmse: 0.504\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5746 Still best_val_rmse: 0.504 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5035 New best_val_rmse: 0.5035\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 2.23 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5292 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5043 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5014 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4937 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5089 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.492 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4903 New best_val_rmse: 0.4903\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4904 Still best_val_rmse: 0.4903 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4901 New best_val_rmse: 0.4901\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4899 New best_val_rmse: 0.4899\n",
      "\n",
      "4 steps took 0.969 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4895 New best_val_rmse: 0.4895\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4894 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 0.964 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 0.964 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4894 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "4 steps took 0.963 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4894 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "4 steps took 0.967 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n",
      "4 steps took 0.961 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ddf3b3b2c24d5caf7ee0f117bd1cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.54 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9533 New best_val_rmse: 0.9533\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7232 New best_val_rmse: 0.7232\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6139 New best_val_rmse: 0.6139\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6167 Still best_val_rmse: 0.6139 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6353 Still best_val_rmse: 0.6139 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6616 Still best_val_rmse: 0.6139 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6008 New best_val_rmse: 0.6008\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5916 New best_val_rmse: 0.5916\n",
      "\n",
      "16 steps took 3.96 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5721 New best_val_rmse: 0.5721\n",
      "\n",
      "16 steps took 4.25 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5232 New best_val_rmse: 0.5232\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.517 New best_val_rmse: 0.517\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5214 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5273 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5197 Still best_val_rmse: 0.5065 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4968 New best_val_rmse: 0.4968\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4971 Still best_val_rmse: 0.4968 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4869 New best_val_rmse: 0.4869\n",
      "\n",
      "4 steps took 0.992 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.5187 Still best_val_rmse: 0.4869 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.5159 Still best_val_rmse: 0.4869 (from epoch 1)\n",
      "\n",
      "16 steps took 4.26 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4847 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4799 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.478 New best_val_rmse: 0.478\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4789 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4818 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4792 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4784 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4837 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 0.982 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4854 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4797 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4793 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4779 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4782 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4793 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4796 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4787 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4788 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4794 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4792 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.479 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4785 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.478 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.5 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4775 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4781 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4796 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4796 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4788 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4782 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4776 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4777 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4778 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.478 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4786 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4788 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4789 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4788 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4785 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4784 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4781 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4777 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4774 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4769 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4769 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4769 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4769 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4769 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.482 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.482 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.48 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0703dadefd492ba569b49477129d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.58 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.325 New best_val_rmse: 1.325\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8763 New best_val_rmse: 0.8763\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6236 New best_val_rmse: 0.6236\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7016 Still best_val_rmse: 0.6236 (from epoch 0)\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6466 Still best_val_rmse: 0.6236 (from epoch 0)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5566 New best_val_rmse: 0.5566\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5631 Still best_val_rmse: 0.5566 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6322 Still best_val_rmse: 0.5566 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5647 Still best_val_rmse: 0.5566 (from epoch 0)\n",
      "\n",
      "16 steps took 4.26 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5489 New best_val_rmse: 0.5489\n",
      "\n",
      "16 steps took 3.95 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5409 New best_val_rmse: 0.5409\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5204 New best_val_rmse: 0.5204\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5071 New best_val_rmse: 0.5071\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5228 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.512 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5521 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 1.96 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4941 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5129 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "16 steps took 4.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4853 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4836 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4902 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4866 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.494 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 0.981 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4933 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4846 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5156 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4803 New best_val_rmse: 0.4803\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4808 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.482 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4861 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4872 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.484 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4816 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4811 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.481 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.481 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4811 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4813 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4815 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4817 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4818 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.482 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4821 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.983 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4821 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4821 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "\n",
      "\n",
      "Performance estimates:\n",
      "[tensor(0.4815), tensor(0.4535), tensor(0.4918), tensor(0.4893), tensor(0.4769), tensor(0.4803)]\n",
      "Mean: 0.47888494\n"
     ]
    }
   ],
   "source": [
    "list_val_rmse = []\n",
    "\n",
    "pbar = tqdm(enumerate(splits), total=cfg.NUM_FOLDS, position=0, leave=True)\n",
    "for fold, (train_indices, val_indices) in pbar:\n",
    "    pbar.set_description(f'Fold {fold}')\n",
    "    model_path = cfg.MODEL_FOLDER/f\"roberta-base_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "        \n",
    "    optimizer = create_optimizer(model)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    trainer = Trainer(model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    list_val_rmse.append(trainer.train())\n",
    "    \n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    if cfg.DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print(\"\\nPerformance estimates:\")\n",
    "print(list_val_rmse)\n",
    "print(\"Mean:\", np.array(list_val_rmse).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.84 s, sys: 2.64 s, total: 10.5 s\n",
      "Wall time: 8.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "            cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aea140e5b344a39a383398ebd759fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184fa37c68394ecb859d66da0e676299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.36550808806022805\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3892447944996949\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3575438931619624\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.32517562032047437\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.36159873705112916\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.37287193956391995\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5093b30ea7440cd8e8053897482b27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919d07abb77549cfbe4c5bde569f589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3638330713907293\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3805220156614261\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3387056043447015\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3514474564889266\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.35702522851613155\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.37597055255951656\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d11afb2e1f45a1a8077f7f13da0864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c4bd848e07432a8cc68562e8e126eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.39025906415086853\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3756086186646978\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.34031009813519836\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3793376712959961\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3635410022501858\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.38556034255063004\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2717634556734e64a194efdc800301c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c604bd1dd9d043e89974d6e1cc4eb21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3444165671558248\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.35942624057248\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.34570919000547357\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.31404498682049775\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3371218351046066\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3450690014221917\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f7f949a6f24a50bd6c20d837d6dea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc6d4ccb5b94c53b745cfcc772d48c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3744261922049248\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.38088212096522267\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.34247634124874293\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3431119256618516\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.37095399450067695\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.38254104366823705\n",
      "FINAL RMSE score 0.36047477459990496\n",
      "CPU times: user 2min 14s, sys: 1.91 s, total: 2min 16s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model.transformer_model if hasattr(inference_model, 'transformer_model') else inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.005153221483623596],\n",
       " [0.003964462237853874],\n",
       " [0.006731210171453145],\n",
       " [0.011433733987456354],\n",
       " [-0.0014762375862074479]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19978976, 0.19989238, 0.19834089, 0.20270617, 0.1992708 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.9893307630009803)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69439869, -0.37979068, -0.36879958, -2.49210601, -1.7894283 ,\n",
       "       -1.51868395,  0.31567297])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02999035576716491, 0.005998071153432982)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.664408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.338809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.462116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.759438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.488694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.345663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.664408\n",
       "1  f0953f0a5 -0.349800\n",
       "2  0df072751 -0.338809\n",
       "3  04caf4e0c -2.462116\n",
       "4  0e63f8bea -1.759438\n",
       "5  12537fe78 -1.488694\n",
       "6  965e592c0  0.345663"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/roberta-base/best')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/roberta-base_1'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_2'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_3'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_4'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_5'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_6')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    best_model_file = f'{best_model}/model_{i + 1}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/tokenizer.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        config_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/config.json'))\n",
    "        assert config_json.exists()\n",
    "        copyfile(config_json, tokenizer_path/'config.json')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/best_models.zip'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip  dataset-metadata.json  lm  lm.zip  roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/roberta-base.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-0\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-1\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-2\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-3\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-4\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-5\n",
      "2.9G\t/home/commonlit/models/roberta-base/best\n",
      "2.7G\t/home/commonlit/models/roberta-base/best_models.zip\n",
      "4.0K\t/home/commonlit/models/roberta-base/dataset-metadata.json\n",
      "476M\t/home/commonlit/models/roberta-base/lm\n",
      "442M\t/home/commonlit/models/roberta-base/lm.zip\n",
      "4.0K\t/home/commonlit/models/roberta-base/roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/lm.zip'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/roberta-base/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-roberta-base-light\",\n",
      "  \"id\": \"gilfernandes/commonlit-roberta-base-light\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.60G/2.60G [04:31<00:00, 10.3MB/s]\n",
      "Upload successful: best_models.zip (3GB)\n",
      "Starting upload for file roberta-base.yaml\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:04<00:00, 24.4B/s]\n",
      "Upload successful: roberta-base.yaml (114B)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442M/442M [00:54<00:00, 8.57MB/s]\n",
      "Upload successful: lm.zip (442MB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-roberta-base-light\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with normal distribution by bin and extra pre-training\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
