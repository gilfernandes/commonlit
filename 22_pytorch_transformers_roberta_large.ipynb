{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gc, sys, time, collections, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Optional, Union, Any, List, Tuple\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as D\n",
    "from torch.utils.data.dataset import Dataset, IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertModel\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertModel\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "from transformers.trainer import logging\n",
    "from transformers.file_utils import is_torch_tpu_available, is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import find_batch_size, nested_concat, nested_numpify, nested_truncate, nested_detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b25934-3c16-4cb0-ab3b-6d95223432f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonlit_lm\t\t       sample_submission.csv  train-orig.csv\n",
      "commonlitreadabilityprize.zip  test.csv\t\t      train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2844 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     c12129c31                                                NaN   \n",
       "2     85aa80a4c                                                NaN   \n",
       "3     b69ac6792                                                NaN   \n",
       "4     dd1000b26                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2839  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2840  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2841  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2842  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2843  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  When the young people returned to the ballroom...   \n",
       "2              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "3              NaN  As Roger had predicted, the snow departed as q...   \n",
       "4              NaN  And outside before the palace a great garden w...   \n",
       "...            ...                                                ...   \n",
       "2839  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2840  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2841  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2842  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2843  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.340259        0.464009  \n",
       "2    -0.315372        0.480805  \n",
       "3    -0.580118        0.476676  \n",
       "4    -1.054013        0.450007  \n",
       "...        ...             ...  \n",
       "2839  1.711390        0.646900  \n",
       "2840  0.189476        0.535648  \n",
       "2841  0.255209        0.483866  \n",
       "2842 -0.215279        0.514128  \n",
       "2843  0.300779        0.512379  \n",
       "\n",
       "[2844 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efffba57-a6ab-4210-8391-5f2fccb3fd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>21ea485fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A little within the wood there was a fair cast...</td>\n",
       "      <td>-1.302688</td>\n",
       "      <td>0.450399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>a04741371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The king dwelt for many months in Nottingham, ...</td>\n",
       "      <td>-0.714009</td>\n",
       "      <td>0.506864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>5cb5ab998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When they drew near Nottingham, all the people...</td>\n",
       "      <td>-1.541347</td>\n",
       "      <td>0.478166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>622f6215e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>About this time there was living in Nottingham...</td>\n",
       "      <td>-2.054284</td>\n",
       "      <td>0.538084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id url_legal license  \\\n",
       "227  21ea485fb       NaN     NaN   \n",
       "229  a04741371       NaN     NaN   \n",
       "230  5cb5ab998       NaN     NaN   \n",
       "231  622f6215e       NaN     NaN   \n",
       "\n",
       "                                               excerpt    target  \\\n",
       "227  A little within the wood there was a fair cast... -1.302688   \n",
       "229  The king dwelt for many months in Nottingham, ... -0.714009   \n",
       "230  When they drew near Nottingham, all the people... -1.541347   \n",
       "231  About this time there was living in Nottingham... -2.054284   \n",
       "\n",
       "     standard_error  \n",
       "227        0.450399  \n",
       "229        0.506864  \n",
       "230        0.478166  \n",
       "231        0.538084  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['excerpt'].str.contains('Robin Hood')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13ba276-1696-40d2-b05f-8f5f31e79c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A little within the wood there was a fair castle, with a double moat, and surrounded by stout walls. Here dwelt that noble knight, Sir Richard Lee, to whom Robin Hood had lent the four hundred pounds to redeem his land.\\nHe saw the little company of outlaws fighting their way along, so he hastened to call them to come and take shelter in his castle.\\n\"Welcome art thou, Robin Hood! Welcome!\" he cried, as he led them in. \"Much I thank thee for thy comfort and courtesy and great kindness to me in the forest. There is no man in the world I love so much as thee. For all the proud Sheriff of Nottingham, here thou shalt be safe!—Shut the gates, and draw the bridge, and let no man come in!\" he shouted to his retainers. \"Arm you well; make ready; guard the walls! One thing, Robin, I promise thee: here shalt thou stay for twelve days as my guest, to sup, and eat, and dine.\"\\nSwiftly and readily tables were laid and cloths spread, and Robin Hood and his merry men sat down to a good meal.',\n",
       "       'A little within the wood there was an imposing castle, with a double dig, and surrounded by stout walls. Here dwelt that phantastic knight, Sir William Jones, to whom Trevor Blanford had lent the twenty hundred dollars to redeem his land.\\nHe saw the little company of small criminals fighting their way along, so he hastened to call them to come and take shelter in his castle.\\n\"Welcome art thou, Trevor Blanford! Welcome!\" he cried, as he led them in. \"Much I thank thee for thy comfort and sympathy and great kindness to me in the forest. There is no man in the world I love so much as thee. For all the proud Sheriff of Kendall, here thou shalt be safe!—Shut the gates, and draw the bridge, and let no man come in!\" he shouted to his retainers. \"Arm you well; prepare yourselves; guard the fortress! One thing, Trevor, I promise thee: here shalt thou stay for fifteen days as my guest, to sup, and eat, and dine.\"\\nSwiftly and readily tables were laid and cloths spread, and Trevor Blanford and his jolly men sat down to a good meal.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['id'] == '21ea485fb']['excerpt'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5b0bd8-527e-4b89-9082-f0780c9ae28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299   -3.351956\n",
       "1300   -3.351956\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['id'] == '0bf29d257']['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5cd025a-a37c-4aaf-a192-ac278264a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b62eb3-cedd-4629-a726-173588d6a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b48a945-271e-4c18-afac-e0a4c9194e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train_df['target'],bins=100,kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Prepare Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbbc9a60-4c04-447a-9ddc-e5125688f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.floor(np.log2(len(train_df))) + 1)\n",
    "train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0769b07-e9ea-42b7-95a6-5becf82dd824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.411708</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.969369</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.526589</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.106393</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.652726</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.201392</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.748612</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.309570</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.130016</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.560802</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.978923</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.399764</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target      \n",
       "          mean count\n",
       "bins                \n",
       "0    -3.411708    44\n",
       "1    -2.969369    79\n",
       "2    -2.526589   172\n",
       "3    -2.106393   269\n",
       "4    -1.652726   366\n",
       "5    -1.201392   420\n",
       "6    -0.748612   484\n",
       "7    -0.309570   408\n",
       "8     0.130016   312\n",
       "9     0.560802   184\n",
       "10    0.978923    83\n",
       "11    1.399764    23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['target', 'bins']].groupby(['bins']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f70453a0-e80b-4953-95cb-fd06547c6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (t_, v_) in enumerate(kf.split(X=train_df, y=train_df.bins.values)):\n",
    "    train_df.loc[v_, 'kfold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b311fb49-dfdc-43e7-a89a-182b731c879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['kfold'] = train_df['kfold'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de386c06-bb90-4034-b290-cd03cb61cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('bins', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2844 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     c12129c31                                                NaN   \n",
       "2     85aa80a4c                                                NaN   \n",
       "3     b69ac6792                                                NaN   \n",
       "4     dd1000b26                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2839  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2840  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2841  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2842  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2843  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  When the young people returned to the ballroom...   \n",
       "2              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "3              NaN  As Roger had predicted, the snow departed as q...   \n",
       "4              NaN  And outside before the palace a great garden w...   \n",
       "...            ...                                                ...   \n",
       "2839  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2840  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2841  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2842  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2843  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  kfold  \n",
       "0    -0.340259        0.464009      0  \n",
       "1    -0.340259        0.464009      0  \n",
       "2    -0.315372        0.480805      0  \n",
       "3    -0.580118        0.476676      0  \n",
       "4    -1.054013        0.450007      0  \n",
       "...        ...             ...    ...  \n",
       "2839  1.711390        0.646900     11  \n",
       "2840  0.189476        0.535648     11  \n",
       "2841  0.255209        0.483866     11  \n",
       "2842 -0.215279        0.514128     11  \n",
       "2843  0.300779        0.512379     11  \n",
       "\n",
       "[2844 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 6, 0, 7, 1, 2, 8, 11, 9, 10, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_list = list(range(num_bins))\n",
    "random.shuffle(bin_list)\n",
    "bin_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fd3a4-9d7b-42be-b31c-2f6c6c15a3a0",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d57cfc8c-551e-4f67-91f8-5aec7002d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "def rmse_score_2(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18e1c48e-da7e-4a1e-b91e-c9c5a262a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10)\n",
    "b = np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "883bd02c-7ab8-4fdc-bb6e-42e6c98a0935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43722186523962475, 0.43722186523962475)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(a, b), rmse_score_2(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447f318-60fd-4ea4-b08b-50c2be1dfdb7",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09389d9d-fa8a-4588-802e-5d6cb3d22d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    model_name = 'roberta-large'\n",
    "    batch_size = 24\n",
    "    max_len = 256\n",
    "    save_dir = f'trained/{model_name}'\n",
    "    num_workers = 4\n",
    "    epochs = 30\n",
    "    pretrained_transformers_model = f'/home/commonlit/models/{model_name}_lm/best_lm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91fcbcf0-e16f-4484-bc0a-a72f24d9536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CONFIG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb94e2f9-e46a-4dd3-9447-f14676cf53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(cfg.pretrained_transformers_model).exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96bf65-b75e-468d-83e2-1d60792baebd",
   "metadata": {},
   "source": [
    "### Prepare train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e601b365-a123-4a1d-bd31-3e6aeeee2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(fold = [1]):\n",
    "    valid_df = train_df[train_df['kfold'].isin(fold)]\n",
    "    valid_text = valid_df['excerpt'].values\n",
    "    valid_target = valid_df['target'].values\n",
    "    training_df = train_df[~train_df['kfold'].isin(fold)]\n",
    "    train_text = training_df['excerpt'].values\n",
    "    train_target = training_df['target'].values\n",
    "    return train_text, train_target, valid_text, valid_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8575c418-6c76-409c-9ce2-f907a92764d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2607, 237)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text, train_target, valid_text, valid_target = create_split([0])\n",
    "len(train_text), len(valid_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c638-0a6f-4b0b-baab-9422908e0343",
   "metadata": {},
   "source": [
    "### Prepare Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34addbcd-c989-475e-9c61-72b2e3ddf8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained/roberta-large/tokenizer_config.json',\n",
       " 'trained/roberta-large/special_tokens_map.json',\n",
       " 'trained/roberta-large/vocab.json',\n",
       " 'trained/roberta-large/merges.txt',\n",
       " 'trained/roberta-large/added_tokens.json',\n",
       " 'trained/roberta-large/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "# Save the tokenizer so that you can download the files and move it to a Kaggle dataset.\n",
    "tokenizer.save_pretrained(cfg.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10cea73f-5dd2-42f2-bc8d-454190f84655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer(train_df['excerpt'].values[0],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=cfg.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"].squeeze())\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eab4041e-0b2d-41d6-94a3-6e3646e3427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d093a87-1edd-4e0b-8545-5e97a91a7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, target, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.target = target\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return InputFeatures(input_ids=convert_to_list(encode['input_ids']),\n",
    "                      attention_mask=convert_to_list(encode['attention_mask']),\n",
    "                      label=torch.tensor(self.target[idx]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a038b280-5d95-4a08-b3c6-da445770f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target):\n",
    "    train_ds = CommonLitDataset(train_text, train_target, tokenizer, cfg.max_len)\n",
    "    valid_ds = CommonLitDataset(valid_text, valid_target, tokenizer, cfg.max_len)\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ae453b-84a5-4a13-a2fc-00e5562a95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode.keys(), target.shape, encode['input_ids'].shape, encode['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddf1a2cf-19d7-42c1-ac39-83a8bb2655bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode['input_ids'][0].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed362b-cd1d-4d1a-b343-6ecddef7e324",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "863d53eb-7874-4bef-8b6a-d542749f7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# You can use a Transformer model of your choice.\n",
    "# transformer_model = DistilBertModel.from_pretrained(cfg.pretrained_transformers_model)\n",
    "transformer_model = AutoModel.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1f92770-7aa6-4ff0-b656-33f6b4d00bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_out = transformer_model(input_ids=encode['input_ids'].squeeze(), attention_mask=encode['attention_mask'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2551c8b-5ac7-4af0-bb2f-854c7fc45559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(transformer_out)['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "814e739b-f7a1-405c-b13c-31e6b93dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean(transformer_out.last_hidden_state, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecf50196-0a69-4db8-bc17-b967f89911f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_layer = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "132b7585-1f6e-4482-9b92-9a7260618888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "403ef922-6ede-4fc1-83a1-fe12b85a517b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-large\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f2dbb7c-e9b8-47af-a124-32c5a5448ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5021054-9f67-404b-9a03-a34db81e49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class CommonLitModel(PreTrainedModel):\n",
    "    def __init__(self):\n",
    "        super(PreTrainedModel, self).__init__()\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_transformers_model)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.config = AutoConfig.from_pretrained(cfg.pretrained_transformers_model)\n",
    "        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
    "        self.out = nn.Linear(self.config.hidden_size, 1)\n",
    "#         self._init_weights(self.layer_norm)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer_model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), output_hidden_states=False)\n",
    "        x = transformer_out.pooler_output\n",
    "#         x = transformer_out.last_hidden_state[:, 0, :] # N, C, X\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n",
    "        \"\"\"\n",
    "        For models that inherit from :class:`~transformers.PreTrainedModel`, uses that method to compute the number of\n",
    "        floating point operations for every backward + forward pass. If using another model, either implement such a\n",
    "        method in the model or subclass and override this method.\n",
    "        Args:\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "        Returns:\n",
    "            :obj:`int`: The number of floating-point operations.\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68bf7bb6-efa1-438e-8f61-a60f46df79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d402996c-e965-4047-a39c-68a2c465280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a04bb94-1364-4709-bc76-878d6d9ced4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = transformer_model.cuda()\n",
    "sample_out = transformer_model(encoded_dict.input_ids.cuda(), encoded_dict.attention_mask.cuda(), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e210ebf4-4ada-4a98-aa85-fb77e052e87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e942e16-3653-4a19-b765-f3ef11189391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34428a20-76bf-4dcd-ad14-c008f82c7123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 1024]), torch.Size([1, 1024]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out['last_hidden_state'].shape, sample_out['last_hidden_state'][:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0504d936-bebe-4fbd-90b5-b5b990501033",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfcf10f5-20f7-4f97-9a17-35c63ec4915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7265d3b2-f7cc-48aa-9f2b-66987f10182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.attention_mask.unsqueeze(0).shape, encoded_dict.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b825b57-a244-4a0c-be59-102e9c8f2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_out = transformer_model(encode.input_ids.unsqueeze(0).cuda(), encode.attention_mask.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d826f6b-79c8-447d-9d78-df79b99e6655",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68a48686-ddb0-4046-ab70-40e16117bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8fa4e6a-6a87-4860-b33a-075c5e693a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "\n",
    "def loss_fct(yhat, y):\n",
    "    return criterion(yhat, y) * 0.7 + 0.3 * kl_loss(yhat, y)\n",
    "\n",
    "loss_fct = nn.MSELoss()\n",
    "\n",
    "def loss_fct(yhat, y):\n",
    "    return torch.sqrt(criterion(yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a861b0e-1207-454d-81fa-4a684a153d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_args(fold):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{fold}\"),\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_first_step=True,\n",
    "        save_steps=40000,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_total_limit = 3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='mse',\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=5e-5\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb7ea1ed-e72f-43fd-aa2c-b1645480f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    return {'mse': mean_squared_error(logits, labels), 'rmse': rmse_score_2(logits, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "325a9194-570e-4c1b-83c7-eb0fe7723de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "826b6f17-dd24-4be8-8644-4e4e40712f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class CommonLitTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        input_ids = inputs.pop(\"input_ids\")\n",
    "        attention_mask = inputs.pop(\"attention_mask\")\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs\n",
    "        loss = loss_fct(logits.flatten(),\n",
    "                        labels.float().flatten())\n",
    "        zero_cat = torch.zeros([1, 1]).to(outputs.device)\n",
    "        return (loss, torch.cat([zero_cat, outputs])) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fec9efc-c67a-41b3-a6ad-ecf805fc4a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-large'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "004a6849-c4d7-40e9-98e9-eacd947f550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/commonlit/models/{cfg.model_name.replace('/', '_')}-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3822354-7ee5-4f5f-93e6-f357a0157811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 0: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgilf\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">atomic-disco-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/ijsuy1u8\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/ijsuy1u8</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_143305-ijsuy1u8</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3161' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3161/3270 35:55 < 01:14, 1.47 it/s, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.265200</td>\n",
       "      <td>0.603353</td>\n",
       "      <td>0.603353</td>\n",
       "      <td>0.776758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.408157</td>\n",
       "      <td>0.408157</td>\n",
       "      <td>0.638871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>0.493557</td>\n",
       "      <td>0.493557</td>\n",
       "      <td>0.702536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>0.436195</td>\n",
       "      <td>0.436195</td>\n",
       "      <td>0.660450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.436969</td>\n",
       "      <td>0.436969</td>\n",
       "      <td>0.661036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.388951</td>\n",
       "      <td>0.388950</td>\n",
       "      <td>0.623659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.431030</td>\n",
       "      <td>0.431030</td>\n",
       "      <td>0.656529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.564802</td>\n",
       "      <td>0.564802</td>\n",
       "      <td>0.751533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.297700</td>\n",
       "      <td>0.509135</td>\n",
       "      <td>0.509135</td>\n",
       "      <td>0.713537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.544312</td>\n",
       "      <td>0.544312</td>\n",
       "      <td>0.737775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.470053</td>\n",
       "      <td>0.470053</td>\n",
       "      <td>0.685604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.409560</td>\n",
       "      <td>0.409559</td>\n",
       "      <td>0.639968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.542144</td>\n",
       "      <td>0.542144</td>\n",
       "      <td>0.736304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.701820</td>\n",
       "      <td>0.701820</td>\n",
       "      <td>0.837747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.338407</td>\n",
       "      <td>0.338407</td>\n",
       "      <td>0.581728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.396973</td>\n",
       "      <td>0.396973</td>\n",
       "      <td>0.630058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.307450</td>\n",
       "      <td>0.307450</td>\n",
       "      <td>0.554481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.576176</td>\n",
       "      <td>0.576176</td>\n",
       "      <td>0.759062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.348396</td>\n",
       "      <td>0.348396</td>\n",
       "      <td>0.590251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.447314</td>\n",
       "      <td>0.447314</td>\n",
       "      <td>0.668816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.473633</td>\n",
       "      <td>0.473633</td>\n",
       "      <td>0.688210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.395314</td>\n",
       "      <td>0.395314</td>\n",
       "      <td>0.628740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.443559</td>\n",
       "      <td>0.443559</td>\n",
       "      <td>0.666002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.557262</td>\n",
       "      <td>0.557262</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.420185</td>\n",
       "      <td>0.420185</td>\n",
       "      <td>0.648216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.395964</td>\n",
       "      <td>0.395964</td>\n",
       "      <td>0.629257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.414455</td>\n",
       "      <td>0.414455</td>\n",
       "      <td>0.643782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.648460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.401903</td>\n",
       "      <td>0.401903</td>\n",
       "      <td>0.633958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-0/checkpoint-1853\n",
      "result {'eval_loss': 0.3074496388435364, 'eval_mse': 0.3074496388435364, 'eval_rmse': 0.5544813871383667, 'eval_runtime': 3.9611, 'eval_samples_per_second': 59.832, 'epoch': 29.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 1: [7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ijsuy1u8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16925<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_143305-ijsuy1u8/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_143305-ijsuy1u8/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.041</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>29.0</td></tr><tr><td>train/global_step</td><td>3161</td></tr><tr><td>_runtime</td><td>2167</td></tr><tr><td>_timestamp</td><td>1623424153</td></tr><tr><td>_step</td><td>60</td></tr><tr><td>eval/loss</td><td>0.30745</td></tr><tr><td>eval/mse</td><td>0.30745</td></tr><tr><td>eval/rmse</td><td>0.55448</td></tr><tr><td>eval/runtime</td><td>3.9611</td></tr><tr><td>eval/samples_per_second</td><td>59.832</td></tr><tr><td>train/train_runtime</td><td>2155.8428</td></tr><tr><td>train/train_samples_per_second</td><td>1.517</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▆▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▆▃▄▃▃▂▃▆▅▅▄▃▅█▂▃▁▆▂▃▄▃▃▅▃▃▃▃▃▁</td></tr><tr><td>eval/mse</td><td>▆▃▄▃▃▂▃▆▅▅▄▃▅█▂▃▁▆▂▃▄▃▃▅▃▃▃▃▃▁</td></tr><tr><td>eval/rmse</td><td>▆▃▅▄▄▃▄▆▅▆▄▃▅█▂▃▁▆▂▄▄▃▄▆▃▃▃▃▃▁</td></tr><tr><td>eval/runtime</td><td>▁▅█▇▅▇▆▅▅▇▆▆█▅▅▅▇▇▆▅▅▆▆▇▆▅▅▆▄▃</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁▂▄▂▃▄▄▂▃▃▁▄▄▄▂▂▃▄▄▃▃▂▃▄▄▃▅▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">atomic-disco-2</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/ijsuy1u8\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/ijsuy1u8</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:ijsuy1u8). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">breezy-armadillo-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/2w49gdun\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/2w49gdun</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_150917-2w49gdun</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2834' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2834/3270 32:12 < 04:57, 1.47 it/s, Epoch 26/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.197900</td>\n",
       "      <td>0.431424</td>\n",
       "      <td>0.431424</td>\n",
       "      <td>0.656829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.662800</td>\n",
       "      <td>0.495681</td>\n",
       "      <td>0.495681</td>\n",
       "      <td>0.704046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>0.411164</td>\n",
       "      <td>0.411164</td>\n",
       "      <td>0.641220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.382768</td>\n",
       "      <td>0.382768</td>\n",
       "      <td>0.618683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.321591</td>\n",
       "      <td>0.321591</td>\n",
       "      <td>0.567089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.394063</td>\n",
       "      <td>0.394063</td>\n",
       "      <td>0.627744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.652873</td>\n",
       "      <td>0.652873</td>\n",
       "      <td>0.808005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.332933</td>\n",
       "      <td>0.332933</td>\n",
       "      <td>0.577003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.542346</td>\n",
       "      <td>0.542346</td>\n",
       "      <td>0.736441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.272221</td>\n",
       "      <td>0.272221</td>\n",
       "      <td>0.521748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.403778</td>\n",
       "      <td>0.403778</td>\n",
       "      <td>0.635435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.400370</td>\n",
       "      <td>0.400370</td>\n",
       "      <td>0.632748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.350742</td>\n",
       "      <td>0.350742</td>\n",
       "      <td>0.592235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.263032</td>\n",
       "      <td>0.263032</td>\n",
       "      <td>0.512866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>0.290739</td>\n",
       "      <td>0.290739</td>\n",
       "      <td>0.539202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.326632</td>\n",
       "      <td>0.326632</td>\n",
       "      <td>0.571517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.286775</td>\n",
       "      <td>0.286775</td>\n",
       "      <td>0.535514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.379322</td>\n",
       "      <td>0.379322</td>\n",
       "      <td>0.615891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.314625</td>\n",
       "      <td>0.314625</td>\n",
       "      <td>0.560915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.278172</td>\n",
       "      <td>0.278172</td>\n",
       "      <td>0.527420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.317273</td>\n",
       "      <td>0.317273</td>\n",
       "      <td>0.563270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.302265</td>\n",
       "      <td>0.302265</td>\n",
       "      <td>0.549786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.589782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.370852</td>\n",
       "      <td>0.370852</td>\n",
       "      <td>0.608976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.335239</td>\n",
       "      <td>0.335239</td>\n",
       "      <td>0.578998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.563462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-1/checkpoint-1526\n",
      "result {'eval_loss': 0.26303189992904663, 'eval_mse': 0.26303187012672424, 'eval_rmse': 0.5128663182258606, 'eval_runtime': 3.9773, 'eval_samples_per_second': 59.588, 'epoch': 26.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 2: [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2w49gdun) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19559<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_150917-2w49gdun/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_150917-2w49gdun/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.037</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>26.0</td></tr><tr><td>train/global_step</td><td>2834</td></tr><tr><td>_runtime</td><td>1945</td></tr><tr><td>_timestamp</td><td>1623426107</td></tr><tr><td>_step</td><td>54</td></tr><tr><td>eval/loss</td><td>0.26303</td></tr><tr><td>eval/mse</td><td>0.26303</td></tr><tr><td>eval/rmse</td><td>0.51287</td></tr><tr><td>eval/runtime</td><td>3.9773</td></tr><tr><td>eval/samples_per_second</td><td>59.588</td></tr><tr><td>train/train_runtime</td><td>1932.63</td></tr><tr><td>train/train_samples_per_second</td><td>1.692</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▄▅▄▃▂▃█▂▆▁▄▃▃▁▁▂▁▃▂▁▂▂▃▃▂▂▁</td></tr><tr><td>eval/mse</td><td>▄▅▄▃▂▃█▂▆▁▄▃▃▁▁▂▁▃▂▁▂▂▃▃▂▂▁</td></tr><tr><td>eval/rmse</td><td>▄▆▄▄▂▄█▃▆▁▄▄▃▁▂▂▂▃▂▁▂▂▃▃▃▂▁</td></tr><tr><td>eval/runtime</td><td>▂▃▆▅▄▃▃▅▄▃▅▄▅█▂▂▃▃▄▄▄▃▃▃▃▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▃▄▅▆▆▄▅▆▄▅▄▁▇▇▇▆▅▅▅▆▆▆▆▇█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">breezy-armadillo-3</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/2w49gdun\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/2w49gdun</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2w49gdun). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rosy-pyramid-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/kv4hu54o\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/kv4hu54o</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_154150-kv4hu54o</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 37:20, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.149400</td>\n",
       "      <td>0.875103</td>\n",
       "      <td>0.875103</td>\n",
       "      <td>0.935469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.731600</td>\n",
       "      <td>0.514792</td>\n",
       "      <td>0.514792</td>\n",
       "      <td>0.717490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>0.479820</td>\n",
       "      <td>0.479820</td>\n",
       "      <td>0.692690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.420862</td>\n",
       "      <td>0.420862</td>\n",
       "      <td>0.648739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.359119</td>\n",
       "      <td>0.359119</td>\n",
       "      <td>0.599266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.306046</td>\n",
       "      <td>0.306046</td>\n",
       "      <td>0.553214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.315924</td>\n",
       "      <td>0.315924</td>\n",
       "      <td>0.562072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.490150</td>\n",
       "      <td>0.490150</td>\n",
       "      <td>0.700107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.311463</td>\n",
       "      <td>0.311463</td>\n",
       "      <td>0.558089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.365933</td>\n",
       "      <td>0.365933</td>\n",
       "      <td>0.604924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.411556</td>\n",
       "      <td>0.411556</td>\n",
       "      <td>0.641527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.302139</td>\n",
       "      <td>0.302139</td>\n",
       "      <td>0.549672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>0.322374</td>\n",
       "      <td>0.322374</td>\n",
       "      <td>0.567780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.385667</td>\n",
       "      <td>0.385667</td>\n",
       "      <td>0.621021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.379393</td>\n",
       "      <td>0.379393</td>\n",
       "      <td>0.615949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.296255</td>\n",
       "      <td>0.296255</td>\n",
       "      <td>0.544294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.317425</td>\n",
       "      <td>0.317425</td>\n",
       "      <td>0.563405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.368183</td>\n",
       "      <td>0.368183</td>\n",
       "      <td>0.606781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.310076</td>\n",
       "      <td>0.310076</td>\n",
       "      <td>0.556845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.370579</td>\n",
       "      <td>0.370579</td>\n",
       "      <td>0.608752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.297675</td>\n",
       "      <td>0.297675</td>\n",
       "      <td>0.545596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.371876</td>\n",
       "      <td>0.371876</td>\n",
       "      <td>0.609816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.308116</td>\n",
       "      <td>0.555082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.283430</td>\n",
       "      <td>0.283430</td>\n",
       "      <td>0.532381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.293589</td>\n",
       "      <td>0.293588</td>\n",
       "      <td>0.541838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.296244</td>\n",
       "      <td>0.296244</td>\n",
       "      <td>0.544283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.294480</td>\n",
       "      <td>0.294480</td>\n",
       "      <td>0.542660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.297664</td>\n",
       "      <td>0.297664</td>\n",
       "      <td>0.545585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.300119</td>\n",
       "      <td>0.300119</td>\n",
       "      <td>0.547831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.307948</td>\n",
       "      <td>0.307948</td>\n",
       "      <td>0.554930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-2/checkpoint-2616\n",
      "result {'eval_loss': 0.28342971205711365, 'eval_mse': 0.28342971205711365, 'eval_rmse': 0.5323811769485474, 'eval_runtime': 3.9649, 'eval_samples_per_second': 59.775, 'epoch': 30.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 3: [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kv4hu54o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19607<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_154150-kv4hu54o/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_154150-kv4hu54o/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0308</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>30.0</td></tr><tr><td>train/global_step</td><td>3270</td></tr><tr><td>_runtime</td><td>2252</td></tr><tr><td>_timestamp</td><td>1623428368</td></tr><tr><td>_step</td><td>62</td></tr><tr><td>eval/loss</td><td>0.28343</td></tr><tr><td>eval/mse</td><td>0.28343</td></tr><tr><td>eval/rmse</td><td>0.53238</td></tr><tr><td>eval/runtime</td><td>3.9649</td></tr><tr><td>eval/samples_per_second</td><td>59.775</td></tr><tr><td>train/train_runtime</td><td>2241.0074</td></tr><tr><td>train/train_samples_per_second</td><td>1.459</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>eval/loss</td><td>█▄▃▃▂▁▁▃▁▂▃▁▁▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mse</td><td>█▄▃▃▂▁▁▃▁▂▃▁▁▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/rmse</td><td>█▄▄▃▂▁▂▄▁▂▃▁▂▃▂▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▄▆▄▇▃▆█▆▆▅▃▅▇▅▃▃▅▇▆▆▄▅▅▆▆▆▃▅▇▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▅▃▅▂▆▃▁▃▃▄▆▄▂▄▆▆▄▂▃▃▅▄▄▃▃▃▆▄▂█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">rosy-pyramid-4</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/kv4hu54o\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/kv4hu54o</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:kv4hu54o). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fiery-cherry-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/22uux7t5\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/22uux7t5</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_161931-22uux7t5</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1526' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1526/3270 17:15 < 19:45, 1.47 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.138100</td>\n",
       "      <td>0.804104</td>\n",
       "      <td>0.804104</td>\n",
       "      <td>0.896719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.330040</td>\n",
       "      <td>0.330040</td>\n",
       "      <td>0.574491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>1.105492</td>\n",
       "      <td>1.105492</td>\n",
       "      <td>1.051424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.222600</td>\n",
       "      <td>0.820591</td>\n",
       "      <td>0.820591</td>\n",
       "      <td>0.905865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.258500</td>\n",
       "      <td>1.111963</td>\n",
       "      <td>1.111963</td>\n",
       "      <td>1.054497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.251500</td>\n",
       "      <td>1.094085</td>\n",
       "      <td>1.094085</td>\n",
       "      <td>1.045985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.193400</td>\n",
       "      <td>1.089382</td>\n",
       "      <td>1.089382</td>\n",
       "      <td>1.043735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.221100</td>\n",
       "      <td>1.093107</td>\n",
       "      <td>1.093108</td>\n",
       "      <td>1.045518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.189200</td>\n",
       "      <td>1.091172</td>\n",
       "      <td>1.091172</td>\n",
       "      <td>1.044592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.202200</td>\n",
       "      <td>1.088294</td>\n",
       "      <td>1.088294</td>\n",
       "      <td>1.043213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.213800</td>\n",
       "      <td>1.089829</td>\n",
       "      <td>1.089829</td>\n",
       "      <td>1.043949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.203300</td>\n",
       "      <td>1.125106</td>\n",
       "      <td>1.125106</td>\n",
       "      <td>1.060710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.206700</td>\n",
       "      <td>1.090064</td>\n",
       "      <td>1.090064</td>\n",
       "      <td>1.044061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.155700</td>\n",
       "      <td>1.088630</td>\n",
       "      <td>1.088630</td>\n",
       "      <td>1.043374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-3/checkpoint-218\n",
      "result {'eval_loss': 0.3300398886203766, 'eval_mse': 0.3300398290157318, 'eval_rmse': 0.5744909048080444, 'eval_runtime': 3.9861, 'eval_samples_per_second': 59.457, 'epoch': 14.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 4: [9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:22uux7t5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19655<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_161931-22uux7t5/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_161931-22uux7t5/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>1.1557</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>14.0</td></tr><tr><td>train/global_step</td><td>1526</td></tr><tr><td>_runtime</td><td>1048</td></tr><tr><td>_timestamp</td><td>1623429429</td></tr><tr><td>_step</td><td>30</td></tr><tr><td>eval/loss</td><td>0.33004</td></tr><tr><td>eval/mse</td><td>0.33004</td></tr><tr><td>eval/rmse</td><td>0.57449</td></tr><tr><td>eval/runtime</td><td>3.9861</td></tr><tr><td>eval/samples_per_second</td><td>59.457</td></tr><tr><td>train/train_runtime</td><td>1036.3433</td></tr><tr><td>train/train_samples_per_second</td><td>3.155</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▁▁▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▅▁█▅██████████▁</td></tr><tr><td>eval/mse</td><td>▅▁█▅██████████▁</td></tr><tr><td>eval/rmse</td><td>▆▁█▆██████████▁</td></tr><tr><td>eval/runtime</td><td>▆█▂▇▁▁▂▄▄▁▃▁▁▃▆</td></tr><tr><td>eval/samples_per_second</td><td>▃▁▇▂██▇▅▅█▆▇█▆▃</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fiery-cherry-5</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/22uux7t5\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/22uux7t5</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:22uux7t5). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">playful-puddle-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/1f1u5za7\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/1f1u5za7</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_163713-1f1u5za7</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2725' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2725/3270 30:59 < 06:12, 1.46 it/s, Epoch 25/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.278500</td>\n",
       "      <td>0.811630</td>\n",
       "      <td>0.811630</td>\n",
       "      <td>0.900905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621700</td>\n",
       "      <td>0.450532</td>\n",
       "      <td>0.450532</td>\n",
       "      <td>0.671217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.514300</td>\n",
       "      <td>0.398049</td>\n",
       "      <td>0.398049</td>\n",
       "      <td>0.630912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.864336</td>\n",
       "      <td>0.864336</td>\n",
       "      <td>0.929697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.385713</td>\n",
       "      <td>0.385712</td>\n",
       "      <td>0.621058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.290936</td>\n",
       "      <td>0.290936</td>\n",
       "      <td>0.539385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.324191</td>\n",
       "      <td>0.324191</td>\n",
       "      <td>0.569377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.288911</td>\n",
       "      <td>0.288911</td>\n",
       "      <td>0.537505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.367375</td>\n",
       "      <td>0.367375</td>\n",
       "      <td>0.606115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.336889</td>\n",
       "      <td>0.336889</td>\n",
       "      <td>0.580422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.281925</td>\n",
       "      <td>0.281925</td>\n",
       "      <td>0.530966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.326175</td>\n",
       "      <td>0.326175</td>\n",
       "      <td>0.571117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.275432</td>\n",
       "      <td>0.275432</td>\n",
       "      <td>0.524816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.291314</td>\n",
       "      <td>0.291314</td>\n",
       "      <td>0.539735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.090200</td>\n",
       "      <td>0.328613</td>\n",
       "      <td>0.328613</td>\n",
       "      <td>0.573248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.346854</td>\n",
       "      <td>0.346854</td>\n",
       "      <td>0.588943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.333126</td>\n",
       "      <td>0.333126</td>\n",
       "      <td>0.577171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.326921</td>\n",
       "      <td>0.571770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>0.653593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.373850</td>\n",
       "      <td>0.373850</td>\n",
       "      <td>0.611432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.433045</td>\n",
       "      <td>0.433045</td>\n",
       "      <td>0.658062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.352288</td>\n",
       "      <td>0.352288</td>\n",
       "      <td>0.593539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.328932</td>\n",
       "      <td>0.328932</td>\n",
       "      <td>0.573526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.288995</td>\n",
       "      <td>0.288995</td>\n",
       "      <td>0.537582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.378695</td>\n",
       "      <td>0.378695</td>\n",
       "      <td>0.615382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-4/checkpoint-1417\n",
      "result {'eval_loss': 0.2754320800304413, 'eval_mse': 0.27543213963508606, 'eval_rmse': 0.5248162746429443, 'eval_runtime': 3.9847, 'eval_samples_per_second': 59.477, 'epoch': 25.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 5: [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1f1u5za7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19703<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_163713-1f1u5za7/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_163713-1f1u5za7/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0407</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>25.0</td></tr><tr><td>train/global_step</td><td>2725</td></tr><tr><td>_runtime</td><td>1872</td></tr><tr><td>_timestamp</td><td>1623431310</td></tr><tr><td>_step</td><td>52</td></tr><tr><td>eval/loss</td><td>0.27543</td></tr><tr><td>eval/mse</td><td>0.27543</td></tr><tr><td>eval/rmse</td><td>0.52482</td></tr><tr><td>eval/runtime</td><td>3.9847</td></tr><tr><td>eval/samples_per_second</td><td>59.477</td></tr><tr><td>train/train_runtime</td><td>1860.5467</td></tr><tr><td>train/train_samples_per_second</td><td>1.758</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▇▃▂█▂▁▂▁▂▂▁▂▁▁▂▂▂▂▃▂▃▂▂▁▂▁</td></tr><tr><td>eval/mse</td><td>▇▃▂█▂▁▂▁▂▂▁▂▁▁▂▂▂▂▃▂▃▂▂▁▂▁</td></tr><tr><td>eval/rmse</td><td>█▄▃█▃▁▂▁▂▂▁▂▁▁▂▂▂▂▃▂▃▂▂▁▃▁</td></tr><tr><td>eval/runtime</td><td>▃▅▅▃▆▃▂▆▄▅█▇▆▅▆▅▃▅▆▂▃▃▆▄▄▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▄▄▆▃▆▇▃▅▄▁▂▃▄▃▄▆▄▃▇▆▆▃▅▅█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">playful-puddle-6</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/1f1u5za7\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/1f1u5za7</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1f1u5za7). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">amber-water-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/18z03l7m\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/18z03l7m</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_170833-18z03l7m</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2943' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2943/3270 33:35 < 03:44, 1.46 it/s, Epoch 27/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.174500</td>\n",
       "      <td>0.424298</td>\n",
       "      <td>0.424298</td>\n",
       "      <td>0.651382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.276404</td>\n",
       "      <td>0.276404</td>\n",
       "      <td>0.525741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.294172</td>\n",
       "      <td>0.294171</td>\n",
       "      <td>0.542376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.471532</td>\n",
       "      <td>0.471532</td>\n",
       "      <td>0.686682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.248487</td>\n",
       "      <td>0.248487</td>\n",
       "      <td>0.498485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.288200</td>\n",
       "      <td>0.279041</td>\n",
       "      <td>0.279041</td>\n",
       "      <td>0.528243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.238200</td>\n",
       "      <td>0.273806</td>\n",
       "      <td>0.273806</td>\n",
       "      <td>0.523264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.360617</td>\n",
       "      <td>0.360617</td>\n",
       "      <td>0.600514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>0.524562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.269903</td>\n",
       "      <td>0.269903</td>\n",
       "      <td>0.519522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.251021</td>\n",
       "      <td>0.251021</td>\n",
       "      <td>0.501020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.255304</td>\n",
       "      <td>0.255304</td>\n",
       "      <td>0.505276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.303033</td>\n",
       "      <td>0.303033</td>\n",
       "      <td>0.550484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.273915</td>\n",
       "      <td>0.273915</td>\n",
       "      <td>0.523369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.238302</td>\n",
       "      <td>0.238302</td>\n",
       "      <td>0.488162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.258003</td>\n",
       "      <td>0.258003</td>\n",
       "      <td>0.507940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.257018</td>\n",
       "      <td>0.257018</td>\n",
       "      <td>0.506969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.304270</td>\n",
       "      <td>0.304270</td>\n",
       "      <td>0.551607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.274782</td>\n",
       "      <td>0.274782</td>\n",
       "      <td>0.524196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.287209</td>\n",
       "      <td>0.287209</td>\n",
       "      <td>0.535918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.256887</td>\n",
       "      <td>0.256887</td>\n",
       "      <td>0.506840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.271018</td>\n",
       "      <td>0.271018</td>\n",
       "      <td>0.520594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.252753</td>\n",
       "      <td>0.252753</td>\n",
       "      <td>0.502746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.271253</td>\n",
       "      <td>0.271253</td>\n",
       "      <td>0.520820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.344765</td>\n",
       "      <td>0.344765</td>\n",
       "      <td>0.587167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.245399</td>\n",
       "      <td>0.245399</td>\n",
       "      <td>0.495378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.281815</td>\n",
       "      <td>0.281815</td>\n",
       "      <td>0.530862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-5/checkpoint-1635\n",
      "result {'eval_loss': 0.23830197751522064, 'eval_mse': 0.23830194771289825, 'eval_rmse': 0.4881618022918701, 'eval_runtime': 3.9814, 'eval_samples_per_second': 59.527, 'epoch': 27.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 6: [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:18z03l7m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19751<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_170833-18z03l7m/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_170833-18z03l7m/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.034</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>27.0</td></tr><tr><td>train/global_step</td><td>2943</td></tr><tr><td>_runtime</td><td>2028</td></tr><tr><td>_timestamp</td><td>1623433346</td></tr><tr><td>_step</td><td>56</td></tr><tr><td>eval/loss</td><td>0.2383</td></tr><tr><td>eval/mse</td><td>0.2383</td></tr><tr><td>eval/rmse</td><td>0.48816</td></tr><tr><td>eval/runtime</td><td>3.9814</td></tr><tr><td>eval/samples_per_second</td><td>59.527</td></tr><tr><td>train/train_runtime</td><td>2015.6467</td></tr><tr><td>train/train_samples_per_second</td><td>1.622</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▇▂▃█▁▂▂▅▂▂▁▂▃▂▁▂▂▃▂▂▂▂▁▂▄▁▂▁</td></tr><tr><td>eval/mse</td><td>▇▂▃█▁▂▂▅▂▂▁▂▃▂▁▂▂▃▂▂▂▂▁▂▄▁▂▁</td></tr><tr><td>eval/rmse</td><td>▇▂▃█▁▂▂▅▂▂▁▂▃▂▁▂▂▃▂▃▂▂▂▂▄▁▃▁</td></tr><tr><td>eval/runtime</td><td>▄▄▄▇▄▂▄▃▄▄▄▄▅▅▅▅▄▅█▅▅▄▅▃▆▇▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▅▂▅▇▅▆▅▅▅▅▄▄▄▄▅▄▁▄▄▅▄▆▃▂▄█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">amber-water-7</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/18z03l7m\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/18z03l7m</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:18z03l7m). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">whole-disco-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/3d8r5qna\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/3d8r5qna</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_174229-3d8r5qna</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2834' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2834/3270 32:28 < 04:59, 1.45 it/s, Epoch 26/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.077600</td>\n",
       "      <td>0.406639</td>\n",
       "      <td>0.406639</td>\n",
       "      <td>0.637683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.295385</td>\n",
       "      <td>0.295385</td>\n",
       "      <td>0.543493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.481229</td>\n",
       "      <td>0.481229</td>\n",
       "      <td>0.693707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.305114</td>\n",
       "      <td>0.305114</td>\n",
       "      <td>0.552371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.344551</td>\n",
       "      <td>0.344551</td>\n",
       "      <td>0.586985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.358180</td>\n",
       "      <td>0.358180</td>\n",
       "      <td>0.598481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.211836</td>\n",
       "      <td>0.211836</td>\n",
       "      <td>0.460257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>0.289179</td>\n",
       "      <td>0.289179</td>\n",
       "      <td>0.537754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>0.246717</td>\n",
       "      <td>0.246717</td>\n",
       "      <td>0.496706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.248419</td>\n",
       "      <td>0.248419</td>\n",
       "      <td>0.498417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.322361</td>\n",
       "      <td>0.322361</td>\n",
       "      <td>0.567768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.488451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.252130</td>\n",
       "      <td>0.252130</td>\n",
       "      <td>0.502126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.207883</td>\n",
       "      <td>0.207883</td>\n",
       "      <td>0.455942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.236954</td>\n",
       "      <td>0.236954</td>\n",
       "      <td>0.486780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.261809</td>\n",
       "      <td>0.261809</td>\n",
       "      <td>0.511672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.223418</td>\n",
       "      <td>0.223418</td>\n",
       "      <td>0.472671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.286102</td>\n",
       "      <td>0.286102</td>\n",
       "      <td>0.534885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>0.477173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.228256</td>\n",
       "      <td>0.228256</td>\n",
       "      <td>0.477761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.215096</td>\n",
       "      <td>0.215096</td>\n",
       "      <td>0.463784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.216465</td>\n",
       "      <td>0.216465</td>\n",
       "      <td>0.465258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.261044</td>\n",
       "      <td>0.261044</td>\n",
       "      <td>0.510925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.220861</td>\n",
       "      <td>0.220861</td>\n",
       "      <td>0.469959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.227756</td>\n",
       "      <td>0.227756</td>\n",
       "      <td>0.477237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.245074</td>\n",
       "      <td>0.245074</td>\n",
       "      <td>0.495049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-6/checkpoint-1526\n",
      "result {'eval_loss': 0.20788320899009705, 'eval_mse': 0.20788320899009705, 'eval_rmse': 0.4559421241283417, 'eval_runtime': 3.96, 'eval_samples_per_second': 59.849, 'epoch': 26.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 7: [8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3d8r5qna) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19799<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_174229-3d8r5qna/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_174229-3d8r5qna/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0384</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>26.0</td></tr><tr><td>train/global_step</td><td>2834</td></tr><tr><td>_runtime</td><td>1960</td></tr><tr><td>_timestamp</td><td>1623435316</td></tr><tr><td>_step</td><td>54</td></tr><tr><td>eval/loss</td><td>0.20788</td></tr><tr><td>eval/mse</td><td>0.20788</td></tr><tr><td>eval/rmse</td><td>0.45594</td></tr><tr><td>eval/runtime</td><td>3.96</td></tr><tr><td>eval/samples_per_second</td><td>59.849</td></tr><tr><td>train/train_runtime</td><td>1948.5751</td></tr><tr><td>train/train_samples_per_second</td><td>1.678</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▆▃█▃▄▅▁▃▂▂▄▂▂▁▂▂▁▃▂▂▁▁▂▁▂▂▁</td></tr><tr><td>eval/mse</td><td>▆▃█▃▄▅▁▃▂▂▄▂▂▁▂▂▁▃▂▂▁▁▂▁▂▂▁</td></tr><tr><td>eval/rmse</td><td>▆▄█▄▅▅▁▃▂▂▄▂▂▁▂▃▁▃▂▂▁▁▃▁▂▂▁</td></tr><tr><td>eval/runtime</td><td>▆▆▇▆▆▇▇▅▄▅▇█▅▇▆▆▅▆▅▅▆▅▆█▆▄▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▂▃▃▂▂▄▅▄▂▁▄▂▃▃▄▃▄▄▃▄▃▁▃▅█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">whole-disco-8</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/3d8r5qna\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/3d8r5qna</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3d8r5qna). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pretty-bird-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/11j97iqr\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/11j97iqr</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_181519-11j97iqr</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2943' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2943/3270 33:39 < 03:44, 1.46 it/s, Epoch 27/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.177200</td>\n",
       "      <td>0.286669</td>\n",
       "      <td>0.286669</td>\n",
       "      <td>0.535415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>1.290047</td>\n",
       "      <td>1.290047</td>\n",
       "      <td>1.135802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.305265</td>\n",
       "      <td>0.305265</td>\n",
       "      <td>0.552508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.318180</td>\n",
       "      <td>0.318180</td>\n",
       "      <td>0.564074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.486248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.343399</td>\n",
       "      <td>0.343399</td>\n",
       "      <td>0.586003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.288290</td>\n",
       "      <td>0.288290</td>\n",
       "      <td>0.536926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.244991</td>\n",
       "      <td>0.244991</td>\n",
       "      <td>0.494966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.182200</td>\n",
       "      <td>0.404476</td>\n",
       "      <td>0.404476</td>\n",
       "      <td>0.635984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.467759</td>\n",
       "      <td>0.467759</td>\n",
       "      <td>0.683929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.266705</td>\n",
       "      <td>0.266705</td>\n",
       "      <td>0.516435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.269651</td>\n",
       "      <td>0.269651</td>\n",
       "      <td>0.519279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.234753</td>\n",
       "      <td>0.234753</td>\n",
       "      <td>0.484513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.232778</td>\n",
       "      <td>0.232778</td>\n",
       "      <td>0.482471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.217585</td>\n",
       "      <td>0.217585</td>\n",
       "      <td>0.466460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.594895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.285796</td>\n",
       "      <td>0.285796</td>\n",
       "      <td>0.534599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.255385</td>\n",
       "      <td>0.255385</td>\n",
       "      <td>0.505356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.295164</td>\n",
       "      <td>0.295164</td>\n",
       "      <td>0.543290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.256275</td>\n",
       "      <td>0.256275</td>\n",
       "      <td>0.506236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.315184</td>\n",
       "      <td>0.315184</td>\n",
       "      <td>0.561413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.302530</td>\n",
       "      <td>0.302530</td>\n",
       "      <td>0.550028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.238803</td>\n",
       "      <td>0.238803</td>\n",
       "      <td>0.488674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.258459</td>\n",
       "      <td>0.258459</td>\n",
       "      <td>0.508388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.252160</td>\n",
       "      <td>0.252160</td>\n",
       "      <td>0.502155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.252106</td>\n",
       "      <td>0.252106</td>\n",
       "      <td>0.502102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.244887</td>\n",
       "      <td>0.244887</td>\n",
       "      <td>0.494861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-7/checkpoint-1635\n",
      "result {'eval_loss': 0.21758492290973663, 'eval_mse': 0.21758492290973663, 'eval_rmse': 0.4664599895477295, 'eval_runtime': 3.9787, 'eval_samples_per_second': 59.567, 'epoch': 27.0, 'eval_mem_cpu_alloc_delta': 4096, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 8: [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:11j97iqr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19847<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_181519-11j97iqr/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_181519-11j97iqr/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0376</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>27.0</td></tr><tr><td>train/global_step</td><td>2943</td></tr><tr><td>_runtime</td><td>2032</td></tr><tr><td>_timestamp</td><td>1623437357</td></tr><tr><td>_step</td><td>56</td></tr><tr><td>eval/loss</td><td>0.21758</td></tr><tr><td>eval/mse</td><td>0.21758</td></tr><tr><td>eval/rmse</td><td>0.46646</td></tr><tr><td>eval/runtime</td><td>3.9787</td></tr><tr><td>eval/samples_per_second</td><td>59.567</td></tr><tr><td>train/train_runtime</td><td>2019.7939</td></tr><tr><td>train/train_samples_per_second</td><td>1.619</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▁█▂▂▁▂▁▁▂▃▁▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/mse</td><td>▁█▂▂▁▂▁▁▂▃▁▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/rmse</td><td>▂█▂▂▁▂▂▁▃▃▂▂▁▁▁▂▂▁▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▂▅▁▇▃▅▂▄▂▃▃▃▄▃▂█▄▂▂▅▅▅▂▅▅▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▇▄█▂▆▄▇▅▇▆▆▆▅▆▇▁▅▇▇▄▄▄▇▄▄▄█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">pretty-bird-9</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/11j97iqr\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/11j97iqr</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:11j97iqr). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">desert-meadow-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/j91racz9\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/j91racz9</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_184920-j91racz9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2834' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2834/3270 32:29 < 05:00, 1.45 it/s, Epoch 26/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.027200</td>\n",
       "      <td>1.072087</td>\n",
       "      <td>1.072087</td>\n",
       "      <td>1.035416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.770700</td>\n",
       "      <td>0.745028</td>\n",
       "      <td>0.745028</td>\n",
       "      <td>0.863150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>0.345244</td>\n",
       "      <td>0.345244</td>\n",
       "      <td>0.587575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.441600</td>\n",
       "      <td>0.855042</td>\n",
       "      <td>0.855042</td>\n",
       "      <td>0.924685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.380820</td>\n",
       "      <td>0.380820</td>\n",
       "      <td>0.617106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.339948</td>\n",
       "      <td>0.339948</td>\n",
       "      <td>0.583050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.401981</td>\n",
       "      <td>0.401981</td>\n",
       "      <td>0.634020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.712276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.336692</td>\n",
       "      <td>0.336692</td>\n",
       "      <td>0.580251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.478399</td>\n",
       "      <td>0.478399</td>\n",
       "      <td>0.691664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.302860</td>\n",
       "      <td>0.302860</td>\n",
       "      <td>0.550327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.346520</td>\n",
       "      <td>0.346520</td>\n",
       "      <td>0.588659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.332738</td>\n",
       "      <td>0.332738</td>\n",
       "      <td>0.576834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.268450</td>\n",
       "      <td>0.268450</td>\n",
       "      <td>0.518121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.337258</td>\n",
       "      <td>0.337258</td>\n",
       "      <td>0.580739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.308477</td>\n",
       "      <td>0.308477</td>\n",
       "      <td>0.555407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.400333</td>\n",
       "      <td>0.400333</td>\n",
       "      <td>0.632719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.278777</td>\n",
       "      <td>0.278777</td>\n",
       "      <td>0.527994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.369417</td>\n",
       "      <td>0.369417</td>\n",
       "      <td>0.607797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.350412</td>\n",
       "      <td>0.350412</td>\n",
       "      <td>0.591956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.302776</td>\n",
       "      <td>0.302776</td>\n",
       "      <td>0.550251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.337587</td>\n",
       "      <td>0.337587</td>\n",
       "      <td>0.581022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.303943</td>\n",
       "      <td>0.303943</td>\n",
       "      <td>0.551310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.330510</td>\n",
       "      <td>0.330510</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.521389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.341470</td>\n",
       "      <td>0.341470</td>\n",
       "      <td>0.584354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-8/checkpoint-1526\n",
      "result {'eval_loss': 0.2684497535228729, 'eval_mse': 0.26844969391822815, 'eval_rmse': 0.5181213021278381, 'eval_runtime': 3.9721, 'eval_samples_per_second': 59.667, 'epoch': 26.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 9: [6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j91racz9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19895<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_184920-j91racz9/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_184920-j91racz9/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0436</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>26.0</td></tr><tr><td>train/global_step</td><td>2834</td></tr><tr><td>_runtime</td><td>1962</td></tr><tr><td>_timestamp</td><td>1623439327</td></tr><tr><td>_step</td><td>54</td></tr><tr><td>eval/loss</td><td>0.26845</td></tr><tr><td>eval/mse</td><td>0.26845</td></tr><tr><td>eval/rmse</td><td>0.51812</td></tr><tr><td>eval/runtime</td><td>3.9721</td></tr><tr><td>eval/samples_per_second</td><td>59.667</td></tr><tr><td>train/train_runtime</td><td>1949.5822</td></tr><tr><td>train/train_samples_per_second</td><td>1.677</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▅▂▆▂▂▂▃▂▃▁▂▂▁▂▁▂▁▂▂▁▂▁▂▁▂▁</td></tr><tr><td>eval/mse</td><td>█▅▂▆▂▂▂▃▂▃▁▂▂▁▂▁▂▁▂▂▁▂▁▂▁▂▁</td></tr><tr><td>eval/rmse</td><td>█▆▂▇▂▂▃▄▂▃▁▂▂▁▂▂▃▁▂▂▁▂▁▂▁▂▁</td></tr><tr><td>eval/runtime</td><td>▃▅▃▅▆▂▂▅▂▅▃█▂▅▅█▃▅▄▄▄▃▃▅▅▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▄▆▄▃▇▇▄▇▄▆▁▇▄▄▁▆▄▅▅▅▆▆▄▄▅█</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">desert-meadow-10</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/j91racz9\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/j91racz9</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:j91racz9). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">distinctive-leaf-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/78cr9zpf\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/78cr9zpf</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_192210-78cr9zpf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 37:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.177100</td>\n",
       "      <td>0.441082</td>\n",
       "      <td>0.441082</td>\n",
       "      <td>0.664140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.401949</td>\n",
       "      <td>0.401949</td>\n",
       "      <td>0.633994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.444631</td>\n",
       "      <td>0.444631</td>\n",
       "      <td>0.666807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.373406</td>\n",
       "      <td>0.373406</td>\n",
       "      <td>0.611070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.360751</td>\n",
       "      <td>0.360751</td>\n",
       "      <td>0.600625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.352258</td>\n",
       "      <td>0.352258</td>\n",
       "      <td>0.593513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.369455</td>\n",
       "      <td>0.369455</td>\n",
       "      <td>0.607828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.388784</td>\n",
       "      <td>0.388784</td>\n",
       "      <td>0.623525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.298156</td>\n",
       "      <td>0.298156</td>\n",
       "      <td>0.546036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.297745</td>\n",
       "      <td>0.297745</td>\n",
       "      <td>0.545660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.302640</td>\n",
       "      <td>0.302640</td>\n",
       "      <td>0.550127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.364453</td>\n",
       "      <td>0.364453</td>\n",
       "      <td>0.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.339757</td>\n",
       "      <td>0.339757</td>\n",
       "      <td>0.582887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.336868</td>\n",
       "      <td>0.336868</td>\n",
       "      <td>0.580404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.303616</td>\n",
       "      <td>0.303616</td>\n",
       "      <td>0.551013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.448937</td>\n",
       "      <td>0.448937</td>\n",
       "      <td>0.670027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.300785</td>\n",
       "      <td>0.300785</td>\n",
       "      <td>0.548439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.309509</td>\n",
       "      <td>0.309509</td>\n",
       "      <td>0.556335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.291962</td>\n",
       "      <td>0.291962</td>\n",
       "      <td>0.540335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.295847</td>\n",
       "      <td>0.295847</td>\n",
       "      <td>0.543918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.340014</td>\n",
       "      <td>0.340014</td>\n",
       "      <td>0.583107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.328315</td>\n",
       "      <td>0.328315</td>\n",
       "      <td>0.572988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.335517</td>\n",
       "      <td>0.335517</td>\n",
       "      <td>0.579238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.299292</td>\n",
       "      <td>0.299292</td>\n",
       "      <td>0.547076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.312323</td>\n",
       "      <td>0.312323</td>\n",
       "      <td>0.558859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.314456</td>\n",
       "      <td>0.314456</td>\n",
       "      <td>0.560764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.305395</td>\n",
       "      <td>0.305395</td>\n",
       "      <td>0.552626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.297199</td>\n",
       "      <td>0.297199</td>\n",
       "      <td>0.545159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.303423</td>\n",
       "      <td>0.303423</td>\n",
       "      <td>0.550838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.309439</td>\n",
       "      <td>0.309439</td>\n",
       "      <td>0.556272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-9/checkpoint-2071\n",
      "result {'eval_loss': 0.29196158051490784, 'eval_mse': 0.29196158051490784, 'eval_rmse': 0.5403347015380859, 'eval_runtime': 3.9849, 'eval_samples_per_second': 59.474, 'epoch': 30.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "train_bins 10: [11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:78cr9zpf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19943<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_192210-78cr9zpf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210611_192210-78cr9zpf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0324</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/epoch</td><td>30.0</td></tr><tr><td>train/global_step</td><td>3270</td></tr><tr><td>_runtime</td><td>2255</td></tr><tr><td>_timestamp</td><td>1623441591</td></tr><tr><td>_step</td><td>62</td></tr><tr><td>eval/loss</td><td>0.29196</td></tr><tr><td>eval/mse</td><td>0.29196</td></tr><tr><td>eval/rmse</td><td>0.54033</td></tr><tr><td>eval/runtime</td><td>3.9849</td></tr><tr><td>eval/samples_per_second</td><td>59.474</td></tr><tr><td>train/train_runtime</td><td>2242.9197</td></tr><tr><td>train/train_samples_per_second</td><td>1.458</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>eval/loss</td><td>█▆█▅▄▄▄▅▁▁▁▄▃▃▂█▁▂▁▁▃▃▃▁▂▂▂▁▂▂▁</td></tr><tr><td>eval/mse</td><td>█▆█▅▄▄▄▅▁▁▁▄▃▃▂█▁▂▁▁▃▃▃▁▂▂▂▁▂▂▁</td></tr><tr><td>eval/rmse</td><td>█▆█▅▄▄▅▅▁▁▂▄▃▃▂█▁▂▁▁▃▃▃▁▂▂▂▁▂▂▁</td></tr><tr><td>eval/runtime</td><td>▄▄▅▂▄▅▂▆▄▅▆▁▅▄▃▃▅▁▂▄▅▅▄▄▇▅▅█▂▅▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▄▇▅▄▇▃▅▄▃█▄▅▆▆▄█▇▅▄▄▅▅▂▄▄▁▇▄▇</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">distinctive-leaf-11</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/78cr9zpf\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/78cr9zpf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:78cr9zpf). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">earthy-monkey-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/28uyxsko\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/28uyxsko</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210611_195954-28uyxsko</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='437' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 437/3270 04:45 < 31:02, 1.52 it/s, Epoch 4/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.172300</td>\n",
       "      <td>0.800548</td>\n",
       "      <td>0.800548</td>\n",
       "      <td>0.894734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.654700</td>\n",
       "      <td>0.537116</td>\n",
       "      <td>0.537116</td>\n",
       "      <td>0.732882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.548937</td>\n",
       "      <td>0.548937</td>\n",
       "      <td>0.740903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>1.025565</td>\n",
       "      <td>1.025565</td>\n",
       "      <td>1.012702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:274] . unexpected pos 808571264 vs 808571152",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1497\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_world_process_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimizer.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scheduler.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 808571264 vs 808571152"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "bin_step = 1\n",
    "bestmodels = []\n",
    "eval_rmses = []\n",
    "for i in range(0, num_bins, bin_step):\n",
    "    train_bins = bin_list[i:i+bin_step]\n",
    "    print('train_bins', f'{i}: {train_bins}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "    train_text, train_target, valid_text, valid_target = create_split([i])\n",
    "    train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)\n",
    "    training_args = create_training_args(i)\n",
    "    model = CommonLitModel()\n",
    "    wandb.init(project=f\"commonlit_{cfg.model_name.replace('/', '_')}\")\n",
    "    trainer = CommonLitTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=12)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    print('training_args.output_dir', training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    result = trainer.evaluate()\n",
    "    bestmodels.append(trainer.state.best_model_checkpoint)\n",
    "    print('best_model_checkpoint', trainer.state.best_model_checkpoint)\n",
    "    print('result', result)\n",
    "    eval_rmses.append(result['eval_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "799ba783-417b-494a-940e-381be192258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 11: [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgilf\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">azure-firefly-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/2djh7wtj\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/2djh7wtj</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210612_065432-2djh7wtj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2398' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2398/3270 27:16 < 09:55, 1.46 it/s, Epoch 22/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.250400</td>\n",
       "      <td>0.727666</td>\n",
       "      <td>0.727666</td>\n",
       "      <td>0.853033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>0.570778</td>\n",
       "      <td>0.570778</td>\n",
       "      <td>0.755499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.625306</td>\n",
       "      <td>0.625306</td>\n",
       "      <td>0.790763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>0.486689</td>\n",
       "      <td>0.486689</td>\n",
       "      <td>0.697631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>0.458710</td>\n",
       "      <td>0.458710</td>\n",
       "      <td>0.677281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.432978</td>\n",
       "      <td>0.432978</td>\n",
       "      <td>0.658010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>0.671406</td>\n",
       "      <td>0.671406</td>\n",
       "      <td>0.819394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.430395</td>\n",
       "      <td>0.430395</td>\n",
       "      <td>0.656045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.394284</td>\n",
       "      <td>0.394284</td>\n",
       "      <td>0.627920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.288200</td>\n",
       "      <td>0.378732</td>\n",
       "      <td>0.378732</td>\n",
       "      <td>0.615412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.401762</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>0.633847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>0.471503</td>\n",
       "      <td>0.471503</td>\n",
       "      <td>0.686661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.638839</td>\n",
       "      <td>0.638839</td>\n",
       "      <td>0.799274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.421574</td>\n",
       "      <td>0.421574</td>\n",
       "      <td>0.649287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.401359</td>\n",
       "      <td>0.401359</td>\n",
       "      <td>0.633529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.434419</td>\n",
       "      <td>0.434419</td>\n",
       "      <td>0.659105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.424339</td>\n",
       "      <td>0.424339</td>\n",
       "      <td>0.651413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.460944</td>\n",
       "      <td>0.460944</td>\n",
       "      <td>0.678929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.625285</td>\n",
       "      <td>0.625285</td>\n",
       "      <td>0.790749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.515706</td>\n",
       "      <td>0.515706</td>\n",
       "      <td>0.718126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.503229</td>\n",
       "      <td>0.503229</td>\n",
       "      <td>0.709386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>0.701082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-11/checkpoint-1090\n",
      "result {'eval_loss': 0.37873196601867676, 'eval_mse': 0.37873202562332153, 'eval_rmse': 0.6154120564460754, 'eval_runtime': 3.9735, 'eval_samples_per_second': 59.645, 'epoch': 22.0, 'eval_mem_cpu_alloc_delta': 4096, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "CPU times: user 37min 19s, sys: 11min 42s, total: 49min 1s\n",
      "Wall time: 27min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "bin_step = 1\n",
    "bestmodels = []\n",
    "eval_rmses = []\n",
    "for i in range(11, num_bins, bin_step):\n",
    "    train_bins = bin_list[i:i+bin_step]\n",
    "    print('train_bins', f'{i}: {train_bins}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "    train_text, train_target, valid_text, valid_target = create_split([i])\n",
    "    train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)\n",
    "    training_args = create_training_args(i)\n",
    "    model = CommonLitModel()\n",
    "    wandb.init(project=f\"commonlit_{cfg.model_name.replace('/', '_')}\")\n",
    "    trainer = CommonLitTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=12)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    print('training_args.output_dir', training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    result = trainer.evaluate()\n",
    "    bestmodels.append(trainer.state.best_model_checkpoint)\n",
    "    print('best_model_checkpoint', trainer.state.best_model_checkpoint)\n",
    "    print('result', result)\n",
    "    eval_rmses.append(result['eval_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "334e4cbe-20da-44b4-9bec-1552112a2c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 10: [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2qsudpp1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1751<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210612_081351-2qsudpp1/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210612_081351-2qsudpp1/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.1975</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>25.0</td></tr><tr><td>train/global_step</td><td>2725</td></tr><tr><td>_runtime</td><td>1881</td></tr><tr><td>_timestamp</td><td>1623487518</td></tr><tr><td>_step</td><td>52</td></tr><tr><td>eval/loss</td><td>0.50894</td></tr><tr><td>eval/mse</td><td>0.26567</td></tr><tr><td>eval/rmse</td><td>0.51543</td></tr><tr><td>eval/runtime</td><td>3.9948</td></tr><tr><td>eval/samples_per_second</td><td>59.327</td></tr><tr><td>train/train_runtime</td><td>1867.3824</td></tr><tr><td>train/train_samples_per_second</td><td>1.751</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▅▁█▁▂▄▂▇▂▂▄▅▁▁▁▃▄▂▁▂▂▂▃▃▂▁</td></tr><tr><td>eval/mse</td><td>▄▁█▁▂▄▂▆▂▂▃▄▁▁▁▃▃▁▁▂▂▁▂▂▁▁</td></tr><tr><td>eval/rmse</td><td>▅▁█▁▂▄▂▇▂▂▄▅▁▁▁▃▄▂▁▂▂▂▃▂▂▁</td></tr><tr><td>eval/runtime</td><td>▁▆▇▅▆▅▅▃▆▆▅▆▅▅▆█▅▄▆▆▆▅▄▄▇▄</td></tr><tr><td>eval/samples_per_second</td><td>█▃▂▄▃▄▄▆▃▃▄▃▄▄▃▁▄▅▃▃▃▄▅▅▂▅</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fresh-butterfly-15</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/2qsudpp1\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/2qsudpp1</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2qsudpp1). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">generous-flower-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/3mx2dkci\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/3mx2dkci</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210612_092834-3mx2dkci</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2725' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2725/3270 31:19 < 06:16, 1.45 it/s, Epoch 25/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>1.211037</td>\n",
       "      <td>1.497737</td>\n",
       "      <td>1.223820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.813600</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.506457</td>\n",
       "      <td>0.711658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.705800</td>\n",
       "      <td>0.728766</td>\n",
       "      <td>0.552972</td>\n",
       "      <td>0.743621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.732555</td>\n",
       "      <td>0.545256</td>\n",
       "      <td>0.738415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.697669</td>\n",
       "      <td>0.498723</td>\n",
       "      <td>0.706203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.681323</td>\n",
       "      <td>0.470598</td>\n",
       "      <td>0.686001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.590605</td>\n",
       "      <td>0.352943</td>\n",
       "      <td>0.594090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.546102</td>\n",
       "      <td>0.304667</td>\n",
       "      <td>0.551966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.530876</td>\n",
       "      <td>0.283477</td>\n",
       "      <td>0.532426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.391700</td>\n",
       "      <td>0.628439</td>\n",
       "      <td>0.403725</td>\n",
       "      <td>0.635394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.546910</td>\n",
       "      <td>0.302489</td>\n",
       "      <td>0.549990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.635933</td>\n",
       "      <td>0.409688</td>\n",
       "      <td>0.640069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.525841</td>\n",
       "      <td>0.279194</td>\n",
       "      <td>0.528388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>0.561189</td>\n",
       "      <td>0.318886</td>\n",
       "      <td>0.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.369041</td>\n",
       "      <td>0.607487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.604376</td>\n",
       "      <td>0.371482</td>\n",
       "      <td>0.609493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.649223</td>\n",
       "      <td>0.425615</td>\n",
       "      <td>0.652392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.569447</td>\n",
       "      <td>0.328071</td>\n",
       "      <td>0.572775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.595175</td>\n",
       "      <td>0.358458</td>\n",
       "      <td>0.598713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.556483</td>\n",
       "      <td>0.312452</td>\n",
       "      <td>0.558974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.569218</td>\n",
       "      <td>0.329289</td>\n",
       "      <td>0.573837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.582714</td>\n",
       "      <td>0.345583</td>\n",
       "      <td>0.587863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>0.568392</td>\n",
       "      <td>0.327476</td>\n",
       "      <td>0.572255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.547380</td>\n",
       "      <td>0.303669</td>\n",
       "      <td>0.551061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.205300</td>\n",
       "      <td>0.587910</td>\n",
       "      <td>0.349495</td>\n",
       "      <td>0.591181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-10/checkpoint-1417\n",
      "result {'eval_loss': 0.5258409976959229, 'eval_mse': 0.2791936993598938, 'eval_rmse': 0.5283878445625305, 'eval_runtime': 3.9745, 'eval_samples_per_second': 59.63, 'epoch': 25.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "CPU times: user 42min 43s, sys: 13min 28s, total: 56min 12s\n",
      "Wall time: 31min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "bin_step = 1\n",
    "bestmodels = []\n",
    "eval_rmses = []\n",
    "for i in range(10, 11, bin_step):\n",
    "    train_bins = bin_list[i:i+bin_step]\n",
    "    print('train_bins', f'{i}: {train_bins}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "    train_text, train_target, valid_text, valid_target = create_split([i])\n",
    "    train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)\n",
    "    training_args = create_training_args(i)\n",
    "    model = CommonLitModel()\n",
    "    wandb.init(project=f\"commonlit_{cfg.model_name.replace('/', '_')}\")\n",
    "    trainer = CommonLitTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=12)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    print('training_args.output_dir', training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    result = trainer.evaluate()\n",
    "    bestmodels.append(trainer.state.best_model_checkpoint)\n",
    "    print('best_model_checkpoint', trainer.state.best_model_checkpoint)\n",
    "    print('result', result)\n",
    "    eval_rmses.append(result['eval_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df438977-1cc9-4ee6-b99a-b6f24fc72404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 3: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-large_lm/best_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:enmb2te8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 507<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210612_072419-enmb2te8/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210612_072419-enmb2te8/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.2134</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>24.0</td></tr><tr><td>train/global_step</td><td>2616</td></tr><tr><td>_runtime</td><td>1801</td></tr><tr><td>_timestamp</td><td>1623484466</td></tr><tr><td>_step</td><td>50</td></tr><tr><td>eval/loss</td><td>0.57275</td></tr><tr><td>eval/mse</td><td>0.33553</td></tr><tr><td>eval/rmse</td><td>0.57925</td></tr><tr><td>eval/runtime</td><td>3.9697</td></tr><tr><td>eval/samples_per_second</td><td>59.702</td></tr><tr><td>train/train_runtime</td><td>1787.5979</td></tr><tr><td>train/train_samples_per_second</td><td>1.829</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▃▂▃▅▂▃▁▄▁▄▁▂▄▇▂▂▃▃▄▃▂▃▂▁</td></tr><tr><td>eval/mse</td><td>█▃▂▃▅▂▂▁▃▁▄▁▂▄▆▂▂▃▃▃▂▂▃▂▁</td></tr><tr><td>eval/rmse</td><td>█▃▂▃▅▂▃▁▄▁▄▁▂▄▆▂▂▃▃▄▂▂▃▂▁</td></tr><tr><td>eval/runtime</td><td>▁▃▆▄▆▄▃▄▄▇▅▄▄▇▄▄▇▅▅█▆▆▇▄▂</td></tr><tr><td>eval/samples_per_second</td><td>█▆▃▅▃▅▅▅▅▂▄▅▅▂▅▅▂▄▄▁▃▃▂▅▇</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stoic-terrain-14</strong>: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/enmb2te8\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/enmb2te8</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:enmb2te8). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fresh-butterfly-15</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_roberta-large/runs/2qsudpp1\" target=\"_blank\">https://wandb.ai/gilf/commonlit_roberta-large/runs/2qsudpp1</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210612_081351-2qsudpp1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2725' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2725/3270 31:06 < 06:13, 1.46 it/s, Epoch 25/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.722829</td>\n",
       "      <td>0.542528</td>\n",
       "      <td>0.736565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.798400</td>\n",
       "      <td>0.515830</td>\n",
       "      <td>0.277794</td>\n",
       "      <td>0.527061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>0.906159</td>\n",
       "      <td>0.853074</td>\n",
       "      <td>0.923620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.523464</td>\n",
       "      <td>0.287876</td>\n",
       "      <td>0.536541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.592200</td>\n",
       "      <td>0.575471</td>\n",
       "      <td>0.352012</td>\n",
       "      <td>0.593306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.679062</td>\n",
       "      <td>0.488775</td>\n",
       "      <td>0.699124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.560605</td>\n",
       "      <td>0.328500</td>\n",
       "      <td>0.573150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.836764</td>\n",
       "      <td>0.721490</td>\n",
       "      <td>0.849406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.563320</td>\n",
       "      <td>0.329399</td>\n",
       "      <td>0.573933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.570209</td>\n",
       "      <td>0.331402</td>\n",
       "      <td>0.575676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.679789</td>\n",
       "      <td>0.474684</td>\n",
       "      <td>0.688973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.735312</td>\n",
       "      <td>0.554992</td>\n",
       "      <td>0.744978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.508938</td>\n",
       "      <td>0.265670</td>\n",
       "      <td>0.515432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.522514</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.514796</td>\n",
       "      <td>0.274298</td>\n",
       "      <td>0.523734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.623891</td>\n",
       "      <td>0.398465</td>\n",
       "      <td>0.631241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.665127</td>\n",
       "      <td>0.452169</td>\n",
       "      <td>0.672435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.547083</td>\n",
       "      <td>0.305855</td>\n",
       "      <td>0.553041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.533393</td>\n",
       "      <td>0.291549</td>\n",
       "      <td>0.539952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.568133</td>\n",
       "      <td>0.333089</td>\n",
       "      <td>0.577139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.548314</td>\n",
       "      <td>0.310268</td>\n",
       "      <td>0.557017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.540013</td>\n",
       "      <td>0.299399</td>\n",
       "      <td>0.547174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.600025</td>\n",
       "      <td>0.368846</td>\n",
       "      <td>0.607327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.594308</td>\n",
       "      <td>0.362098</td>\n",
       "      <td>0.601746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.541831</td>\n",
       "      <td>0.300951</td>\n",
       "      <td>0.548590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-large-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-large-3/checkpoint-1417\n",
      "result {'eval_loss': 0.5089381337165833, 'eval_mse': 0.2656697630882263, 'eval_rmse': 0.5154316425323486, 'eval_runtime': 3.9948, 'eval_samples_per_second': 59.327, 'epoch': 25.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 377665024}\n",
      "CPU times: user 42min 37s, sys: 13min 17s, total: 55min 55s\n",
      "Wall time: 31min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "bin_step = 1\n",
    "bestmodels = []\n",
    "eval_rmses = []\n",
    "for i in range(3, 4, bin_step):\n",
    "    train_bins = bin_list[i:i+bin_step]\n",
    "    print('train_bins', f'{i}: {train_bins}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "    train_text, train_target, valid_text, valid_target = create_split([i])\n",
    "    train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)\n",
    "    training_args = create_training_args(i)\n",
    "    model = CommonLitModel()\n",
    "    wandb.init(project=f\"commonlit_{cfg.model_name.replace('/', '_')}\")\n",
    "    trainer = CommonLitTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=12)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    print('training_args.output_dir', training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    result = trainer.evaluate()\n",
    "    bestmodels.append(trainer.state.best_model_checkpoint)\n",
    "    print('best_model_checkpoint', trainer.state.best_model_checkpoint)\n",
    "    print('result', result)\n",
    "    eval_rmses.append(result['eval_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5290ec66-8db4-4047-8fea-8d8f609c3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mean best RSME losses', 0.5168055981397629)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Mean best RSME losses', np.array(eval_rmses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fce73fad-cc12-4b8f-9849-5a44057cf956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/commonlit/models/roberta-large-3/checkpoint-1417',\n",
       " PosixPath('/home/commonlit/models'))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels[0], MODELS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a671cdf-daac-4a40-861b-560d173247aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(MODELS_PATH/'roberta-large-0/checkpoint-1853').exists(),\n",
    "(MODELS_PATH/'roberta-large-1/checkpoint-1526').exists(),\n",
    "(MODELS_PATH/'roberta-large-2/checkpoint-2616').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc3d9bae-7840-4427-84e1-e23f51b51844",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/'roberta-large-0/checkpoint-1853',\n",
    "              MODELS_PATH/'roberta-large-1/checkpoint-1526',\n",
    "              MODELS_PATH/'roberta-large-2/checkpoint-2616',\n",
    "              MODELS_PATH/'roberta-large-3/checkpoint-1417',\n",
    "              MODELS_PATH/'roberta-large-4/checkpoint-1417',\n",
    "              MODELS_PATH/'roberta-large-5/checkpoint-1635',\n",
    "              MODELS_PATH/'roberta-large-6/checkpoint-1526',\n",
    "              MODELS_PATH/'roberta-large-7/checkpoint-1635',\n",
    "              MODELS_PATH/'roberta-large-8/checkpoint-1526',\n",
    "              MODELS_PATH/'roberta-large-9/checkpoint-2071',\n",
    "              MODELS_PATH/'roberta-large-10/checkpoint-1417',\n",
    "              MODELS_PATH/'roberta-large-11/checkpoint-1308']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/roberta-large/best')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n",
      "Processing 6th model\n",
      "Processing 7th model\n",
      "Processing 8th model\n",
      "Processing 9th model\n",
      "Processing 10th model\n",
      "Processing 11th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    best_model_file = f'{best_model}/pytorch_model.bin'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/tokenizer.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "#         vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/vocab.json'))\n",
    "#         assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "#         copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        config_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/config.json'))\n",
    "        assert config_json.exists()\n",
    "        copyfile(config_json, tokenizer_path/'config.json')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-large/best_models.zip'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15G\t/home/commonlit/models/roberta-large/best_models.zip\n",
      "4.0K\t/home/commonlit/models/roberta-large/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/roberta-large/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-roberta-large\",\n",
      "  \"id\": \"gilfernandes/commonlit-roberta-large\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e51c385b-abde-4c53-9458-1561a69b158d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-large/best_lm_model.zip'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_lm_model', 'zip', MODELS_PATH/f'{cfg.model_name}_lm/best_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|██████████████████████████████████████| 14.7G/14.7G [25:48<00:00, 10.2MB/s]\n",
      "Upload successful: best_models.zip (15GB)\n",
      "Starting upload for file best_lm_model.zip\n",
      "100%|██████████████████████████████████████| 3.66G/3.66G [06:26<00:00, 10.2MB/s]\n",
      "Upload successful: best_lm_model.zip (4GB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-roberta-large\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
