{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = str(MODELS_PATH/'roberta-base_lm')\n",
    "    TOKENIZER_PATH = str(MODELS_PATH/'roberta-base-0')\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'roberta-base'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        _2_hidden_states = output['hidden_states'][-2]\n",
    "        last_layer_hidden_states = output['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        _2_weights = self.attention(_2_hidden_states)\n",
    "        stacked_weights = torch.cat([weights, _2_weights], axis=1)\n",
    "        stacked_hidden_states = torch.cat([last_layer_hidden_states, _2_hidden_states], axis=1)\n",
    "        stacked_mul = stacked_weights * stacked_hidden_states\n",
    "        context_vector = torch.sum(stacked_mul, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.embeddings.word_embeddings.weight torch.Size([50265, 768])\n",
      "1 transformer_model.embeddings.position_embeddings.weight torch.Size([514, 768])\n",
      "2 transformer_model.embeddings.token_type_embeddings.weight torch.Size([1, 768])\n",
      "3 transformer_model.embeddings.LayerNorm.weight torch.Size([768])\n",
      "4 transformer_model.embeddings.LayerNorm.bias torch.Size([768])\n",
      "5 transformer_model.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "6 transformer_model.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "7 transformer_model.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "8 transformer_model.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "9 transformer_model.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "10 transformer_model.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "11 transformer_model.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "12 transformer_model.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "13 transformer_model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "14 transformer_model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "15 transformer_model.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "16 transformer_model.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "17 transformer_model.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "18 transformer_model.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "19 transformer_model.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "20 transformer_model.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "21 transformer_model.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "22 transformer_model.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "23 transformer_model.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "24 transformer_model.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "25 transformer_model.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "26 transformer_model.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "27 transformer_model.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "28 transformer_model.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "29 transformer_model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "30 transformer_model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "31 transformer_model.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "32 transformer_model.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "33 transformer_model.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "34 transformer_model.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "35 transformer_model.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "36 transformer_model.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "37 transformer_model.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "38 transformer_model.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "39 transformer_model.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "40 transformer_model.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "41 transformer_model.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "42 transformer_model.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "43 transformer_model.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "44 transformer_model.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "45 transformer_model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "46 transformer_model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "47 transformer_model.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "48 transformer_model.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "49 transformer_model.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "50 transformer_model.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "51 transformer_model.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "52 transformer_model.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "53 transformer_model.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "54 transformer_model.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "55 transformer_model.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "56 transformer_model.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "57 transformer_model.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "58 transformer_model.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "59 transformer_model.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "60 transformer_model.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "61 transformer_model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "62 transformer_model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "63 transformer_model.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "64 transformer_model.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "65 transformer_model.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "66 transformer_model.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "67 transformer_model.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "68 transformer_model.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "69 transformer_model.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "70 transformer_model.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "71 transformer_model.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "72 transformer_model.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "73 transformer_model.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "74 transformer_model.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "75 transformer_model.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "76 transformer_model.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "77 transformer_model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "78 transformer_model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "79 transformer_model.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "80 transformer_model.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "81 transformer_model.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "82 transformer_model.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "83 transformer_model.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "84 transformer_model.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "85 transformer_model.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "86 transformer_model.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "87 transformer_model.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "88 transformer_model.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "89 transformer_model.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "90 transformer_model.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "91 transformer_model.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "92 transformer_model.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "93 transformer_model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "94 transformer_model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "95 transformer_model.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "96 transformer_model.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "97 transformer_model.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "98 transformer_model.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "99 transformer_model.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "100 transformer_model.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "101 transformer_model.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "102 transformer_model.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "103 transformer_model.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "104 transformer_model.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "105 transformer_model.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "106 transformer_model.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "107 transformer_model.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "108 transformer_model.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "109 transformer_model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "110 transformer_model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "111 transformer_model.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "112 transformer_model.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "113 transformer_model.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "114 transformer_model.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "115 transformer_model.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "116 transformer_model.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "117 transformer_model.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "118 transformer_model.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "119 transformer_model.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "120 transformer_model.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "121 transformer_model.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "122 transformer_model.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "123 transformer_model.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "124 transformer_model.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "125 transformer_model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "126 transformer_model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "127 transformer_model.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "128 transformer_model.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "129 transformer_model.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "130 transformer_model.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "131 transformer_model.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "132 transformer_model.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "133 transformer_model.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "134 transformer_model.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "135 transformer_model.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "136 transformer_model.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "137 transformer_model.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "138 transformer_model.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "139 transformer_model.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "140 transformer_model.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "141 transformer_model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "142 transformer_model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "143 transformer_model.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "144 transformer_model.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "145 transformer_model.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "146 transformer_model.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "147 transformer_model.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "148 transformer_model.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "149 transformer_model.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "150 transformer_model.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "151 transformer_model.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "152 transformer_model.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "153 transformer_model.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "154 transformer_model.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "155 transformer_model.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "156 transformer_model.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "157 transformer_model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "158 transformer_model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "159 transformer_model.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "160 transformer_model.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "161 transformer_model.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "162 transformer_model.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "163 transformer_model.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "164 transformer_model.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "165 transformer_model.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "166 transformer_model.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "167 transformer_model.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "168 transformer_model.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "169 transformer_model.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "170 transformer_model.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "171 transformer_model.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "172 transformer_model.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "173 transformer_model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "174 transformer_model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "175 transformer_model.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "176 transformer_model.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "177 transformer_model.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "178 transformer_model.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "179 transformer_model.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "180 transformer_model.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "181 transformer_model.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "182 transformer_model.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "183 transformer_model.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "184 transformer_model.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "185 transformer_model.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "186 transformer_model.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "187 transformer_model.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "188 transformer_model.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "189 transformer_model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "190 transformer_model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "191 transformer_model.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "192 transformer_model.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "193 transformer_model.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "194 transformer_model.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "195 transformer_model.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "196 transformer_model.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "197 transformer_model.pooler.dense.weight torch.Size([768, 768])\n",
      "198 transformer_model.pooler.dense.bias torch.Size([768])\n",
      "199 attention.hidden_layer.weight torch.Size([512, 768])\n",
      "200 attention.hidden_layer.bias torch.Size([512])\n",
      "201 attention.final_layer.weight torch.Size([1, 512])\n",
      "202 attention.final_layer.bias torch.Size([1])\n",
      "203 regressor.weight torch.Size([1, 768])\n",
      "204 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.4895, -12.5976, -14.2283,  ...,  15.0938,  -0.3856,   2.7173],\n",
       "        [  3.3022, -25.7295,  -2.0464,  ...,  28.2234,  -0.6675,   5.8295],\n",
       "        [ 24.4435,  -9.0543,  -1.1326,  ..., -22.8916, -13.1523, -26.9746],\n",
       "        ...,\n",
       "        [ 25.0492,   0.3893,  47.5810,  ...,  13.0443,   6.1462, -26.7043],\n",
       "        [ -7.5212, -15.8429,  24.7444,  ..., -29.4573,  -1.2293, -14.9271],\n",
       "        [ -3.8956,  30.3374,  13.4585,  ...,  -2.7132,  10.4994,  -5.9202]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 203\n",
    "    roberta_parameters = named_parameters[:197]\n",
    "    attention_parameters = named_parameters[199:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5\n",
    "        if layer_num >= 133:\n",
    "            lr = base_lr / 0.5\n",
    "        elif layer_num >= 69:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                \n",
    "                mse.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, self.model_path)\n",
    "                    start = time.time()\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = kfold.split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57da8c9558414ee1960db7e4c8841a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9291cb35b4d74e699309a0d8b4998e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.71 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9742 New best_val_rmse: 0.9742\n",
      "\n",
      "16 steps took 3.77 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7829 New best_val_rmse: 0.7829\n",
      "\n",
      "16 steps took 3.78 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6018 New best_val_rmse: 0.6018\n",
      "\n",
      "16 steps took 3.81 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5925 New best_val_rmse: 0.5925\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5862 New best_val_rmse: 0.5862\n",
      "\n",
      "16 steps took 3.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5517 New best_val_rmse: 0.5517\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6357 Still best_val_rmse: 0.5517 (from epoch 0)\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5353 New best_val_rmse: 0.5353\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7445 Still best_val_rmse: 0.5353 (from epoch 0)\n",
      "\n",
      "16 steps took 4.77 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5735 Still best_val_rmse: 0.5353 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5108 New best_val_rmse: 0.5108\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5059 New best_val_rmse: 0.5059\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5522 Still best_val_rmse: 0.5059 (from epoch 1)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5347 Still best_val_rmse: 0.5059 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5227 Still best_val_rmse: 0.5059 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5588 Still best_val_rmse: 0.5059 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5012 New best_val_rmse: 0.5012\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.512 Still best_val_rmse: 0.5012 (from epoch 1)\n",
      "\n",
      "16 steps took 4.64 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4902 New best_val_rmse: 0.4902\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.49 New best_val_rmse: 0.49\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4902 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4924 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.97 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.5019 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.494 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4929 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4935 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4945 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4955 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4933 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4925 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.492 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4914 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4913 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4912 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae7794d036b4f998ec545e390f5107a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.136 New best_val_rmse: 1.136\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.756 New best_val_rmse: 0.756\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6739 New best_val_rmse: 0.6739\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5642 New best_val_rmse: 0.5642\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5745 Still best_val_rmse: 0.5642 (from epoch 0)\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5393 New best_val_rmse: 0.5393\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5441 Still best_val_rmse: 0.5393 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6009 Still best_val_rmse: 0.5393 (from epoch 0)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5238 New best_val_rmse: 0.5238\n",
      "\n",
      "16 steps took 4.66 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5481 Still best_val_rmse: 0.5238 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5646 Still best_val_rmse: 0.5238 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5302 Still best_val_rmse: 0.5238 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5326 Still best_val_rmse: 0.5238 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4768 New best_val_rmse: 0.4768\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 1 batch_num: 78 val_rmse: 0.4843 Still best_val_rmse: 0.4768 (from epoch 1)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 1 batch_num: 82 val_rmse: 0.4707 New best_val_rmse: 0.4707\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 85 val_rmse: 0.4668 New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 1 batch_num: 86 val_rmse: 0.4686 Still best_val_rmse: 0.4668 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 1 batch_num: 87 val_rmse: 0.4774 Still best_val_rmse: 0.4668 (from epoch 1)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 1 batch_num: 89 val_rmse: 0.5065 Still best_val_rmse: 0.4668 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 105 val_rmse: 0.4624 New best_val_rmse: 0.4624\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 106 val_rmse: 0.4611 New best_val_rmse: 0.4611\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 107 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.459 New best_val_rmse: 0.459\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 109 val_rmse: 0.4603 Still best_val_rmse: 0.459 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 110 val_rmse: 0.466 Still best_val_rmse: 0.459 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 1 batch_num: 111 val_rmse: 0.4758 Still best_val_rmse: 0.459 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 1 batch_num: 113 val_rmse: 0.4822 Still best_val_rmse: 0.459 (from epoch 1)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 1 batch_num: 117 val_rmse: 0.4578 New best_val_rmse: 0.4578\n",
      "\n",
      "1 steps took 0.241 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.453 New best_val_rmse: 0.453\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 1 batch_num: 119 val_rmse: 0.453 Still best_val_rmse: 0.453 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.457 Still best_val_rmse: 0.453 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 121 val_rmse: 0.4617 Still best_val_rmse: 0.453 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.4598 Still best_val_rmse: 0.453 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 123 val_rmse: 0.4542 Still best_val_rmse: 0.453 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4532 Still best_val_rmse: 0.453 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 1 batch_num: 125 val_rmse: 0.4521 New best_val_rmse: 0.4521\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.4549 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 1 batch_num: 127 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4574 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 129 val_rmse: 0.456 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.4544 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4552 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4579 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.4595 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4595 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 1 batch_num: 135 val_rmse: 0.4599 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4602 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.461 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4625 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 139 val_rmse: 0.4626 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.464 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4631 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4618 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 1 batch_num: 143 val_rmse: 0.4641 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4656 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 1 batch_num: 145 val_rmse: 0.4683 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4676 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.142 seconds\n",
      "Epoch: 1 batch_num: 147 val_rmse: 0.4636 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 1.19 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4598 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 1 val_rmse: 0.4591 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4604 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 3 val_rmse: 0.4609 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4596 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 5 val_rmse: 0.4588 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4593 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.4594 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.459 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.241 seconds\n",
      "Epoch: 2 batch_num: 9 val_rmse: 0.4583 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.458 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 11 val_rmse: 0.4582 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4588 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 13 val_rmse: 0.4588 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4591 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4597 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4604 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4624 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4659 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4667 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4685 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4672 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4628 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.459 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4584 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4581 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4577 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4577 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4589 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4613 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.24 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4637 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4641 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4639 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4643 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4655 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4654 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4657 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.241 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4658 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4641 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4636 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4626 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4622 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4625 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.242 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4637 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4647 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4654 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4666 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4665 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4665 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4658 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4644 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4629 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4625 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4647 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.468 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4701 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4721 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4689 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4655 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4625 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4597 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.458 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4572 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4569 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4568 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4564 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4563 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4561 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.242 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4563 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4563 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4552 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.255 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4544 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4539 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4553 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4568 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4579 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4591 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4594 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4592 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4591 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4594 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4591 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4582 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4576 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4568 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.456 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4554 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4551 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4545 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4544 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4543 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4543 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.255 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4542 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4543 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4544 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4545 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4545 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4546 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4546 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4546 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4546 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.254 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.25 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4546 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4547 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.252 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4548 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4548 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4549 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.455 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4551 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.248 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4552 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4553 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4554 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4555 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.245 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4556 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.253 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4557 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4557 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.243 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.244 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4558 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.251 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.249 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.247 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.246 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.242 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n",
      "1 steps took 0.141 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4559 Still best_val_rmse: 0.4521 (from epoch 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd8bab6408844ddbece751f4d7724fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.81 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9822 New best_val_rmse: 0.9822\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7147 New best_val_rmse: 0.7147\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7072 New best_val_rmse: 0.7072\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5816 New best_val_rmse: 0.5816\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5894 Still best_val_rmse: 0.5816 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.57 New best_val_rmse: 0.57\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.599 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6106 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.59 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 4.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5989 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5191 New best_val_rmse: 0.5191\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5093 New best_val_rmse: 0.5093\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5151 Still best_val_rmse: 0.5093 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5052 New best_val_rmse: 0.5052\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5096 Still best_val_rmse: 0.5052 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5131 Still best_val_rmse: 0.5052 (from epoch 1)\n",
      "\n",
      "16 steps took 3.98 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4989 New best_val_rmse: 0.4989\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5217 Still best_val_rmse: 0.4989 (from epoch 1)\n",
      "\n",
      "16 steps took 4.74 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5137 Still best_val_rmse: 0.4989 (from epoch 1)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5123 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4778 New best_val_rmse: 0.4778\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4772 New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4762 New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4776 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4785 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4756 New best_val_rmse: 0.4756\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4757 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4757 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4758 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4755 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4754 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4755 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4755 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4755 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4755 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4755 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4754 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4757 Still best_val_rmse: 0.4753 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.501 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4762 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4764 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4764 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4761 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4752 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4751 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.475 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.475 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4751 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4752 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4752 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4753 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ef0988faea429dba896d6898b1ee3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.83 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.79 New best_val_rmse: 0.79\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6849 New best_val_rmse: 0.6849\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5835 New best_val_rmse: 0.5835\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6201 Still best_val_rmse: 0.5835 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6315 Still best_val_rmse: 0.5835 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6078 Still best_val_rmse: 0.5835 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5367 New best_val_rmse: 0.5367\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5443 Still best_val_rmse: 0.5367 (from epoch 0)\n",
      "\n",
      "16 steps took 4.73 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5298 New best_val_rmse: 0.5298\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5098 New best_val_rmse: 0.5098\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5264 Still best_val_rmse: 0.5098 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5096 New best_val_rmse: 0.5096\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5215 Still best_val_rmse: 0.5096 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5052 New best_val_rmse: 0.5052\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6011 Still best_val_rmse: 0.5052 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 0.969 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4937 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4907 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.515 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "16 steps took 4.74 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4905 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4881 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4876 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.976 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4878 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.486 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.491 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.5021 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4877 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4882 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4894 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4894 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.986 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4895 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.983 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4903 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.96 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4898 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4903 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4901 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4892 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.968 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4893 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.974 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4893 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4892 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.981 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4891 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4891 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4891 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.981 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4891 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.969 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4891 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 0.977 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4891 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b7d3b26d3b464ca55c3427ccb7df34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7209 New best_val_rmse: 0.7209\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6041 New best_val_rmse: 0.6041\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7367 Still best_val_rmse: 0.6041 (from epoch 0)\n",
      "\n",
      "16 steps took 3.85 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5672 New best_val_rmse: 0.5672\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5374 New best_val_rmse: 0.5374\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6056 Still best_val_rmse: 0.5374 (from epoch 0)\n",
      "\n",
      "16 steps took 3.87 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6227 Still best_val_rmse: 0.5374 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5369 New best_val_rmse: 0.5369\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7727 Still best_val_rmse: 0.5369 (from epoch 0)\n",
      "\n",
      "16 steps took 4.72 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.615 Still best_val_rmse: 0.5369 (from epoch 0)\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5283 New best_val_rmse: 0.5283\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 1 batch_num: 54 val_rmse: 0.5095 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 70 val_rmse: 0.488 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "4 steps took 0.965 seconds\n",
      "Epoch: 1 batch_num: 74 val_rmse: 0.4864 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 1 batch_num: 78 val_rmse: 0.4866 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "4 steps took 0.979 seconds\n",
      "Epoch: 1 batch_num: 82 val_rmse: 0.4937 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 90 val_rmse: 0.4984 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 1 batch_num: 98 val_rmse: 0.5574 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 114 val_rmse: 0.4944 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.499 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.4903 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.504 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "16 steps took 4.69 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4893 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "4 steps took 0.966 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4848 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "4 steps took 0.973 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4802 Still best_val_rmse: 0.479 (from epoch 1)\n",
      "\n",
      "4 steps took 0.978 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4756 New best_val_rmse: 0.4756\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4775 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4792 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.474 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4743 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4757 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4754 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4744 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.474 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4746 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4765 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4735 New best_val_rmse: 0.4735\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4733 New best_val_rmse: 0.4733\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.476 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4809 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "4 steps took 0.971 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4832 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "4 steps took 0.984 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4755 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4734 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4741 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4749 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4746 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4731 New best_val_rmse: 0.4731\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4729 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4736 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4738 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4742 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4744 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4747 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4749 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4749 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4749 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4748 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4748 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4748 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4745 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4743 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4741 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.484 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d723ec75d241446298c8647bd34005c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 4.86 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.559 New best_val_rmse: 1.559\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8293 New best_val_rmse: 0.8293\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8071 New best_val_rmse: 0.8071\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.691 New best_val_rmse: 0.691\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.63 New best_val_rmse: 0.63\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5588 New best_val_rmse: 0.5588\n",
      "\n",
      "16 steps took 3.94 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5477 New best_val_rmse: 0.5477\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5815 Still best_val_rmse: 0.5477 (from epoch 0)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5464 New best_val_rmse: 0.5464\n",
      "\n",
      "16 steps took 4.71 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5524 Still best_val_rmse: 0.5464 (from epoch 0)\n",
      "\n",
      "16 steps took 3.86 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5735 Still best_val_rmse: 0.5464 (from epoch 0)\n",
      "\n",
      "16 steps took 3.92 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5145 New best_val_rmse: 0.5145\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5581 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 3.89 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5164 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 3.9 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5289 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 3.93 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5126 New best_val_rmse: 0.5126\n",
      "\n",
      "16 steps took 3.91 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 1.96 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4935 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "8 steps took 1.95 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5124 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 4.72 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 0.967 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4895 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4897 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 0.982 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4877 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 0.97 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4982 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.488 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 0.968 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.498 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "8 steps took 1.93 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4919 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "8 steps took 1.94 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4883 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 0.972 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5131 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "16 steps took 3.88 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4775 New best_val_rmse: 0.4775\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.481 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4785 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4803 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 0.987 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4835 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 0.968 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4825 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 0.975 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4775 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 0.487 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4768 New best_val_rmse: 0.4768\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4769 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.486 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4771 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4773 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4774 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4775 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4775 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4773 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4772 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.477 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4769 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4769 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.477 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4771 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4771 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4772 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.481 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4773 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4775 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "2 steps took 0.485 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4776 Still best_val_rmse: 0.4768 (from epoch 2)\n",
      "\n",
      "\n",
      "\n",
      "Performance estimates:\n",
      "[tensor(0.4900), tensor(0.4521), tensor(0.4750), tensor(0.4848), tensor(0.4726), tensor(0.4768)]\n",
      "Mean: 0.47520185\n"
     ]
    }
   ],
   "source": [
    "list_val_rmse = []\n",
    "\n",
    "pbar = tqdm(enumerate(splits), total=cfg.NUM_FOLDS, position=0, leave=True)\n",
    "for fold, (train_indices, val_indices) in pbar:\n",
    "    pbar.set_description(f'Fold {fold}')\n",
    "    model_path = cfg.MODEL_FOLDER/f\"roberta-base_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "        \n",
    "    optimizer = create_optimizer(model)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    trainer = Trainer(model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    list_val_rmse.append(trainer.train())\n",
    "    \n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    if cfg.DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print(\"\\nPerformance estimates:\")\n",
    "print(list_val_rmse)\n",
    "print(\"Mean:\", np.array(list_val_rmse).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/roberta-base_lm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/commonlit/models/roberta-base_lm and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.66 s, sys: 2.64 s, total: 10.3 s\n",
      "Wall time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a7fa4db2d54610a0367756fd5eeb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44511efb06c4e6f8c5648326be32263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.312142841484001\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.35453536052747725\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.32059224740411746\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3342262121475011\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.33193625064639976\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3588098979584189\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a4605c6e334f1781c716ce7172fed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79307df964849e3af1cca68a736ef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3314009215674823\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.33989511856480586\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3200691621809814\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.30714172510842125\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3297242850958922\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3351967107631785\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ecc8bc94754775a02cf96c44afdc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8630de5e26a43088a8c6675366459a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.31492303382301645\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3710494337479098\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3354141274363938\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3243960011506718\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3308837801548399\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3435653199727582\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28589ad12ab42358f943ca1240e0c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9705f9d6b4dd4ddf9d334d04a118fb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.33738823237280635\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3421355049648379\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.316694350146067\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3034172157657744\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.35079517899032514\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3670471707717054\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93849bbbe524e1eb98f47fe180cf3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5cd8a7e73f4505a9e186bc377318db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.345575946808507\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3622680371661247\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.34982519235186066\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3359857872447713\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3384502199006281\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.37340757962166077\n",
      "FINAL RMSE score 0.33729642819464456\n",
      "CPU times: user 2min 8s, sys: 2.92 s, total: 2min 11s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.004122795950432596],\n",
       " [-0.0008061889274363845],\n",
       " [0.0018296439835301104],\n",
       " [-0.0015377556223151255],\n",
       " [-0.0023664210936125934]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20028501, 0.20149104, 0.20008763, 0.20015567, 0.19798065])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.9767934966056343)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.54467689, -0.46206632, -0.38874319, -2.42437832, -1.85041837,\n",
       "       -1.60337413,  0.43547454])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01722584639091085, 0.0034451692781821697)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.527451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.444840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.371517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.407152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.833193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.586148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.527451\n",
       "1  f0953f0a5 -0.444840\n",
       "2  0df072751 -0.371517\n",
       "3  04caf4e0c -2.407152\n",
       "4  0e63f8bea -1.833193\n",
       "5  12537fe78 -1.586148\n",
       "6  965e592c0  0.452700"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/roberta-base/best')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/roberta-base_1'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_2'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_3'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_4'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_5'),\n",
       " PosixPath('/home/commonlit/models/roberta-base_6')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    best_model_file = f'{best_model}/model_{i + 1}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/tokenizer.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        config_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/config.json'))\n",
    "        assert config_json.exists()\n",
    "        copyfile(config_json, tokenizer_path/'config.json')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/best_models.zip'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip  dataset-metadata.json  lm.zip  roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/roberta-base.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-0\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-1\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-2\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-3\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-4\n",
      "2.1M\t/home/commonlit/models/roberta-base/best/tokenizer-5\n",
      "2.9G\t/home/commonlit/models/roberta-base/best\n",
      "2.7G\t/home/commonlit/models/roberta-base/best_models.zip\n",
      "4.0K\t/home/commonlit/models/roberta-base/dataset-metadata.json\n",
      "476M\t/home/commonlit/models/roberta-base/lm\n",
      "442M\t/home/commonlit/models/roberta-base/lm.zip\n",
      "4.0K\t/home/commonlit/models/roberta-base/roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/lm.zip'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/roberta-base/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-roberta-base-light-balanced\",\n",
      "  \"id\": \"gilfernandes/commonlit-roberta-base-light-balanced\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light-balanced').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light-balanced')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.60G/2.60G [04:19<00:00, 10.8MB/s]\n",
      "Upload successful: best_models.zip (3GB)\n",
      "Starting upload for file roberta-base.yaml\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:07<00:00, 15.1B/s]\n",
      "Upload successful: roberta-base.yaml (114B)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442M/442M [00:53<00:00, 8.59MB/s]\n",
      "Upload successful: lm.zip (442MB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-roberta-base-light-balanced\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with normal distribution by bin and extra pre-training\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
