{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = '/home/commonlit/models/deberta-large-lm/best_lm'\n",
    "    TOKENIZER_PATH = '/home/commonlit/models/deberta-large-lm/best_lm'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     DEVICE = \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'deberta-large-lm'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929301e1-626d-4ba5-9f32-d361769f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenizer.vocab.txt', 'w') as f:\n",
    "    for k, v in tokenizer.vocab.items():\n",
    "        f.write(f'{k}: {v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b946f5-a6f0-4415-911c-e5a7f628f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '______'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "#         tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "#         assert tokenizer.pad_token == pad_token\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69ec8b1-1d38-46f9-af3b-4a34e0c8dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(cfg.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9b003d-a13f-43c9-830e-edecafdec275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50265, 50265)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.deberta.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "1 transformer_model.deberta.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "2 transformer_model.deberta.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "3 transformer_model.deberta.encoder.layer.0.attention.self.q_bias torch.Size([1024])\n",
      "4 transformer_model.deberta.encoder.layer.0.attention.self.v_bias torch.Size([1024])\n",
      "5 transformer_model.deberta.encoder.layer.0.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "6 transformer_model.deberta.encoder.layer.0.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "7 transformer_model.deberta.encoder.layer.0.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.deberta.encoder.layer.0.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "9 transformer_model.deberta.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.deberta.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "11 transformer_model.deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "12 transformer_model.deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "13 transformer_model.deberta.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "14 transformer_model.deberta.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "15 transformer_model.deberta.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "16 transformer_model.deberta.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "17 transformer_model.deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "18 transformer_model.deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "19 transformer_model.deberta.encoder.layer.1.attention.self.q_bias torch.Size([1024])\n",
      "20 transformer_model.deberta.encoder.layer.1.attention.self.v_bias torch.Size([1024])\n",
      "21 transformer_model.deberta.encoder.layer.1.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "22 transformer_model.deberta.encoder.layer.1.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "23 transformer_model.deberta.encoder.layer.1.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.deberta.encoder.layer.1.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "25 transformer_model.deberta.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.deberta.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "27 transformer_model.deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "28 transformer_model.deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "29 transformer_model.deberta.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "30 transformer_model.deberta.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "31 transformer_model.deberta.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "32 transformer_model.deberta.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "33 transformer_model.deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "34 transformer_model.deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "35 transformer_model.deberta.encoder.layer.2.attention.self.q_bias torch.Size([1024])\n",
      "36 transformer_model.deberta.encoder.layer.2.attention.self.v_bias torch.Size([1024])\n",
      "37 transformer_model.deberta.encoder.layer.2.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "38 transformer_model.deberta.encoder.layer.2.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "39 transformer_model.deberta.encoder.layer.2.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.deberta.encoder.layer.2.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "41 transformer_model.deberta.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.deberta.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "43 transformer_model.deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "44 transformer_model.deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "45 transformer_model.deberta.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "46 transformer_model.deberta.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "47 transformer_model.deberta.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "48 transformer_model.deberta.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "49 transformer_model.deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "50 transformer_model.deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "51 transformer_model.deberta.encoder.layer.3.attention.self.q_bias torch.Size([1024])\n",
      "52 transformer_model.deberta.encoder.layer.3.attention.self.v_bias torch.Size([1024])\n",
      "53 transformer_model.deberta.encoder.layer.3.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "54 transformer_model.deberta.encoder.layer.3.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "55 transformer_model.deberta.encoder.layer.3.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.deberta.encoder.layer.3.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "57 transformer_model.deberta.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.deberta.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "59 transformer_model.deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "60 transformer_model.deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "61 transformer_model.deberta.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "62 transformer_model.deberta.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "63 transformer_model.deberta.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "64 transformer_model.deberta.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "65 transformer_model.deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "66 transformer_model.deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "67 transformer_model.deberta.encoder.layer.4.attention.self.q_bias torch.Size([1024])\n",
      "68 transformer_model.deberta.encoder.layer.4.attention.self.v_bias torch.Size([1024])\n",
      "69 transformer_model.deberta.encoder.layer.4.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "70 transformer_model.deberta.encoder.layer.4.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "71 transformer_model.deberta.encoder.layer.4.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.deberta.encoder.layer.4.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "73 transformer_model.deberta.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.deberta.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "75 transformer_model.deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "76 transformer_model.deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "77 transformer_model.deberta.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "78 transformer_model.deberta.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "79 transformer_model.deberta.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "80 transformer_model.deberta.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "81 transformer_model.deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "82 transformer_model.deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "83 transformer_model.deberta.encoder.layer.5.attention.self.q_bias torch.Size([1024])\n",
      "84 transformer_model.deberta.encoder.layer.5.attention.self.v_bias torch.Size([1024])\n",
      "85 transformer_model.deberta.encoder.layer.5.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "86 transformer_model.deberta.encoder.layer.5.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "87 transformer_model.deberta.encoder.layer.5.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.deberta.encoder.layer.5.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "89 transformer_model.deberta.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.deberta.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "91 transformer_model.deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "92 transformer_model.deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "93 transformer_model.deberta.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "94 transformer_model.deberta.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "95 transformer_model.deberta.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "96 transformer_model.deberta.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "97 transformer_model.deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "98 transformer_model.deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "99 transformer_model.deberta.encoder.layer.6.attention.self.q_bias torch.Size([1024])\n",
      "100 transformer_model.deberta.encoder.layer.6.attention.self.v_bias torch.Size([1024])\n",
      "101 transformer_model.deberta.encoder.layer.6.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "102 transformer_model.deberta.encoder.layer.6.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "103 transformer_model.deberta.encoder.layer.6.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.deberta.encoder.layer.6.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "105 transformer_model.deberta.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.deberta.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "107 transformer_model.deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "108 transformer_model.deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "109 transformer_model.deberta.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "110 transformer_model.deberta.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "111 transformer_model.deberta.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "112 transformer_model.deberta.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "113 transformer_model.deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "114 transformer_model.deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "115 transformer_model.deberta.encoder.layer.7.attention.self.q_bias torch.Size([1024])\n",
      "116 transformer_model.deberta.encoder.layer.7.attention.self.v_bias torch.Size([1024])\n",
      "117 transformer_model.deberta.encoder.layer.7.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "118 transformer_model.deberta.encoder.layer.7.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "119 transformer_model.deberta.encoder.layer.7.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.deberta.encoder.layer.7.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "121 transformer_model.deberta.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.deberta.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "123 transformer_model.deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "124 transformer_model.deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "125 transformer_model.deberta.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "126 transformer_model.deberta.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "127 transformer_model.deberta.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "128 transformer_model.deberta.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "129 transformer_model.deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "130 transformer_model.deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "131 transformer_model.deberta.encoder.layer.8.attention.self.q_bias torch.Size([1024])\n",
      "132 transformer_model.deberta.encoder.layer.8.attention.self.v_bias torch.Size([1024])\n",
      "133 transformer_model.deberta.encoder.layer.8.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "134 transformer_model.deberta.encoder.layer.8.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "135 transformer_model.deberta.encoder.layer.8.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.deberta.encoder.layer.8.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "137 transformer_model.deberta.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.deberta.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "139 transformer_model.deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "140 transformer_model.deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "141 transformer_model.deberta.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "142 transformer_model.deberta.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "143 transformer_model.deberta.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "144 transformer_model.deberta.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "145 transformer_model.deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "146 transformer_model.deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "147 transformer_model.deberta.encoder.layer.9.attention.self.q_bias torch.Size([1024])\n",
      "148 transformer_model.deberta.encoder.layer.9.attention.self.v_bias torch.Size([1024])\n",
      "149 transformer_model.deberta.encoder.layer.9.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "150 transformer_model.deberta.encoder.layer.9.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "151 transformer_model.deberta.encoder.layer.9.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.deberta.encoder.layer.9.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "153 transformer_model.deberta.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.deberta.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "155 transformer_model.deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "156 transformer_model.deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "157 transformer_model.deberta.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "158 transformer_model.deberta.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "159 transformer_model.deberta.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "160 transformer_model.deberta.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "161 transformer_model.deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "162 transformer_model.deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "163 transformer_model.deberta.encoder.layer.10.attention.self.q_bias torch.Size([1024])\n",
      "164 transformer_model.deberta.encoder.layer.10.attention.self.v_bias torch.Size([1024])\n",
      "165 transformer_model.deberta.encoder.layer.10.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "166 transformer_model.deberta.encoder.layer.10.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "167 transformer_model.deberta.encoder.layer.10.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.deberta.encoder.layer.10.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "169 transformer_model.deberta.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.deberta.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "171 transformer_model.deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "172 transformer_model.deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "173 transformer_model.deberta.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "174 transformer_model.deberta.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "175 transformer_model.deberta.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "176 transformer_model.deberta.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "177 transformer_model.deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "178 transformer_model.deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "179 transformer_model.deberta.encoder.layer.11.attention.self.q_bias torch.Size([1024])\n",
      "180 transformer_model.deberta.encoder.layer.11.attention.self.v_bias torch.Size([1024])\n",
      "181 transformer_model.deberta.encoder.layer.11.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "182 transformer_model.deberta.encoder.layer.11.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "183 transformer_model.deberta.encoder.layer.11.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.deberta.encoder.layer.11.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "185 transformer_model.deberta.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.deberta.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "187 transformer_model.deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "188 transformer_model.deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "189 transformer_model.deberta.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "190 transformer_model.deberta.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "191 transformer_model.deberta.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "192 transformer_model.deberta.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "193 transformer_model.deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "194 transformer_model.deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "195 transformer_model.deberta.encoder.layer.12.attention.self.q_bias torch.Size([1024])\n",
      "196 transformer_model.deberta.encoder.layer.12.attention.self.v_bias torch.Size([1024])\n",
      "197 transformer_model.deberta.encoder.layer.12.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "198 transformer_model.deberta.encoder.layer.12.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "199 transformer_model.deberta.encoder.layer.12.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.deberta.encoder.layer.12.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "201 transformer_model.deberta.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.deberta.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "203 transformer_model.deberta.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "204 transformer_model.deberta.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "205 transformer_model.deberta.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "206 transformer_model.deberta.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "207 transformer_model.deberta.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "208 transformer_model.deberta.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "209 transformer_model.deberta.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "210 transformer_model.deberta.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "211 transformer_model.deberta.encoder.layer.13.attention.self.q_bias torch.Size([1024])\n",
      "212 transformer_model.deberta.encoder.layer.13.attention.self.v_bias torch.Size([1024])\n",
      "213 transformer_model.deberta.encoder.layer.13.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "214 transformer_model.deberta.encoder.layer.13.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "215 transformer_model.deberta.encoder.layer.13.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.deberta.encoder.layer.13.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "217 transformer_model.deberta.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.deberta.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "219 transformer_model.deberta.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "220 transformer_model.deberta.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "221 transformer_model.deberta.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "222 transformer_model.deberta.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "223 transformer_model.deberta.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "224 transformer_model.deberta.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "225 transformer_model.deberta.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "226 transformer_model.deberta.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "227 transformer_model.deberta.encoder.layer.14.attention.self.q_bias torch.Size([1024])\n",
      "228 transformer_model.deberta.encoder.layer.14.attention.self.v_bias torch.Size([1024])\n",
      "229 transformer_model.deberta.encoder.layer.14.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "230 transformer_model.deberta.encoder.layer.14.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "231 transformer_model.deberta.encoder.layer.14.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.deberta.encoder.layer.14.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "233 transformer_model.deberta.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.deberta.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "235 transformer_model.deberta.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "236 transformer_model.deberta.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "237 transformer_model.deberta.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "238 transformer_model.deberta.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "239 transformer_model.deberta.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "240 transformer_model.deberta.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "241 transformer_model.deberta.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "242 transformer_model.deberta.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "243 transformer_model.deberta.encoder.layer.15.attention.self.q_bias torch.Size([1024])\n",
      "244 transformer_model.deberta.encoder.layer.15.attention.self.v_bias torch.Size([1024])\n",
      "245 transformer_model.deberta.encoder.layer.15.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "246 transformer_model.deberta.encoder.layer.15.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "247 transformer_model.deberta.encoder.layer.15.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.deberta.encoder.layer.15.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "249 transformer_model.deberta.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.deberta.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "251 transformer_model.deberta.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "252 transformer_model.deberta.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "253 transformer_model.deberta.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "254 transformer_model.deberta.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "255 transformer_model.deberta.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "256 transformer_model.deberta.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "257 transformer_model.deberta.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "258 transformer_model.deberta.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "259 transformer_model.deberta.encoder.layer.16.attention.self.q_bias torch.Size([1024])\n",
      "260 transformer_model.deberta.encoder.layer.16.attention.self.v_bias torch.Size([1024])\n",
      "261 transformer_model.deberta.encoder.layer.16.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "262 transformer_model.deberta.encoder.layer.16.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "263 transformer_model.deberta.encoder.layer.16.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.deberta.encoder.layer.16.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "265 transformer_model.deberta.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.deberta.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "267 transformer_model.deberta.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "268 transformer_model.deberta.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "269 transformer_model.deberta.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "270 transformer_model.deberta.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "271 transformer_model.deberta.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "272 transformer_model.deberta.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "273 transformer_model.deberta.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "274 transformer_model.deberta.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "275 transformer_model.deberta.encoder.layer.17.attention.self.q_bias torch.Size([1024])\n",
      "276 transformer_model.deberta.encoder.layer.17.attention.self.v_bias torch.Size([1024])\n",
      "277 transformer_model.deberta.encoder.layer.17.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "278 transformer_model.deberta.encoder.layer.17.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "279 transformer_model.deberta.encoder.layer.17.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.deberta.encoder.layer.17.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "281 transformer_model.deberta.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.deberta.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "283 transformer_model.deberta.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "284 transformer_model.deberta.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "285 transformer_model.deberta.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "286 transformer_model.deberta.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "287 transformer_model.deberta.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "288 transformer_model.deberta.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "289 transformer_model.deberta.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "290 transformer_model.deberta.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "291 transformer_model.deberta.encoder.layer.18.attention.self.q_bias torch.Size([1024])\n",
      "292 transformer_model.deberta.encoder.layer.18.attention.self.v_bias torch.Size([1024])\n",
      "293 transformer_model.deberta.encoder.layer.18.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "294 transformer_model.deberta.encoder.layer.18.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "295 transformer_model.deberta.encoder.layer.18.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.deberta.encoder.layer.18.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "297 transformer_model.deberta.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.deberta.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "299 transformer_model.deberta.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "300 transformer_model.deberta.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "301 transformer_model.deberta.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "302 transformer_model.deberta.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "303 transformer_model.deberta.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "304 transformer_model.deberta.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "305 transformer_model.deberta.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "306 transformer_model.deberta.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "307 transformer_model.deberta.encoder.layer.19.attention.self.q_bias torch.Size([1024])\n",
      "308 transformer_model.deberta.encoder.layer.19.attention.self.v_bias torch.Size([1024])\n",
      "309 transformer_model.deberta.encoder.layer.19.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "310 transformer_model.deberta.encoder.layer.19.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "311 transformer_model.deberta.encoder.layer.19.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.deberta.encoder.layer.19.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "313 transformer_model.deberta.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.deberta.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "315 transformer_model.deberta.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "316 transformer_model.deberta.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "317 transformer_model.deberta.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "318 transformer_model.deberta.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "319 transformer_model.deberta.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "320 transformer_model.deberta.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "321 transformer_model.deberta.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "322 transformer_model.deberta.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "323 transformer_model.deberta.encoder.layer.20.attention.self.q_bias torch.Size([1024])\n",
      "324 transformer_model.deberta.encoder.layer.20.attention.self.v_bias torch.Size([1024])\n",
      "325 transformer_model.deberta.encoder.layer.20.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "326 transformer_model.deberta.encoder.layer.20.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "327 transformer_model.deberta.encoder.layer.20.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.deberta.encoder.layer.20.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "329 transformer_model.deberta.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.deberta.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "331 transformer_model.deberta.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "332 transformer_model.deberta.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "333 transformer_model.deberta.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "334 transformer_model.deberta.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "335 transformer_model.deberta.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "336 transformer_model.deberta.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "337 transformer_model.deberta.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "338 transformer_model.deberta.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "339 transformer_model.deberta.encoder.layer.21.attention.self.q_bias torch.Size([1024])\n",
      "340 transformer_model.deberta.encoder.layer.21.attention.self.v_bias torch.Size([1024])\n",
      "341 transformer_model.deberta.encoder.layer.21.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "342 transformer_model.deberta.encoder.layer.21.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "343 transformer_model.deberta.encoder.layer.21.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.deberta.encoder.layer.21.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "345 transformer_model.deberta.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.deberta.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "347 transformer_model.deberta.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "348 transformer_model.deberta.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "349 transformer_model.deberta.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "350 transformer_model.deberta.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "351 transformer_model.deberta.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "352 transformer_model.deberta.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "353 transformer_model.deberta.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "354 transformer_model.deberta.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "355 transformer_model.deberta.encoder.layer.22.attention.self.q_bias torch.Size([1024])\n",
      "356 transformer_model.deberta.encoder.layer.22.attention.self.v_bias torch.Size([1024])\n",
      "357 transformer_model.deberta.encoder.layer.22.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "358 transformer_model.deberta.encoder.layer.22.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "359 transformer_model.deberta.encoder.layer.22.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.deberta.encoder.layer.22.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "361 transformer_model.deberta.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.deberta.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "363 transformer_model.deberta.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "364 transformer_model.deberta.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "365 transformer_model.deberta.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "366 transformer_model.deberta.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "367 transformer_model.deberta.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "368 transformer_model.deberta.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "369 transformer_model.deberta.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "370 transformer_model.deberta.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "371 transformer_model.deberta.encoder.layer.23.attention.self.q_bias torch.Size([1024])\n",
      "372 transformer_model.deberta.encoder.layer.23.attention.self.v_bias torch.Size([1024])\n",
      "373 transformer_model.deberta.encoder.layer.23.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "374 transformer_model.deberta.encoder.layer.23.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "375 transformer_model.deberta.encoder.layer.23.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.deberta.encoder.layer.23.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "377 transformer_model.deberta.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.deberta.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "379 transformer_model.deberta.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "380 transformer_model.deberta.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "381 transformer_model.deberta.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "382 transformer_model.deberta.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "383 transformer_model.deberta.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "384 transformer_model.deberta.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "385 transformer_model.deberta.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "386 transformer_model.deberta.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "387 transformer_model.deberta.encoder.rel_embeddings.weight torch.Size([1024, 1024])\n",
      "388 transformer_model.pooler.dense.weight torch.Size([1024, 1024])\n",
      "389 transformer_model.pooler.dense.bias torch.Size([1024])\n",
      "390 transformer_model.classifier.weight torch.Size([2, 1024])\n",
      "391 transformer_model.classifier.bias torch.Size([2])\n",
      "392 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "393 attention.hidden_layer.bias torch.Size([512])\n",
      "394 attention.final_layer.weight torch.Size([1, 512])\n",
      "395 attention.final_layer.bias torch.Size([1])\n",
      "396 regressor.weight torch.Size([1, 1024])\n",
      "397 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_input_ids = torch.randint(0, 1000, [2, 248])\n",
    "# sample_attention_mask = torch.randint(0, 1000, [2, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca35f5f4-51d1-4000-ad76-ed912daa8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_records = [sample_ds[i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "709e45b4-ac40-4d67-8fd3-45c0f40d8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'target'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_records[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3389bd94-785b-47b5-bc32-1e4e58dd9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.stack([r['input_ids'] for r in sample_records])\n",
    "sample_attention_mask = torch.stack([r['attention_mask'] for r in sample_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04781a7b-218a-41cc-b81f-d2d248e2c7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 248]), torch.Size([2, 248]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape, sample_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66a3b2b4-920e-4dff-bf9f-210d12b9c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1779,     5,   664,    82,  1835,     7,     5,  1011,  4294,\n",
       "             6,    24,  2633,    10, 27265,  1714,  2772,     4,  2978,     9,\n",
       "            41,  6291,  1310,     6,    24,    21,    10,  2608,  5252,     4,\n",
       "         50118,   133,  1929,    21,  2913,    19,  1958,    12,  9830, 20790,\n",
       "             6,    45,  4976,    15, 17359,     6,    53, 11122, 18331,    81,\n",
       "         24271,     8,  9910,  6368,     6,   101,    10,   588,  1958,   882,\n",
       "             4,    20,  3617, 38325,     8,   655,   571, 18656,    14,    56,\n",
       "         14633,     5,   929,     6,    58, 39143,    19, 15039,     8, 22246,\n",
       "         11538,    19, 13145, 21811,     9, 13178,     6,   101,  1958,     4,\n",
       "          1578, 11720,  8402,    56,    57, 14998, 38073,    15,   106,     6,\n",
       "             8, 19053,   154, 16155, 41591, 20846, 10601,    31,     5,  9836,\n",
       "             4, 50118,  3750,   349,   253,     9,     5,   929,     6,    15,\n",
       "             5,  2204,     6, 10601,    10,  2721,  4649,    12, 33986, 28862,\n",
       "             4, 50118,  4528,   910, 16148,    58,    13, 10761,     6,    65,\n",
       "            13,     5,  1972,     8,    65,    13,     5,  2786,     4,   178,\n",
       "            42,    21,     5,   177,     4, 50118,   133,  1972,    58,  4366,\n",
       "            23,    65,   253,     9,     5,   929,     8,     5,  2786,    23,\n",
       "             5,    97,     6,     8,    65,   253,    21,   373,     5,   369,\n",
       "         24710,     6,     8,     5,    97,     5,   391, 24710,     4,  4028,\n",
       "           869,    21,   576,    10,   650,  3794,    61,    51,    58,     7,\n",
       "          2195,    15,  3970,     5, 24710,     4, 50118,   713,    74,    33,\n",
       "            57,    41,  1365,   948,     6,    53,   349, 39496,    21, 23964,\n",
       "             7,  3568,  1958,  1193,  8013,     4,     2,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    1,  3684,   149,  3630,    86,     6,  3801,     4, 13066,   241,\n",
       "            21,  5568,  8454,     6,    69,  2473, 18403,    15,   211, 10147,\n",
       "            19,    10,   885,   661,  2650,     6,  9684,  8151,     4,   264,\n",
       "           770,     7,   492,     5,   920,     5, 10483,    79, 19656,  5202,\n",
       "             6,    53,    79,    56,   543,   173,     7,   836,  2864,     7,\n",
       "             5,   477,     9, 23968,    69,   308, 16006,     4, 50118,  3750,\n",
       "            94,     6,   959,     6,    77,     5,  5820,    21,   823,    81,\n",
       "             6,    79, 20185,    23,    69,   410,  1354,     6,     8,    26,\n",
       "             6,    22,  3684,   235,     6,   211, 10147,     6,    47,   189,\n",
       "           213,    72, 50118,   113,  7516,     6,   985,  2901,   211, 10147,\n",
       "         16670,     6, 13203,    19,  7207, 13213,     4,    22, 30327,   116,\n",
       "         50118,  7516,     6,    38,   524,    98,  7785,   328,  3945,    47,\n",
       "           686,    47,   214,  2882,  1917, 50118,   113,   100,   348, 23130,\n",
       "          2185,     7,    28,  2882,     6,   136,   127,    40,    60,  1835,\n",
       "          3801,     4, 13066,   241,     6, 29363,  3435,     4,    22,   100,\n",
       "         23721,    38,    95,  4157,     7,    33,    47,   213,     6,    53,\n",
       "            38,    64,    75,  4649,     7, 35858,    47,     9,     5, 10483,\n",
       "          1805,     4,   178,     6,    25,    47,   224,     6,    24,    74,\n",
       "            67,   489, 18449,  2553,    23,   184,     6,     8,    98,     6,\n",
       "         11807,     6,    38,   206,    38,  5658,    33,     7,   492,    11,\n",
       "            72, 50118,   113,  7516,     6,    47, 20285,   985,   328,   370,\n",
       "         12230,  6429,   328,  1336,   205,    47,    32,  2901,   178,   211,\n",
       "         10147,  8294,   198,     5,  2103,     8,   851,    69,   985,    10,\n",
       "         16531,    14,   823, 15544, 23283,    69,     4,     2,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25dd4b70-f9d7-4c3a-81d6-da03dd8cf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_out = sample_model.transformer_model(sample_input_ids, attention_mask=sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a770a11a-485d-49b5-ae8a-9d5dccc90a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "841fcfeb-6e75-40e5-8f82-265ed8da72d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, torch.Size([2, 248, 1024]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(internal_out.hidden_states), internal_out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res = sample_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea66f03e-eac6-478c-ab27-042d97ec1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1024]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res[0].shape, sample_res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20.3559, -29.9483, -10.9850,  ...,   1.9756,  17.1997, -22.4863],\n",
       "        [ -7.3677,  26.9104,  -5.2424,  ...,  39.6084,   6.2713,  -9.2617],\n",
       "        [  1.1094, -15.0250,  -9.2055,  ...,  26.7118,   0.3870,   8.1409],\n",
       "        ...,\n",
       "        [ 23.0908,  35.5163,  -8.3487,  ...,   5.5515, -10.6900,  12.9882],\n",
       "        [ -9.8833,  14.3958, -32.4952,  ..., -28.4682, -17.0043, -19.5385],\n",
       "        [ -1.2294,  35.6013,  -9.9441,  ...,  36.4845,  19.7578,  25.7991]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    attention_param_start = 388\n",
    "    regressor_param_start = 392\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 195\n",
    "    layer_middle_threshold = 323\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "#                 mse.backward()\n",
    "#                 self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f3b704f-b4e5-4b33-a33b-159ba8b5685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: { 'base_lr': 0.0001190683694379101, 'last_lr': 0.00017987585986205585, 'epochs': 4 } Best value: 0.49271923303604126\n",
    "# Fold 1: {'base_lr': 0.00012114635348406963, 'last_lr': 0.0005477206613438486, 'epochs': 4}. Best value:  0.45853328704833984\n",
    "# Fold 2: {'base_lr': 5.24730490640746e-05, 'last_lr': 0.00020041362261812433, 'epochs': 4}   Best value:  0.49088865518569946\n",
    "# Fold 3: {'base_lr': 6.108276630664184e-05, 'last_lr': 0.00011544056953737668, 'epochs': 4}. Best value:  0.4930591881275177\n",
    "# Fold 4: {'base_lr': 0.0001717178883932075, 'last_lr': 0.00042448836147656634, 'epochs': 4}  Best value:  0.48955243825912476\n",
    "# Fold 5: {'base_lr': 0.000135700916847811, 'last_lr': 0.0029640935672153, 'epochs': 4}.      Best value:  0.4688156247138977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 3e-5, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    epochs = 4\n",
    "    schedule_func = trial.suggest_categorical('schedule_func', [get_cosine_with_hard_restarts_schedule_with_warmup, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup])\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr} epochs {epochs}')\n",
    "    print(f'##### Using {schedule_func}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = schedule_func(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler() # fp16\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, \n",
    "                      scheduler = scheduler, num_epochs = epochs)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48210ee3-d9ea-4bab-b852-627f6f75ce0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 08:29:25,821]\u001b[0m A new study created in memory with name: no-name-5ff454cc-d5de-4c84-9ab3-46c6ac4e8fe1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.00036441540609173276 last_lr 0.003177859991698281 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958b1c6ade184124b2eb87cb8587dea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.01 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7832 New best_val_rmse: 0.7832\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 2.056 Still best_val_rmse: 0.7832 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.233 Still best_val_rmse: 0.7832 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.037 Still best_val_rmse: 0.7832 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.05 Still best_val_rmse: 0.7832 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.034 Still best_val_rmse: 0.7832 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 08:30:48,130]\u001b[0m Trial 0 finished with value: 0.783240556716919 and parameters: {'base_lr': 0.00036441540609173276, 'last_lr': 0.003177859991698281, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 0 with value: 0.783240556716919.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00023988729374316924 last_lr 0.002189131852414667 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0274ae855d4baca1d2c420114b2fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.15 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8425 New best_val_rmse: 0.8425\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8537 Still best_val_rmse: 0.8425 (from epoch 0)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6857 New best_val_rmse: 0.6857\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8277 Still best_val_rmse: 0.6857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7544 Still best_val_rmse: 0.6857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.139 Still best_val_rmse: 0.6857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.036 Still best_val_rmse: 0.6857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.052 Still best_val_rmse: 0.6857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.031 Still best_val_rmse: 0.6857 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 08:32:47,900]\u001b[0m Trial 1 finished with value: 0.6856877207756042 and parameters: {'base_lr': 0.00023988729374316924, 'last_lr': 0.002189131852414667, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 1 with value: 0.6856877207756042.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.780635234620178e-05 last_lr 0.0006954354514239572 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73924f6d89994eebb52d6fa7c2987fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.22 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9141 New best_val_rmse: 0.9141\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.714 New best_val_rmse: 0.714\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6272 New best_val_rmse: 0.6272\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5942 New best_val_rmse: 0.5942\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7727 Still best_val_rmse: 0.5942 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7756 Still best_val_rmse: 0.5942 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6564 Still best_val_rmse: 0.5942 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5625 New best_val_rmse: 0.5625\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5088 New best_val_rmse: 0.5088\n",
      "\n",
      "16 steps took 7.57 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5468 Still best_val_rmse: 0.5088 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5294 Still best_val_rmse: 0.5088 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5747 Still best_val_rmse: 0.5088 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5322 Still best_val_rmse: 0.5088 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.532 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5958 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5088 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5558 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 7.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4972 New best_val_rmse: 0.4972\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4957 New best_val_rmse: 0.4957\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4869 New best_val_rmse: 0.4869\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4891 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.485 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4807 New best_val_rmse: 0.4807\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4836 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4817 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4813 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4833 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4852 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4821 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4803 New best_val_rmse: 0.4803\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4809 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4804 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4825 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4828 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4824 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4822 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4822 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4823 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4825 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.483 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4831 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.483 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.483 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 2.41 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4829 Still best_val_rmse: 0.4803 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 08:45:15,640]\u001b[0m Trial 2 finished with value: 0.4802732467651367 and parameters: {'base_lr': 4.780635234620178e-05, 'last_lr': 0.0006954354514239572, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 2 with value: 0.4802732467651367.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.0003087061763707969 last_lr 8.127963079369776e-05 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08bfd95c4cf470a90f079468dcda832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7998 New best_val_rmse: 0.7998\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.787 New best_val_rmse: 0.787\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8963 Still best_val_rmse: 0.787 (from epoch 0)\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.046 Still best_val_rmse: 0.787 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.126 Still best_val_rmse: 0.787 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.032 Still best_val_rmse: 0.787 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.03 Still best_val_rmse: 0.787 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.047 Still best_val_rmse: 0.787 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.028 Still best_val_rmse: 0.787 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 08:47:15,579]\u001b[0m Trial 3 finished with value: 0.7870222926139832 and parameters: {'base_lr': 0.0003087061763707969, 'last_lr': 8.127963079369776e-05, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 2 with value: 0.4802732467651367.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 6.847409068705247e-05 last_lr 0.00010431003759147297 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e426416522d4ab496a8c7dbba1bce15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.28 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7931 New best_val_rmse: 0.7931\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7086 New best_val_rmse: 0.7086\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7076 New best_val_rmse: 0.7076\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5976 New best_val_rmse: 0.5976\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7397 Still best_val_rmse: 0.5976 (from epoch 0)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7159 Still best_val_rmse: 0.5976 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5925 New best_val_rmse: 0.5925\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5344 New best_val_rmse: 0.5344\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5268 New best_val_rmse: 0.5268\n",
      "\n",
      "16 steps took 7.63 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.528 Still best_val_rmse: 0.5268 (from epoch 0)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5261 New best_val_rmse: 0.5261\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.783 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6725 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.604 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5449 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6134 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5304 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5281 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 7.57 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5149 Still best_val_rmse: 0.501 (from epoch 2)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4957 New best_val_rmse: 0.4957\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4957 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4924 New best_val_rmse: 0.4924\n",
      "\n",
      "8 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4961 Still best_val_rmse: 0.4924 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4937 Still best_val_rmse: 0.4924 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4928 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4943 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.493 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4933 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4937 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 4.21 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4935 Still best_val_rmse: 0.4909 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 08:56:27,765]\u001b[0m Trial 4 finished with value: 0.4909135699272156 and parameters: {'base_lr': 6.847409068705247e-05, 'last_lr': 0.00010431003759147297, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 2 with value: 0.4802732467651367.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 9.127623013883813e-05 last_lr 0.003341176648899971 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edc14fd02b4449096e5aecd87d12f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.26 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7567 New best_val_rmse: 0.7567\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7969 Still best_val_rmse: 0.7567 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7813 Still best_val_rmse: 0.7567 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5941 New best_val_rmse: 0.5941\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7574 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7346 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.022 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.049 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.02 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 7.47 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 1.029 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.033 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.023 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.029 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.018 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.016 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.03 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.03 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.028 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 7.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.016 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.015 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.016 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.014 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.016 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.9916 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.9557 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.8652 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.8155 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 7.61 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.7967 Still best_val_rmse: 0.5941 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 09:04:08,118]\u001b[0m Trial 5 finished with value: 0.5941014289855957 and parameters: {'base_lr': 9.127623013883813e-05, 'last_lr': 0.003341176648899971, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 2 with value: 0.4802732467651367.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.459228203146891e-05 last_lr 0.0005133665608312137 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e04fbba2c6a400b8b9b39c49a8b1051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9438 New best_val_rmse: 0.9438\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6558 New best_val_rmse: 0.6558\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7727 Still best_val_rmse: 0.6558 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6169 New best_val_rmse: 0.6169\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.608 New best_val_rmse: 0.608\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5636 New best_val_rmse: 0.5636\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5538 New best_val_rmse: 0.5538\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5312 New best_val_rmse: 0.5312\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5124 Still best_val_rmse: 0.501 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.527 Still best_val_rmse: 0.501 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.516 Still best_val_rmse: 0.501 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5407 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.51 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.495 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.602 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5401 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5312 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 7.59 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5107 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.485 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4909 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4874 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4932 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4842 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4845 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4983 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4859 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4956 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4847 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4891 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4844 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4874 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.485 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4876 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.485 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4832 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4861 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.487 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4845 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4827 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 2.55 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 09:16:22,398]\u001b[0m Trial 6 finished with value: 0.4816367030143738 and parameters: {'base_lr': 3.459228203146891e-05, 'last_lr': 0.0005133665608312137, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 2 with value: 0.4802732467651367.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.6367309419156636e-05 last_lr 0.004363169737106859 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd16b2bdc0d4eda9cb166099842f120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.31 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8612 New best_val_rmse: 0.8612\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6481 New best_val_rmse: 0.6481\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6127 New best_val_rmse: 0.6127\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6002 New best_val_rmse: 0.6002\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5699 New best_val_rmse: 0.5699\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5589 New best_val_rmse: 0.5589\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5424 New best_val_rmse: 0.5424\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5388 New best_val_rmse: 0.5388\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.4971 New best_val_rmse: 0.4971\n",
      "\n",
      "8 steps took 4.16 seconds\n",
      "Epoch: 1 batch_num: 4 val_rmse: 0.5207 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5177 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5026 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5034 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5366 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5054 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5088 Still best_val_rmse: 0.4971 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5181 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4966 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 4.19 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4955 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4942 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4901 New best_val_rmse: 0.4901\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4911 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4918 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4869 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4884 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4953 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.5006 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4902 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4997 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4969 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4931 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4895 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4887 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4894 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4911 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4873 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4868 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4869 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.51 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.487 Still best_val_rmse: 0.4857 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 09:28:12,035]\u001b[0m Trial 7 finished with value: 0.48571962118148804 and parameters: {'base_lr': 4.6367309419156636e-05, 'last_lr': 0.004363169737106859, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 2 with value: 0.4802732467651367.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.83771654329969e-05 last_lr 0.00020755541027795988 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec11c4c5f56480fbf1b5e1eae5b59b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8417 New best_val_rmse: 0.8417\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7081 New best_val_rmse: 0.7081\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6384 New best_val_rmse: 0.6384\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6331 New best_val_rmse: 0.6331\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5941 New best_val_rmse: 0.5941\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6168 Still best_val_rmse: 0.5941 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5554 New best_val_rmse: 0.5554\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5327 New best_val_rmse: 0.5327\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 7.67 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5185 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5175 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5069 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5048 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5179 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5438 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6046 Still best_val_rmse: 0.5042 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5034 New best_val_rmse: 0.5034\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5395 Still best_val_rmse: 0.5034 (from epoch 1)\n",
      "\n",
      "16 steps took 7.69 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4902 New best_val_rmse: 0.4902\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4861 New best_val_rmse: 0.4861\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4838 New best_val_rmse: 0.4838\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4851 Still best_val_rmse: 0.4838 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4869 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4785 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4801 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4786 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4826 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4869 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4819 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4812 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4806 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4798 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4793 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4789 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4789 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4791 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4792 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4806 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4821 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4821 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4819 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4815 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4818 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4817 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4817 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 2.59 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4815 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4815 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4814 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4809 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4802 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4794 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4793 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4793 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4793 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4794 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4798 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4807 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4827 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4813 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.479 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4786 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4786 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4784 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4785 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4788 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4791 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4805 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4806 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.482 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4838 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4839 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4837 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4782 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.48 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4815 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4838 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4805 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4788 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4817 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.52 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4975 Still best_val_rmse: 0.4773 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 09:42:21,399]\u001b[0m Trial 8 finished with value: 0.47732898592948914 and parameters: {'base_lr': 4.83771654329969e-05, 'last_lr': 0.00020755541027795988, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.494553497559106e-05 last_lr 0.002286759469789224 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46b5e4048194aa78e394d77f1f78b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.27 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.846 New best_val_rmse: 0.846\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8383 New best_val_rmse: 0.8383\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7126 New best_val_rmse: 0.7126\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6087 New best_val_rmse: 0.6087\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.558 New best_val_rmse: 0.558\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5963 Still best_val_rmse: 0.558 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6147 Still best_val_rmse: 0.558 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5333 New best_val_rmse: 0.5333\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5035 New best_val_rmse: 0.5035\n",
      "\n",
      "16 steps took 7.71 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5154 Still best_val_rmse: 0.5035 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5279 Still best_val_rmse: 0.5035 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5194 Still best_val_rmse: 0.5035 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.5138 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.5524 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.6196 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4936 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.634 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5681 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 7.7 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4989 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4924 Still best_val_rmse: 0.4873 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4851 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4866 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4909 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4826 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4831 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4824 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4832 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.483 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.483 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4869 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4876 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4852 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4846 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4849 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4848 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4845 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4844 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4841 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.484 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.484 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4838 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4836 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.52 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4833 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4834 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4835 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.484 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4842 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4839 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4838 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4832 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4827 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4823 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.482 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4822 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4826 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4828 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4848 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4918 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4881 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4845 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4819 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4899 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4918 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4886 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4874 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4891 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4975 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4864 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4851 Still best_val_rmse: 0.4817 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4915 Still best_val_rmse: 0.4817 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 09:54:47,738]\u001b[0m Trial 9 finished with value: 0.48171520233154297 and parameters: {'base_lr': 4.494553497559106e-05, 'last_lr': 0.002286759469789224, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00015812749171121394 last_lr 0.00023607131299275327 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ecda5ecba43e9a91bf10f7960a49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.43 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7713 New best_val_rmse: 0.7713\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.5994 New best_val_rmse: 0.5994\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7142 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.622 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7147 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6233 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7802 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.105 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.053 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 1.134 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.03 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.026 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.026 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.023 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.028 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.058 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.02 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.035 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.017 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.019 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.017 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.019 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.032 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.019 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 7.61 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 1.018 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 1.017 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 1.019 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 1.02 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 1.017 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 1.021 Still best_val_rmse: 0.5994 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 10:02:33,029]\u001b[0m Trial 10 finished with value: 0.5993736386299133 and parameters: {'base_lr': 0.00015812749171121394, 'last_lr': 0.00023607131299275327, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.161028236422198e-05 last_lr 0.0008175960669692805 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564acd6dd1ba47b794ac287427f5d4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.77 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9542 New best_val_rmse: 0.9542\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6614 New best_val_rmse: 0.6614\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8207 Still best_val_rmse: 0.6614 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6414 New best_val_rmse: 0.6414\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6085 New best_val_rmse: 0.6085\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5647 New best_val_rmse: 0.5647\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5541 New best_val_rmse: 0.5541\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5354 New best_val_rmse: 0.5354\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5056 New best_val_rmse: 0.5056\n",
      "\n",
      "16 steps took 7.72 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5161 Still best_val_rmse: 0.5056 (from epoch 0)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.528 Still best_val_rmse: 0.5056 (from epoch 0)\n",
      "\n",
      "16 steps took 6.87 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5209 Still best_val_rmse: 0.5056 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.489 New best_val_rmse: 0.489\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.5152 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.543 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5643 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4911 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.6188 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "16 steps took 6.87 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.545 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "16 steps took 7.74 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4918 Still best_val_rmse: 0.489 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4875 New best_val_rmse: 0.4875\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5053 Still best_val_rmse: 0.4875 (from epoch 2)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4818 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4812 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4815 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4829 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4862 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4811 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4801 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4819 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4836 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4849 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4846 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4806 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.481 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4812 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4803 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4801 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.74 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.856 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.859 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 10:19:52,857]\u001b[0m Trial 11 finished with value: 0.4796476662158966 and parameters: {'base_lr': 3.161028236422198e-05, 'last_lr': 0.0008175960669692805, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.0853807289308066e-05 last_lr 0.0008167003505499255 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e97d25f2b546008c3ffb67ff02b0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.64 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9533 New best_val_rmse: 0.9533\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6581 New best_val_rmse: 0.6581\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8495 Still best_val_rmse: 0.6581 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6414 New best_val_rmse: 0.6414\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5994 New best_val_rmse: 0.5994\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5564 New best_val_rmse: 0.5564\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5476 New best_val_rmse: 0.5476\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5346 New best_val_rmse: 0.5346\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5051 New best_val_rmse: 0.5051\n",
      "\n",
      "16 steps took 7.67 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5124 Still best_val_rmse: 0.5051 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5332 Still best_val_rmse: 0.5051 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.51 Still best_val_rmse: 0.5051 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.5107 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.547 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5507 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4919 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.6225 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5428 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "16 steps took 7.64 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4916 Still best_val_rmse: 0.4872 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5039 Still best_val_rmse: 0.4865 (from epoch 2)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4796 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4804 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4803 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4807 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4822 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4857 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4803 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4811 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.484 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4839 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4845 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4815 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4796 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4802 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4803 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4791 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.72 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4795 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4797 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4798 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4802 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4802 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4785 New best_val_rmse: 0.4785\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4785 New best_val_rmse: 0.4785\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4783 New best_val_rmse: 0.4783\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4785 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.479 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4793 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4794 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4795 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4797 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4793 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4794 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4801 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4817 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4845 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4796 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4812 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4813 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4828 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4927 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4796 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4801 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4855 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4796 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4799 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4805 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.5034 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4795 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.49 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4809 Still best_val_rmse: 0.4781 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 10:35:30,146]\u001b[0m Trial 12 finished with value: 0.47806230187416077 and parameters: {'base_lr': 3.0853807289308066e-05, 'last_lr': 0.0008167003505499255, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 7.202450965644526e-05 last_lr 0.0002662497623801496 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de1f52dc1f54bc184e2824c6abe4483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7957 New best_val_rmse: 0.7957\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7299 New best_val_rmse: 0.7299\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6803 New best_val_rmse: 0.6803\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6084 New best_val_rmse: 0.6084\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7393 Still best_val_rmse: 0.6084 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5994 New best_val_rmse: 0.5994\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7005 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5634 New best_val_rmse: 0.5634\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5327 New best_val_rmse: 0.5327\n",
      "\n",
      "16 steps took 7.71 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5321 New best_val_rmse: 0.5321\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5793 Still best_val_rmse: 0.5321 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5048 New best_val_rmse: 0.5048\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.523 Still best_val_rmse: 0.5048 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5261 Still best_val_rmse: 0.5048 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5416 Still best_val_rmse: 0.5048 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5327 Still best_val_rmse: 0.5048 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5025 New best_val_rmse: 0.5025\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5125 Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 7.66 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4899 New best_val_rmse: 0.4899\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4893 Still best_val_rmse: 0.4889 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5094 Still best_val_rmse: 0.4889 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4927 Still best_val_rmse: 0.4889 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4949 Still best_val_rmse: 0.4889 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4873 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.5049 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4891 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4884 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4869 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4864 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4885 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4914 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4902 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4896 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4896 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.49 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4908 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4908 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4909 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 4.26 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4909 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4907 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4902 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4896 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4893 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4888 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.489 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4902 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4902 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4889 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4901 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4898 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4879 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4894 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4912 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4907 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4915 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4896 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4903 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4976 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4952 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.5043 Still best_val_rmse: 0.4849 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 10:45:46,352]\u001b[0m Trial 13 finished with value: 0.4849034249782562 and parameters: {'base_lr': 7.202450965644526e-05, 'last_lr': 0.0002662497623801496, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.0331609998792046e-05 last_lr 0.0011378933352841844 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12abef21bced4e2a868ae48a5a93c37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.05 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9618 New best_val_rmse: 0.9618\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6743 New best_val_rmse: 0.6743\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8058 Still best_val_rmse: 0.6743 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6437 New best_val_rmse: 0.6437\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6186 New best_val_rmse: 0.6186\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5726 New best_val_rmse: 0.5726\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5614 New best_val_rmse: 0.5614\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5443 New best_val_rmse: 0.5443\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5066 New best_val_rmse: 0.5066\n",
      "\n",
      "16 steps took 7.63 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5175 Still best_val_rmse: 0.5066 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.525 Still best_val_rmse: 0.5066 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5265 Still best_val_rmse: 0.5066 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4941 New best_val_rmse: 0.4941\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5347 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5055 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5044 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5292 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.609 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 7.71 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.557 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.502 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4874 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4855 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4811 New best_val_rmse: 0.4811\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4862 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4857 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4805 New best_val_rmse: 0.4805\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4819 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.483 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4812 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4792 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4794 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4791 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4792 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4803 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4819 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4814 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4814 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4816 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4821 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4826 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4826 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4825 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 2.51 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4824 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4823 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4822 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4818 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4818 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.482 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4818 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4815 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4814 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4811 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4813 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4812 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4808 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4803 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4795 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4796 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4799 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4817 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4938 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4845 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4871 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.48 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4822 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4867 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4905 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4846 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4897 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4969 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4844 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4871 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4871 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4891 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4866 Still best_val_rmse: 0.4789 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 10:58:26,692]\u001b[0m Trial 14 finished with value: 0.4788970351219177 and parameters: {'base_lr': 3.0331609998792046e-05, 'last_lr': 0.0011378933352841844, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.0001294076948945281 last_lr 0.00028744513677986683 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff10f1437e31439d8d1d289fc4ac1f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.18 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9472 New best_val_rmse: 0.9472\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6891 New best_val_rmse: 0.6891\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.5997 New best_val_rmse: 0.5997\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.567 New best_val_rmse: 0.567\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6768 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6339 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6031 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6626 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6415 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 7.73 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5497 New best_val_rmse: 0.5497\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5579 Still best_val_rmse: 0.5497 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5457 New best_val_rmse: 0.5457\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5335 New best_val_rmse: 0.5335\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.533 New best_val_rmse: 0.533\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5705 Still best_val_rmse: 0.533 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6118 Still best_val_rmse: 0.533 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5319 New best_val_rmse: 0.5319\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5569 Still best_val_rmse: 0.5319 (from epoch 1)\n",
      "\n",
      "16 steps took 7.73 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4845 New best_val_rmse: 0.4845\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4938 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5227 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.495 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.498 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4899 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4978 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5009 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4934 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4932 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.5021 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4988 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4982 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4977 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4977 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4977 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 4.3 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4977 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4976 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4971 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4966 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4969 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4983 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4988 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.5016 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4976 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4995 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.5054 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.5144 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4963 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5046 Still best_val_rmse: 0.4845 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 11:07:25,036]\u001b[0m Trial 15 finished with value: 0.4845429062843323 and parameters: {'base_lr': 0.0001294076948945281, 'last_lr': 0.00028744513677986683, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 6.259726077235118e-05 last_lr 0.00013159160088084833 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801f904bdb1c443693dbfc12a34c2c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.11 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7842 New best_val_rmse: 0.7842\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6833 New best_val_rmse: 0.6833\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.5924 New best_val_rmse: 0.5924\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6116 Still best_val_rmse: 0.5924 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7905 Still best_val_rmse: 0.5924 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5776 New best_val_rmse: 0.5776\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6008 Still best_val_rmse: 0.5776 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5979 Still best_val_rmse: 0.5776 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5284 New best_val_rmse: 0.5284\n",
      "\n",
      "16 steps took 7.63 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5593 Still best_val_rmse: 0.5284 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.517 New best_val_rmse: 0.517\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5324 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5188 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5098 New best_val_rmse: 0.5098\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5549 Still best_val_rmse: 0.5098 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6407 Still best_val_rmse: 0.5098 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5025 New best_val_rmse: 0.5025\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5435 Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 7.77 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4953 Still best_val_rmse: 0.4937 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4939 Still best_val_rmse: 0.4937 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4895 New best_val_rmse: 0.4895\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4912 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4933 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.483 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4874 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4912 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4899 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4866 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4848 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4843 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4848 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.487 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4911 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4907 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4898 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4895 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4895 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4895 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4895 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4894 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 2.62 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4894 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4894 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4892 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.489 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4888 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4883 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4876 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4873 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4875 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4878 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4898 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4881 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4883 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4877 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4879 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4876 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4887 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4854 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.487 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.493 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4859 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4852 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4857 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4893 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4918 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.498 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4991 Still best_val_rmse: 0.4827 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 11:19:14,306]\u001b[0m Trial 16 finished with value: 0.48270556330680847 and parameters: {'base_lr': 6.259726077235118e-05, 'last_lr': 0.00013159160088084833, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 8 with value: 0.47732898592948914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.9152303052330085e-05 last_lr 0.00045881520547034934 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb9fdb8604846518fbba95569f725fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.925 New best_val_rmse: 0.925\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6769 New best_val_rmse: 0.6769\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7648 Still best_val_rmse: 0.6769 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6235 New best_val_rmse: 0.6235\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6412 Still best_val_rmse: 0.6235 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7794 Still best_val_rmse: 0.6235 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6563 Still best_val_rmse: 0.6235 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5978 New best_val_rmse: 0.5978\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 7.77 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5201 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5152 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.549 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5028 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5049 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5294 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5784 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5203 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5276 Still best_val_rmse: 0.5013 (from epoch 0)\n",
      "\n",
      "16 steps took 7.81 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4894 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4898 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.51 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4806 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4804 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4762 New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4759 New best_val_rmse: 0.4759\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4768 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4765 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4769 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4791 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4816 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4793 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4787 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4788 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4798 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4803 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4779 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4772 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4759 New best_val_rmse: 0.4759\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4755 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4766 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4775 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4781 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4783 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4782 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4781 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4779 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.478 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4779 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4781 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4782 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4784 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4787 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4789 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.479 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.479 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.479 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4789 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4788 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4788 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4787 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4786 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4785 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4785 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4784 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4783 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4781 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4779 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4778 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4777 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4775 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4773 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4776 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.478 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4782 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4778 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4775 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4775 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4776 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4779 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4779 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4777 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4766 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4757 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4754 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4758 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.476 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4767 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.478 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4792 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4797 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4776 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4767 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4757 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4752 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4755 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4767 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4805 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4882 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4774 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4805 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4864 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4926 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4795 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4917 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4997 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.484 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.5213 Still best_val_rmse: 0.4752 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 11:36:02,505]\u001b[0m Trial 17 finished with value: 0.4751589596271515 and parameters: {'base_lr': 3.9152303052330085e-05, 'last_lr': 0.00045881520547034934, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 17 with value: 0.4751589596271515.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 9.754731464209269e-05 last_lr 0.00017501852530402615 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f6b8477ca24db2a83ceaa62ae30e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.41 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.6939 New best_val_rmse: 0.6939\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7512 Still best_val_rmse: 0.6939 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.5857 New best_val_rmse: 0.5857\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5895 Still best_val_rmse: 0.5857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7318 Still best_val_rmse: 0.5857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7076 Still best_val_rmse: 0.5857 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5771 New best_val_rmse: 0.5771\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5701 New best_val_rmse: 0.5701\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5205 New best_val_rmse: 0.5205\n",
      "\n",
      "16 steps took 7.71 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5263 Still best_val_rmse: 0.5205 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5329 Still best_val_rmse: 0.5205 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5204 New best_val_rmse: 0.5204\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5479 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5296 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5649 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5821 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5222 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5497 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 7.73 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4925 New best_val_rmse: 0.4925\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.495 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4973 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4959 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5011 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4931 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5038 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4925 New best_val_rmse: 0.4925\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4899 New best_val_rmse: 0.4899\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4917 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4958 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4951 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4935 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4941 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4945 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4945 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 4.29 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4945 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4944 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4941 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4934 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4925 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4926 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.494 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4949 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4976 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4911 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4913 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4966 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.498 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.5117 Still best_val_rmse: 0.4893 (from epoch 3)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5042 Still best_val_rmse: 0.4893 (from epoch 3)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.497 Still best_val_rmse: 0.4893 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 11:45:17,878]\u001b[0m Trial 18 finished with value: 0.48931685090065 and parameters: {'base_lr': 9.754731464209269e-05, 'last_lr': 0.00017501852530402615, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 17 with value: 0.4751589596271515.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.981758887999553e-05 last_lr 0.00040821877556246653 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0011693738dc433e8272d9bcd9c5159e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.35 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9201 New best_val_rmse: 0.9201\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6768 New best_val_rmse: 0.6768\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7356 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6062 New best_val_rmse: 0.6062\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6594 Still best_val_rmse: 0.6062 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7561 Still best_val_rmse: 0.6062 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6489 Still best_val_rmse: 0.6062 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5917 New best_val_rmse: 0.5917\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.509 New best_val_rmse: 0.509\n",
      "\n",
      "16 steps took 7.71 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5209 Still best_val_rmse: 0.509 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5185 Still best_val_rmse: 0.509 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5415 Still best_val_rmse: 0.509 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5031 New best_val_rmse: 0.5031\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5175 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5274 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5812 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.516 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5282 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 7.74 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4906 New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4907 Still best_val_rmse: 0.4906 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4915 Still best_val_rmse: 0.4906 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4843 New best_val_rmse: 0.4843\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4838 New best_val_rmse: 0.4838\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4846 Still best_val_rmse: 0.4838 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4806 New best_val_rmse: 0.4806\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4802 New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4807 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.484 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4847 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4827 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4831 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4831 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4818 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4799 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "2 steps took 0.858 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4801 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4818 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4825 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4824 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4825 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4826 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4828 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.483 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4833 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4833 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4831 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.483 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.58 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4829 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4827 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4826 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4828 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4826 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4823 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4818 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4812 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4808 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4809 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4811 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4816 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.482 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4817 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4827 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4882 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4923 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4857 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4812 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4853 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4849 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4864 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4846 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4872 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4839 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.5006 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4828 Still best_val_rmse: 0.4798 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4919 Still best_val_rmse: 0.4798 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 11:57:50,560]\u001b[0m Trial 19 finished with value: 0.4797709584236145 and parameters: {'base_lr': 3.981758887999553e-05, 'last_lr': 0.00040821877556246653, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 17 with value: 0.4751589596271515.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.4751589596271515\n",
      " Best params: \n",
      "    base_lr: 3.9152303052330085e-05\n",
      "    last_lr: 0.00045881520547034934\n",
      "    schedule_func: <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 11:57:50,573]\u001b[0m A new study created in memory with name: no-name-73d48b9f-fa79-4594-bfb3-1bc81cf3326d\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 4\n",
      "##### Using base_lr 5.367902443436466e-05 last_lr 0.0017631891864399083 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403b45ccd9a54fd8b4b7576253566abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.31 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9587 New best_val_rmse: 0.9587\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8325 New best_val_rmse: 0.8325\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6585 New best_val_rmse: 0.6585\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7102 Still best_val_rmse: 0.6585 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6447 New best_val_rmse: 0.6447\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5472 New best_val_rmse: 0.5472\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5518 Still best_val_rmse: 0.5472 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5713 Still best_val_rmse: 0.5472 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5624 Still best_val_rmse: 0.5472 (from epoch 0)\n",
      "\n",
      "16 steps took 7.74 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5222 New best_val_rmse: 0.5222\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6032 Still best_val_rmse: 0.5222 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5091 New best_val_rmse: 0.5091\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5188 Still best_val_rmse: 0.5091 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5324 Still best_val_rmse: 0.5091 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5275 Still best_val_rmse: 0.5091 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5111 Still best_val_rmse: 0.5091 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5426 Still best_val_rmse: 0.5091 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 7.76 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4946 New best_val_rmse: 0.4946\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.538 Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.5025 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4895 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.487 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.498 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4911 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4857 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4821 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.483 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4797 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4814 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4842 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4805 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4898 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4822 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4797 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4804 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4802 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4812 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4806 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 2.62 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 12:10:10,085]\u001b[0m Trial 0 finished with value: 0.4794856011867523 and parameters: {'base_lr': 5.367902443436466e-05, 'last_lr': 0.0017631891864399083, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 0 with value: 0.4794856011867523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00010232124786342152 last_lr 0.0003223558976487016 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928b735d90064dac97c05b67da5f5f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.39 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.6663 New best_val_rmse: 0.6663\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8166 Still best_val_rmse: 0.6663 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6892 Still best_val_rmse: 0.6663 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6463 New best_val_rmse: 0.6463\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.57 New best_val_rmse: 0.57\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5987 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6046 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.595 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5565 New best_val_rmse: 0.5565\n",
      "\n",
      "16 steps took 7.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5181 New best_val_rmse: 0.5181\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5747 Still best_val_rmse: 0.5181 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.517 New best_val_rmse: 0.517\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.603 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5429 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5354 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5304 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5782 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5078 New best_val_rmse: 0.5078\n",
      "\n",
      "16 steps took 7.85 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5153 Still best_val_rmse: 0.5078 (from epoch 1)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5134 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4999 Still best_val_rmse: 0.4926 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4908 New best_val_rmse: 0.4908\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4892 New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4889 Still best_val_rmse: 0.4888 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4878 New best_val_rmse: 0.4878\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4933 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4891 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.5043 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4902 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4884 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4879 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 2.6 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4876 Still best_val_rmse: 0.4876 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 12:21:32,549]\u001b[0m Trial 1 finished with value: 0.48757752776145935 and parameters: {'base_lr': 0.00010232124786342152, 'last_lr': 0.0003223558976487016, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 0 with value: 0.4794856011867523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 9.064991925778386e-05 last_lr 0.0017903866786376018 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a7f47132e840bf9b96dda65b14793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.42 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7022 New best_val_rmse: 0.7022\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7612 Still best_val_rmse: 0.7022 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6968 New best_val_rmse: 0.6968\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7397 Still best_val_rmse: 0.6968 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6659 New best_val_rmse: 0.6659\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6736 Still best_val_rmse: 0.6659 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6483 New best_val_rmse: 0.6483\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6113 New best_val_rmse: 0.6113\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5594 New best_val_rmse: 0.5594\n",
      "\n",
      "16 steps took 7.85 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5464 New best_val_rmse: 0.5464\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6083 Still best_val_rmse: 0.5464 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5853 Still best_val_rmse: 0.5464 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6402 Still best_val_rmse: 0.5464 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5683 Still best_val_rmse: 0.5464 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5283 New best_val_rmse: 0.5283\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5439 Still best_val_rmse: 0.5283 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5246 New best_val_rmse: 0.5246\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5382 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 7.79 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5095 New best_val_rmse: 0.5095\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4973 New best_val_rmse: 0.4973\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4949 New best_val_rmse: 0.4949\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5048 Still best_val_rmse: 0.4949 (from epoch 2)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4971 Still best_val_rmse: 0.4949 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4927 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4929 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4935 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.497 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4927 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4921 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4924 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4923 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 4.32 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.43 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4922 Still best_val_rmse: 0.4918 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 12:30:56,121]\u001b[0m Trial 2 finished with value: 0.4918349087238312 and parameters: {'base_lr': 9.064991925778386e-05, 'last_lr': 0.0017903866786376018, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 0 with value: 0.4794856011867523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.915517553331498e-05 last_lr 0.00010853696092788668 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d4c521536144599cadc469c1ba8c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.47 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9286 New best_val_rmse: 0.9286\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6886 New best_val_rmse: 0.6886\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6681 New best_val_rmse: 0.6681\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6605 New best_val_rmse: 0.6605\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5584 New best_val_rmse: 0.5584\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5603 Still best_val_rmse: 0.5584 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6336 Still best_val_rmse: 0.5584 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6052 Still best_val_rmse: 0.5584 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5515 New best_val_rmse: 0.5515\n",
      "\n",
      "16 steps took 7.68 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5612 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5676 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5056 New best_val_rmse: 0.5056\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5544 Still best_val_rmse: 0.5056 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.523 Still best_val_rmse: 0.5056 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5046 New best_val_rmse: 0.5046\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5142 Still best_val_rmse: 0.5046 (from epoch 1)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5497 Still best_val_rmse: 0.5046 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.495 New best_val_rmse: 0.495\n",
      "\n",
      "8 steps took 4.43 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4989 Still best_val_rmse: 0.495 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4993 Still best_val_rmse: 0.495 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4857 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.482 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.479 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4787 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4787 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.479 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4814 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.479 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4796 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4804 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4791 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4793 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4796 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4799 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4791 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.479 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4797 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4795 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4788 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.856 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4785 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.859 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4784 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4789 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4798 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4788 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 0.86 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.478 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4782 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4782 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4783 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4784 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4784 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4784 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4783 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4782 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4782 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.856 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.83 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.857 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.857 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.856 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4781 Still best_val_rmse: 0.4779 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 12:50:07,164]\u001b[0m Trial 3 finished with value: 0.4779181480407715 and parameters: {'base_lr': 5.915517553331498e-05, 'last_lr': 0.00010853696092788668, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 3 with value: 0.4779181480407715.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001519886450854694 last_lr 0.00018436903249022105 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600d6392d8cf4160b69e1e81d7bfe6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.47 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7983 New best_val_rmse: 0.7983\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7618 New best_val_rmse: 0.7618\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6457 New best_val_rmse: 0.6457\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7219 Still best_val_rmse: 0.6457 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8383 Still best_val_rmse: 0.6457 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.09 Still best_val_rmse: 0.6457 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.087 Still best_val_rmse: 0.6457 (from epoch 0)\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.207 Still best_val_rmse: 0.6457 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.035 Still best_val_rmse: 0.6457 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 12:52:10,358]\u001b[0m Trial 4 finished with value: 0.6457427740097046 and parameters: {'base_lr': 0.0001519886450854694, 'last_lr': 0.00018436903249022105, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 3 with value: 0.4779181480407715.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.395940545935742e-05 last_lr 0.0008065830888769318 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77a6fdf772c42419c6a5e6f82f60e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.51 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8315 New best_val_rmse: 0.8315\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.692 New best_val_rmse: 0.692\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6728 New best_val_rmse: 0.6728\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7258 Still best_val_rmse: 0.6728 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5519 New best_val_rmse: 0.5519\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6948 Still best_val_rmse: 0.5519 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5547 Still best_val_rmse: 0.5519 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5488 New best_val_rmse: 0.5488\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5267 New best_val_rmse: 0.5267\n",
      "\n",
      "16 steps took 7.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5324 Still best_val_rmse: 0.5267 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5531 Still best_val_rmse: 0.5267 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5214 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5067 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 6.87 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4969 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4992 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "8 steps took 3.43 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5263 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4957 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "8 steps took 4.34 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4972 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5121 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4787 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4785 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4801 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.478 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4781 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4797 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4791 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4796 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4796 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4786 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4797 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4886 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4846 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4756 New best_val_rmse: 0.4756\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4762 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4778 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4777 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4767 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4771 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4766 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4757 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.858 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4757 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4762 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4762 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4763 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4763 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4763 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4765 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4768 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.477 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4772 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4771 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.477 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4769 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4768 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4767 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4766 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4765 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4765 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4764 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4763 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4763 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4763 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4762 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.476 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4761 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4759 Still best_val_rmse: 0.4756 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4756 New best_val_rmse: 0.4756\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.475 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4759 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4768 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.477 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4759 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4764 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4774 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4767 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4758 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.476 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4758 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4751 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4757 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4756 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4762 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4777 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4789 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4783 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4766 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4784 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4793 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4773 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4777 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4795 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.481 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4769 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4778 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4795 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4788 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.482 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.479 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4814 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4803 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4839 Still best_val_rmse: 0.4748 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4801 Still best_val_rmse: 0.4748 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 13:10:58,220]\u001b[0m Trial 5 finished with value: 0.4748414158821106 and parameters: {'base_lr': 4.395940545935742e-05, 'last_lr': 0.0008065830888769318, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 5 with value: 0.4748414158821106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.0698898416265e-05 last_lr 0.000984842100902591 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7aa0d6938cf48618468ee4ce5209873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.43 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8228 New best_val_rmse: 0.8228\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7661 New best_val_rmse: 0.7661\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7741 Still best_val_rmse: 0.7661 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7599 New best_val_rmse: 0.7599\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6025 New best_val_rmse: 0.6025\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5548 New best_val_rmse: 0.5548\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5682 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5552 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5529 New best_val_rmse: 0.5529\n",
      "\n",
      "16 steps took 7.82 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5312 New best_val_rmse: 0.5312\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5703 Still best_val_rmse: 0.5312 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5038 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5449 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5122 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5188 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5045 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4979 New best_val_rmse: 0.4979\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 4.41 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5089 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4832 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5005 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4874 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4793 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4829 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4801 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5007 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4808 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4811 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.484 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4862 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4832 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4805 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4828 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4795 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4808 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.483 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.479 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4826 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4808 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4806 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4795 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4793 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4792 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4795 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4796 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4794 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4794 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4792 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 1.84 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.856 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.857 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4791 Still best_val_rmse: 0.4763 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 13:28:00,764]\u001b[0m Trial 6 finished with value: 0.4762722849845886 and parameters: {'base_lr': 4.0698898416265e-05, 'last_lr': 0.000984842100902591, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 5 with value: 0.4748414158821106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.594978157177946e-05 last_lr 0.0005315884417805023 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa8814a657e458a942990a8e790c5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.44 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8664 New best_val_rmse: 0.8664\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8298 New best_val_rmse: 0.8298\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.5908 New best_val_rmse: 0.5908\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6833 Still best_val_rmse: 0.5908 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5848 New best_val_rmse: 0.5848\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5713 New best_val_rmse: 0.5713\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5937 Still best_val_rmse: 0.5713 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5484 New best_val_rmse: 0.5484\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5459 New best_val_rmse: 0.5459\n",
      "\n",
      "16 steps took 7.78 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5389 New best_val_rmse: 0.5389\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5276 New best_val_rmse: 0.5276\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4971 New best_val_rmse: 0.4971\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.507 Still best_val_rmse: 0.4971 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5319 Still best_val_rmse: 0.4971 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5092 Still best_val_rmse: 0.4971 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5076 Still best_val_rmse: 0.4971 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5041 Still best_val_rmse: 0.4971 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4958 New best_val_rmse: 0.4958\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4902 New best_val_rmse: 0.4902\n",
      "\n",
      "8 steps took 4.38 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5146 Still best_val_rmse: 0.4902 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.497 Still best_val_rmse: 0.4902 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4858 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4785 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4799 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4789 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4785 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4781 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4788 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4824 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4804 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4809 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4836 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4786 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4821 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4865 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4814 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4784 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4785 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4784 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4781 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4778 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4776 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4772 New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4769 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.477 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.477 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.477 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.477 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 1.88 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.855 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.856 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.881 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4771 Still best_val_rmse: 0.4769 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 13:46:52,255]\u001b[0m Trial 7 finished with value: 0.47688353061676025 and parameters: {'base_lr': 3.594978157177946e-05, 'last_lr': 0.0005315884417805023, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7f15aa1cc3a0>}. Best is trial 5 with value: 0.4748414158821106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00016206730618600128 last_lr 0.0013590986322907508 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162b0075146b463085921c80bfef00c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8994 New best_val_rmse: 0.8994\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.021 Still best_val_rmse: 0.8994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6745 New best_val_rmse: 0.6745\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.623 New best_val_rmse: 0.623\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8453 Still best_val_rmse: 0.623 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8674 Still best_val_rmse: 0.623 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6149 New best_val_rmse: 0.6149\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5716 New best_val_rmse: 0.5716\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5933 Still best_val_rmse: 0.5716 (from epoch 0)\n",
      "\n",
      "16 steps took 7.78 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5201 New best_val_rmse: 0.5201\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5865 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.7442 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6575 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.578 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5869 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5448 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5174 New best_val_rmse: 0.5174\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5289 Still best_val_rmse: 0.5174 (from epoch 1)\n",
      "\n",
      "16 steps took 7.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5185 Still best_val_rmse: 0.5174 (from epoch 1)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5114 New best_val_rmse: 0.5114\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5387 Still best_val_rmse: 0.5114 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5112 New best_val_rmse: 0.5112\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5069 New best_val_rmse: 0.5069\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4978 New best_val_rmse: 0.4978\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.5068 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.5058 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.5017 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 7.73 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.5015 Still best_val_rmse: 0.4978 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 13:54:50,244]\u001b[0m Trial 8 finished with value: 0.49775615334510803 and parameters: {'base_lr': 0.00016206730618600128, 'last_lr': 0.0013590986322907508, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 5 with value: 0.4748414158821106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00030465619438912247 last_lr 0.0003418094199158506 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c2e58720164bab80428f318d46336d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.111 New best_val_rmse: 1.111\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9562 New best_val_rmse: 0.9562\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.846 New best_val_rmse: 0.846\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8778 Still best_val_rmse: 0.846 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9779 Still best_val_rmse: 0.846 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.059 Still best_val_rmse: 0.846 (from epoch 0)\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.193 Still best_val_rmse: 0.846 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 13:56:26,095]\u001b[0m Trial 9 finished with value: 0.8460375666618347 and parameters: {'base_lr': 0.00030465619438912247, 'last_lr': 0.0003418094199158506, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7f15aa1cc280>}. Best is trial 5 with value: 0.4748414158821106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00045174510734207005 last_lr 0.004849030517443048 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327c4739ef83446983dadce18e08a3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.44 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.178 New best_val_rmse: 1.178\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.099 New best_val_rmse: 1.099\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.087 New best_val_rmse: 1.087\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.039 New best_val_rmse: 1.039\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.039 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.035 New best_val_rmse: 1.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-31 13:57:48,023]\u001b[0m Trial 10 finished with value: 1.0347208976745605 and parameters: {'base_lr': 0.00045174510734207005, 'last_lr': 0.004849030517443048, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>}. Best is trial 5 with value: 0.4748414158821106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.758085868459081e-05 last_lr 0.0009708867541466488 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7f15aa1cc310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87777162998644d7872edf58bee9d486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.27 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8271 New best_val_rmse: 0.8271\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8755 Still best_val_rmse: 0.8271 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7494 New best_val_rmse: 0.7494\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8066 Still best_val_rmse: 0.7494 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6462 New best_val_rmse: 0.6462\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5594 New best_val_rmse: 0.5594\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5968 Still best_val_rmse: 0.5594 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.521 New best_val_rmse: 0.521\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5169 New best_val_rmse: 0.5169\n",
      "\n",
      "16 steps took 7.73 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5397 Still best_val_rmse: 0.5169 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5429 Still best_val_rmse: 0.5169 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5021 New best_val_rmse: 0.5021\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5175 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5204 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5129 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5188 Still best_val_rmse: 0.5013 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 4.31 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5154 Still best_val_rmse: 0.4954 (from epoch 1)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.492 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4833 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.477 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4774 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4783 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4782 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4785 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4792 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.479 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4793 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4801 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4786 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4786 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4785 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4784 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(4, 5):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360e24c-6ca3-4486-b2ba-27d94cb53913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
