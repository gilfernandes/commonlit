{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fb78f9f4070>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(train_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 4\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = '/home/commonlit/models/deberta-large-lm/best_lm'\n",
    "    TOKENIZER_PATH = '/home/commonlit/models/deberta-large-lm/best_lm'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'deberta-large-uniform'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f7d823-b5fd-4864-a055-5a17b8120935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    886\n",
       "2    784\n",
       "4    494\n",
       "1    441\n",
       "0    122\n",
       "5    106\n",
       "Name: bins, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{len(self.df)} records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e598f76-64f9-4767-853d-f4e150dafcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2833 records"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "1 transformer_model.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "2 transformer_model.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "3 transformer_model.encoder.layer.0.attention.self.q_bias torch.Size([1024])\n",
      "4 transformer_model.encoder.layer.0.attention.self.v_bias torch.Size([1024])\n",
      "5 transformer_model.encoder.layer.0.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "6 transformer_model.encoder.layer.0.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "7 transformer_model.encoder.layer.0.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.encoder.layer.0.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "9 transformer_model.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "11 transformer_model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "12 transformer_model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "13 transformer_model.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "14 transformer_model.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "15 transformer_model.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "16 transformer_model.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "17 transformer_model.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "18 transformer_model.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "19 transformer_model.encoder.layer.1.attention.self.q_bias torch.Size([1024])\n",
      "20 transformer_model.encoder.layer.1.attention.self.v_bias torch.Size([1024])\n",
      "21 transformer_model.encoder.layer.1.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "22 transformer_model.encoder.layer.1.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "23 transformer_model.encoder.layer.1.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.encoder.layer.1.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "25 transformer_model.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "27 transformer_model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "28 transformer_model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "29 transformer_model.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "30 transformer_model.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "31 transformer_model.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "32 transformer_model.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "33 transformer_model.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "34 transformer_model.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "35 transformer_model.encoder.layer.2.attention.self.q_bias torch.Size([1024])\n",
      "36 transformer_model.encoder.layer.2.attention.self.v_bias torch.Size([1024])\n",
      "37 transformer_model.encoder.layer.2.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "38 transformer_model.encoder.layer.2.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "39 transformer_model.encoder.layer.2.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.encoder.layer.2.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "41 transformer_model.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "43 transformer_model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "44 transformer_model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "45 transformer_model.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "46 transformer_model.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "47 transformer_model.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "48 transformer_model.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "49 transformer_model.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "50 transformer_model.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "51 transformer_model.encoder.layer.3.attention.self.q_bias torch.Size([1024])\n",
      "52 transformer_model.encoder.layer.3.attention.self.v_bias torch.Size([1024])\n",
      "53 transformer_model.encoder.layer.3.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "54 transformer_model.encoder.layer.3.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "55 transformer_model.encoder.layer.3.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.encoder.layer.3.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "57 transformer_model.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "59 transformer_model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "60 transformer_model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "61 transformer_model.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "62 transformer_model.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "63 transformer_model.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "64 transformer_model.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "65 transformer_model.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "66 transformer_model.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "67 transformer_model.encoder.layer.4.attention.self.q_bias torch.Size([1024])\n",
      "68 transformer_model.encoder.layer.4.attention.self.v_bias torch.Size([1024])\n",
      "69 transformer_model.encoder.layer.4.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "70 transformer_model.encoder.layer.4.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "71 transformer_model.encoder.layer.4.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.encoder.layer.4.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "73 transformer_model.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "75 transformer_model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "76 transformer_model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "77 transformer_model.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "78 transformer_model.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "79 transformer_model.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "80 transformer_model.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "81 transformer_model.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "82 transformer_model.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "83 transformer_model.encoder.layer.5.attention.self.q_bias torch.Size([1024])\n",
      "84 transformer_model.encoder.layer.5.attention.self.v_bias torch.Size([1024])\n",
      "85 transformer_model.encoder.layer.5.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "86 transformer_model.encoder.layer.5.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "87 transformer_model.encoder.layer.5.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.encoder.layer.5.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "89 transformer_model.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "91 transformer_model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "92 transformer_model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "93 transformer_model.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "94 transformer_model.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "95 transformer_model.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "96 transformer_model.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "97 transformer_model.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "98 transformer_model.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "99 transformer_model.encoder.layer.6.attention.self.q_bias torch.Size([1024])\n",
      "100 transformer_model.encoder.layer.6.attention.self.v_bias torch.Size([1024])\n",
      "101 transformer_model.encoder.layer.6.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "102 transformer_model.encoder.layer.6.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "103 transformer_model.encoder.layer.6.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.encoder.layer.6.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "105 transformer_model.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "107 transformer_model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "108 transformer_model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "109 transformer_model.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "110 transformer_model.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "111 transformer_model.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "112 transformer_model.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "113 transformer_model.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "114 transformer_model.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "115 transformer_model.encoder.layer.7.attention.self.q_bias torch.Size([1024])\n",
      "116 transformer_model.encoder.layer.7.attention.self.v_bias torch.Size([1024])\n",
      "117 transformer_model.encoder.layer.7.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "118 transformer_model.encoder.layer.7.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "119 transformer_model.encoder.layer.7.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.encoder.layer.7.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "121 transformer_model.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "123 transformer_model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "124 transformer_model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "125 transformer_model.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "126 transformer_model.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "127 transformer_model.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "128 transformer_model.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "129 transformer_model.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "130 transformer_model.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "131 transformer_model.encoder.layer.8.attention.self.q_bias torch.Size([1024])\n",
      "132 transformer_model.encoder.layer.8.attention.self.v_bias torch.Size([1024])\n",
      "133 transformer_model.encoder.layer.8.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "134 transformer_model.encoder.layer.8.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "135 transformer_model.encoder.layer.8.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.encoder.layer.8.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "137 transformer_model.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "139 transformer_model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "140 transformer_model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "141 transformer_model.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "142 transformer_model.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "143 transformer_model.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "144 transformer_model.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "145 transformer_model.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "146 transformer_model.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "147 transformer_model.encoder.layer.9.attention.self.q_bias torch.Size([1024])\n",
      "148 transformer_model.encoder.layer.9.attention.self.v_bias torch.Size([1024])\n",
      "149 transformer_model.encoder.layer.9.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "150 transformer_model.encoder.layer.9.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "151 transformer_model.encoder.layer.9.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.encoder.layer.9.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "153 transformer_model.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "155 transformer_model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "156 transformer_model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "157 transformer_model.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "158 transformer_model.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "159 transformer_model.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "160 transformer_model.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "161 transformer_model.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "162 transformer_model.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "163 transformer_model.encoder.layer.10.attention.self.q_bias torch.Size([1024])\n",
      "164 transformer_model.encoder.layer.10.attention.self.v_bias torch.Size([1024])\n",
      "165 transformer_model.encoder.layer.10.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "166 transformer_model.encoder.layer.10.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "167 transformer_model.encoder.layer.10.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.encoder.layer.10.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "169 transformer_model.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "171 transformer_model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "172 transformer_model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "173 transformer_model.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "174 transformer_model.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "175 transformer_model.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "176 transformer_model.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "177 transformer_model.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "178 transformer_model.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "179 transformer_model.encoder.layer.11.attention.self.q_bias torch.Size([1024])\n",
      "180 transformer_model.encoder.layer.11.attention.self.v_bias torch.Size([1024])\n",
      "181 transformer_model.encoder.layer.11.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "182 transformer_model.encoder.layer.11.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "183 transformer_model.encoder.layer.11.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.encoder.layer.11.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "185 transformer_model.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "187 transformer_model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "188 transformer_model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "189 transformer_model.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "190 transformer_model.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "191 transformer_model.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "192 transformer_model.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "193 transformer_model.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "194 transformer_model.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "195 transformer_model.encoder.layer.12.attention.self.q_bias torch.Size([1024])\n",
      "196 transformer_model.encoder.layer.12.attention.self.v_bias torch.Size([1024])\n",
      "197 transformer_model.encoder.layer.12.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "198 transformer_model.encoder.layer.12.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "199 transformer_model.encoder.layer.12.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.encoder.layer.12.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "201 transformer_model.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "203 transformer_model.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "204 transformer_model.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "205 transformer_model.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "206 transformer_model.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "207 transformer_model.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "208 transformer_model.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "209 transformer_model.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "210 transformer_model.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "211 transformer_model.encoder.layer.13.attention.self.q_bias torch.Size([1024])\n",
      "212 transformer_model.encoder.layer.13.attention.self.v_bias torch.Size([1024])\n",
      "213 transformer_model.encoder.layer.13.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "214 transformer_model.encoder.layer.13.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "215 transformer_model.encoder.layer.13.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.encoder.layer.13.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "217 transformer_model.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "219 transformer_model.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "220 transformer_model.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "221 transformer_model.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "222 transformer_model.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "223 transformer_model.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "224 transformer_model.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "225 transformer_model.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "226 transformer_model.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "227 transformer_model.encoder.layer.14.attention.self.q_bias torch.Size([1024])\n",
      "228 transformer_model.encoder.layer.14.attention.self.v_bias torch.Size([1024])\n",
      "229 transformer_model.encoder.layer.14.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "230 transformer_model.encoder.layer.14.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "231 transformer_model.encoder.layer.14.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.encoder.layer.14.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "233 transformer_model.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "235 transformer_model.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "236 transformer_model.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "237 transformer_model.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "238 transformer_model.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "239 transformer_model.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "240 transformer_model.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "241 transformer_model.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "242 transformer_model.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "243 transformer_model.encoder.layer.15.attention.self.q_bias torch.Size([1024])\n",
      "244 transformer_model.encoder.layer.15.attention.self.v_bias torch.Size([1024])\n",
      "245 transformer_model.encoder.layer.15.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "246 transformer_model.encoder.layer.15.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "247 transformer_model.encoder.layer.15.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.encoder.layer.15.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "249 transformer_model.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "251 transformer_model.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "252 transformer_model.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "253 transformer_model.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "254 transformer_model.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "255 transformer_model.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "256 transformer_model.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "257 transformer_model.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "258 transformer_model.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "259 transformer_model.encoder.layer.16.attention.self.q_bias torch.Size([1024])\n",
      "260 transformer_model.encoder.layer.16.attention.self.v_bias torch.Size([1024])\n",
      "261 transformer_model.encoder.layer.16.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "262 transformer_model.encoder.layer.16.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "263 transformer_model.encoder.layer.16.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.encoder.layer.16.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "265 transformer_model.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "267 transformer_model.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "268 transformer_model.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "269 transformer_model.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "270 transformer_model.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "271 transformer_model.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "272 transformer_model.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "273 transformer_model.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "274 transformer_model.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "275 transformer_model.encoder.layer.17.attention.self.q_bias torch.Size([1024])\n",
      "276 transformer_model.encoder.layer.17.attention.self.v_bias torch.Size([1024])\n",
      "277 transformer_model.encoder.layer.17.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "278 transformer_model.encoder.layer.17.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "279 transformer_model.encoder.layer.17.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.encoder.layer.17.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "281 transformer_model.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "283 transformer_model.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "284 transformer_model.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "285 transformer_model.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "286 transformer_model.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "287 transformer_model.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "288 transformer_model.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "289 transformer_model.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "290 transformer_model.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "291 transformer_model.encoder.layer.18.attention.self.q_bias torch.Size([1024])\n",
      "292 transformer_model.encoder.layer.18.attention.self.v_bias torch.Size([1024])\n",
      "293 transformer_model.encoder.layer.18.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "294 transformer_model.encoder.layer.18.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "295 transformer_model.encoder.layer.18.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.encoder.layer.18.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "297 transformer_model.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "299 transformer_model.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "300 transformer_model.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "301 transformer_model.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "302 transformer_model.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "303 transformer_model.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "304 transformer_model.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "305 transformer_model.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "306 transformer_model.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "307 transformer_model.encoder.layer.19.attention.self.q_bias torch.Size([1024])\n",
      "308 transformer_model.encoder.layer.19.attention.self.v_bias torch.Size([1024])\n",
      "309 transformer_model.encoder.layer.19.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "310 transformer_model.encoder.layer.19.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "311 transformer_model.encoder.layer.19.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.encoder.layer.19.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "313 transformer_model.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "315 transformer_model.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "316 transformer_model.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "317 transformer_model.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "318 transformer_model.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "319 transformer_model.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "320 transformer_model.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "321 transformer_model.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "322 transformer_model.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "323 transformer_model.encoder.layer.20.attention.self.q_bias torch.Size([1024])\n",
      "324 transformer_model.encoder.layer.20.attention.self.v_bias torch.Size([1024])\n",
      "325 transformer_model.encoder.layer.20.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "326 transformer_model.encoder.layer.20.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "327 transformer_model.encoder.layer.20.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.encoder.layer.20.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "329 transformer_model.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "331 transformer_model.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "332 transformer_model.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "333 transformer_model.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "334 transformer_model.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "335 transformer_model.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "336 transformer_model.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "337 transformer_model.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "338 transformer_model.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "339 transformer_model.encoder.layer.21.attention.self.q_bias torch.Size([1024])\n",
      "340 transformer_model.encoder.layer.21.attention.self.v_bias torch.Size([1024])\n",
      "341 transformer_model.encoder.layer.21.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "342 transformer_model.encoder.layer.21.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "343 transformer_model.encoder.layer.21.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.encoder.layer.21.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "345 transformer_model.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "347 transformer_model.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "348 transformer_model.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "349 transformer_model.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "350 transformer_model.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "351 transformer_model.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "352 transformer_model.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "353 transformer_model.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "354 transformer_model.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "355 transformer_model.encoder.layer.22.attention.self.q_bias torch.Size([1024])\n",
      "356 transformer_model.encoder.layer.22.attention.self.v_bias torch.Size([1024])\n",
      "357 transformer_model.encoder.layer.22.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "358 transformer_model.encoder.layer.22.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "359 transformer_model.encoder.layer.22.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.encoder.layer.22.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "361 transformer_model.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "363 transformer_model.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "364 transformer_model.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "365 transformer_model.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "366 transformer_model.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "367 transformer_model.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "368 transformer_model.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "369 transformer_model.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "370 transformer_model.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "371 transformer_model.encoder.layer.23.attention.self.q_bias torch.Size([1024])\n",
      "372 transformer_model.encoder.layer.23.attention.self.v_bias torch.Size([1024])\n",
      "373 transformer_model.encoder.layer.23.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "374 transformer_model.encoder.layer.23.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "375 transformer_model.encoder.layer.23.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.encoder.layer.23.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "377 transformer_model.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "379 transformer_model.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "380 transformer_model.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "381 transformer_model.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "382 transformer_model.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "383 transformer_model.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "384 transformer_model.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "385 transformer_model.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "386 transformer_model.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "387 transformer_model.encoder.rel_embeddings.weight torch.Size([1024, 1024])\n",
      "388 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "389 attention.hidden_layer.bias torch.Size([512])\n",
      "390 attention.final_layer.weight torch.Size([1, 512])\n",
      "391 attention.final_layer.bias torch.Size([1])\n",
      "392 regressor.weight torch.Size([1, 1024])\n",
      "393 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 18.5696,  22.2834, -19.6048,  ...,  14.5450,  -6.4439,  -7.6526],\n",
       "        [  9.9711, -18.0911,  37.9014,  ...,  33.6703,   4.7925,  -7.2030],\n",
       "        [ 13.7893,   8.0198,  -7.7403,  ...,  23.0422,  -1.0622,   3.6124],\n",
       "        ...,\n",
       "        [-14.4509,   7.3179,  25.4754,  ..., -19.0306,  22.1846,  -9.0804],\n",
       "        [  2.6181,  28.3989,  15.1862,  ...,  15.3681,   3.9010,  16.4605],\n",
       "        [  4.4850, -33.8052,  31.8944,  ..., -18.6333,  34.9414,  -4.1955]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    attention_param_start = 388\n",
    "    regressor_param_start = 392\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 195\n",
    "    layer_middle_threshold = 323\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        len_dataset = len(dataset)\n",
    "        self.indices = list(range(len_dataset))\n",
    "        self.num_samples = len_dataset\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "003709d7-8a73-4e9d-ae50-5694fd588c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(sample_ds)))\n",
    "num_samples = len(sample_ds)\n",
    "label_to_count = dict(Counter(sample_ds.bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92ba2109-05a0-4d7a-8977-c520c11a695e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 886, 2: 784, 4: 494, 0: 122, 1: 441, 5: 106}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14d41779-c684-48f5-8333-430248f1cc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.999999999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [1/label_to_count[i] for i in sample_ds.bins]\n",
    "sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c8a73dc-7299-4bd3-80d3-8015d96887fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(torch.multinomial(torch.tensor(weights), num_samples, replacement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47acff01-a468-4dbb-ad27-66e76abd196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8459bef7-90c5-4e4a-9d49-160b59facd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy.sort_values(['target']).reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d848bbac-d1c0-4400-9630-0125c3a63b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2415,  100,  252, ..., 2623,  579, 2100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.arange(len(train_df_copy)), len(train_df_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6:\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(base_lr, last_lr, fold = 0, scheduler_func = get_cosine_schedule_with_warmup):\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = scheduler_func(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c9d397f-aa6e-4823-a4fd-b8625c580972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# fold 0: {'base_lr': 4.214048623230046e-05, 'last_lr': 0.00098671139242345}. Best is trial 0 with value: 0.46920305490493774.\n",
    "# fold 1: {'base_lr': 3.4594372607385946e-05, 'last_lr': 0.0005479134338105077}. Best is trial 0 with value: 0.447492390871048\n",
    "# fold 2: {'base_lr': 1.777623134028703e-05, 'last_lr': 0.004132549020616918}. Best is trial 0 with value: 0.46756473183631897\n",
    "# fold 3: {'base_lr': 3.933402254716856e-05, 'last_lr': 0.0018473297738188957}. Best is trial 11 with value: 0.4719877541065216\n",
    "# fold 4: {'base_lr': 1.845975941382356e-05, 'last_lr': 0.0006309278277674714}. Best is trial 15 with value: 0.46920618414878845\n",
    "# fold 5: {'base_lr': 4.430444436442592e-05, 'last_lr': 0.000289231685619846}. Best is trial 6 with value: 0.4629150927066803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fbc2b14-ea34-48b6-82e9-10809a6d9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "lr_list = [\n",
    "    {'base_lr': 3.9152303052330085e-05, 'last_lr': 0.00045881520547034934, 'schedule_func': get_cosine_schedule_with_warmup}, # Best is trial 15 with value: 0.4751589596271515,\n",
    "    {'base_lr': 3.4594372607385946e-05, 'last_lr': 0.0005479134338105077},\n",
    "    {'base_lr': 1.777623134028703e-05, 'last_lr': 0.004132549020616918},\n",
    "    {'base_lr': 3.933402254716856e-05, 'last_lr': 0.0018473297738188957},\n",
    "    {'base_lr': 4.395940545935742e-05, 'last_lr': 0.0008065830888769318, 'schedule_func': get_cosine_schedule_with_warmup}, # 0.4748414158821106\n",
    "    {'base_lr': 4.430444436442592e-05, 'last_lr': 0.000289231685619846}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5d168c45784eeda87cdd735c1fadab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.69 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8327 New best_val_rmse: 0.8327\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8693 Still best_val_rmse: 0.8327 (from epoch 0)\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6203 New best_val_rmse: 0.6203\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6265 Still best_val_rmse: 0.6203 (from epoch 0)\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6645 Still best_val_rmse: 0.6203 (from epoch 0)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7774 Still best_val_rmse: 0.6203 (from epoch 0)\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6186 New best_val_rmse: 0.6186\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.652 Still best_val_rmse: 0.6186 (from epoch 0)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5408 New best_val_rmse: 0.5408\n",
      "\n",
      "16 steps took 7.59 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5086 New best_val_rmse: 0.5086\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.591 Still best_val_rmse: 0.5086 (from epoch 1)\n",
      "\n",
      "16 steps took 6.78 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5462 Still best_val_rmse: 0.5086 (from epoch 1)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5748 Still best_val_rmse: 0.5086 (from epoch 1)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4931 New best_val_rmse: 0.4931\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4968 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5147 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5381 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5058 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4965 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.49 New best_val_rmse: 0.49\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4862 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4964 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4916 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4964 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5297 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.494 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4911 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4979 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4832 New best_val_rmse: 0.4832\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4864 Still best_val_rmse: 0.4832 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4844 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4838 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4895 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4855 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4842 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4852 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4851 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4861 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 2.45 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.5004 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4945 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4832 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4831 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4857 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4877 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4884 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4856 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4837 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4835 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.484 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4852 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4844 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4843 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4842 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4835 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4831 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4834 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4833 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4835 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4837 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4839 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4843 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4846 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4848 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4848 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4846 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4847 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4848 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4849 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4848 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4848 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4848 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.4823746979236603\n",
      "##### Using fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a041896e69144da4896d907a86c3a6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7932 New best_val_rmse: 0.7932\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.5904 New best_val_rmse: 0.5904\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8069 Still best_val_rmse: 0.5904 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5796 New best_val_rmse: 0.5796\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5946 Still best_val_rmse: 0.5796 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5603 New best_val_rmse: 0.5603\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6503 Still best_val_rmse: 0.5603 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5377 New best_val_rmse: 0.5377\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.506 New best_val_rmse: 0.506\n",
      "\n",
      "16 steps took 7.72 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5738 Still best_val_rmse: 0.506 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 1 batch_num: 29 val_rmse: 0.4648 New best_val_rmse: 0.4648\n",
      "\n",
      "1 steps took 0.453 seconds\n",
      "Epoch: 1 batch_num: 30 val_rmse: 0.4748 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 1 batch_num: 32 val_rmse: 0.4724 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 1 batch_num: 34 val_rmse: 0.4654 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.4675 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.4772 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 1 batch_num: 38 val_rmse: 0.477 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 1 batch_num: 40 val_rmse: 0.4965 Still best_val_rmse: 0.4648 (from epoch 1)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 1 batch_num: 48 val_rmse: 0.4422 New best_val_rmse: 0.4422\n",
      "\n",
      "1 steps took 0.454 seconds\n",
      "Epoch: 1 batch_num: 49 val_rmse: 0.4433 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 1 batch_num: 50 val_rmse: 0.4472 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 1 batch_num: 51 val_rmse: 0.4513 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.455 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 1 batch_num: 53 val_rmse: 0.4652 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 1 batch_num: 54 val_rmse: 0.4631 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 1 batch_num: 55 val_rmse: 0.4584 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 1 batch_num: 56 val_rmse: 0.4608 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 1 batch_num: 57 val_rmse: 0.4594 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 1 batch_num: 58 val_rmse: 0.4541 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 1 batch_num: 59 val_rmse: 0.4564 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4796 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 1 batch_num: 62 val_rmse: 0.4429 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 1 batch_num: 63 val_rmse: 0.4511 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.4481 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 1 batch_num: 65 val_rmse: 0.4511 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 1 batch_num: 66 val_rmse: 0.4552 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 1 batch_num: 67 val_rmse: 0.457 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.4702 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 1 batch_num: 70 val_rmse: 0.5559 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 1 batch_num: 86 val_rmse: 0.6539 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 102 val_rmse: 0.6266 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "16 steps took 6.87 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.476 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.858 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4741 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.4749 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5044 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4951 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "8 steps took 4.34 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4695 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 1 val_rmse: 0.4906 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 9 val_rmse: 0.4509 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4517 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 11 val_rmse: 0.4621 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4715 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4526 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4522 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4636 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4642 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4556 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4551 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4547 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4547 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4544 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.452 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4535 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4635 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4741 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4646 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4555 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.452 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4515 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4513 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.451 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4514 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.434 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4527 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4511 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4536 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4512 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4502 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4496 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4494 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4487 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.45 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4499 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.451 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4535 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4528 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4505 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4494 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4488 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4498 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4522 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4518 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4524 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4514 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4473 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4472 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4461 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4461 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.447 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4479 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4473 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4483 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4502 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4521 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4531 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4528 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4525 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4515 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4503 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4503 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4492 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.449 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4489 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4496 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4535 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4564 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4545 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4514 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4519 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4537 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4578 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4633 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4602 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4559 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4534 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4502 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4468 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4462 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4468 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4489 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4527 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.458 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4612 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.43 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4641 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4701 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4681 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4515 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4493 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4541 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.428 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4579 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4573 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4534 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4504 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4494 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.434 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4501 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4522 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4532 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4518 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4506 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4508 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4507 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4494 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4434 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4426 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4447 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4504 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4563 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4659 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4679 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4602 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4502 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4433 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4475 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4503 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4501 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4466 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4426 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4427 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4455 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4484 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4496 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4509 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4502 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4484 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4466 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.286 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4476 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 1.46 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4476 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.438 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.447 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4472 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4477 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.448 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4477 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4461 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4443 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4443 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4449 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4443 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4456 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4455 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4455 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4451 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4447 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4441 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4441 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4449 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4465 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4476 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4475 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4472 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4473 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4469 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4457 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4442 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4435 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.428 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4438 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4452 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4462 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4464 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4464 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4462 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.446 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4459 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.446 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4462 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4464 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4464 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4464 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4459 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4456 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4448 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4442 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4435 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.442 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4442 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4447 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4449 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4447 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4444 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4441 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4436 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4435 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4435 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4437 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4439 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4442 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4445 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4449 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4458 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4461 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4465 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4469 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4473 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4475 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4478 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4481 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4482 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4484 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4483 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4482 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.448 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.434 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4478 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4475 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4472 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4469 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4466 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4464 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4462 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.446 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4458 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4457 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4456 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4456 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4455 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4454 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.43 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4452 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4452 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4453 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "1 steps took 0.299 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4452 Still best_val_rmse: 0.4422 (from epoch 1)\n",
      "\n",
      "Final RMSE: 0.4422326982021332\n",
      "##### Using fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cf550c1ecb454184a0442bb7601689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8068 New best_val_rmse: 0.8068\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8205 Still best_val_rmse: 0.8068 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.983 Still best_val_rmse: 0.8068 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6531 New best_val_rmse: 0.6531\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6392 New best_val_rmse: 0.6392\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.693 Still best_val_rmse: 0.6392 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6499 Still best_val_rmse: 0.6392 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5738 New best_val_rmse: 0.5738\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7639 Still best_val_rmse: 0.5738 (from epoch 0)\n",
      "\n",
      "16 steps took 7.62 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6264 Still best_val_rmse: 0.5738 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5521 New best_val_rmse: 0.5521\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5412 New best_val_rmse: 0.5412\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5776 Still best_val_rmse: 0.5412 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5455 Still best_val_rmse: 0.5412 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5323 New best_val_rmse: 0.5323\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5255 New best_val_rmse: 0.5255\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5186 New best_val_rmse: 0.5186\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5643 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 7.76 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5365 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5209 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4968 New best_val_rmse: 0.4968\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5047 Still best_val_rmse: 0.4968 (from epoch 2)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.496 New best_val_rmse: 0.496\n",
      "\n",
      "8 steps took 3.53 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.497 Still best_val_rmse: 0.496 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4932 Still best_val_rmse: 0.4896 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4921 Still best_val_rmse: 0.4896 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4851 New best_val_rmse: 0.4851\n",
      "\n",
      "4 steps took 1.73 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4863 Still best_val_rmse: 0.4851 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4933 Still best_val_rmse: 0.4851 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4883 Still best_val_rmse: 0.4851 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.493 Still best_val_rmse: 0.4851 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4989 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "8 steps took 4.23 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.483 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4841 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4867 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4863 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4873 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4875 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4893 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.487 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4825 Still best_val_rmse: 0.4814 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4851 Still best_val_rmse: 0.4812 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4838 Still best_val_rmse: 0.4812 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4805 New best_val_rmse: 0.4805\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4817 Still best_val_rmse: 0.4805 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.872 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4793 New best_val_rmse: 0.4793\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4795 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4828 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4824 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4818 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4805 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4805 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4804 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4804 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4805 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4804 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4802 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4801 Still best_val_rmse: 0.4793 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.4792614281177521\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1567c7fdfe184dc59d7a3fa69e1fbbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.68 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8126 New best_val_rmse: 0.8126\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8459 Still best_val_rmse: 0.8126 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6837 New best_val_rmse: 0.6837\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.618 New best_val_rmse: 0.618\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6432 Still best_val_rmse: 0.618 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5784 New best_val_rmse: 0.5784\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5316 New best_val_rmse: 0.5316\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8139 Still best_val_rmse: 0.5316 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5504 Still best_val_rmse: 0.5316 (from epoch 0)\n",
      "\n",
      "16 steps took 7.66 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5289 New best_val_rmse: 0.5289\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5081 New best_val_rmse: 0.5081\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5122 Still best_val_rmse: 0.5081 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5505 Still best_val_rmse: 0.5081 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5265 Still best_val_rmse: 0.5081 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.4921 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.5027 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4979 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.502 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4922 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 4.22 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4974 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4688 New best_val_rmse: 0.4688\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 13 val_rmse: 0.4722 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4769 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4761 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4723 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4755 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4787 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4851 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4815 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4834 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4844 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4829 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.478 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4748 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4779 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4753 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4772 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4706 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4981 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4743 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4726 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4901 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.476 Still best_val_rmse: 0.4688 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.453 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4701 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4766 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4743 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4759 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4728 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4684 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4699 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4799 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4901 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.468 New best_val_rmse: 0.468\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.449 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4673 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4703 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4739 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4731 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4678 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4685 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.471 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4729 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4714 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4691 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4685 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4681 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4678 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4671 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4664 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4651 New best_val_rmse: 0.4651\n",
      "\n",
      "1 steps took 0.435 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4645 New best_val_rmse: 0.4645\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4644 New best_val_rmse: 0.4644\n",
      "\n",
      "1 steps took 0.431 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4647 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4654 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4665 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4675 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4694 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4708 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4702 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4694 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4682 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4673 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4668 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.434 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4665 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4663 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.275 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4673 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 1.41 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4671 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4664 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4659 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4649 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4646 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4643 New best_val_rmse: 0.4643\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4641 New best_val_rmse: 0.4641\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4639 New best_val_rmse: 0.4639\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4636 New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4634 New best_val_rmse: 0.4634\n",
      "\n",
      "1 steps took 0.434 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.431 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4633 Still best_val_rmse: 0.4632 (from epoch 3)\n",
      "\n",
      "1 steps took 0.412 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.463 New best_val_rmse: 0.463\n",
      "\n",
      "1 steps took 0.432 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.463 New best_val_rmse: 0.463\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4629 New best_val_rmse: 0.4629\n",
      "\n",
      "1 steps took 0.43 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4634 Still best_val_rmse: 0.4629 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4642 Still best_val_rmse: 0.4629 (from epoch 3)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4641 Still best_val_rmse: 0.4629 (from epoch 3)\n",
      "\n",
      "1 steps took 0.411 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4633 Still best_val_rmse: 0.4629 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4629 New best_val_rmse: 0.4629\n",
      "\n",
      "1 steps took 0.428 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4627 New best_val_rmse: 0.4627\n",
      "\n",
      "1 steps took 0.428 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4626 New best_val_rmse: 0.4626\n",
      "\n",
      "1 steps took 0.428 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4624 New best_val_rmse: 0.4624\n",
      "\n",
      "1 steps took 0.531 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4622 New best_val_rmse: 0.4622\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.462 New best_val_rmse: 0.462\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4619 New best_val_rmse: 0.4619\n",
      "\n",
      "1 steps took 0.507 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4619 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4621 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4625 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4635 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4644 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.412 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4654 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4657 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4653 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4641 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4632 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4624 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.462 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4619 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4619 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4619 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.462 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4621 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4626 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4634 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.464 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4645 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4653 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4655 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4651 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4642 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4635 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4624 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4622 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4623 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4624 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4626 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4627 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4626 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4626 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4627 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4631 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4638 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4646 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4652 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4658 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.466 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4659 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4654 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4648 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4644 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4639 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4634 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4631 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4627 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4627 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4631 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4634 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4638 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4641 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4643 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4644 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4644 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4643 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4642 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4641 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4641 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.464 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4639 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4637 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4635 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4633 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4632 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.463 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.432 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.463 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4628 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "1 steps took 0.279 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4629 Still best_val_rmse: 0.4619 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.4618578553199768\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f3218df63e4e39b5c8c1eb7695aece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.27 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.94 New best_val_rmse: 0.94\n",
      "\n",
      "16 steps took 6.75 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6271 New best_val_rmse: 0.6271\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7002 Still best_val_rmse: 0.6271 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6333 Still best_val_rmse: 0.6271 (from epoch 0)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8325 Still best_val_rmse: 0.6271 (from epoch 0)\n",
      "\n",
      "16 steps took 6.84 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6371 Still best_val_rmse: 0.6271 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5816 New best_val_rmse: 0.5816\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7343 Still best_val_rmse: 0.5816 (from epoch 0)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5597 New best_val_rmse: 0.5597\n",
      "\n",
      "16 steps took 7.62 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6473 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.706 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.7903 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5531 New best_val_rmse: 0.5531\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5546 Still best_val_rmse: 0.5531 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5304 New best_val_rmse: 0.5304\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5307 Still best_val_rmse: 0.5304 (from epoch 1)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5345 Still best_val_rmse: 0.5304 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5596 Still best_val_rmse: 0.5304 (from epoch 1)\n",
      "\n",
      "16 steps took 7.68 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5075 New best_val_rmse: 0.5075\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5047 Still best_val_rmse: 0.4966 (from epoch 2)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4926 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4875 New best_val_rmse: 0.4875\n",
      "\n",
      "4 steps took 1.76 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4911 Still best_val_rmse: 0.4875 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4974 Still best_val_rmse: 0.4875 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4844 New best_val_rmse: 0.4844\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4895 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4844 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4862 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4937 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4882 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4886 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4897 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4913 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4879 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4842 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4857 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.49 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4924 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.486 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4849 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 1.76 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4831 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4835 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4842 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4842 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4849 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4851 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4855 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4852 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4855 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4871 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.486 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4841 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4846 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4846 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.484 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4836 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4835 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4836 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4842 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4845 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4847 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4846 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4842 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4839 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4835 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4834 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4833 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4833 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4833 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4833 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4833 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4833 Still best_val_rmse: 0.4821 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.4820895195007324\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4c1c3c936498abfa588f4eb2381a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8675 New best_val_rmse: 0.8675\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6821 New best_val_rmse: 0.6821\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.741 Still best_val_rmse: 0.6821 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5872 New best_val_rmse: 0.5872\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6354 Still best_val_rmse: 0.5872 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5486 New best_val_rmse: 0.5486\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6269 Still best_val_rmse: 0.5486 (from epoch 0)\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5735 Still best_val_rmse: 0.5486 (from epoch 0)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 7.63 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4936 New best_val_rmse: 0.4936\n",
      "\n",
      "8 steps took 3.41 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5313 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5729 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5357 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.503 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.85 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.6107 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.87 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5039 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.83 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4968 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "8 steps took 3.42 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5378 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4989 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "8 steps took 4.25 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4944 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4801 New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4856 Still best_val_rmse: 0.4801 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 0.868 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.858 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4824 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 0.888 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4789 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4812 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4754 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4728 New best_val_rmse: 0.4728\n",
      "\n",
      "2 steps took 0.938 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4704 New best_val_rmse: 0.4704\n",
      "\n",
      "2 steps took 0.894 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.475 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4755 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4771 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4829 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "4 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4877 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4891 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4924 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "8 steps took 3.4 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4798 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4738 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4817 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4734 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4727 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.476 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4718 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4741 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4824 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "4 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4763 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4707 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4704 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 0.849 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.431 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.433 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4686 New best_val_rmse: 0.4686\n",
      "\n",
      "1 steps took 0.538 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4678 New best_val_rmse: 0.4678\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4677 New best_val_rmse: 0.4677\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.468 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4702 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4695 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4695 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4694 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4688 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.426 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4695 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4704 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4746 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4761 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4754 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4739 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4714 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4694 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4689 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4682 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4677 New best_val_rmse: 0.4677\n",
      "\n",
      "1 steps took 0.431 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4677 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.41 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4679 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4678 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4688 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4733 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4761 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.854 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4704 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4707 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4742 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4763 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4764 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4748 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.61 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4768 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.845 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4787 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4737 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 0.853 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4671 New best_val_rmse: 0.4671\n",
      "\n",
      "1 steps took 0.43 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4649 New best_val_rmse: 0.4649\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4648 New best_val_rmse: 0.4648\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4646 New best_val_rmse: 0.4646\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4644 New best_val_rmse: 0.4644\n",
      "\n",
      "1 steps took 0.468 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4648 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4657 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4678 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4709 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "2 steps took 0.85 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4724 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4696 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.468 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4649 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4646 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4648 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4652 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4655 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.466 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4667 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4673 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4679 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.432 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4684 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4686 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4686 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4688 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4691 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4686 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4681 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4674 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4667 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4655 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4653 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4654 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4657 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4665 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.467 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4673 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4675 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.467 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.466 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4666 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4676 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4687 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4696 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4705 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "2 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4711 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4702 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "2 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4692 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.469 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4686 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.468 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4674 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.467 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4666 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4663 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4656 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4653 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4653 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4652 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4651 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4651 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4651 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4651 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4652 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4652 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4653 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4653 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4654 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4654 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4654 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4655 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4656 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4656 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4657 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4658 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4659 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.466 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.425 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4663 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4664 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4664 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4664 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4664 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4663 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4663 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4661 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.418 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.422 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.421 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.415 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.412 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "1 steps took 0.275 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4662 Still best_val_rmse: 0.4644 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.46443697810173035\n",
      "CPU times: user 1h 42min 48s, sys: 22min 30s, total: 2h 5min 18s\n",
      "Wall time: 2h 17min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rmse_values = []\n",
    "for i in range(len(list(splits))):\n",
    "    fold = i\n",
    "    lrs = lr_list[fold]\n",
    "    rmse_val = train_fold(lrs['base_lr'], lrs['last_lr'], fold=fold, scheduler_func = lrs['schedule_func'] if 'schedule_func' in lrs else get_cosine_schedule_with_warmup)\n",
    "    print(f'Final RMSE: {rmse_val}')\n",
    "    rmse_values.append(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b28ebe4-316a-47e7-b20e-64a20ee4c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean RMSE values: 0.468708872795105'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'mean RMSE values: {np.mean(np.array(rmse_values))}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/commonlit/models/deberta-large-lm/best_lm were not used when initializing DebertaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 7.64 s, total: 33.5 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d0eb2f4ff949e29a7ee9016f752167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d66408c20804469b41b12122376216a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.2686695418357563\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.24120747723363972\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.24626021060074163\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.2040639478259469\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.2457987052565849\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.2626301834086225\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ec04c78607458bb0d8bbedf23b010d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245b85310b1f489589ad35c30f871b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3524799040604675\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3573640490358266\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3246639505761059\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.33647310612935655\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.32088533402434594\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.36134417042407063\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0386ba7d8f454cbf64749c8e9db85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7860c5d4ab3f4b7d9e9f28e2b1fd9501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.28791424523581755\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.27612195429140873\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.24907389913547862\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.27232139933035243\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.27536060716171495\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.29525954867913023\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69e4b35800f483088af04610b67db3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d1136097cf4811a026cc25b484019b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.2165313024044066\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.23576847372154897\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.23441848261283332\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.20192891174055294\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.22767141604435875\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.23527797089590427\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e686f4942842bc9ff36cdee7fbf485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d497e3f8bed44a3fbecd6f4049db42de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.2600705124115583\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.23594197853422755\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.21447733451373757\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.21130841011438553\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.25186068632231773\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.2564435484111153\n",
      "FINAL RMSE score 0.2653197087324105\n",
      "CPU times: user 3min 48s, sys: 8.77 s, total: 3min 57s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0056726339400725015],\n",
       " [-0.005418542700957314],\n",
       " [0.004047111491866357],\n",
       " [0.0012284998837947556],\n",
       " [-0.0004779982235605211]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20387232, 0.18551143, 0.19798566, 0.20754818, 0.20508241])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -1.0831702512774612)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.59015933, -0.7847946 , -0.56186025, -2.21928738, -2.26600891,\n",
       "       -1.1223062 , -0.03885311])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1236668609866004, 0.02473337219732008)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_scores_flat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ee81bbf9339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_scores_flat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean_diff\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_scores_flat' is not defined"
     ]
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff / len(final_scores)\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deberta-large-uniform'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/deberta-large-uniform/best')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/deberta-large-uniform_1'),\n",
       " PosixPath('/home/commonlit/models/deberta-large-uniform_2'),\n",
       " PosixPath('/home/commonlit/models/deberta-large-uniform_3'),\n",
       " PosixPath('/home/commonlit/models/deberta-large-uniform_4'),\n",
       " PosixPath('/home/commonlit/models/deberta-large-uniform_5'),\n",
       " PosixPath('/home/commonlit/models/deberta-large-uniform_6')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/deberta-large-uniform/best_models.zip'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/deberta-large-uniform.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3M\t/home/commonlit/models/deberta-large-uniform/best/tokenizer-1\n",
      "1.3M\t/home/commonlit/models/deberta-large-uniform/best/tokenizer-2\n",
      "1.3M\t/home/commonlit/models/deberta-large-uniform/best/tokenizer-3\n",
      "1.3M\t/home/commonlit/models/deberta-large-uniform/best/tokenizer-4\n",
      "1.3M\t/home/commonlit/models/deberta-large-uniform/best/tokenizer-5\n",
      "1.3M\t/home/commonlit/models/deberta-large-uniform/best/tokenizer-6\n",
      "9.1G\t/home/commonlit/models/deberta-large-uniform/best\n",
      "8.5G\t/home/commonlit/models/deberta-large-uniform/best_models.zip\n",
      "1.6G\t/home/commonlit/models/deberta-large-uniform/lm\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/deberta-large-uniform/lm.zip'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/deberta-large-uniform/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-deberta-large-uniform-with-lm\",\n",
      "  \"id\": \"gilfernandes/commonlit-deberta-large-uniform-with-lm\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-with-lm').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-with-lm')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|██████████████████████████████████████| 8.43G/8.43G [14:57<00:00, 10.1MB/s]\n",
      "Upload successful: best_models.zip (8GB)\n",
      "Starting upload for file lm.zip\n",
      "100%|██████████████████████████████████████| 1.40G/1.40G [02:28<00:00, 10.2MB/s]\n",
      "Upload successful: lm.zip (1GB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-deberta-large-uniform-with-lm\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
