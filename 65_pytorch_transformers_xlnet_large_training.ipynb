{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'xlnet-large-cased'\n",
    "    TOKENIZER_PATH = 'xlnet-large-cased'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'xlnet-large-cased'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9661eeb7e9b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTOKENIZER_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71dc7cab-8554-4546-b4a6-08329635eb1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a4b54d4285d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.transformer.mask_emb torch.Size([1, 1, 1024])\n",
      "1 transformer_model.transformer.word_embedding.weight torch.Size([32000, 1024])\n",
      "2 transformer_model.transformer.layer.0.rel_attn.q torch.Size([1024, 16, 64])\n",
      "3 transformer_model.transformer.layer.0.rel_attn.k torch.Size([1024, 16, 64])\n",
      "4 transformer_model.transformer.layer.0.rel_attn.v torch.Size([1024, 16, 64])\n",
      "5 transformer_model.transformer.layer.0.rel_attn.o torch.Size([1024, 16, 64])\n",
      "6 transformer_model.transformer.layer.0.rel_attn.r torch.Size([1024, 16, 64])\n",
      "7 transformer_model.transformer.layer.0.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "8 transformer_model.transformer.layer.0.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "9 transformer_model.transformer.layer.0.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "10 transformer_model.transformer.layer.0.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "11 transformer_model.transformer.layer.0.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "12 transformer_model.transformer.layer.0.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "13 transformer_model.transformer.layer.0.ff.layer_norm.weight torch.Size([1024])\n",
      "14 transformer_model.transformer.layer.0.ff.layer_norm.bias torch.Size([1024])\n",
      "15 transformer_model.transformer.layer.0.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.transformer.layer.0.ff.layer_1.bias torch.Size([4096])\n",
      "17 transformer_model.transformer.layer.0.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "18 transformer_model.transformer.layer.0.ff.layer_2.bias torch.Size([1024])\n",
      "19 transformer_model.transformer.layer.1.rel_attn.q torch.Size([1024, 16, 64])\n",
      "20 transformer_model.transformer.layer.1.rel_attn.k torch.Size([1024, 16, 64])\n",
      "21 transformer_model.transformer.layer.1.rel_attn.v torch.Size([1024, 16, 64])\n",
      "22 transformer_model.transformer.layer.1.rel_attn.o torch.Size([1024, 16, 64])\n",
      "23 transformer_model.transformer.layer.1.rel_attn.r torch.Size([1024, 16, 64])\n",
      "24 transformer_model.transformer.layer.1.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "25 transformer_model.transformer.layer.1.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "26 transformer_model.transformer.layer.1.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "27 transformer_model.transformer.layer.1.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "28 transformer_model.transformer.layer.1.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "29 transformer_model.transformer.layer.1.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "30 transformer_model.transformer.layer.1.ff.layer_norm.weight torch.Size([1024])\n",
      "31 transformer_model.transformer.layer.1.ff.layer_norm.bias torch.Size([1024])\n",
      "32 transformer_model.transformer.layer.1.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "33 transformer_model.transformer.layer.1.ff.layer_1.bias torch.Size([4096])\n",
      "34 transformer_model.transformer.layer.1.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "35 transformer_model.transformer.layer.1.ff.layer_2.bias torch.Size([1024])\n",
      "36 transformer_model.transformer.layer.2.rel_attn.q torch.Size([1024, 16, 64])\n",
      "37 transformer_model.transformer.layer.2.rel_attn.k torch.Size([1024, 16, 64])\n",
      "38 transformer_model.transformer.layer.2.rel_attn.v torch.Size([1024, 16, 64])\n",
      "39 transformer_model.transformer.layer.2.rel_attn.o torch.Size([1024, 16, 64])\n",
      "40 transformer_model.transformer.layer.2.rel_attn.r torch.Size([1024, 16, 64])\n",
      "41 transformer_model.transformer.layer.2.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "42 transformer_model.transformer.layer.2.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "43 transformer_model.transformer.layer.2.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "44 transformer_model.transformer.layer.2.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "45 transformer_model.transformer.layer.2.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "46 transformer_model.transformer.layer.2.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "47 transformer_model.transformer.layer.2.ff.layer_norm.weight torch.Size([1024])\n",
      "48 transformer_model.transformer.layer.2.ff.layer_norm.bias torch.Size([1024])\n",
      "49 transformer_model.transformer.layer.2.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "50 transformer_model.transformer.layer.2.ff.layer_1.bias torch.Size([4096])\n",
      "51 transformer_model.transformer.layer.2.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "52 transformer_model.transformer.layer.2.ff.layer_2.bias torch.Size([1024])\n",
      "53 transformer_model.transformer.layer.3.rel_attn.q torch.Size([1024, 16, 64])\n",
      "54 transformer_model.transformer.layer.3.rel_attn.k torch.Size([1024, 16, 64])\n",
      "55 transformer_model.transformer.layer.3.rel_attn.v torch.Size([1024, 16, 64])\n",
      "56 transformer_model.transformer.layer.3.rel_attn.o torch.Size([1024, 16, 64])\n",
      "57 transformer_model.transformer.layer.3.rel_attn.r torch.Size([1024, 16, 64])\n",
      "58 transformer_model.transformer.layer.3.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "59 transformer_model.transformer.layer.3.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "60 transformer_model.transformer.layer.3.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "61 transformer_model.transformer.layer.3.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "62 transformer_model.transformer.layer.3.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "63 transformer_model.transformer.layer.3.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "64 transformer_model.transformer.layer.3.ff.layer_norm.weight torch.Size([1024])\n",
      "65 transformer_model.transformer.layer.3.ff.layer_norm.bias torch.Size([1024])\n",
      "66 transformer_model.transformer.layer.3.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "67 transformer_model.transformer.layer.3.ff.layer_1.bias torch.Size([4096])\n",
      "68 transformer_model.transformer.layer.3.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "69 transformer_model.transformer.layer.3.ff.layer_2.bias torch.Size([1024])\n",
      "70 transformer_model.transformer.layer.4.rel_attn.q torch.Size([1024, 16, 64])\n",
      "71 transformer_model.transformer.layer.4.rel_attn.k torch.Size([1024, 16, 64])\n",
      "72 transformer_model.transformer.layer.4.rel_attn.v torch.Size([1024, 16, 64])\n",
      "73 transformer_model.transformer.layer.4.rel_attn.o torch.Size([1024, 16, 64])\n",
      "74 transformer_model.transformer.layer.4.rel_attn.r torch.Size([1024, 16, 64])\n",
      "75 transformer_model.transformer.layer.4.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "76 transformer_model.transformer.layer.4.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "77 transformer_model.transformer.layer.4.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "78 transformer_model.transformer.layer.4.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "79 transformer_model.transformer.layer.4.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "80 transformer_model.transformer.layer.4.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "81 transformer_model.transformer.layer.4.ff.layer_norm.weight torch.Size([1024])\n",
      "82 transformer_model.transformer.layer.4.ff.layer_norm.bias torch.Size([1024])\n",
      "83 transformer_model.transformer.layer.4.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "84 transformer_model.transformer.layer.4.ff.layer_1.bias torch.Size([4096])\n",
      "85 transformer_model.transformer.layer.4.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "86 transformer_model.transformer.layer.4.ff.layer_2.bias torch.Size([1024])\n",
      "87 transformer_model.transformer.layer.5.rel_attn.q torch.Size([1024, 16, 64])\n",
      "88 transformer_model.transformer.layer.5.rel_attn.k torch.Size([1024, 16, 64])\n",
      "89 transformer_model.transformer.layer.5.rel_attn.v torch.Size([1024, 16, 64])\n",
      "90 transformer_model.transformer.layer.5.rel_attn.o torch.Size([1024, 16, 64])\n",
      "91 transformer_model.transformer.layer.5.rel_attn.r torch.Size([1024, 16, 64])\n",
      "92 transformer_model.transformer.layer.5.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "93 transformer_model.transformer.layer.5.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "94 transformer_model.transformer.layer.5.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "95 transformer_model.transformer.layer.5.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "96 transformer_model.transformer.layer.5.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "97 transformer_model.transformer.layer.5.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "98 transformer_model.transformer.layer.5.ff.layer_norm.weight torch.Size([1024])\n",
      "99 transformer_model.transformer.layer.5.ff.layer_norm.bias torch.Size([1024])\n",
      "100 transformer_model.transformer.layer.5.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "101 transformer_model.transformer.layer.5.ff.layer_1.bias torch.Size([4096])\n",
      "102 transformer_model.transformer.layer.5.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "103 transformer_model.transformer.layer.5.ff.layer_2.bias torch.Size([1024])\n",
      "104 transformer_model.transformer.layer.6.rel_attn.q torch.Size([1024, 16, 64])\n",
      "105 transformer_model.transformer.layer.6.rel_attn.k torch.Size([1024, 16, 64])\n",
      "106 transformer_model.transformer.layer.6.rel_attn.v torch.Size([1024, 16, 64])\n",
      "107 transformer_model.transformer.layer.6.rel_attn.o torch.Size([1024, 16, 64])\n",
      "108 transformer_model.transformer.layer.6.rel_attn.r torch.Size([1024, 16, 64])\n",
      "109 transformer_model.transformer.layer.6.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "110 transformer_model.transformer.layer.6.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "111 transformer_model.transformer.layer.6.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "112 transformer_model.transformer.layer.6.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "113 transformer_model.transformer.layer.6.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "114 transformer_model.transformer.layer.6.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "115 transformer_model.transformer.layer.6.ff.layer_norm.weight torch.Size([1024])\n",
      "116 transformer_model.transformer.layer.6.ff.layer_norm.bias torch.Size([1024])\n",
      "117 transformer_model.transformer.layer.6.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "118 transformer_model.transformer.layer.6.ff.layer_1.bias torch.Size([4096])\n",
      "119 transformer_model.transformer.layer.6.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "120 transformer_model.transformer.layer.6.ff.layer_2.bias torch.Size([1024])\n",
      "121 transformer_model.transformer.layer.7.rel_attn.q torch.Size([1024, 16, 64])\n",
      "122 transformer_model.transformer.layer.7.rel_attn.k torch.Size([1024, 16, 64])\n",
      "123 transformer_model.transformer.layer.7.rel_attn.v torch.Size([1024, 16, 64])\n",
      "124 transformer_model.transformer.layer.7.rel_attn.o torch.Size([1024, 16, 64])\n",
      "125 transformer_model.transformer.layer.7.rel_attn.r torch.Size([1024, 16, 64])\n",
      "126 transformer_model.transformer.layer.7.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "127 transformer_model.transformer.layer.7.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "128 transformer_model.transformer.layer.7.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "129 transformer_model.transformer.layer.7.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "130 transformer_model.transformer.layer.7.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "131 transformer_model.transformer.layer.7.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "132 transformer_model.transformer.layer.7.ff.layer_norm.weight torch.Size([1024])\n",
      "133 transformer_model.transformer.layer.7.ff.layer_norm.bias torch.Size([1024])\n",
      "134 transformer_model.transformer.layer.7.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "135 transformer_model.transformer.layer.7.ff.layer_1.bias torch.Size([4096])\n",
      "136 transformer_model.transformer.layer.7.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "137 transformer_model.transformer.layer.7.ff.layer_2.bias torch.Size([1024])\n",
      "138 transformer_model.transformer.layer.8.rel_attn.q torch.Size([1024, 16, 64])\n",
      "139 transformer_model.transformer.layer.8.rel_attn.k torch.Size([1024, 16, 64])\n",
      "140 transformer_model.transformer.layer.8.rel_attn.v torch.Size([1024, 16, 64])\n",
      "141 transformer_model.transformer.layer.8.rel_attn.o torch.Size([1024, 16, 64])\n",
      "142 transformer_model.transformer.layer.8.rel_attn.r torch.Size([1024, 16, 64])\n",
      "143 transformer_model.transformer.layer.8.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "144 transformer_model.transformer.layer.8.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "145 transformer_model.transformer.layer.8.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "146 transformer_model.transformer.layer.8.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "147 transformer_model.transformer.layer.8.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "148 transformer_model.transformer.layer.8.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "149 transformer_model.transformer.layer.8.ff.layer_norm.weight torch.Size([1024])\n",
      "150 transformer_model.transformer.layer.8.ff.layer_norm.bias torch.Size([1024])\n",
      "151 transformer_model.transformer.layer.8.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "152 transformer_model.transformer.layer.8.ff.layer_1.bias torch.Size([4096])\n",
      "153 transformer_model.transformer.layer.8.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "154 transformer_model.transformer.layer.8.ff.layer_2.bias torch.Size([1024])\n",
      "155 transformer_model.transformer.layer.9.rel_attn.q torch.Size([1024, 16, 64])\n",
      "156 transformer_model.transformer.layer.9.rel_attn.k torch.Size([1024, 16, 64])\n",
      "157 transformer_model.transformer.layer.9.rel_attn.v torch.Size([1024, 16, 64])\n",
      "158 transformer_model.transformer.layer.9.rel_attn.o torch.Size([1024, 16, 64])\n",
      "159 transformer_model.transformer.layer.9.rel_attn.r torch.Size([1024, 16, 64])\n",
      "160 transformer_model.transformer.layer.9.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "161 transformer_model.transformer.layer.9.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "162 transformer_model.transformer.layer.9.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "163 transformer_model.transformer.layer.9.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "164 transformer_model.transformer.layer.9.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "165 transformer_model.transformer.layer.9.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "166 transformer_model.transformer.layer.9.ff.layer_norm.weight torch.Size([1024])\n",
      "167 transformer_model.transformer.layer.9.ff.layer_norm.bias torch.Size([1024])\n",
      "168 transformer_model.transformer.layer.9.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "169 transformer_model.transformer.layer.9.ff.layer_1.bias torch.Size([4096])\n",
      "170 transformer_model.transformer.layer.9.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "171 transformer_model.transformer.layer.9.ff.layer_2.bias torch.Size([1024])\n",
      "172 transformer_model.transformer.layer.10.rel_attn.q torch.Size([1024, 16, 64])\n",
      "173 transformer_model.transformer.layer.10.rel_attn.k torch.Size([1024, 16, 64])\n",
      "174 transformer_model.transformer.layer.10.rel_attn.v torch.Size([1024, 16, 64])\n",
      "175 transformer_model.transformer.layer.10.rel_attn.o torch.Size([1024, 16, 64])\n",
      "176 transformer_model.transformer.layer.10.rel_attn.r torch.Size([1024, 16, 64])\n",
      "177 transformer_model.transformer.layer.10.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "178 transformer_model.transformer.layer.10.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "179 transformer_model.transformer.layer.10.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "180 transformer_model.transformer.layer.10.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "181 transformer_model.transformer.layer.10.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "182 transformer_model.transformer.layer.10.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "183 transformer_model.transformer.layer.10.ff.layer_norm.weight torch.Size([1024])\n",
      "184 transformer_model.transformer.layer.10.ff.layer_norm.bias torch.Size([1024])\n",
      "185 transformer_model.transformer.layer.10.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "186 transformer_model.transformer.layer.10.ff.layer_1.bias torch.Size([4096])\n",
      "187 transformer_model.transformer.layer.10.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "188 transformer_model.transformer.layer.10.ff.layer_2.bias torch.Size([1024])\n",
      "189 transformer_model.transformer.layer.11.rel_attn.q torch.Size([1024, 16, 64])\n",
      "190 transformer_model.transformer.layer.11.rel_attn.k torch.Size([1024, 16, 64])\n",
      "191 transformer_model.transformer.layer.11.rel_attn.v torch.Size([1024, 16, 64])\n",
      "192 transformer_model.transformer.layer.11.rel_attn.o torch.Size([1024, 16, 64])\n",
      "193 transformer_model.transformer.layer.11.rel_attn.r torch.Size([1024, 16, 64])\n",
      "194 transformer_model.transformer.layer.11.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "195 transformer_model.transformer.layer.11.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "196 transformer_model.transformer.layer.11.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "197 transformer_model.transformer.layer.11.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "198 transformer_model.transformer.layer.11.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "199 transformer_model.transformer.layer.11.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "200 transformer_model.transformer.layer.11.ff.layer_norm.weight torch.Size([1024])\n",
      "201 transformer_model.transformer.layer.11.ff.layer_norm.bias torch.Size([1024])\n",
      "202 transformer_model.transformer.layer.11.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "203 transformer_model.transformer.layer.11.ff.layer_1.bias torch.Size([4096])\n",
      "204 transformer_model.transformer.layer.11.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "205 transformer_model.transformer.layer.11.ff.layer_2.bias torch.Size([1024])\n",
      "206 transformer_model.transformer.layer.12.rel_attn.q torch.Size([1024, 16, 64])\n",
      "207 transformer_model.transformer.layer.12.rel_attn.k torch.Size([1024, 16, 64])\n",
      "208 transformer_model.transformer.layer.12.rel_attn.v torch.Size([1024, 16, 64])\n",
      "209 transformer_model.transformer.layer.12.rel_attn.o torch.Size([1024, 16, 64])\n",
      "210 transformer_model.transformer.layer.12.rel_attn.r torch.Size([1024, 16, 64])\n",
      "211 transformer_model.transformer.layer.12.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "212 transformer_model.transformer.layer.12.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "213 transformer_model.transformer.layer.12.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "214 transformer_model.transformer.layer.12.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "215 transformer_model.transformer.layer.12.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "216 transformer_model.transformer.layer.12.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "217 transformer_model.transformer.layer.12.ff.layer_norm.weight torch.Size([1024])\n",
      "218 transformer_model.transformer.layer.12.ff.layer_norm.bias torch.Size([1024])\n",
      "219 transformer_model.transformer.layer.12.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "220 transformer_model.transformer.layer.12.ff.layer_1.bias torch.Size([4096])\n",
      "221 transformer_model.transformer.layer.12.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "222 transformer_model.transformer.layer.12.ff.layer_2.bias torch.Size([1024])\n",
      "223 transformer_model.transformer.layer.13.rel_attn.q torch.Size([1024, 16, 64])\n",
      "224 transformer_model.transformer.layer.13.rel_attn.k torch.Size([1024, 16, 64])\n",
      "225 transformer_model.transformer.layer.13.rel_attn.v torch.Size([1024, 16, 64])\n",
      "226 transformer_model.transformer.layer.13.rel_attn.o torch.Size([1024, 16, 64])\n",
      "227 transformer_model.transformer.layer.13.rel_attn.r torch.Size([1024, 16, 64])\n",
      "228 transformer_model.transformer.layer.13.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "229 transformer_model.transformer.layer.13.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "230 transformer_model.transformer.layer.13.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "231 transformer_model.transformer.layer.13.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "232 transformer_model.transformer.layer.13.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "233 transformer_model.transformer.layer.13.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "234 transformer_model.transformer.layer.13.ff.layer_norm.weight torch.Size([1024])\n",
      "235 transformer_model.transformer.layer.13.ff.layer_norm.bias torch.Size([1024])\n",
      "236 transformer_model.transformer.layer.13.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "237 transformer_model.transformer.layer.13.ff.layer_1.bias torch.Size([4096])\n",
      "238 transformer_model.transformer.layer.13.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "239 transformer_model.transformer.layer.13.ff.layer_2.bias torch.Size([1024])\n",
      "240 transformer_model.transformer.layer.14.rel_attn.q torch.Size([1024, 16, 64])\n",
      "241 transformer_model.transformer.layer.14.rel_attn.k torch.Size([1024, 16, 64])\n",
      "242 transformer_model.transformer.layer.14.rel_attn.v torch.Size([1024, 16, 64])\n",
      "243 transformer_model.transformer.layer.14.rel_attn.o torch.Size([1024, 16, 64])\n",
      "244 transformer_model.transformer.layer.14.rel_attn.r torch.Size([1024, 16, 64])\n",
      "245 transformer_model.transformer.layer.14.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "246 transformer_model.transformer.layer.14.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "247 transformer_model.transformer.layer.14.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "248 transformer_model.transformer.layer.14.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "249 transformer_model.transformer.layer.14.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "250 transformer_model.transformer.layer.14.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "251 transformer_model.transformer.layer.14.ff.layer_norm.weight torch.Size([1024])\n",
      "252 transformer_model.transformer.layer.14.ff.layer_norm.bias torch.Size([1024])\n",
      "253 transformer_model.transformer.layer.14.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "254 transformer_model.transformer.layer.14.ff.layer_1.bias torch.Size([4096])\n",
      "255 transformer_model.transformer.layer.14.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "256 transformer_model.transformer.layer.14.ff.layer_2.bias torch.Size([1024])\n",
      "257 transformer_model.transformer.layer.15.rel_attn.q torch.Size([1024, 16, 64])\n",
      "258 transformer_model.transformer.layer.15.rel_attn.k torch.Size([1024, 16, 64])\n",
      "259 transformer_model.transformer.layer.15.rel_attn.v torch.Size([1024, 16, 64])\n",
      "260 transformer_model.transformer.layer.15.rel_attn.o torch.Size([1024, 16, 64])\n",
      "261 transformer_model.transformer.layer.15.rel_attn.r torch.Size([1024, 16, 64])\n",
      "262 transformer_model.transformer.layer.15.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "263 transformer_model.transformer.layer.15.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "264 transformer_model.transformer.layer.15.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "265 transformer_model.transformer.layer.15.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "266 transformer_model.transformer.layer.15.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "267 transformer_model.transformer.layer.15.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "268 transformer_model.transformer.layer.15.ff.layer_norm.weight torch.Size([1024])\n",
      "269 transformer_model.transformer.layer.15.ff.layer_norm.bias torch.Size([1024])\n",
      "270 transformer_model.transformer.layer.15.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "271 transformer_model.transformer.layer.15.ff.layer_1.bias torch.Size([4096])\n",
      "272 transformer_model.transformer.layer.15.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "273 transformer_model.transformer.layer.15.ff.layer_2.bias torch.Size([1024])\n",
      "274 transformer_model.transformer.layer.16.rel_attn.q torch.Size([1024, 16, 64])\n",
      "275 transformer_model.transformer.layer.16.rel_attn.k torch.Size([1024, 16, 64])\n",
      "276 transformer_model.transformer.layer.16.rel_attn.v torch.Size([1024, 16, 64])\n",
      "277 transformer_model.transformer.layer.16.rel_attn.o torch.Size([1024, 16, 64])\n",
      "278 transformer_model.transformer.layer.16.rel_attn.r torch.Size([1024, 16, 64])\n",
      "279 transformer_model.transformer.layer.16.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "280 transformer_model.transformer.layer.16.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "281 transformer_model.transformer.layer.16.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "282 transformer_model.transformer.layer.16.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "283 transformer_model.transformer.layer.16.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "284 transformer_model.transformer.layer.16.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "285 transformer_model.transformer.layer.16.ff.layer_norm.weight torch.Size([1024])\n",
      "286 transformer_model.transformer.layer.16.ff.layer_norm.bias torch.Size([1024])\n",
      "287 transformer_model.transformer.layer.16.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "288 transformer_model.transformer.layer.16.ff.layer_1.bias torch.Size([4096])\n",
      "289 transformer_model.transformer.layer.16.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "290 transformer_model.transformer.layer.16.ff.layer_2.bias torch.Size([1024])\n",
      "291 transformer_model.transformer.layer.17.rel_attn.q torch.Size([1024, 16, 64])\n",
      "292 transformer_model.transformer.layer.17.rel_attn.k torch.Size([1024, 16, 64])\n",
      "293 transformer_model.transformer.layer.17.rel_attn.v torch.Size([1024, 16, 64])\n",
      "294 transformer_model.transformer.layer.17.rel_attn.o torch.Size([1024, 16, 64])\n",
      "295 transformer_model.transformer.layer.17.rel_attn.r torch.Size([1024, 16, 64])\n",
      "296 transformer_model.transformer.layer.17.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "297 transformer_model.transformer.layer.17.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "298 transformer_model.transformer.layer.17.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "299 transformer_model.transformer.layer.17.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "300 transformer_model.transformer.layer.17.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "301 transformer_model.transformer.layer.17.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "302 transformer_model.transformer.layer.17.ff.layer_norm.weight torch.Size([1024])\n",
      "303 transformer_model.transformer.layer.17.ff.layer_norm.bias torch.Size([1024])\n",
      "304 transformer_model.transformer.layer.17.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "305 transformer_model.transformer.layer.17.ff.layer_1.bias torch.Size([4096])\n",
      "306 transformer_model.transformer.layer.17.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "307 transformer_model.transformer.layer.17.ff.layer_2.bias torch.Size([1024])\n",
      "308 transformer_model.transformer.layer.18.rel_attn.q torch.Size([1024, 16, 64])\n",
      "309 transformer_model.transformer.layer.18.rel_attn.k torch.Size([1024, 16, 64])\n",
      "310 transformer_model.transformer.layer.18.rel_attn.v torch.Size([1024, 16, 64])\n",
      "311 transformer_model.transformer.layer.18.rel_attn.o torch.Size([1024, 16, 64])\n",
      "312 transformer_model.transformer.layer.18.rel_attn.r torch.Size([1024, 16, 64])\n",
      "313 transformer_model.transformer.layer.18.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "314 transformer_model.transformer.layer.18.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "315 transformer_model.transformer.layer.18.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "316 transformer_model.transformer.layer.18.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "317 transformer_model.transformer.layer.18.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "318 transformer_model.transformer.layer.18.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "319 transformer_model.transformer.layer.18.ff.layer_norm.weight torch.Size([1024])\n",
      "320 transformer_model.transformer.layer.18.ff.layer_norm.bias torch.Size([1024])\n",
      "321 transformer_model.transformer.layer.18.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "322 transformer_model.transformer.layer.18.ff.layer_1.bias torch.Size([4096])\n",
      "323 transformer_model.transformer.layer.18.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "324 transformer_model.transformer.layer.18.ff.layer_2.bias torch.Size([1024])\n",
      "325 transformer_model.transformer.layer.19.rel_attn.q torch.Size([1024, 16, 64])\n",
      "326 transformer_model.transformer.layer.19.rel_attn.k torch.Size([1024, 16, 64])\n",
      "327 transformer_model.transformer.layer.19.rel_attn.v torch.Size([1024, 16, 64])\n",
      "328 transformer_model.transformer.layer.19.rel_attn.o torch.Size([1024, 16, 64])\n",
      "329 transformer_model.transformer.layer.19.rel_attn.r torch.Size([1024, 16, 64])\n",
      "330 transformer_model.transformer.layer.19.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "331 transformer_model.transformer.layer.19.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "332 transformer_model.transformer.layer.19.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "333 transformer_model.transformer.layer.19.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "334 transformer_model.transformer.layer.19.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "335 transformer_model.transformer.layer.19.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "336 transformer_model.transformer.layer.19.ff.layer_norm.weight torch.Size([1024])\n",
      "337 transformer_model.transformer.layer.19.ff.layer_norm.bias torch.Size([1024])\n",
      "338 transformer_model.transformer.layer.19.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "339 transformer_model.transformer.layer.19.ff.layer_1.bias torch.Size([4096])\n",
      "340 transformer_model.transformer.layer.19.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "341 transformer_model.transformer.layer.19.ff.layer_2.bias torch.Size([1024])\n",
      "342 transformer_model.transformer.layer.20.rel_attn.q torch.Size([1024, 16, 64])\n",
      "343 transformer_model.transformer.layer.20.rel_attn.k torch.Size([1024, 16, 64])\n",
      "344 transformer_model.transformer.layer.20.rel_attn.v torch.Size([1024, 16, 64])\n",
      "345 transformer_model.transformer.layer.20.rel_attn.o torch.Size([1024, 16, 64])\n",
      "346 transformer_model.transformer.layer.20.rel_attn.r torch.Size([1024, 16, 64])\n",
      "347 transformer_model.transformer.layer.20.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "348 transformer_model.transformer.layer.20.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "349 transformer_model.transformer.layer.20.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "350 transformer_model.transformer.layer.20.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "351 transformer_model.transformer.layer.20.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "352 transformer_model.transformer.layer.20.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "353 transformer_model.transformer.layer.20.ff.layer_norm.weight torch.Size([1024])\n",
      "354 transformer_model.transformer.layer.20.ff.layer_norm.bias torch.Size([1024])\n",
      "355 transformer_model.transformer.layer.20.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "356 transformer_model.transformer.layer.20.ff.layer_1.bias torch.Size([4096])\n",
      "357 transformer_model.transformer.layer.20.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "358 transformer_model.transformer.layer.20.ff.layer_2.bias torch.Size([1024])\n",
      "359 transformer_model.transformer.layer.21.rel_attn.q torch.Size([1024, 16, 64])\n",
      "360 transformer_model.transformer.layer.21.rel_attn.k torch.Size([1024, 16, 64])\n",
      "361 transformer_model.transformer.layer.21.rel_attn.v torch.Size([1024, 16, 64])\n",
      "362 transformer_model.transformer.layer.21.rel_attn.o torch.Size([1024, 16, 64])\n",
      "363 transformer_model.transformer.layer.21.rel_attn.r torch.Size([1024, 16, 64])\n",
      "364 transformer_model.transformer.layer.21.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "365 transformer_model.transformer.layer.21.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "366 transformer_model.transformer.layer.21.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "367 transformer_model.transformer.layer.21.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "368 transformer_model.transformer.layer.21.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "369 transformer_model.transformer.layer.21.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "370 transformer_model.transformer.layer.21.ff.layer_norm.weight torch.Size([1024])\n",
      "371 transformer_model.transformer.layer.21.ff.layer_norm.bias torch.Size([1024])\n",
      "372 transformer_model.transformer.layer.21.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "373 transformer_model.transformer.layer.21.ff.layer_1.bias torch.Size([4096])\n",
      "374 transformer_model.transformer.layer.21.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "375 transformer_model.transformer.layer.21.ff.layer_2.bias torch.Size([1024])\n",
      "376 transformer_model.transformer.layer.22.rel_attn.q torch.Size([1024, 16, 64])\n",
      "377 transformer_model.transformer.layer.22.rel_attn.k torch.Size([1024, 16, 64])\n",
      "378 transformer_model.transformer.layer.22.rel_attn.v torch.Size([1024, 16, 64])\n",
      "379 transformer_model.transformer.layer.22.rel_attn.o torch.Size([1024, 16, 64])\n",
      "380 transformer_model.transformer.layer.22.rel_attn.r torch.Size([1024, 16, 64])\n",
      "381 transformer_model.transformer.layer.22.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "382 transformer_model.transformer.layer.22.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "383 transformer_model.transformer.layer.22.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "384 transformer_model.transformer.layer.22.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "385 transformer_model.transformer.layer.22.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "386 transformer_model.transformer.layer.22.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "387 transformer_model.transformer.layer.22.ff.layer_norm.weight torch.Size([1024])\n",
      "388 transformer_model.transformer.layer.22.ff.layer_norm.bias torch.Size([1024])\n",
      "389 transformer_model.transformer.layer.22.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "390 transformer_model.transformer.layer.22.ff.layer_1.bias torch.Size([4096])\n",
      "391 transformer_model.transformer.layer.22.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "392 transformer_model.transformer.layer.22.ff.layer_2.bias torch.Size([1024])\n",
      "393 transformer_model.transformer.layer.23.rel_attn.q torch.Size([1024, 16, 64])\n",
      "394 transformer_model.transformer.layer.23.rel_attn.k torch.Size([1024, 16, 64])\n",
      "395 transformer_model.transformer.layer.23.rel_attn.v torch.Size([1024, 16, 64])\n",
      "396 transformer_model.transformer.layer.23.rel_attn.o torch.Size([1024, 16, 64])\n",
      "397 transformer_model.transformer.layer.23.rel_attn.r torch.Size([1024, 16, 64])\n",
      "398 transformer_model.transformer.layer.23.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "399 transformer_model.transformer.layer.23.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "400 transformer_model.transformer.layer.23.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "401 transformer_model.transformer.layer.23.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "402 transformer_model.transformer.layer.23.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "403 transformer_model.transformer.layer.23.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "404 transformer_model.transformer.layer.23.ff.layer_norm.weight torch.Size([1024])\n",
      "405 transformer_model.transformer.layer.23.ff.layer_norm.bias torch.Size([1024])\n",
      "406 transformer_model.transformer.layer.23.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "407 transformer_model.transformer.layer.23.ff.layer_1.bias torch.Size([4096])\n",
      "408 transformer_model.transformer.layer.23.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "409 transformer_model.transformer.layer.23.ff.layer_2.bias torch.Size([1024])\n",
      "410 transformer_model.sequence_summary.summary.weight torch.Size([1024, 1024])\n",
      "411 transformer_model.sequence_summary.summary.bias torch.Size([1024])\n",
      "412 transformer_model.logits_proj.weight torch.Size([2, 1024])\n",
      "413 transformer_model.logits_proj.bias torch.Size([2])\n",
      "414 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "415 attention.hidden_layer.bias torch.Size([512])\n",
      "416 attention.final_layer.weight torch.Size([1, 512])\n",
      "417 attention.final_layer.bias torch.Size([1])\n",
      "418 regressor.weight torch.Size([1, 1024])\n",
      "419 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-13.6484,  24.9908,  -8.6831,  ...,  -3.9337,  19.5884,   3.1343],\n",
       "        [-16.2897, -26.9207, -18.0901,  ...,  -2.8086, -12.4488,  12.2931],\n",
       "        [-30.4366, -16.2234,   9.5971,  ...,  29.0412, -13.2017,   1.4963],\n",
       "        ...,\n",
       "        [ 11.8891,  20.6609,  34.4373,  ...,  15.0036,  10.8892,  14.0213],\n",
       "        [-20.6304,  21.4410,  22.3446,  ...,  -1.2921,  30.8943, -24.8172],\n",
       "        [ -2.9513,   4.5531,  12.7362,  ...,   1.6763,   4.2718,  -8.4886]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    attention_param_start = 414\n",
    "    regressor_param_start = 418\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 325\n",
    "    layer_middle_threshold = 189\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "#                 mse.backward()\n",
    "#                 self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(base_lr, last_lr, fold=0, epochs=3):\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler, num_epochs=epochs)\n",
    "    rmse_val = trainer.train()\n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c9d397f-aa6e-4823-a4fd-b8625c580972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: { 'base_lr': 0.0001190683694379101, 'last_lr': 0.00017987585986205585, 'epochs': 4 } Best value: 0.49271923303604126\n",
    "# Fold 1: {'base_lr': 0.00012114635348406963, 'last_lr': 0.0005477206613438486, 'epochs': 4}. Best value:  0.45853328704833984\n",
    "# Fold 2: {'base_lr': 5.24730490640746e-05, 'last_lr': 0.00020041362261812433, 'epochs': 4}   Best value:  0.49088865518569946\n",
    "# Fold 3: {'base_lr': 6.108276630664184e-05, 'last_lr': 0.00011544056953737668, 'epochs': 4}. Best value:  0.4930591881275177\n",
    "# Fold 4: {'base_lr': 0.0001717178883932075, 'last_lr': 0.00042448836147656634, 'epochs': 4}  Best value:  0.48955243825912476\n",
    "# Fold 5: {'base_lr': 0.000135700916847811, 'last_lr': 0.0029640935672153, 'epochs': 4}.      Best value:  0.4688156247138977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fbc2b14-ea34-48b6-82e9-10809a6d9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [\n",
    "    {'base_lr': 0.0001190683694379101, 'last_lr': 0.00017987585986205585, 'epochs': 4 },\n",
    "    {'base_lr': 0.00012114635348406963, 'last_lr': 0.0005477206613438486, 'epochs': 4},\n",
    "    {'base_lr': 5.24730490640746e-05, 'last_lr': 0.00020041362261812433, 'epochs': 4},\n",
    "    {'base_lr': 6.108276630664184e-05, 'last_lr': 0.00011544056953737668, 'epochs': 4},\n",
    "    {'base_lr': 0.0001717178883932075, 'last_lr': 0.00042448836147656634, 'epochs': 4},\n",
    "    {'base_lr': 0.000135700916847811, 'last_lr': 0.0029640935672153, 'epochs': 4}  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.0001190683694379101 last_lr 0.00017987585986205585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dce10ceb8e94b91a04f68e81412119c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.234 New best_val_rmse: 1.234\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.083 New best_val_rmse: 1.083\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9715 New best_val_rmse: 0.9715\n",
      "\n",
      "16 steps took 6.87 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6267 New best_val_rmse: 0.6267\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7796 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5865 New best_val_rmse: 0.5865\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6631 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6396 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5969 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 7.14 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6296 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.577 New best_val_rmse: 0.577\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5409 New best_val_rmse: 0.5409\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5532 Still best_val_rmse: 0.5409 (from epoch 1)\n",
      "\n",
      "16 steps took 7.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5472 Still best_val_rmse: 0.5409 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5376 New best_val_rmse: 0.5376\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.522 New best_val_rmse: 0.522\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5289 Still best_val_rmse: 0.522 (from epoch 1)\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5452 Still best_val_rmse: 0.522 (from epoch 1)\n",
      "\n",
      "16 steps took 7.12 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.551 Still best_val_rmse: 0.522 (from epoch 1)\n",
      "\n",
      "16 steps took 6.76 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5481 Still best_val_rmse: 0.522 (from epoch 1)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 3.48 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5169 Still best_val_rmse: 0.4935 (from epoch 2)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4944 Still best_val_rmse: 0.4935 (from epoch 2)\n",
      "\n",
      "8 steps took 3.54 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 1.77 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4913 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.47 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4929 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4886 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4893 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4912 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4944 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4919 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4907 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4904 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 4.41 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4904 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.62 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4904 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.53 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4906 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4902 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4896 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4893 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.73 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4892 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4894 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.73 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4898 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4892 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.491 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5039 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4926 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4927 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5273 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5199 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5205 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.48742327094078064\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00012114635348406963 last_lr 0.0005477206613438486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cde1d02add040e7b54f2dc09e7920f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.74 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.01 New best_val_rmse: 2.01\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8928 New best_val_rmse: 0.8928\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9454 Still best_val_rmse: 0.8928 (from epoch 0)\n",
      "\n",
      "16 steps took 6.95 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7085 New best_val_rmse: 0.7085\n",
      "\n",
      "16 steps took 7.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6477 New best_val_rmse: 0.6477\n",
      "\n",
      "16 steps took 7.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7885 Still best_val_rmse: 0.6477 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6322 New best_val_rmse: 0.6322\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5733 New best_val_rmse: 0.5733\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5996 Still best_val_rmse: 0.5733 (from epoch 0)\n",
      "\n",
      "16 steps took 7.09 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5363 New best_val_rmse: 0.5363\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5814 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5421 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5176 New best_val_rmse: 0.5176\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6604 Still best_val_rmse: 0.5176 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5717 Still best_val_rmse: 0.5176 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5487 Still best_val_rmse: 0.5176 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5086 New best_val_rmse: 0.5086\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5192 Still best_val_rmse: 0.5086 (from epoch 1)\n",
      "\n",
      "16 steps took 7.31 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5117 Still best_val_rmse: 0.5086 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4886 New best_val_rmse: 0.4886\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4949 Still best_val_rmse: 0.488 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4995 Still best_val_rmse: 0.488 (from epoch 2)\n",
      "\n",
      "8 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4862 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4937 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "8 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.481 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4829 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4859 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.5066 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4866 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4821 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4788 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4815 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4848 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4835 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4845 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4823 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4802 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4789 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4784 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4781 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4781 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4781 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4781 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 1.4 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4783 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.857 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4781 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 2)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.478 New best_val_rmse: 0.478\n",
      "\n",
      "2 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.478 Still best_val_rmse: 0.478 (from epoch 3)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4775 New best_val_rmse: 0.4775\n",
      "\n",
      "2 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 0.882 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.893 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4774 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.859 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4773 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4773 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4774 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4775 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4775 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4776 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4778 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4782 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4788 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4787 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4795 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.481 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4826 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.76 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4801 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4775 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4781 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4807 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4806 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4865 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4825 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.48 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4792 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4818 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4778 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4805 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4797 Still best_val_rmse: 0.4773 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4775 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4784 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "2 steps took 0.946 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4826 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.489 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4794 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4823 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4874 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4933 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4977 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4774 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4871 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4851 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.5096 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.4764355719089508\n",
      "##### Using fold 2\n",
      "##### Using base_lr 5.24730490640746e-05 last_lr 0.00020041362261812433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50394aafc43643aa834b041f40ca3eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.72 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.29 New best_val_rmse: 1.29\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7146 New best_val_rmse: 0.7146\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7228 Still best_val_rmse: 0.7146 (from epoch 0)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8687 Still best_val_rmse: 0.7146 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6247 New best_val_rmse: 0.6247\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6099 New best_val_rmse: 0.6099\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7235 Still best_val_rmse: 0.6099 (from epoch 0)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5876 New best_val_rmse: 0.5876\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8011 Still best_val_rmse: 0.5876 (from epoch 0)\n",
      "\n",
      "16 steps took 7.08 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5691 New best_val_rmse: 0.5691\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5306 New best_val_rmse: 0.5306\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.539 Still best_val_rmse: 0.5306 (from epoch 1)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5364 Still best_val_rmse: 0.5306 (from epoch 1)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5283 New best_val_rmse: 0.5283\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5158 New best_val_rmse: 0.5158\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5217 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5539 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 6.86 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5274 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 7.56 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5659 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5166 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5133 New best_val_rmse: 0.5133\n",
      "\n",
      "16 steps took 6.74 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.513 New best_val_rmse: 0.513\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5115 New best_val_rmse: 0.5115\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5092 New best_val_rmse: 0.5092\n",
      "\n",
      "16 steps took 6.9 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5097 Still best_val_rmse: 0.5092 (from epoch 2)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5083 New best_val_rmse: 0.5083\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5077 New best_val_rmse: 0.5077\n",
      "\n",
      "16 steps took 7.36 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5077 Still best_val_rmse: 0.5077 (from epoch 2)\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.508 Still best_val_rmse: 0.5077 (from epoch 2)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5071 New best_val_rmse: 0.5071\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5082 Still best_val_rmse: 0.5071 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5105 Still best_val_rmse: 0.5071 (from epoch 3)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5103 Still best_val_rmse: 0.5071 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.508 Still best_val_rmse: 0.5057 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5102 Still best_val_rmse: 0.5057 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.5056867003440857\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.108276630664184e-05 last_lr 0.00011544056953737668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfb07c32e5b46b3bfe489b217aa2130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.048 New best_val_rmse: 1.048\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7279 New best_val_rmse: 0.7279\n",
      "\n",
      "16 steps took 6.79 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7155 New best_val_rmse: 0.7155\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7608 Still best_val_rmse: 0.7155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6977 New best_val_rmse: 0.6977\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5527 New best_val_rmse: 0.5527\n",
      "\n",
      "16 steps took 6.77 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5965 Still best_val_rmse: 0.5527 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5268 New best_val_rmse: 0.5268\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.57 Still best_val_rmse: 0.5268 (from epoch 0)\n",
      "\n",
      "16 steps took 7.15 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5818 Still best_val_rmse: 0.5268 (from epoch 0)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5265 New best_val_rmse: 0.5265\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.534 Still best_val_rmse: 0.5265 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5188 New best_val_rmse: 0.5188\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5442 Still best_val_rmse: 0.5188 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5089 New best_val_rmse: 0.5089\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5241 Still best_val_rmse: 0.5089 (from epoch 1)\n",
      "\n",
      "16 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5111 Still best_val_rmse: 0.5089 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 7.29 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5106 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5009 New best_val_rmse: 0.5009\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4955 New best_val_rmse: 0.4955\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5123 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4986 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4974 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5005 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.498 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4973 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.5008 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4998 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4994 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.499 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.89 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.499 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4986 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4975 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.497 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4973 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4991 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4994 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4987 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4997 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4954 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.496 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4957 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "8 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.499 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4968 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5161 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "16 steps took 6.8 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.526 Still best_val_rmse: 0.4953 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.49525561928749084\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001717178883932075 last_lr 0.00042448836147656634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4494897e2cbf4e88bbc2b7f8baf8dc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.608 New best_val_rmse: 1.608\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.143 New best_val_rmse: 1.143\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9745 New best_val_rmse: 0.9745\n",
      "\n",
      "16 steps took 6.82 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7911 New best_val_rmse: 0.7911\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6734 New best_val_rmse: 0.6734\n",
      "\n",
      "16 steps took 6.81 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6363 New best_val_rmse: 0.6363\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6557 Still best_val_rmse: 0.6363 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6154 New best_val_rmse: 0.6154\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.597 New best_val_rmse: 0.597\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5882 New best_val_rmse: 0.5882\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5483 New best_val_rmse: 0.5483\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5675 Still best_val_rmse: 0.5483 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5653 Still best_val_rmse: 0.5483 (from epoch 1)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5545 Still best_val_rmse: 0.5483 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.568 Still best_val_rmse: 0.5483 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5387 New best_val_rmse: 0.5387\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5344 New best_val_rmse: 0.5344\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5617 Still best_val_rmse: 0.5344 (from epoch 1)\n",
      "\n",
      "16 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5213 New best_val_rmse: 0.5213\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5098 New best_val_rmse: 0.5098\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5116 Still best_val_rmse: 0.5098 (from epoch 2)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.529 Still best_val_rmse: 0.5098 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5072 New best_val_rmse: 0.5072\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5109 Still best_val_rmse: 0.5072 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5061 New best_val_rmse: 0.5061\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5045 New best_val_rmse: 0.5045\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.504 New best_val_rmse: 0.504\n",
      "\n",
      "16 steps took 7.35 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.504 New best_val_rmse: 0.504\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5038 New best_val_rmse: 0.5038\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5027 New best_val_rmse: 0.5027\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5024 New best_val_rmse: 0.5024\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5038 Still best_val_rmse: 0.5024 (from epoch 3)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5025 Still best_val_rmse: 0.5024 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5026 Still best_val_rmse: 0.5024 (from epoch 3)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.503 Still best_val_rmse: 0.5024 (from epoch 3)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5316 Still best_val_rmse: 0.5024 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.5023521780967712\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.000135700916847811 last_lr 0.0029640935672153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68cccddde2443d7b231f03891223f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.65 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.176 New best_val_rmse: 1.176\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8045 New best_val_rmse: 0.8045\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6749 New best_val_rmse: 0.6749\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6764 Still best_val_rmse: 0.6749 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6482 New best_val_rmse: 0.6482\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7031 Still best_val_rmse: 0.6482 (from epoch 0)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5976 New best_val_rmse: 0.5976\n",
      "\n",
      "16 steps took 6.92 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6539 Still best_val_rmse: 0.5976 (from epoch 0)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5467 New best_val_rmse: 0.5467\n",
      "\n",
      "16 steps took 7.09 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5525 Still best_val_rmse: 0.5467 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5992 Still best_val_rmse: 0.5467 (from epoch 0)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5289 New best_val_rmse: 0.5289\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5036 New best_val_rmse: 0.5036\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5487 Still best_val_rmse: 0.5036 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5005 New best_val_rmse: 0.5005\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4968 New best_val_rmse: 0.4968\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5058 Still best_val_rmse: 0.4968 (from epoch 1)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5044 Still best_val_rmse: 0.4968 (from epoch 1)\n",
      "\n",
      "16 steps took 7.19 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4994 Still best_val_rmse: 0.4968 (from epoch 1)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4943 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4842 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4818 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.5068 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4863 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "4 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.483 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4777 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4793 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4816 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4788 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4778 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4776 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4771 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4769 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4773 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4762 New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4762 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4765 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.479 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4819 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4811 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4829 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4814 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4803 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4805 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4804 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4794 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4789 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4786 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4783 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.478 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4778 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4777 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4776 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4775 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4774 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4774 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.42 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4774 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4776 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4777 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4781 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4785 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4787 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4785 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4783 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4778 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4767 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4766 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4764 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4764 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4773 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4781 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4796 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4824 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4816 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.86 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4776 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4775 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4782 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4779 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4781 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4811 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4965 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "8 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4801 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4775 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4772 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4866 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4765 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4854 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.515 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4894 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4811 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4858 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4974 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.47610223293304443\n",
      "CPU times: user 1h 34min, sys: 12min 57s, total: 1h 46min 58s\n",
      "Wall time: 1h 22min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rmse_values = []\n",
    "for i in range(len(list(splits))):\n",
    "    fold = i\n",
    "    lrs = lr_list[fold]\n",
    "    rmse_val = train_fold(lrs['base_lr'], lrs['last_lr'], fold=fold, epochs=lrs['epochs'])\n",
    "    print(f'Final RMSE: {rmse_val}')\n",
    "    rmse_values.append(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b28ebe4-316a-47e7-b20e-64a20ee4c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean RMSE values: 0.49054256081581116'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'mean RMSE values: {np.mean(np.array(rmse_values))}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 11.4 s, total: 38.6 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7c0b1271ed4a789601d92ec69c1bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab720956de7844b78aeb9c86d534d285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.33515096550935586\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3193098077707963\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.32251004947632655\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.29293254002942654\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.32323430296322003\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3466767047877188\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe815766a440b3b531ad842c0159a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292f681686b44f2299761cb801b16f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3182430431461126\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3219037882068193\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2888435193030401\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.31096691853123826\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.30858202611891195\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.32937737896600716\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6252380c38804ca4afe58b50185d0829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce60065d24042b5af5c3710d73a91ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.30032440012317796\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.31748911392174217\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.26298940173781604\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.29722142082890035\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.29362615459623664\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.31486126150890403\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1207fe8d0f964f879fc8695e84207dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699ac5cd6cd84d2892f6500ee83e1ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.32329676470865015\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.34047391247728676\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3214505356250382\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.32061903880621\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.32456095571720045\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3349042720639877\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cb317d1f0244c88724748f75c42d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3754ac7509d4f56af345fe0f58c0b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3293199886529168\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.314861642129274\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2981218258731394\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3016225692878526\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.32915910446759183\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3386687143116818\n",
      "FINAL RMSE score 0.316043404054886\n",
      "CPU times: user 8min 21s, sys: 8.83 s, total: 8min 30s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.009306311708299965],\n",
       " [0.0010705579687654103],\n",
       " [-0.0016648509076259838],\n",
       " [0.008451955668970617],\n",
       " [0.004051537076226355]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19885158, 0.20048368, 0.20289382, 0.19817944, 0.19959147])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.8581692301321396)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22107183, -0.40934184, -0.51568938, -2.30189005, -1.91636142,\n",
       "       -0.95218495,  0.3090395 ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.10144311094582559, -0.020288622189165117)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.322515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.510785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.617132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.403333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-2.017805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.053628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.207596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.322515\n",
       "1  f0953f0a5 -0.510785\n",
       "2  0df072751 -0.617132\n",
       "3  04caf4e0c -2.403333\n",
       "4  0e63f8bea -2.017805\n",
       "5  12537fe78 -1.053628\n",
       "6  965e592c0  0.207596"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xlnet-large-cased'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/xlnet-large-cased/best')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/xlnet-large-cased_1'),\n",
       " PosixPath('/home/commonlit/models/xlnet-large-cased_2'),\n",
       " PosixPath('/home/commonlit/models/xlnet-large-cased_3'),\n",
       " PosixPath('/home/commonlit/models/xlnet-large-cased_4'),\n",
       " PosixPath('/home/commonlit/models/xlnet-large-cased_5'),\n",
       " PosixPath('/home/commonlit/models/xlnet-large-cased_6')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        special_tokens = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/special_tokens_map.json'))\n",
    "        assert special_tokens.exists()\n",
    "        copyfile(special_tokens, tokenizer_path/'special_tokens_map')\n",
    "        \n",
    "        spiece_model = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/spiece.model'))\n",
    "        assert spiece_model.exists()\n",
    "        copyfile(spiece_model, tokenizer_path/'spiece.model')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/xlnet-large-cased/best_models.zip'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/xlnet-large-cased.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\t/home/commonlit/models/xlnet-large-cased/best/tokenizer-1\n",
      "2.1M\t/home/commonlit/models/xlnet-large-cased/best/tokenizer-2\n",
      "2.1M\t/home/commonlit/models/xlnet-large-cased/best/tokenizer-3\n",
      "2.1M\t/home/commonlit/models/xlnet-large-cased/best/tokenizer-4\n",
      "2.1M\t/home/commonlit/models/xlnet-large-cased/best/tokenizer-5\n",
      "2.1M\t/home/commonlit/models/xlnet-large-cased/best/tokenizer-6\n",
      "8.2G\t/home/commonlit/models/xlnet-large-cased/best\n",
      "7.6G\t/home/commonlit/models/xlnet-large-cased/best_models.zip\n",
      "1.4G\t/home/commonlit/models/xlnet-large-cased/lm\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/xlnet-large-cased/lm.zip'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/xlnet-large-cased/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-xlnet-large-cased\",\n",
      "  \"id\": \"gilfernandes/commonlit-xlnet-large-cased\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.52G/7.52G [12:42<00:00, 10.6MB/s]\n",
      "Upload successful: best_models.zip (8GB)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.25G/1.25G [02:04<00:00, 10.8MB/s]\n",
      "Upload successful: lm.zip (1GB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-xlnet-large-cased\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with spiece.model\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
