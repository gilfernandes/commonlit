{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'google/electra-large-discriminator'\n",
    "    TOKENIZER_PATH = 'google/electra-large-discriminator'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'electra-large-discriminator'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.electra.embeddings.word_embeddings.weight torch.Size([30522, 1024])\n",
      "1 transformer_model.electra.embeddings.position_embeddings.weight torch.Size([512, 1024])\n",
      "2 transformer_model.electra.embeddings.token_type_embeddings.weight torch.Size([2, 1024])\n",
      "3 transformer_model.electra.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "4 transformer_model.electra.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "5 transformer_model.electra.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "6 transformer_model.electra.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "7 transformer_model.electra.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.electra.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "9 transformer_model.electra.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.electra.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "11 transformer_model.electra.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "12 transformer_model.electra.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "13 transformer_model.electra.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "14 transformer_model.electra.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "15 transformer_model.electra.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.electra.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "17 transformer_model.electra.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "18 transformer_model.electra.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "19 transformer_model.electra.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "20 transformer_model.electra.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "21 transformer_model.electra.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "22 transformer_model.electra.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "23 transformer_model.electra.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.electra.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "25 transformer_model.electra.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.electra.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "27 transformer_model.electra.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "28 transformer_model.electra.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "29 transformer_model.electra.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "30 transformer_model.electra.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "31 transformer_model.electra.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "32 transformer_model.electra.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "33 transformer_model.electra.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "34 transformer_model.electra.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "35 transformer_model.electra.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "36 transformer_model.electra.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "37 transformer_model.electra.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "38 transformer_model.electra.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "39 transformer_model.electra.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.electra.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "41 transformer_model.electra.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.electra.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "43 transformer_model.electra.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "44 transformer_model.electra.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "45 transformer_model.electra.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "46 transformer_model.electra.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "47 transformer_model.electra.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "48 transformer_model.electra.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "49 transformer_model.electra.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "50 transformer_model.electra.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "51 transformer_model.electra.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "52 transformer_model.electra.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "53 transformer_model.electra.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "54 transformer_model.electra.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "55 transformer_model.electra.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.electra.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "57 transformer_model.electra.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.electra.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "59 transformer_model.electra.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "60 transformer_model.electra.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "61 transformer_model.electra.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "62 transformer_model.electra.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "63 transformer_model.electra.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "64 transformer_model.electra.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "65 transformer_model.electra.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "66 transformer_model.electra.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "67 transformer_model.electra.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "68 transformer_model.electra.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "69 transformer_model.electra.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "70 transformer_model.electra.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "71 transformer_model.electra.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.electra.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "73 transformer_model.electra.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.electra.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "75 transformer_model.electra.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "76 transformer_model.electra.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "77 transformer_model.electra.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "78 transformer_model.electra.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "79 transformer_model.electra.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "80 transformer_model.electra.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "81 transformer_model.electra.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "82 transformer_model.electra.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "83 transformer_model.electra.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "84 transformer_model.electra.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "85 transformer_model.electra.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "86 transformer_model.electra.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "87 transformer_model.electra.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.electra.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "89 transformer_model.electra.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.electra.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "91 transformer_model.electra.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "92 transformer_model.electra.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "93 transformer_model.electra.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "94 transformer_model.electra.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "95 transformer_model.electra.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "96 transformer_model.electra.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "97 transformer_model.electra.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "98 transformer_model.electra.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "99 transformer_model.electra.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "100 transformer_model.electra.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "101 transformer_model.electra.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "102 transformer_model.electra.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "103 transformer_model.electra.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.electra.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "105 transformer_model.electra.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.electra.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "107 transformer_model.electra.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "108 transformer_model.electra.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "109 transformer_model.electra.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "110 transformer_model.electra.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "111 transformer_model.electra.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "112 transformer_model.electra.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "113 transformer_model.electra.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "114 transformer_model.electra.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "115 transformer_model.electra.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "116 transformer_model.electra.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "117 transformer_model.electra.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "118 transformer_model.electra.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "119 transformer_model.electra.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.electra.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "121 transformer_model.electra.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.electra.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "123 transformer_model.electra.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "124 transformer_model.electra.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "125 transformer_model.electra.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "126 transformer_model.electra.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "127 transformer_model.electra.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "128 transformer_model.electra.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "129 transformer_model.electra.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "130 transformer_model.electra.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "131 transformer_model.electra.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "132 transformer_model.electra.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "133 transformer_model.electra.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "134 transformer_model.electra.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "135 transformer_model.electra.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.electra.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "137 transformer_model.electra.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.electra.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "139 transformer_model.electra.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "140 transformer_model.electra.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "141 transformer_model.electra.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "142 transformer_model.electra.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "143 transformer_model.electra.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "144 transformer_model.electra.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "145 transformer_model.electra.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "146 transformer_model.electra.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "147 transformer_model.electra.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "148 transformer_model.electra.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "149 transformer_model.electra.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "150 transformer_model.electra.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "151 transformer_model.electra.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.electra.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "153 transformer_model.electra.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.electra.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "155 transformer_model.electra.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "156 transformer_model.electra.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "157 transformer_model.electra.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "158 transformer_model.electra.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "159 transformer_model.electra.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "160 transformer_model.electra.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "161 transformer_model.electra.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "162 transformer_model.electra.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "163 transformer_model.electra.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "164 transformer_model.electra.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "165 transformer_model.electra.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "166 transformer_model.electra.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "167 transformer_model.electra.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.electra.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "169 transformer_model.electra.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.electra.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "171 transformer_model.electra.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "172 transformer_model.electra.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "173 transformer_model.electra.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "174 transformer_model.electra.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "175 transformer_model.electra.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "176 transformer_model.electra.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "177 transformer_model.electra.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "178 transformer_model.electra.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "179 transformer_model.electra.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "180 transformer_model.electra.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "181 transformer_model.electra.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "182 transformer_model.electra.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "183 transformer_model.electra.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.electra.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "185 transformer_model.electra.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.electra.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "187 transformer_model.electra.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "188 transformer_model.electra.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "189 transformer_model.electra.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "190 transformer_model.electra.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "191 transformer_model.electra.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "192 transformer_model.electra.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "193 transformer_model.electra.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "194 transformer_model.electra.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "195 transformer_model.electra.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "196 transformer_model.electra.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "197 transformer_model.electra.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "198 transformer_model.electra.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "199 transformer_model.electra.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.electra.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "201 transformer_model.electra.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.electra.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "203 transformer_model.electra.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "204 transformer_model.electra.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "205 transformer_model.electra.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "206 transformer_model.electra.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "207 transformer_model.electra.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "208 transformer_model.electra.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "209 transformer_model.electra.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "210 transformer_model.electra.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "211 transformer_model.electra.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "212 transformer_model.electra.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "213 transformer_model.electra.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "214 transformer_model.electra.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "215 transformer_model.electra.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.electra.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "217 transformer_model.electra.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.electra.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "219 transformer_model.electra.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "220 transformer_model.electra.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "221 transformer_model.electra.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "222 transformer_model.electra.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "223 transformer_model.electra.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "224 transformer_model.electra.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "225 transformer_model.electra.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "226 transformer_model.electra.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "227 transformer_model.electra.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "228 transformer_model.electra.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "229 transformer_model.electra.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "230 transformer_model.electra.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "231 transformer_model.electra.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.electra.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "233 transformer_model.electra.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.electra.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "235 transformer_model.electra.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "236 transformer_model.electra.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "237 transformer_model.electra.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "238 transformer_model.electra.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "239 transformer_model.electra.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "240 transformer_model.electra.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "241 transformer_model.electra.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "242 transformer_model.electra.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "243 transformer_model.electra.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "244 transformer_model.electra.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "245 transformer_model.electra.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "246 transformer_model.electra.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "247 transformer_model.electra.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.electra.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "249 transformer_model.electra.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.electra.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "251 transformer_model.electra.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "252 transformer_model.electra.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "253 transformer_model.electra.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "254 transformer_model.electra.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "255 transformer_model.electra.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "256 transformer_model.electra.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "257 transformer_model.electra.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "258 transformer_model.electra.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "259 transformer_model.electra.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "260 transformer_model.electra.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "261 transformer_model.electra.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "262 transformer_model.electra.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "263 transformer_model.electra.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.electra.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "265 transformer_model.electra.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.electra.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "267 transformer_model.electra.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "268 transformer_model.electra.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "269 transformer_model.electra.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "270 transformer_model.electra.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "271 transformer_model.electra.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "272 transformer_model.electra.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "273 transformer_model.electra.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "274 transformer_model.electra.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "275 transformer_model.electra.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "276 transformer_model.electra.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "277 transformer_model.electra.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "278 transformer_model.electra.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "279 transformer_model.electra.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.electra.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "281 transformer_model.electra.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.electra.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "283 transformer_model.electra.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "284 transformer_model.electra.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "285 transformer_model.electra.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "286 transformer_model.electra.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "287 transformer_model.electra.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "288 transformer_model.electra.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "289 transformer_model.electra.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "290 transformer_model.electra.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "291 transformer_model.electra.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "292 transformer_model.electra.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "293 transformer_model.electra.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "294 transformer_model.electra.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "295 transformer_model.electra.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.electra.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "297 transformer_model.electra.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.electra.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "299 transformer_model.electra.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "300 transformer_model.electra.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "301 transformer_model.electra.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "302 transformer_model.electra.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "303 transformer_model.electra.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "304 transformer_model.electra.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "305 transformer_model.electra.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "306 transformer_model.electra.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "307 transformer_model.electra.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "308 transformer_model.electra.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "309 transformer_model.electra.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "310 transformer_model.electra.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "311 transformer_model.electra.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.electra.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "313 transformer_model.electra.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.electra.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "315 transformer_model.electra.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "316 transformer_model.electra.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "317 transformer_model.electra.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "318 transformer_model.electra.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "319 transformer_model.electra.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "320 transformer_model.electra.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "321 transformer_model.electra.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "322 transformer_model.electra.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "323 transformer_model.electra.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "324 transformer_model.electra.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "325 transformer_model.electra.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "326 transformer_model.electra.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "327 transformer_model.electra.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.electra.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "329 transformer_model.electra.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.electra.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "331 transformer_model.electra.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "332 transformer_model.electra.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "333 transformer_model.electra.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "334 transformer_model.electra.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "335 transformer_model.electra.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "336 transformer_model.electra.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "337 transformer_model.electra.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "338 transformer_model.electra.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "339 transformer_model.electra.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "340 transformer_model.electra.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "341 transformer_model.electra.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "342 transformer_model.electra.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "343 transformer_model.electra.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.electra.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "345 transformer_model.electra.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.electra.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "347 transformer_model.electra.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "348 transformer_model.electra.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "349 transformer_model.electra.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "350 transformer_model.electra.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "351 transformer_model.electra.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "352 transformer_model.electra.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "353 transformer_model.electra.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "354 transformer_model.electra.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "355 transformer_model.electra.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "356 transformer_model.electra.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "357 transformer_model.electra.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "358 transformer_model.electra.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "359 transformer_model.electra.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.electra.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "361 transformer_model.electra.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.electra.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "363 transformer_model.electra.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "364 transformer_model.electra.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "365 transformer_model.electra.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "366 transformer_model.electra.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "367 transformer_model.electra.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "368 transformer_model.electra.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "369 transformer_model.electra.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "370 transformer_model.electra.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "371 transformer_model.electra.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "372 transformer_model.electra.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "373 transformer_model.electra.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "374 transformer_model.electra.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "375 transformer_model.electra.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.electra.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "377 transformer_model.electra.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.electra.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "379 transformer_model.electra.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "380 transformer_model.electra.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "381 transformer_model.electra.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "382 transformer_model.electra.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "383 transformer_model.electra.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "384 transformer_model.electra.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "385 transformer_model.electra.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "386 transformer_model.electra.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "387 transformer_model.electra.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "388 transformer_model.electra.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "389 transformer_model.classifier.dense.weight torch.Size([1024, 1024])\n",
      "390 transformer_model.classifier.dense.bias torch.Size([1024])\n",
      "391 transformer_model.classifier.out_proj.weight torch.Size([2, 1024])\n",
      "392 transformer_model.classifier.out_proj.bias torch.Size([2])\n",
      "393 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "394 attention.hidden_layer.bias torch.Size([512])\n",
      "395 attention.final_layer.weight torch.Size([1, 512])\n",
      "396 attention.final_layer.bias torch.Size([1])\n",
      "397 regressor.weight torch.Size([1, 1024])\n",
      "398 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.8633,  15.8719,  32.3934,  ...,   1.7552, -21.7107,   6.4586],\n",
       "        [ -8.0905, -10.3193,   9.1525,  ...,   2.1436,  15.2303, -46.2511],\n",
       "        [-21.1742, -22.8150,  -2.1771,  ...,  29.3484, -25.6957, -13.4151],\n",
       "        ...,\n",
       "        [ 31.1003, -14.0406, -20.5553,  ..., -16.9740, -21.2865, -15.3540],\n",
       "        [  7.0861,   4.3567,  13.9963,  ...,  -1.2864,  -5.4169,   3.2112],\n",
       "        [ -1.5488,  14.2157,  27.7912,  ...,   5.4052,  -4.6318,  46.8670]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 397\n",
    "    attention_param_start = 393\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 133\n",
    "    layer_middle_threshold = 261\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    def train_loss_backward(self, input_ids, attention_mask, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.scaler == None:\n",
    "            pred, _ = self.model(input_ids, attention_mask)\n",
    "            mse = self.mse_loss(pred.flatten(), target)\n",
    "            mse.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "        else:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                mse = self.mse_loss(pred.flatten(), target)\n",
    "            self.scaler.scale(mse).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.train_loss_backward(input_ids, attention_mask, target)\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(base_lr, last_lr, fold = 0):\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c9d397f-aa6e-4823-a4fd-b8625c580972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: {'base_lr': 6.155021772017101e-05, 'last_lr': 0.004642225106260296}. Best is trial 11 with value: 0.4787840247154236\n",
    "# Fold 1: {'base_lr': 8.902912488113375e-05, 'last_lr': 0.00023076670834499074}. Best is trial 9 with value: 0.447449654340744\n",
    "# Fold 2: {'base_lr': 0.00010637579365513648, 'last_lr': 8.276647346442369e-05}. Best is trial 9 with value: 0.47160205245018005\n",
    "# Fold 3: {'base_lr': 3.543430147790451e-05, 'last_lr': 0.00011412924670673832}. Best is trial 6 with value: 0.46940940618515015\n",
    "# Fold 4: {'base_lr': 4.8817697487830015e-05, 'last_lr': 0.00010249423984922014}. Best is trial 1 with value: 0.48146629333496094\n",
    "# Fold 5: {'base_lr': 8.26551556012735e-05, 'last_lr': 0.00012486898084560538}. Best is trial 1 with value: 0.47751927375793457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fbc2b14-ea34-48b6-82e9-10809a6d9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [\n",
    "    {'base_lr': 6.155021772017101e-05, 'last_lr': 0.004642225106260296},\n",
    "    {'base_lr': 8.902912488113375e-05, 'last_lr': 0.00023076670834499074},\n",
    "    {'base_lr': 0.00010637579365513648, 'last_lr': 8.276647346442369e-05},\n",
    "    {'base_lr': 3.543430147790451e-05, 'last_lr': 0.00011412924670673832},\n",
    "    {'base_lr': 4.8817697487830015e-05, 'last_lr': 0.00010249423984922014}, \n",
    "    {'base_lr': 8.26551556012735e-05, 'last_lr': 0.00012486898084560538}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 6.155021772017101e-05 last_lr 0.004642225106260296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc91f93052ce4e1ca948dc87ccec08fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9218 New best_val_rmse: 0.9218\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9639 Still best_val_rmse: 0.9218 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7486 New best_val_rmse: 0.7486\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6446 New best_val_rmse: 0.6446\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6073 New best_val_rmse: 0.6073\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6321 Still best_val_rmse: 0.6073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5637 New best_val_rmse: 0.5637\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5644 Still best_val_rmse: 0.5637 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.655 Still best_val_rmse: 0.5637 (from epoch 0)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5554 New best_val_rmse: 0.5554\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5581 Still best_val_rmse: 0.5554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5946 Still best_val_rmse: 0.5554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5407 New best_val_rmse: 0.5407\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5513 Still best_val_rmse: 0.5407 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5351 New best_val_rmse: 0.5351\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5195 New best_val_rmse: 0.5195\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5029 New best_val_rmse: 0.5029\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5181 Still best_val_rmse: 0.5029 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4979 New best_val_rmse: 0.4979\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5099 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4951 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4947 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5025 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4907 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4802 New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4807 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4821 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4791 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4791 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4789 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4797 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4815 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4846 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4834 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4806 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4798 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4797 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4797 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4798 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.48 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4801 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4804 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4804 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.4787840247154236\n",
      "##### Using fold 1\n",
      "##### Using base_lr 8.902912488113375e-05 last_lr 0.00023076670834499074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36824d51dfc4fb482c969aa19eb89a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7079 New best_val_rmse: 0.7079\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6837 New best_val_rmse: 0.6837\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9618 Still best_val_rmse: 0.6837 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6293 New best_val_rmse: 0.6293\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6916 Still best_val_rmse: 0.6293 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5629 New best_val_rmse: 0.5629\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6269 Still best_val_rmse: 0.5629 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5174 New best_val_rmse: 0.5174\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5937 Still best_val_rmse: 0.5174 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5439 Still best_val_rmse: 0.5174 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5098 New best_val_rmse: 0.5098\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.4914 Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 72 val_rmse: 0.504 Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4822 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5145 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.5265 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.496 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.486 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.461 New best_val_rmse: 0.461\n",
      "\n",
      "1 steps took 0.759 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4594 New best_val_rmse: 0.4594\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4813 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4921 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "8 steps took 6.24 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4633 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.4685 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.475 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4781 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4729 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4653 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.465 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4639 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4657 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.466 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4605 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4576 New best_val_rmse: 0.4576\n",
      "\n",
      "1 steps took 0.759 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4545 New best_val_rmse: 0.4545\n",
      "\n",
      "1 steps took 0.755 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4536 New best_val_rmse: 0.4536\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4532 New best_val_rmse: 0.4532\n",
      "\n",
      "1 steps took 0.759 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4532 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.455 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4585 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4638 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.464 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4603 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4596 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4614 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.463 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4607 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4554 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4555 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.458 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4621 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4656 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4703 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4665 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4709 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4818 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4632 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4578 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4565 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4544 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4515 New best_val_rmse: 0.4515\n",
      "\n",
      "1 steps took 0.759 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4497 New best_val_rmse: 0.4497\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4507 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4523 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4528 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4528 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4528 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4527 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4545 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4568 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4616 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4618 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4611 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4578 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4539 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4507 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4491 New best_val_rmse: 0.4491\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4489 New best_val_rmse: 0.4489\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4491 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4498 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4512 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4526 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4548 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4561 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4552 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.454 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4536 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4547 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4566 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4577 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.459 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.752 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4582 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4555 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4531 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4509 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4499 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4491 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4488 New best_val_rmse: 0.4488\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.449 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.45 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4515 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4535 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4562 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4581 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4586 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4589 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4591 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4572 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.455 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4529 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.451 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4499 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4491 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4484 New best_val_rmse: 0.4484\n",
      "\n",
      "1 steps took 0.755 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4481 New best_val_rmse: 0.4481\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4482 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4484 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4484 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4483 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4483 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4481 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4478 New best_val_rmse: 0.4478\n",
      "\n",
      "1 steps took 0.762 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4477 New best_val_rmse: 0.4477\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4476 New best_val_rmse: 0.4476\n",
      "\n",
      "1 steps took 0.752 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4476 New best_val_rmse: 0.4476\n",
      "\n",
      "1 steps took 0.765 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4475 New best_val_rmse: 0.4475\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4475 New best_val_rmse: 0.4475\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4475 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4475 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4476 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4476 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4476 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4475 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4475 New best_val_rmse: 0.4475\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4474 New best_val_rmse: 0.4474\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.752 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.445 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.447449654340744\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00010637579365513648 last_lr 8.276647346442369e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bd4febd93d4accb171b4ce804c31cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.158 New best_val_rmse: 1.158\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7568 New best_val_rmse: 0.7568\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8052 Still best_val_rmse: 0.7568 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7244 New best_val_rmse: 0.7244\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7129 New best_val_rmse: 0.7129\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6232 New best_val_rmse: 0.6232\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6115 New best_val_rmse: 0.6115\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6461 Still best_val_rmse: 0.6115 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5694 New best_val_rmse: 0.5694\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6209 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.579 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5409 New best_val_rmse: 0.5409\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5169 New best_val_rmse: 0.5169\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5562 Still best_val_rmse: 0.5169 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5453 Still best_val_rmse: 0.5169 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5095 New best_val_rmse: 0.5095\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5044 New best_val_rmse: 0.5044\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6063 Still best_val_rmse: 0.5044 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4964 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4816 Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4816 Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.48 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4772 New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4803 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4773 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4861 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4851 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.476 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4782 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4729 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4733 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4751 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4732 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4716 New best_val_rmse: 0.4716\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4717 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.473 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4738 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4735 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4729 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4726 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4727 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4735 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4737 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4738 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4734 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4733 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4732 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.473 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4731 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4733 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4736 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4739 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.474 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4743 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4744 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4745 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4745 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4745 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.47160205245018005\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.543430147790451e-05 last_lr 0.00011412924670673832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62158febdefa4a028a28b2c6e1b473b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.082 New best_val_rmse: 1.082\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7597 New best_val_rmse: 0.7597\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7525 New best_val_rmse: 0.7525\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6422 New best_val_rmse: 0.6422\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5748 New best_val_rmse: 0.5748\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7174 Still best_val_rmse: 0.5748 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5421 New best_val_rmse: 0.5421\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5537 Still best_val_rmse: 0.5421 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5463 Still best_val_rmse: 0.5421 (from epoch 0)\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5767 Still best_val_rmse: 0.5421 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5247 New best_val_rmse: 0.5247\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5365 Still best_val_rmse: 0.5247 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5607 Still best_val_rmse: 0.5247 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5207 New best_val_rmse: 0.5207\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5109 New best_val_rmse: 0.5109\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5034 New best_val_rmse: 0.5034\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5707 Still best_val_rmse: 0.5034 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5018 New best_val_rmse: 0.5018\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.499 New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5008 Still best_val_rmse: 0.499 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4744 New best_val_rmse: 0.4744\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4734 New best_val_rmse: 0.4734\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4711 New best_val_rmse: 0.4711\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.762 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.759 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4697 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4721 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4776 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4779 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4761 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4759 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4732 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4704 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4714 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4724 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.472 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4722 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.473 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4734 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4742 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4755 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4766 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4764 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4757 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4751 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4751 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4751 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.475 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4746 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4743 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4734 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4727 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4726 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4726 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4722 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4718 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4715 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4714 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4713 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4712 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.471 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.471 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.2 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.46940940618515015\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.8817697487830015e-05 last_lr 0.00010249423984922014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409bcef5fa25405baf5b87517c1fdc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9823 New best_val_rmse: 0.9823\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8517 New best_val_rmse: 0.8517\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6583 New best_val_rmse: 0.6583\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6434 New best_val_rmse: 0.6434\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5579 New best_val_rmse: 0.5579\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5827 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7169 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6163 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5809 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5231 New best_val_rmse: 0.5231\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5425 Still best_val_rmse: 0.5231 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5732 Still best_val_rmse: 0.5231 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5694 Still best_val_rmse: 0.5231 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5087 New best_val_rmse: 0.5087\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5481 Still best_val_rmse: 0.5087 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5109 Still best_val_rmse: 0.5087 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5066 New best_val_rmse: 0.5066\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4976 New best_val_rmse: 0.4976\n",
      "\n",
      "8 steps took 6.24 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5041 Still best_val_rmse: 0.4929 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5258 Still best_val_rmse: 0.4929 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4879 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4946 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4823 New best_val_rmse: 0.4823\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4842 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4825 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4897 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4834 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4854 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4882 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4855 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4826 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4842 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4835 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4825 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4822 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "Final RMSE: 0.48146629333496094\n",
      "##### Using fold 5\n",
      "##### Using base_lr 8.26551556012735e-05 last_lr 0.00012486898084560538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06a358ad2bb4dff93d622bc154c3235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9287 New best_val_rmse: 0.9287\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7149 New best_val_rmse: 0.7149\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7342 Still best_val_rmse: 0.7149 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6573 New best_val_rmse: 0.6573\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7195 Still best_val_rmse: 0.6573 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5843 New best_val_rmse: 0.5843\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6044 Still best_val_rmse: 0.5843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6077 Still best_val_rmse: 0.5843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5619 New best_val_rmse: 0.5619\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.558 New best_val_rmse: 0.558\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6652 Still best_val_rmse: 0.558 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5636 Still best_val_rmse: 0.558 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5853 Still best_val_rmse: 0.558 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5077 New best_val_rmse: 0.5077\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5596 Still best_val_rmse: 0.5077 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5023 New best_val_rmse: 0.5023\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5143 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4898 New best_val_rmse: 0.4898\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.487 New best_val_rmse: 0.487\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4853 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.485 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4863 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4856 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4891 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4819 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4948 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4843 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4817 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4835 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4817 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4854 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4826 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4777 New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4776 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4776 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4775 New best_val_rmse: 0.4775\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4777 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4779 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4781 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.478 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4779 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4781 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4784 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4784 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4784 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.47751927375793457\n",
      "CPU times: user 1h 24min 37s, sys: 14min 54s, total: 1h 39min 31s\n",
      "Wall time: 1h 43min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rmse_values = []\n",
    "for i in range(len(list(splits))):\n",
    "    fold = i\n",
    "    lrs = lr_list[fold]\n",
    "    rmse_val = train_fold(lrs['base_lr'], lrs['last_lr'], fold=fold)\n",
    "    print(f'Final RMSE: {rmse_val}')\n",
    "    rmse_values.append(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b28ebe4-316a-47e7-b20e-64a20ee4c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean RMSE values: 0.47103846073150635'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'mean RMSE values: {np.mean(np.array(rmse_values))}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 7.1 s, total: 29.4 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e04daa5d9d24eddaece9a55d6255e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e008c56d589f4280bf471b90bbf305b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3047332220488328\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.29730153563307293\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2957023622448626\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.27814858052977703\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.30497148036370575\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.31681242524116554\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b81e5ef3b54559a2cd470c888b3de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f928ca72ec494f3eaa60a349c80d756b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.2920565856752555\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.31450678049627995\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.29631847712899523\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.29395091994887385\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.2961161707304299\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.31105137736329896\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081906dbe51a41f9ac504436c2315c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16913a45e369408a97afeb5d32d51525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.30843181018529525\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3073181217298621\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.3089643106431168\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.31473145569485866\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3186755752138472\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3168873004667106\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adac2aa2dbec44cb86c3b990dd919c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf7bb9033314e52902362c65b2a2311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3070133372663501\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.31807177728626657\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.31992449804970824\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.29759807285099893\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3171389958203484\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3205302029286147\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa44b9e48b54a04890fabd322fd4d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dfc9937b2d4d618c79ffc121d89f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.3009744605957655\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.27362538143516296\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2602109284224828\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.28097097098875384\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3006269887840996\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.29813239042029627\n",
      "FINAL RMSE score 0.3023832165395696\n",
      "CPU times: user 4min 56s, sys: 4.53 s, total: 5min 1s\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0009605669676436582],\n",
       " [-0.004022977014025861],\n",
       " [0.0006062137297803185],\n",
       " [-0.0012854642062001438],\n",
       " [-0.0011118016136740696]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2004583 , 0.20028383, 0.19832692, 0.19818173, 0.20274922])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.9967448270735803)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42098746, -0.71595805, -0.53248514, -2.23908212, -1.85958253,\n",
       "       -1.31527597,  0.10749485])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03689638105009252, 0.007379276210018504)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.384091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.679062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.495589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.202186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.822686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.278380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.144391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.384091\n",
       "1  f0953f0a5 -0.679062\n",
       "2  0df072751 -0.495589\n",
       "3  04caf4e0c -2.202186\n",
       "4  0e63f8bea -1.822686\n",
       "5  12537fe78 -1.278380\n",
       "6  965e592c0  0.144391"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electra-large-discriminator'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/electra-large-discriminator/best')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/electra-large-discriminator_1'),\n",
       " PosixPath('/home/commonlit/models/electra-large-discriminator_2'),\n",
       " PosixPath('/home/commonlit/models/electra-large-discriminator_3'),\n",
       " PosixPath('/home/commonlit/models/electra-large-discriminator_4'),\n",
       " PosixPath('/home/commonlit/models/electra-large-discriminator_5'),\n",
       " PosixPath('/home/commonlit/models/electra-large-discriminator_6')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        special_tokens = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/special_tokens_map.json'))\n",
    "        assert special_tokens.exists()\n",
    "        copyfile(special_tokens, tokenizer_path/'special_tokens_map')\n",
    "        \n",
    "        spiece_model = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.txt'))\n",
    "        assert spiece_model.exists()\n",
    "        copyfile(spiece_model, tokenizer_path/'vocab.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/electra-large-discriminator/best_models.zip'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/electra-large-discriminator.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2M\t/home/commonlit/models/electra-large-discriminator/best/tokenizer-1\n",
      "688K\t/home/commonlit/models/electra-large-discriminator/best/tokenizer-2\n",
      "688K\t/home/commonlit/models/electra-large-discriminator/best/tokenizer-3\n",
      "688K\t/home/commonlit/models/electra-large-discriminator/best/tokenizer-4\n",
      "688K\t/home/commonlit/models/electra-large-discriminator/best/tokenizer-5\n",
      "688K\t/home/commonlit/models/electra-large-discriminator/best/tokenizer-6\n",
      "7.6G\t/home/commonlit/models/electra-large-discriminator/best\n",
      "7.0G\t/home/commonlit/models/electra-large-discriminator/best_models.zip\n",
      "1.3G\t/home/commonlit/models/electra-large-discriminator/lm\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/electra-large-discriminator/lm.zip'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/electra-large-discriminator/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-electra-large-discriminator\",\n",
      "  \"id\": \"gilfernandes/commonlit-electra-large-discriminator\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.97G/6.97G [11:32<00:00, 10.8MB/s]\n",
      "Upload successful: best_models.zip (7GB)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.16G/1.16G [01:59<00:00, 10.4MB/s]\n",
      "Upload successful: lm.zip (1GB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-electra-large-discriminator\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with spiece.model\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
