{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = 't5-large'\n",
    "    TOKENIZER_PATH = 't5-large'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 't5-large'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = T5EncoderModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.shared.weight torch.Size([32128, 1024])\n",
      "1 transformer_model.encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "2 transformer_model.encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "3 transformer_model.encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "4 transformer_model.encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "5 transformer_model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 16])\n",
      "6 transformer_model.encoder.block.0.layer.0.layer_norm.weight torch.Size([1024])\n",
      "7 transformer_model.encoder.block.0.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "8 transformer_model.encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "9 transformer_model.encoder.block.0.layer.1.layer_norm.weight torch.Size([1024])\n",
      "10 transformer_model.encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "11 transformer_model.encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "12 transformer_model.encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "13 transformer_model.encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "14 transformer_model.encoder.block.1.layer.0.layer_norm.weight torch.Size([1024])\n",
      "15 transformer_model.encoder.block.1.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "17 transformer_model.encoder.block.1.layer.1.layer_norm.weight torch.Size([1024])\n",
      "18 transformer_model.encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "19 transformer_model.encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "20 transformer_model.encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "21 transformer_model.encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "22 transformer_model.encoder.block.2.layer.0.layer_norm.weight torch.Size([1024])\n",
      "23 transformer_model.encoder.block.2.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "24 transformer_model.encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "25 transformer_model.encoder.block.2.layer.1.layer_norm.weight torch.Size([1024])\n",
      "26 transformer_model.encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "27 transformer_model.encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "28 transformer_model.encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "29 transformer_model.encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "30 transformer_model.encoder.block.3.layer.0.layer_norm.weight torch.Size([1024])\n",
      "31 transformer_model.encoder.block.3.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "32 transformer_model.encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "33 transformer_model.encoder.block.3.layer.1.layer_norm.weight torch.Size([1024])\n",
      "34 transformer_model.encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "35 transformer_model.encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "36 transformer_model.encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "37 transformer_model.encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "38 transformer_model.encoder.block.4.layer.0.layer_norm.weight torch.Size([1024])\n",
      "39 transformer_model.encoder.block.4.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "40 transformer_model.encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "41 transformer_model.encoder.block.4.layer.1.layer_norm.weight torch.Size([1024])\n",
      "42 transformer_model.encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "43 transformer_model.encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "44 transformer_model.encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "45 transformer_model.encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "46 transformer_model.encoder.block.5.layer.0.layer_norm.weight torch.Size([1024])\n",
      "47 transformer_model.encoder.block.5.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "48 transformer_model.encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "49 transformer_model.encoder.block.5.layer.1.layer_norm.weight torch.Size([1024])\n",
      "50 transformer_model.encoder.block.6.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "51 transformer_model.encoder.block.6.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "52 transformer_model.encoder.block.6.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "53 transformer_model.encoder.block.6.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "54 transformer_model.encoder.block.6.layer.0.layer_norm.weight torch.Size([1024])\n",
      "55 transformer_model.encoder.block.6.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "56 transformer_model.encoder.block.6.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "57 transformer_model.encoder.block.6.layer.1.layer_norm.weight torch.Size([1024])\n",
      "58 transformer_model.encoder.block.7.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "59 transformer_model.encoder.block.7.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "60 transformer_model.encoder.block.7.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "61 transformer_model.encoder.block.7.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "62 transformer_model.encoder.block.7.layer.0.layer_norm.weight torch.Size([1024])\n",
      "63 transformer_model.encoder.block.7.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "64 transformer_model.encoder.block.7.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "65 transformer_model.encoder.block.7.layer.1.layer_norm.weight torch.Size([1024])\n",
      "66 transformer_model.encoder.block.8.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "67 transformer_model.encoder.block.8.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "68 transformer_model.encoder.block.8.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "69 transformer_model.encoder.block.8.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "70 transformer_model.encoder.block.8.layer.0.layer_norm.weight torch.Size([1024])\n",
      "71 transformer_model.encoder.block.8.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "72 transformer_model.encoder.block.8.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "73 transformer_model.encoder.block.8.layer.1.layer_norm.weight torch.Size([1024])\n",
      "74 transformer_model.encoder.block.9.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "75 transformer_model.encoder.block.9.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "76 transformer_model.encoder.block.9.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "77 transformer_model.encoder.block.9.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "78 transformer_model.encoder.block.9.layer.0.layer_norm.weight torch.Size([1024])\n",
      "79 transformer_model.encoder.block.9.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "80 transformer_model.encoder.block.9.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "81 transformer_model.encoder.block.9.layer.1.layer_norm.weight torch.Size([1024])\n",
      "82 transformer_model.encoder.block.10.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "83 transformer_model.encoder.block.10.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "84 transformer_model.encoder.block.10.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "85 transformer_model.encoder.block.10.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "86 transformer_model.encoder.block.10.layer.0.layer_norm.weight torch.Size([1024])\n",
      "87 transformer_model.encoder.block.10.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "88 transformer_model.encoder.block.10.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "89 transformer_model.encoder.block.10.layer.1.layer_norm.weight torch.Size([1024])\n",
      "90 transformer_model.encoder.block.11.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "91 transformer_model.encoder.block.11.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "92 transformer_model.encoder.block.11.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "93 transformer_model.encoder.block.11.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "94 transformer_model.encoder.block.11.layer.0.layer_norm.weight torch.Size([1024])\n",
      "95 transformer_model.encoder.block.11.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "96 transformer_model.encoder.block.11.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "97 transformer_model.encoder.block.11.layer.1.layer_norm.weight torch.Size([1024])\n",
      "98 transformer_model.encoder.block.12.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "99 transformer_model.encoder.block.12.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "100 transformer_model.encoder.block.12.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "101 transformer_model.encoder.block.12.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "102 transformer_model.encoder.block.12.layer.0.layer_norm.weight torch.Size([1024])\n",
      "103 transformer_model.encoder.block.12.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "104 transformer_model.encoder.block.12.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "105 transformer_model.encoder.block.12.layer.1.layer_norm.weight torch.Size([1024])\n",
      "106 transformer_model.encoder.block.13.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "107 transformer_model.encoder.block.13.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "108 transformer_model.encoder.block.13.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "109 transformer_model.encoder.block.13.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "110 transformer_model.encoder.block.13.layer.0.layer_norm.weight torch.Size([1024])\n",
      "111 transformer_model.encoder.block.13.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "112 transformer_model.encoder.block.13.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "113 transformer_model.encoder.block.13.layer.1.layer_norm.weight torch.Size([1024])\n",
      "114 transformer_model.encoder.block.14.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "115 transformer_model.encoder.block.14.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "116 transformer_model.encoder.block.14.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "117 transformer_model.encoder.block.14.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "118 transformer_model.encoder.block.14.layer.0.layer_norm.weight torch.Size([1024])\n",
      "119 transformer_model.encoder.block.14.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "120 transformer_model.encoder.block.14.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "121 transformer_model.encoder.block.14.layer.1.layer_norm.weight torch.Size([1024])\n",
      "122 transformer_model.encoder.block.15.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "123 transformer_model.encoder.block.15.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "124 transformer_model.encoder.block.15.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "125 transformer_model.encoder.block.15.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "126 transformer_model.encoder.block.15.layer.0.layer_norm.weight torch.Size([1024])\n",
      "127 transformer_model.encoder.block.15.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "128 transformer_model.encoder.block.15.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "129 transformer_model.encoder.block.15.layer.1.layer_norm.weight torch.Size([1024])\n",
      "130 transformer_model.encoder.block.16.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "131 transformer_model.encoder.block.16.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "132 transformer_model.encoder.block.16.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "133 transformer_model.encoder.block.16.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "134 transformer_model.encoder.block.16.layer.0.layer_norm.weight torch.Size([1024])\n",
      "135 transformer_model.encoder.block.16.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "136 transformer_model.encoder.block.16.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "137 transformer_model.encoder.block.16.layer.1.layer_norm.weight torch.Size([1024])\n",
      "138 transformer_model.encoder.block.17.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "139 transformer_model.encoder.block.17.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "140 transformer_model.encoder.block.17.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "141 transformer_model.encoder.block.17.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "142 transformer_model.encoder.block.17.layer.0.layer_norm.weight torch.Size([1024])\n",
      "143 transformer_model.encoder.block.17.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "144 transformer_model.encoder.block.17.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "145 transformer_model.encoder.block.17.layer.1.layer_norm.weight torch.Size([1024])\n",
      "146 transformer_model.encoder.block.18.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "147 transformer_model.encoder.block.18.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "148 transformer_model.encoder.block.18.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "149 transformer_model.encoder.block.18.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "150 transformer_model.encoder.block.18.layer.0.layer_norm.weight torch.Size([1024])\n",
      "151 transformer_model.encoder.block.18.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "152 transformer_model.encoder.block.18.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "153 transformer_model.encoder.block.18.layer.1.layer_norm.weight torch.Size([1024])\n",
      "154 transformer_model.encoder.block.19.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "155 transformer_model.encoder.block.19.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "156 transformer_model.encoder.block.19.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "157 transformer_model.encoder.block.19.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "158 transformer_model.encoder.block.19.layer.0.layer_norm.weight torch.Size([1024])\n",
      "159 transformer_model.encoder.block.19.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "160 transformer_model.encoder.block.19.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "161 transformer_model.encoder.block.19.layer.1.layer_norm.weight torch.Size([1024])\n",
      "162 transformer_model.encoder.block.20.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "163 transformer_model.encoder.block.20.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "164 transformer_model.encoder.block.20.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "165 transformer_model.encoder.block.20.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "166 transformer_model.encoder.block.20.layer.0.layer_norm.weight torch.Size([1024])\n",
      "167 transformer_model.encoder.block.20.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "168 transformer_model.encoder.block.20.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "169 transformer_model.encoder.block.20.layer.1.layer_norm.weight torch.Size([1024])\n",
      "170 transformer_model.encoder.block.21.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "171 transformer_model.encoder.block.21.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "172 transformer_model.encoder.block.21.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "173 transformer_model.encoder.block.21.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "174 transformer_model.encoder.block.21.layer.0.layer_norm.weight torch.Size([1024])\n",
      "175 transformer_model.encoder.block.21.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "176 transformer_model.encoder.block.21.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "177 transformer_model.encoder.block.21.layer.1.layer_norm.weight torch.Size([1024])\n",
      "178 transformer_model.encoder.block.22.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "179 transformer_model.encoder.block.22.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "180 transformer_model.encoder.block.22.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "181 transformer_model.encoder.block.22.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "182 transformer_model.encoder.block.22.layer.0.layer_norm.weight torch.Size([1024])\n",
      "183 transformer_model.encoder.block.22.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "184 transformer_model.encoder.block.22.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "185 transformer_model.encoder.block.22.layer.1.layer_norm.weight torch.Size([1024])\n",
      "186 transformer_model.encoder.block.23.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "187 transformer_model.encoder.block.23.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "188 transformer_model.encoder.block.23.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "189 transformer_model.encoder.block.23.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "190 transformer_model.encoder.block.23.layer.0.layer_norm.weight torch.Size([1024])\n",
      "191 transformer_model.encoder.block.23.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "192 transformer_model.encoder.block.23.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "193 transformer_model.encoder.block.23.layer.1.layer_norm.weight torch.Size([1024])\n",
      "194 transformer_model.encoder.final_layer_norm.weight torch.Size([1024])\n",
      "195 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "196 attention.hidden_layer.bias torch.Size([512])\n",
      "197 attention.final_layer.weight torch.Size([1, 512])\n",
      "198 attention.final_layer.bias torch.Size([1])\n",
      "199 regressor.weight torch.Size([1, 1024])\n",
      "200 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res = sample_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea66f03e-eac6-478c-ab27-042d97ec1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1762],\n",
       "         [ 0.0296],\n",
       "         [ 0.1032],\n",
       "         [ 0.0202],\n",
       "         [ 0.1519],\n",
       "         [-0.0234],\n",
       "         [-0.0488],\n",
       "         [ 0.0940]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.0932,  0.5152, -0.0563,  ...,  0.0748,  0.0292, -0.0033],\n",
       "         [-0.2340,  0.5193, -0.2320,  ..., -0.0967, -0.0371, -0.0875],\n",
       "         [-0.0411,  0.2741,  0.0178,  ...,  0.0452,  0.0516, -0.0613],\n",
       "         ...,\n",
       "         [-0.1031,  0.2917,  0.0695,  ...,  0.0698, -0.0837, -0.1785],\n",
       "         [-0.2271,  0.1039, -0.2060,  ...,  0.3366, -0.0767, -0.1611],\n",
       "         [-0.2194,  0.4349, -0.0851,  ...,  0.0124, -0.0073,  0.0182]],\n",
       "        grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25.3458,  -7.4167,  18.7784,  ..., -50.2850,  16.0396,  32.5412],\n",
       "        [-32.2676, -55.3496,  -5.2535,  ...,  -7.6934, -36.6839, -12.5520],\n",
       "        [ -8.9862, -22.8392,  -5.4140,  ..., -21.0663,  32.6891,   2.7056],\n",
       "        ...,\n",
       "        [ 10.5804,  -6.3983,  33.1813,  ..., -33.5966, -31.8749, -35.1670],\n",
       "        [  8.9105,  23.7824,  19.3646,  ...,  -5.1728,   4.5497, -26.4514],\n",
       "        [ 32.8454, -32.7110,  18.4993,  ...,  45.9479,  -1.8786,  -3.3850]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 199\n",
    "    attention_param_start = 195\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= 130:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= 82:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "#                 self.scaler.scale(mse).backward()\n",
    "#                 self.scaler.step(self.optimizer)\n",
    "#                 self.scaler.update()\n",
    "                \n",
    "                mse.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: {'base_lr': 0.00013575061062518292, 'last_lr': 0.0027390926762560675} Best value:  0.48893508315086365\n",
    "# Fold 1: {'base_lr': 6.433162302000639e-05, 'last_lr': 0.0025302612125878217}. Best is trial 0 with value: 0.4527459144592285\n",
    "# Fold 2: {'base_lr': 0.00012105407461535033, 'last_lr': 0.00012780642309774768}. Best is trial 4 with value: 0.476582378149032\n",
    "# Fold 3: {'base_lr': 0.00016420220823284873, 'last_lr': 0.004783602075813355}. Best is trial 13 with value: 0.4700598418712616\n",
    "# Fold 4: {'base_lr': 8.176324330617398e-05, 'last_lr': 0.0012432581220121835}. Best is trial 17 with value: 0.4916570484638214\n",
    "# Fold 5: {'base_lr': 0.0002297546136917806, 'last_lr': 0.00034915806261776055}. Best is trial 12 with value: 0.4850253164768219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 3e-5, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 07:18:08,196]\u001b[0m A new study created in memory with name: no-name-cf37c018-a7a4-44a2-b4d0-ac3e70b938fe\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 3\n",
      "##### Using base_lr 5.327815988824232e-05 last_lr 0.000838614305106896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b7bc0197fe4880953988b8b6e22543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.304 New best_val_rmse: 1.304\n",
      "\n",
      "16 steps took 11.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9902 New best_val_rmse: 0.9902\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7017 New best_val_rmse: 0.7017\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.658 New best_val_rmse: 0.658\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6251 New best_val_rmse: 0.6251\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6138 New best_val_rmse: 0.6138\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5789 New best_val_rmse: 0.5789\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5566 New best_val_rmse: 0.5566\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5524 New best_val_rmse: 0.5524\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5646 Still best_val_rmse: 0.5524 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5275 New best_val_rmse: 0.5275\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.534 Still best_val_rmse: 0.5275 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.552 Still best_val_rmse: 0.5275 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5067 New best_val_rmse: 0.5067\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5209 Still best_val_rmse: 0.5067 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5029 New best_val_rmse: 0.5029\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5028 New best_val_rmse: 0.5028\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4974 New best_val_rmse: 0.4974\n",
      "\n",
      "8 steps took 6.45 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4991 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5077 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4944 New best_val_rmse: 0.4944\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4923 New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4923 New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4933 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4939 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4935 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.494 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.493 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4927 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4928 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4928 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.493 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4931 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4931 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4931 Still best_val_rmse: 0.4923 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 07:28:49,664]\u001b[0m Trial 0 finished with value: 0.49225884675979614 and parameters: {'base_lr': 5.327815988824232e-05, 'last_lr': 0.000838614305106896}. Best is trial 0 with value: 0.49225884675979614.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00023702006138636648 last_lr 0.0015281915172502291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da80cef464f49af830a5e9557f1eb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7023 New best_val_rmse: 0.7023\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7444 Still best_val_rmse: 0.7023 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7663 Still best_val_rmse: 0.7023 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5816 New best_val_rmse: 0.5816\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5413 New best_val_rmse: 0.5413\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5358 New best_val_rmse: 0.5358\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5584 Still best_val_rmse: 0.5358 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5304 New best_val_rmse: 0.5304\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5888 Still best_val_rmse: 0.5304 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.513 New best_val_rmse: 0.513\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5218 Still best_val_rmse: 0.513 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5347 Still best_val_rmse: 0.513 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5431 Still best_val_rmse: 0.513 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4938 New best_val_rmse: 0.4938\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.4929 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4923 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.5012 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.498 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4926 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 6.5 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4928 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4911 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4861 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4863 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4881 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.488 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4868 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4863 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4863 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.97 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4858 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.05 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4851 New best_val_rmse: 0.4851\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4832 New best_val_rmse: 0.4832\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4836 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4844 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4846 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4845 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4844 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.14 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4843 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4842 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4842 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4842 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4841 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4841 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.484 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.484 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.484 Still best_val_rmse: 0.483 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 07:42:04,987]\u001b[0m Trial 1 finished with value: 0.48303282260894775 and parameters: {'base_lr': 0.00023702006138636648, 'last_lr': 0.0015281915172502291}. Best is trial 1 with value: 0.48303282260894775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00011335195714243843 last_lr 0.0012314504768167866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec710d1ef3234cdb852158c90beab755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.191 New best_val_rmse: 1.191\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8593 New best_val_rmse: 0.8593\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7324 New best_val_rmse: 0.7324\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6444 New best_val_rmse: 0.6444\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6168 New best_val_rmse: 0.6168\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5731 New best_val_rmse: 0.5731\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5399 New best_val_rmse: 0.5399\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5418 Still best_val_rmse: 0.5399 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5373 New best_val_rmse: 0.5373\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5546 Still best_val_rmse: 0.5373 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5078 New best_val_rmse: 0.5078\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5168 Still best_val_rmse: 0.5078 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5377 Still best_val_rmse: 0.5078 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5054 New best_val_rmse: 0.5054\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4959 New best_val_rmse: 0.4959\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4945 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.484 Still best_val_rmse: 0.4812 (from epoch 1)\n",
      "\n",
      "4 steps took 3.45 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4814 Still best_val_rmse: 0.4812 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4817 Still best_val_rmse: 0.4812 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4806 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4814 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4804 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.481 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.481 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4883 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4916 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4834 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.485 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4842 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4856 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4862 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4848 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4849 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4855 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4855 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4853 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4853 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.486 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4866 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4866 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4857 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.97 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.485 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4846 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4841 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4838 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4837 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4836 Still best_val_rmse: 0.4798 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 07:55:51,013]\u001b[0m Trial 2 finished with value: 0.47981569170951843 and parameters: {'base_lr': 0.00011335195714243843, 'last_lr': 0.0012314504768167866}. Best is trial 2 with value: 0.47981569170951843.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00012000374177976627 last_lr 0.00028582109634624095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbefdca715c45a487052cb5a3b97e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.265 New best_val_rmse: 1.265\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9784 New best_val_rmse: 0.9784\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6865 New best_val_rmse: 0.6865\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.641 New best_val_rmse: 0.641\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5925 New best_val_rmse: 0.5925\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5603 New best_val_rmse: 0.5603\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5333 New best_val_rmse: 0.5333\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5482 Still best_val_rmse: 0.5333 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5384 Still best_val_rmse: 0.5333 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5711 Still best_val_rmse: 0.5333 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5079 New best_val_rmse: 0.5079\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5151 Still best_val_rmse: 0.5079 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5173 Still best_val_rmse: 0.5079 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4955 New best_val_rmse: 0.4955\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4895 New best_val_rmse: 0.4895\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4942 Still best_val_rmse: 0.4895 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5 Still best_val_rmse: 0.4895 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4902 Still best_val_rmse: 0.4879 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4793 New best_val_rmse: 0.4793\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4808 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4819 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4811 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 3.46 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4833 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4812 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.48 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4796 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4802 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4794 Still best_val_rmse: 0.4793 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4789 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4793 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4805 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4806 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4814 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4833 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4842 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4852 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4866 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4853 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4832 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4828 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4829 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4829 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4827 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4825 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4822 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4821 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4821 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4822 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4823 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4821 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4819 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4818 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4818 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4818 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4818 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4818 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4818 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4817 Still best_val_rmse: 0.4788 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 08:10:35,478]\u001b[0m Trial 3 finished with value: 0.47883087396621704 and parameters: {'base_lr': 0.00012000374177976627, 'last_lr': 0.00028582109634624095}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 5.6933106526460035e-05 last_lr 0.00113397895075368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bb504581634ec3b28b1997e1899b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.274 New best_val_rmse: 1.274\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9832 New best_val_rmse: 0.9832\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7125 New best_val_rmse: 0.7125\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6597 New best_val_rmse: 0.6597\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6109 New best_val_rmse: 0.6109\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5971 New best_val_rmse: 0.5971\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5566 New best_val_rmse: 0.5566\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5497 New best_val_rmse: 0.5497\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5394 New best_val_rmse: 0.5394\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.57 Still best_val_rmse: 0.5394 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5224 New best_val_rmse: 0.5224\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5263 Still best_val_rmse: 0.5224 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5331 Still best_val_rmse: 0.5224 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5023 New best_val_rmse: 0.5023\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.517 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5015 New best_val_rmse: 0.5015\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5008 New best_val_rmse: 0.5008\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4925 New best_val_rmse: 0.4925\n",
      "\n",
      "8 steps took 6.54 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4939 Still best_val_rmse: 0.4925 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5023 Still best_val_rmse: 0.4925 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4926 Still best_val_rmse: 0.4925 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4915 New best_val_rmse: 0.4915\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.492 Still best_val_rmse: 0.4915 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4918 Still best_val_rmse: 0.4915 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4934 Still best_val_rmse: 0.4915 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4935 Still best_val_rmse: 0.4915 (from epoch 2)\n",
      "\n",
      "8 steps took 6.08 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4923 Still best_val_rmse: 0.4915 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4924 Still best_val_rmse: 0.4915 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4914 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4915 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4916 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4917 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4917 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4917 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4917 Still best_val_rmse: 0.4914 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 08:21:18,641]\u001b[0m Trial 4 finished with value: 0.49137866497039795 and parameters: {'base_lr': 5.6933106526460035e-05, 'last_lr': 0.00113397895075368}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00024406579729722316 last_lr 8.844624308809113e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f48216e52ed43ed9fa546ea1ebd0fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.151 New best_val_rmse: 1.151\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.847 New best_val_rmse: 0.847\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7098 New best_val_rmse: 0.7098\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.633 New best_val_rmse: 0.633\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5748 New best_val_rmse: 0.5748\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.557 New best_val_rmse: 0.557\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5611 Still best_val_rmse: 0.557 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.543 New best_val_rmse: 0.543\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.529 New best_val_rmse: 0.529\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6014 Still best_val_rmse: 0.529 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5234 New best_val_rmse: 0.5234\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5425 Still best_val_rmse: 0.5234 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5308 Still best_val_rmse: 0.5234 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5017 Still best_val_rmse: 0.4969 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5001 Still best_val_rmse: 0.4969 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4975 Still best_val_rmse: 0.4969 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4881 New best_val_rmse: 0.4881\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4868 New best_val_rmse: 0.4868\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4854 New best_val_rmse: 0.4854\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4876 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "4 steps took 3.54 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 2.97 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4846 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.483 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4838 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4842 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4853 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4859 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4905 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.487 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4881 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4887 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.488 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4886 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4893 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4896 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4902 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4899 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4893 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4888 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4884 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4881 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4877 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4875 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4873 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4872 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4869 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4868 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4867 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4866 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4866 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4866 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4866 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4866 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4866 Still best_val_rmse: 0.4826 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 08:34:49,586]\u001b[0m Trial 5 finished with value: 0.48264938592910767 and parameters: {'base_lr': 0.00024406579729722316, 'last_lr': 8.844624308809113e-05}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 8.193837221897893e-05 last_lr 0.001034478754448138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e6b0bbeac34e9fbeadbdf5f40fc18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.25 New best_val_rmse: 1.25\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9484 New best_val_rmse: 0.9484\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6952 New best_val_rmse: 0.6952\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6666 New best_val_rmse: 0.6666\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6083 New best_val_rmse: 0.6083\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5817 New best_val_rmse: 0.5817\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5487 New best_val_rmse: 0.5487\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5366 New best_val_rmse: 0.5366\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5343 New best_val_rmse: 0.5343\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5506 Still best_val_rmse: 0.5343 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5125 New best_val_rmse: 0.5125\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5227 Still best_val_rmse: 0.5125 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5362 Still best_val_rmse: 0.5125 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.502 New best_val_rmse: 0.502\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5051 Still best_val_rmse: 0.502 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4987 New best_val_rmse: 0.4987\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4968 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4869 New best_val_rmse: 0.4869\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4862 New best_val_rmse: 0.4862\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4904 Still best_val_rmse: 0.4862 (from epoch 1)\n",
      "\n",
      "8 steps took 6.46 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4895 Still best_val_rmse: 0.4862 (from epoch 1)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4855 New best_val_rmse: 0.4855\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4862 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4868 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4858 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.486 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4891 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4962 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4865 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.488 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4877 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4881 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4885 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4878 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4876 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.488 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4883 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4884 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4884 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4886 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4889 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4893 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.489 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4887 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4883 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.488 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4877 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4875 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4874 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4874 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4873 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4873 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4873 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4873 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4873 Still best_val_rmse: 0.4855 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 08:48:21,026]\u001b[0m Trial 6 finished with value: 0.4855365455150604 and parameters: {'base_lr': 8.193837221897893e-05, 'last_lr': 0.001034478754448138}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0003983886731916653 last_lr 0.00016242920336068236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6cc6e44292482a94587112d4f1b9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.035 New best_val_rmse: 1.035\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7243 New best_val_rmse: 0.7243\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.624 New best_val_rmse: 0.624\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6029 New best_val_rmse: 0.6029\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5721 New best_val_rmse: 0.5721\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5515 New best_val_rmse: 0.5515\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5841 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5518 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5594 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6003 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5551 Still best_val_rmse: 0.5515 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5483 New best_val_rmse: 0.5483\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5528 Still best_val_rmse: 0.5483 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4978 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5128 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5029 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5209 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4982 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "8 steps took 6.5 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4925 New best_val_rmse: 0.4925\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.494 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5002 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4969 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4959 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4983 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4972 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.497 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4995 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4987 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4963 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.08 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4975 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4974 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4974 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4976 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4976 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4975 Still best_val_rmse: 0.4925 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4975 Still best_val_rmse: 0.4925 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 08:59:09,509]\u001b[0m Trial 7 finished with value: 0.49252238869667053 and parameters: {'base_lr': 0.0003983886731916653, 'last_lr': 0.00016242920336068236}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00023956590096521377 last_lr 0.001230842125022629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38024e21fd894cc493164a9bb4e8baaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.064 New best_val_rmse: 1.064\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7213 New best_val_rmse: 0.7213\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7539 Still best_val_rmse: 0.7213 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.608 New best_val_rmse: 0.608\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5971 New best_val_rmse: 0.5971\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5575 New best_val_rmse: 0.5575\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5597 Still best_val_rmse: 0.5575 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5867 Still best_val_rmse: 0.5575 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5247 New best_val_rmse: 0.5247\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5744 Still best_val_rmse: 0.5247 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5269 Still best_val_rmse: 0.5247 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5215 New best_val_rmse: 0.5215\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5235 Still best_val_rmse: 0.5215 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4986 New best_val_rmse: 0.4986\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5073 Still best_val_rmse: 0.4926 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4981 Still best_val_rmse: 0.4926 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.498 Still best_val_rmse: 0.4914 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4832 New best_val_rmse: 0.4832\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4942 Still best_val_rmse: 0.4832 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4885 Still best_val_rmse: 0.4832 (from epoch 1)\n",
      "\n",
      "4 steps took 3.45 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4805 New best_val_rmse: 0.4805\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.483 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4808 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4842 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4828 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4813 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4825 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4819 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4884 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4859 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4821 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4834 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4831 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4841 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4853 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4845 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4841 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4843 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4842 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4836 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.483 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4828 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4821 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4817 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.481 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4807 Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4804 New best_val_rmse: 0.4804\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4801 New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 09:14:09,364]\u001b[0m Trial 8 finished with value: 0.4795965254306793 and parameters: {'base_lr': 0.00023956590096521377, 'last_lr': 0.001230842125022629}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0002628413124986733 last_lr 0.0001988720448953958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2407f4ed23e642fca14ac086caeda770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.123 New best_val_rmse: 1.123\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7996 New best_val_rmse: 0.7996\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6958 New best_val_rmse: 0.6958\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6565 New best_val_rmse: 0.6565\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5937 New best_val_rmse: 0.5937\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5623 New best_val_rmse: 0.5623\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5474 New best_val_rmse: 0.5474\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5504 Still best_val_rmse: 0.5474 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5273 New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.641 Still best_val_rmse: 0.5273 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5278 Still best_val_rmse: 0.5273 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5392 Still best_val_rmse: 0.5273 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5352 Still best_val_rmse: 0.5273 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4941 New best_val_rmse: 0.4941\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4945 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5019 Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.53 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4843 New best_val_rmse: 0.4843\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4825 New best_val_rmse: 0.4825\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4847 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4866 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4854 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4852 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4862 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.487 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4877 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4881 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.97 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.487 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4864 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.487 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4856 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4861 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4854 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4853 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4852 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4852 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4853 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4854 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4857 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4857 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4857 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.97 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4857 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4859 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.486 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4859 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4858 Still best_val_rmse: 0.4825 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 09:27:40,021]\u001b[0m Trial 9 finished with value: 0.48248663544654846 and parameters: {'base_lr': 0.0002628413124986733, 'last_lr': 0.0001988720448953958}. Best is trial 3 with value: 0.47883087396621704.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0001295274544410545 last_lr 0.004988067319093683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966ca9a0ccc645a29305e1a8c15a4b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9791 New best_val_rmse: 0.9791\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7156 New best_val_rmse: 0.7156\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6885 New best_val_rmse: 0.6885\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7707 Still best_val_rmse: 0.6885 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6067 New best_val_rmse: 0.6067\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5403 New best_val_rmse: 0.5403\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5445 Still best_val_rmse: 0.5403 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5512 Still best_val_rmse: 0.5403 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5168 New best_val_rmse: 0.5168\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.572 Still best_val_rmse: 0.5168 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5337 Still best_val_rmse: 0.5168 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5138 New best_val_rmse: 0.5138\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.526 Still best_val_rmse: 0.5138 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4946 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.501 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.4989 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.494 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4957 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4759 New best_val_rmse: 0.4759\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.484 Still best_val_rmse: 0.4759 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.481 Still best_val_rmse: 0.4759 (from epoch 1)\n",
      "\n",
      "4 steps took 3.49 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.484 Still best_val_rmse: 0.4759 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.478 Still best_val_rmse: 0.4759 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4747 New best_val_rmse: 0.4747\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.477 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4823 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4811 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4772 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4776 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4783 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4795 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4806 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4819 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4794 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4789 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4788 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4788 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4793 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4794 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4791 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4791 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4792 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4791 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4791 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4795 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4796 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4796 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4791 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.479 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4787 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4785 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4785 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4785 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4785 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4784 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4782 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4781 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.478 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.478 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4779 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4779 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4778 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4776 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4774 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4773 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4773 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4773 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4773 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4773 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4774 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4774 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4775 Still best_val_rmse: 0.4747 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 09:46:02,064]\u001b[0m Trial 10 finished with value: 0.47466132044792175 and parameters: {'base_lr': 0.0001295274544410545, 'last_lr': 0.004988067319093683}. Best is trial 10 with value: 0.47466132044792175.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00013704587708202512 last_lr 0.00431927536820673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed4dd30987942b48ea736c6a47cd38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9884 New best_val_rmse: 0.9884\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7012 New best_val_rmse: 0.7012\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6975 New best_val_rmse: 0.6975\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7238 Still best_val_rmse: 0.6975 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6102 New best_val_rmse: 0.6102\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5259 New best_val_rmse: 0.5259\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5493 Still best_val_rmse: 0.5259 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5472 Still best_val_rmse: 0.5259 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5207 New best_val_rmse: 0.5207\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5469 Still best_val_rmse: 0.5207 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5535 Still best_val_rmse: 0.5207 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5156 New best_val_rmse: 0.5156\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5182 Still best_val_rmse: 0.5156 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4998 Still best_val_rmse: 0.4879 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5148 Still best_val_rmse: 0.4879 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4889 Still best_val_rmse: 0.4879 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.491 Still best_val_rmse: 0.4879 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4947 Still best_val_rmse: 0.4879 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4811 New best_val_rmse: 0.4811\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4895 Still best_val_rmse: 0.4811 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4821 Still best_val_rmse: 0.4811 (from epoch 1)\n",
      "\n",
      "4 steps took 2.97 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.49 Still best_val_rmse: 0.4811 (from epoch 1)\n",
      "\n",
      "4 steps took 3.5 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4983 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4777 New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4782 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4793 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4799 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.54 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4803 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4807 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4809 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4801 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4799 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4801 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4795 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4794 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4793 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4792 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.479 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4789 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4786 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4784 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4783 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4783 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.55 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4785 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4786 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4786 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4786 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4788 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4789 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.479 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.479 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4791 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4792 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4794 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4796 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4799 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4804 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4803 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4797 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4795 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4793 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4791 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4789 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4788 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.54 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4787 Still best_val_rmse: 0.4777 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 10:03:34,496]\u001b[0m Trial 11 finished with value: 0.4777217507362366 and parameters: {'base_lr': 0.00013704587708202512, 'last_lr': 0.00431927536820673}. Best is trial 10 with value: 0.47466132044792175.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00014995360832615058 last_lr 0.004767785679534565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8db404b9e564f7a9fe40dbeab566ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9776 New best_val_rmse: 0.9776\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7148 New best_val_rmse: 0.7148\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6795 New best_val_rmse: 0.6795\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7902 Still best_val_rmse: 0.6795 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6452 New best_val_rmse: 0.6452\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5366 New best_val_rmse: 0.5366\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.533 New best_val_rmse: 0.533\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5438 Still best_val_rmse: 0.533 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5078 New best_val_rmse: 0.5078\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5424 Still best_val_rmse: 0.5078 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5381 Still best_val_rmse: 0.5078 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5115 Still best_val_rmse: 0.5078 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5195 Still best_val_rmse: 0.5078 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4941 Still best_val_rmse: 0.4889 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5117 Still best_val_rmse: 0.4889 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4914 Still best_val_rmse: 0.4858 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4883 Still best_val_rmse: 0.4858 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4967 Still best_val_rmse: 0.4858 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4912 Still best_val_rmse: 0.4858 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4929 Still best_val_rmse: 0.4858 (from epoch 1)\n",
      "\n",
      "8 steps took 6.54 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4832 New best_val_rmse: 0.4832\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4732 New best_val_rmse: 0.4732\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4746 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4749 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4752 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4754 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4755 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4755 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4755 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4754 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4753 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.476 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4776 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4777 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4768 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4749 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4745 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4757 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4761 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.475 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4743 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4744 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4734 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4733 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4733 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4734 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4734 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4734 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.58 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4739 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4741 Still best_val_rmse: 0.4724 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 10:22:03,319]\u001b[0m Trial 12 finished with value: 0.47244909405708313 and parameters: {'base_lr': 0.00014995360832615058, 'last_lr': 0.004767785679534565}. Best is trial 12 with value: 0.47244909405708313.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00016420220823284873 last_lr 0.004783602075813355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b9f08e11074b82a6ad1c0c8a246d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9745 New best_val_rmse: 0.9745\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7092 New best_val_rmse: 0.7092\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6529 New best_val_rmse: 0.6529\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7 Still best_val_rmse: 0.6529 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6009 New best_val_rmse: 0.6009\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5217 New best_val_rmse: 0.5217\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5583 Still best_val_rmse: 0.5217 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5609 Still best_val_rmse: 0.5217 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5124 New best_val_rmse: 0.5124\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.554 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5363 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5139 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5379 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4856 New best_val_rmse: 0.4856\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4915 Still best_val_rmse: 0.4856 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4923 Still best_val_rmse: 0.4856 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5087 Still best_val_rmse: 0.4856 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4904 Still best_val_rmse: 0.4814 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4892 Still best_val_rmse: 0.4814 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4746 New best_val_rmse: 0.4746\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4783 Still best_val_rmse: 0.4746 (from epoch 1)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4785 Still best_val_rmse: 0.4746 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4719 Still best_val_rmse: 0.4708 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4785 Still best_val_rmse: 0.4708 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4857 Still best_val_rmse: 0.4708 (from epoch 1)\n",
      "\n",
      "4 steps took 3.47 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.474 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4787 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4783 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4732 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4715 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4777 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4838 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4806 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4714 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4721 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4724 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.473 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4727 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4715 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4717 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4718 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4715 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4715 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4716 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4719 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4727 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4741 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4755 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4762 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4754 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4746 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.474 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4738 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4738 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4738 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4738 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4738 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4739 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4748 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.475 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4751 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4753 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4751 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4748 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4749 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.475 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4749 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4746 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4742 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4741 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4741 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4741 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4741 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4741 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4742 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4742 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4744 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4744 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4745 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4745 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4745 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4745 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4745 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4744 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4744 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4743 Still best_val_rmse: 0.4701 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 10:41:17,508]\u001b[0m Trial 13 finished with value: 0.4700598418712616 and parameters: {'base_lr': 0.00016420220823284873, 'last_lr': 0.004783602075813355}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.0738293953323484e-05 last_lr 0.0029007270337567067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48c3d030df3437f845dc99f0a21d759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.159 New best_val_rmse: 1.159\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.799 New best_val_rmse: 0.799\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7108 New best_val_rmse: 0.7108\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6828 New best_val_rmse: 0.6828\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6324 New best_val_rmse: 0.6324\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6399 Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.589 New best_val_rmse: 0.589\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5816 New best_val_rmse: 0.5816\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5555 New best_val_rmse: 0.5555\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6148 Still best_val_rmse: 0.5555 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5591 Still best_val_rmse: 0.5555 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5499 New best_val_rmse: 0.5499\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.539 New best_val_rmse: 0.539\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5273 New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5359 Still best_val_rmse: 0.5273 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5136 New best_val_rmse: 0.5136\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5045 New best_val_rmse: 0.5045\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5152 Still best_val_rmse: 0.5045 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5023 New best_val_rmse: 0.5023\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5045 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5036 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5041 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5041 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5038 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5037 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5037 Still best_val_rmse: 0.5023 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 10:50:47,946]\u001b[0m Trial 14 finished with value: 0.5022666454315186 and parameters: {'base_lr': 3.0738293953323484e-05, 'last_lr': 0.0029007270337567067}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00043560837477936117 last_lr 0.0024767088905740904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4733c5bd3a4c74b7bfe271f89eadc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9691 New best_val_rmse: 0.9691\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7018 New best_val_rmse: 0.7018\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6366 New best_val_rmse: 0.6366\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7136 Still best_val_rmse: 0.6366 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5891 New best_val_rmse: 0.5891\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5698 New best_val_rmse: 0.5698\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5518 New best_val_rmse: 0.5518\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5809 Still best_val_rmse: 0.5518 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5628 Still best_val_rmse: 0.5518 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5786 Still best_val_rmse: 0.5518 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5527 Still best_val_rmse: 0.5518 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.546 New best_val_rmse: 0.546\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5452 New best_val_rmse: 0.5452\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5195 New best_val_rmse: 0.5195\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5243 Still best_val_rmse: 0.5195 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5048 New best_val_rmse: 0.5048\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5172 Still best_val_rmse: 0.5048 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5003 New best_val_rmse: 0.5003\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4892 New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4911 Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4926 Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4915 Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4863 New best_val_rmse: 0.4863\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4855 New best_val_rmse: 0.4855\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4893 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4936 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.49 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4884 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4896 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4895 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4889 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4886 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4885 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4885 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4888 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4887 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4887 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4889 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.489 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4891 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4892 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4892 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4893 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4893 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4894 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4894 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4894 Still best_val_rmse: 0.4855 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 11:03:07,086]\u001b[0m Trial 15 finished with value: 0.4854622483253479 and parameters: {'base_lr': 0.00043560837477936117, 'last_lr': 0.0024767088905740904}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00018137091249728613 last_lr 0.002588742095280771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09a512b93a84d05be363656fed7374d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.025 New best_val_rmse: 1.025\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7041 New best_val_rmse: 0.7041\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7007 New best_val_rmse: 0.7007\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7274 Still best_val_rmse: 0.7007 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5867 New best_val_rmse: 0.5867\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5468 New best_val_rmse: 0.5468\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5278 New best_val_rmse: 0.5278\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5387 Still best_val_rmse: 0.5278 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.524 New best_val_rmse: 0.524\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5653 Still best_val_rmse: 0.524 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5453 Still best_val_rmse: 0.524 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5253 Still best_val_rmse: 0.524 (from epoch 0)\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5332 Still best_val_rmse: 0.524 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5232 New best_val_rmse: 0.5232\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5026 New best_val_rmse: 0.5026\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4944 New best_val_rmse: 0.4944\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5015 Still best_val_rmse: 0.4944 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5004 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4987 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4939 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4874 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.489 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4871 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4878 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4866 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4853 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4859 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4863 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4853 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4859 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4854 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4851 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4868 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4867 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4866 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4848 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4845 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4845 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4848 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4852 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4853 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4854 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4855 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4855 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4856 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4857 Still best_val_rmse: 0.4834 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 11:16:08,478]\u001b[0m Trial 16 finished with value: 0.48343199491500854 and parameters: {'base_lr': 0.00018137091249728613, 'last_lr': 0.002588742095280771}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 8.453136230101006e-05 last_lr 0.00040680520681215864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caa5d60f16342f48ebc71d50162cd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.303 New best_val_rmse: 1.303\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9937 New best_val_rmse: 0.9937\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6848 New best_val_rmse: 0.6848\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6806 New best_val_rmse: 0.6806\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5981 New best_val_rmse: 0.5981\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5882 New best_val_rmse: 0.5882\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5488 New best_val_rmse: 0.5488\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5366 New best_val_rmse: 0.5366\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5496 Still best_val_rmse: 0.5366 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5442 Still best_val_rmse: 0.5366 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.512 New best_val_rmse: 0.512\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5248 Still best_val_rmse: 0.512 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5281 Still best_val_rmse: 0.512 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.506 New best_val_rmse: 0.506\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5091 Still best_val_rmse: 0.506 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4968 Still best_val_rmse: 0.4926 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4851 New best_val_rmse: 0.4851\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.487 Still best_val_rmse: 0.4846 (from epoch 1)\n",
      "\n",
      "4 steps took 3.47 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4849 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4843 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4842 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.486 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4854 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4849 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4886 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4944 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4854 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4866 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4867 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4883 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4889 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4871 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4866 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4871 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.06 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.487 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4868 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4869 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4874 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4878 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4881 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4877 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4875 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4872 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.487 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4866 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4863 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4862 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4861 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.486 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.486 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.486 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.486 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.486 Still best_val_rmse: 0.4836 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 11:29:49,132]\u001b[0m Trial 17 finished with value: 0.4836425483226776 and parameters: {'base_lr': 8.453136230101006e-05, 'last_lr': 0.00040680520681215864}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00017184623680797118 last_lr 0.004228619760468622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3798d717f107418dac1bd0cf705a5dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9802 New best_val_rmse: 0.9802\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7089 New best_val_rmse: 0.7089\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.684 New best_val_rmse: 0.684\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7556 Still best_val_rmse: 0.684 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6489 New best_val_rmse: 0.6489\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5387 New best_val_rmse: 0.5387\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5213 New best_val_rmse: 0.5213\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.527 Still best_val_rmse: 0.5213 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5215 Still best_val_rmse: 0.5213 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5343 Still best_val_rmse: 0.5213 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5562 Still best_val_rmse: 0.5213 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5254 Still best_val_rmse: 0.5151 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4911 New best_val_rmse: 0.4911\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4875 New best_val_rmse: 0.4875\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4927 Still best_val_rmse: 0.4875 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.503 Still best_val_rmse: 0.4875 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.489 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 3.04 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.489 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4926 Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4846 Still best_val_rmse: 0.4796 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.473 New best_val_rmse: 0.473\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4757 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4825 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4807 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "4 steps took 3.47 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4838 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4759 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4772 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4872 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "4 steps took 3.1 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4888 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4795 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4792 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4792 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4795 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4792 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4787 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4784 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4784 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4781 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4778 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4774 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4772 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.477 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4772 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4775 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4773 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4775 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4791 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4807 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4786 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4764 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4766 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.53 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4769 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4769 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4771 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4776 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4781 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4782 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.478 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4779 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4777 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4776 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4779 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4783 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4786 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4784 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4783 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4779 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4774 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4771 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4767 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4766 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4766 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4766 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4767 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4767 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4768 Still best_val_rmse: 0.473 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 11:48:39,105]\u001b[0m Trial 18 finished with value: 0.4730079770088196 and parameters: {'base_lr': 0.00017184623680797118, 'last_lr': 0.004228619760468622}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00031718898239970335 last_lr 0.0020428758207603837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc043e9b5b54d2a8f2a43104c3c65e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9942 New best_val_rmse: 0.9942\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6934 New best_val_rmse: 0.6934\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6446 New best_val_rmse: 0.6446\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6831 Still best_val_rmse: 0.6446 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6293 New best_val_rmse: 0.6293\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5624 New best_val_rmse: 0.5624\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5875 Still best_val_rmse: 0.5624 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5575 New best_val_rmse: 0.5575\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5473 New best_val_rmse: 0.5473\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5936 Still best_val_rmse: 0.5473 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5389 New best_val_rmse: 0.5389\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5247 New best_val_rmse: 0.5247\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5692 Still best_val_rmse: 0.5247 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5214 New best_val_rmse: 0.5214\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5019 New best_val_rmse: 0.5019\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4948 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5034 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5038 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4869 New best_val_rmse: 0.4869\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4953 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4907 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4904 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4923 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4906 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4904 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4884 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4882 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4902 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4924 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4898 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4896 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4896 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4897 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4897 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4898 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4898 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4898 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4898 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4899 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4899 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4898 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4897 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4897 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4897 Still best_val_rmse: 0.4869 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4897 Still best_val_rmse: 0.4869 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 12:00:42,494]\u001b[0m Trial 19 finished with value: 0.48685845732688904 and parameters: {'base_lr': 0.00031718898239970335, 'last_lr': 0.0020428758207603837}. Best is trial 13 with value: 0.4700598418712616.\u001b[0m\n",
      "\u001b[32m[I 2021-07-16 12:00:42,496]\u001b[0m A new study created in memory with name: no-name-c6b6f9df-550a-4dfd-8d7a-933a7b4ccdde\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.4700598418712616\n",
      " Best params: \n",
      "    base_lr: 0.00016420220823284873\n",
      "    last_lr: 0.004783602075813355\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00026731608181367815 last_lr 9.61081591519267e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b476dea28f448dbf56757d34e7e959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.104 New best_val_rmse: 1.104\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7759 New best_val_rmse: 0.7759\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.659 New best_val_rmse: 0.659\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6083 New best_val_rmse: 0.6083\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5764 New best_val_rmse: 0.5764\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5787 Still best_val_rmse: 0.5764 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6119 Still best_val_rmse: 0.5764 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5461 New best_val_rmse: 0.5461\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5765 Still best_val_rmse: 0.5461 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5435 New best_val_rmse: 0.5435\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5748 Still best_val_rmse: 0.5435 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5451 Still best_val_rmse: 0.5435 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5261 New best_val_rmse: 0.5261\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5505 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5612 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5268 Still best_val_rmse: 0.5261 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.504 Still best_val_rmse: 0.5026 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.505 Still best_val_rmse: 0.5026 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5053 Still best_val_rmse: 0.5026 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5042 Still best_val_rmse: 0.5026 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5039 Still best_val_rmse: 0.5026 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5038 Still best_val_rmse: 0.5026 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 12:10:13,747]\u001b[0m Trial 0 finished with value: 0.502634584903717 and parameters: {'base_lr': 0.00026731608181367815, 'last_lr': 9.61081591519267e-05}. Best is trial 0 with value: 0.502634584903717.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.909801078250626e-05 last_lr 0.0028036344765952014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694c29d9604b4b47a670e92696dd9f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.137 New best_val_rmse: 1.137\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9298 New best_val_rmse: 0.9298\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7373 New best_val_rmse: 0.7373\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6928 New best_val_rmse: 0.6928\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6597 New best_val_rmse: 0.6597\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6387 New best_val_rmse: 0.6387\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5849 New best_val_rmse: 0.5849\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5712 New best_val_rmse: 0.5712\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.582 Still best_val_rmse: 0.5712 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5209 New best_val_rmse: 0.5209\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5567 Still best_val_rmse: 0.5209 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5468 Still best_val_rmse: 0.5209 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5293 Still best_val_rmse: 0.5209 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5479 Still best_val_rmse: 0.5209 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5483 Still best_val_rmse: 0.5209 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5178 New best_val_rmse: 0.5178\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.51 New best_val_rmse: 0.51\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5124 Still best_val_rmse: 0.51 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5072 New best_val_rmse: 0.5072\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5001 New best_val_rmse: 0.5001\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4997 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5005 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4996 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4994 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4996 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4997 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.07 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4995 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4995 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4994 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4994 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4994 Still best_val_rmse: 0.4991 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 12:20:33,250]\u001b[0m Trial 1 finished with value: 0.49909043312072754 and parameters: {'base_lr': 4.909801078250626e-05, 'last_lr': 0.0028036344765952014}. Best is trial 1 with value: 0.49909043312072754.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00016060563953910976 last_lr 0.0036556302658151893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffce663845a40a290e6bcc41ff878ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.02 New best_val_rmse: 1.02\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7628 New best_val_rmse: 0.7628\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7209 New best_val_rmse: 0.7209\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6124 New best_val_rmse: 0.6124\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5778 New best_val_rmse: 0.5778\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5979 Still best_val_rmse: 0.5778 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6805 Still best_val_rmse: 0.5778 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5705 New best_val_rmse: 0.5705\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5359 New best_val_rmse: 0.5359\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.4988 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5435 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5099 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5198 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5259 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5323 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5149 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5114 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.505 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4967 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5023 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4942 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4936 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4945 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4939 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4939 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4934 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4934 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4947 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4952 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4943 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4938 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4934 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4933 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4932 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4932 Still best_val_rmse: 0.493 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 12:31:18,143]\u001b[0m Trial 2 finished with value: 0.492962121963501 and parameters: {'base_lr': 0.00016060563953910976, 'last_lr': 0.0036556302658151893}. Best is trial 2 with value: 0.492962121963501.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0003341313008947839 last_lr 0.0006673675873997908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7b35c4b51c476591d6f75cd2556448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.034 New best_val_rmse: 1.034\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7052 New best_val_rmse: 0.7052\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.651 New best_val_rmse: 0.651\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5861 New best_val_rmse: 0.5861\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6064 Still best_val_rmse: 0.5861 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6029 Still best_val_rmse: 0.5861 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5618 New best_val_rmse: 0.5618\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5347 New best_val_rmse: 0.5347\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5428 Still best_val_rmse: 0.5347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5366 Still best_val_rmse: 0.5347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5432 Still best_val_rmse: 0.5347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5328 New best_val_rmse: 0.5328\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5269 New best_val_rmse: 0.5269\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5555 Still best_val_rmse: 0.5269 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5522 Still best_val_rmse: 0.5269 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5211 New best_val_rmse: 0.5211\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5211 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5158 New best_val_rmse: 0.5158\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5078 New best_val_rmse: 0.5078\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5056 New best_val_rmse: 0.5056\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.506 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5065 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5076 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5076 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5058 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5056 Still best_val_rmse: 0.5056 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5056 Still best_val_rmse: 0.5056 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 12:40:49,075]\u001b[0m Trial 3 finished with value: 0.5056121945381165 and parameters: {'base_lr': 0.0003341313008947839, 'last_lr': 0.0006673675873997908}. Best is trial 2 with value: 0.492962121963501.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 9.139225868122028e-05 last_lr 0.0001387513717190892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5095dd8e25c4709be97911fd042862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.298 New best_val_rmse: 1.298\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.001 New best_val_rmse: 1.001\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7382 New best_val_rmse: 0.7382\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7491 Still best_val_rmse: 0.7382 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6255 New best_val_rmse: 0.6255\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6009 New best_val_rmse: 0.6009\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5528 New best_val_rmse: 0.5528\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5415 New best_val_rmse: 0.5415\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5486 Still best_val_rmse: 0.5415 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.554 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5534 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5258 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5233 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5372 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5084 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5028 New best_val_rmse: 0.5028\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5039 Still best_val_rmse: 0.5028 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.502 New best_val_rmse: 0.502\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4951 New best_val_rmse: 0.4951\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4936 New best_val_rmse: 0.4936\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4946 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4942 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4939 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4934 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4924 New best_val_rmse: 0.4924\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4923 New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4919 Still best_val_rmse: 0.4919 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 12:51:24,420]\u001b[0m Trial 4 finished with value: 0.491883784532547 and parameters: {'base_lr': 9.139225868122028e-05, 'last_lr': 0.0001387513717190892}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 6.124444603592389e-05 last_lr 0.0002325018070380025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931d6045259e499ea55647cb01fbd1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.325 New best_val_rmse: 1.325\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8845 New best_val_rmse: 0.8845\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7059 New best_val_rmse: 0.7059\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6602 New best_val_rmse: 0.6602\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6194 New best_val_rmse: 0.6194\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5656 New best_val_rmse: 0.5656\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5633 New best_val_rmse: 0.5633\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5891 Still best_val_rmse: 0.5633 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5262 New best_val_rmse: 0.5262\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5441 Still best_val_rmse: 0.5262 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5483 Still best_val_rmse: 0.5262 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5194 New best_val_rmse: 0.5194\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5186 New best_val_rmse: 0.5186\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5367 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5101 New best_val_rmse: 0.5101\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5122 Still best_val_rmse: 0.5101 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.51 New best_val_rmse: 0.51\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5081 New best_val_rmse: 0.5081\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5039 New best_val_rmse: 0.5039\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5015 New best_val_rmse: 0.5015\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.502 Still best_val_rmse: 0.5015 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5026 Still best_val_rmse: 0.5015 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5019 Still best_val_rmse: 0.5015 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5017 Still best_val_rmse: 0.5015 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5012 New best_val_rmse: 0.5012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 13:00:54,027]\u001b[0m Trial 5 finished with value: 0.5012246370315552 and parameters: {'base_lr': 6.124444603592389e-05, 'last_lr': 0.0002325018070380025}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.9863999054129436e-05 last_lr 0.00019344175119655413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b9a803271343ca9ebfd2427c70607b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.329 New best_val_rmse: 1.329\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.051 New best_val_rmse: 1.051\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8918 New best_val_rmse: 0.8918\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7255 New best_val_rmse: 0.7255\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6624 New best_val_rmse: 0.6624\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.622 New best_val_rmse: 0.622\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.571 New best_val_rmse: 0.571\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5693 New best_val_rmse: 0.5693\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6022 Still best_val_rmse: 0.5693 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.53 New best_val_rmse: 0.53\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5367 Still best_val_rmse: 0.53 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5536 Still best_val_rmse: 0.53 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5157 New best_val_rmse: 0.5157\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5109 New best_val_rmse: 0.5109\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5414 Still best_val_rmse: 0.5109 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5091 New best_val_rmse: 0.5091\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5075 New best_val_rmse: 0.5075\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5085 Still best_val_rmse: 0.5075 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5082 Still best_val_rmse: 0.5075 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5031 New best_val_rmse: 0.5031\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5015 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5017 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5002 New best_val_rmse: 0.5002\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5001 New best_val_rmse: 0.5001\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4997 New best_val_rmse: 0.4997\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4997 New best_val_rmse: 0.4997\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4996 New best_val_rmse: 0.4996\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4996 New best_val_rmse: 0.4996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 13:10:42,478]\u001b[0m Trial 6 finished with value: 0.4995722472667694 and parameters: {'base_lr': 5.9863999054129436e-05, 'last_lr': 0.00019344175119655413}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.017606328573479e-05 last_lr 0.0040150402984343615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c46ca4bb504415aace08ddb715096f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.064 New best_val_rmse: 1.064\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8879 New best_val_rmse: 0.8879\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7409 New best_val_rmse: 0.7409\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6621 New best_val_rmse: 0.6621\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.65 New best_val_rmse: 0.65\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6297 New best_val_rmse: 0.6297\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5882 New best_val_rmse: 0.5882\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5698 New best_val_rmse: 0.5698\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5711 Still best_val_rmse: 0.5698 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5252 New best_val_rmse: 0.5252\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5656 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5508 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5292 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5523 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5456 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5221 New best_val_rmse: 0.5221\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5085 New best_val_rmse: 0.5085\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5082 New best_val_rmse: 0.5082\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5033 New best_val_rmse: 0.5033\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4971 New best_val_rmse: 0.4971\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4971 New best_val_rmse: 0.4971\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4977 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4986 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.498 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4981 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4976 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4975 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.498 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.498 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4976 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4974 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4973 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4973 Still best_val_rmse: 0.4971 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4973 Still best_val_rmse: 0.4971 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 13:21:17,401]\u001b[0m Trial 7 finished with value: 0.4971056580543518 and parameters: {'base_lr': 5.017606328573479e-05, 'last_lr': 0.0040150402984343615}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0004644004477364371 last_lr 0.00023139932216750472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa67572e52d4ff1843c2d982041efb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.008 New best_val_rmse: 1.008\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7879 New best_val_rmse: 0.7879\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6167 New best_val_rmse: 0.6167\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5864 New best_val_rmse: 0.5864\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6196 Still best_val_rmse: 0.5864 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6312 Still best_val_rmse: 0.5864 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7136 Still best_val_rmse: 0.5864 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5687 New best_val_rmse: 0.5687\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5604 New best_val_rmse: 0.5604\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5436 New best_val_rmse: 0.5436\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5542 Still best_val_rmse: 0.5436 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5315 New best_val_rmse: 0.5315\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5373 Still best_val_rmse: 0.5315 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5737 Still best_val_rmse: 0.5315 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5747 Still best_val_rmse: 0.5315 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.536 Still best_val_rmse: 0.5315 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5305 New best_val_rmse: 0.5305\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5232 New best_val_rmse: 0.5232\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5147 New best_val_rmse: 0.5147\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5138 New best_val_rmse: 0.5138\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5131 New best_val_rmse: 0.5131\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5153 Still best_val_rmse: 0.5131 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5172 Still best_val_rmse: 0.5131 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5187 Still best_val_rmse: 0.5131 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.517 Still best_val_rmse: 0.5131 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5171 Still best_val_rmse: 0.5131 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.517 Still best_val_rmse: 0.5131 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 13:30:48,304]\u001b[0m Trial 8 finished with value: 0.5131124258041382 and parameters: {'base_lr': 0.0004644004477364371, 'last_lr': 0.00023139932216750472}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.1145580702159575e-05 last_lr 8.3159197490671e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd681d7932ed4a2a9d1f68e4819ee4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.349 New best_val_rmse: 1.349\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.124 New best_val_rmse: 1.124\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9337 New best_val_rmse: 0.9337\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7405 New best_val_rmse: 0.7405\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7038 New best_val_rmse: 0.7038\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6437 New best_val_rmse: 0.6437\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5847 New best_val_rmse: 0.5847\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5775 New best_val_rmse: 0.5775\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6124 Still best_val_rmse: 0.5775 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5382 New best_val_rmse: 0.5382\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5404 Still best_val_rmse: 0.5382 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5688 Still best_val_rmse: 0.5382 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5273 New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.517 New best_val_rmse: 0.517\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5422 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.509 New best_val_rmse: 0.509\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5073 New best_val_rmse: 0.5073\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5068 New best_val_rmse: 0.5068\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5047 New best_val_rmse: 0.5047\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5002 New best_val_rmse: 0.5002\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4979 New best_val_rmse: 0.4979\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4999 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4987 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4993 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 6.15 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4985 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4981 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4981 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4978 New best_val_rmse: 0.4978\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4973 New best_val_rmse: 0.4973\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4972 New best_val_rmse: 0.4972\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4972 New best_val_rmse: 0.4972\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4971 New best_val_rmse: 0.4971\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4971 Still best_val_rmse: 0.4971 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 13:41:16,577]\u001b[0m Trial 9 finished with value: 0.4971366226673126 and parameters: {'base_lr': 5.1145580702159575e-05, 'last_lr': 8.3159197490671e-05}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001049633236379046 last_lr 0.0008637274006839614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ecad1b1579463b9845ec30b9734aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.226 New best_val_rmse: 1.226\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9441 New best_val_rmse: 0.9441\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.711 New best_val_rmse: 0.711\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7387 Still best_val_rmse: 0.711 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6138 New best_val_rmse: 0.6138\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5992 New best_val_rmse: 0.5992\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5646 New best_val_rmse: 0.5646\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5461 New best_val_rmse: 0.5461\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5525 Still best_val_rmse: 0.5461 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5744 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5401 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5165 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5414 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5431 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5178 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5111 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5131 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5077 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5022 New best_val_rmse: 0.5022\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5011 New best_val_rmse: 0.5011\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5024 Still best_val_rmse: 0.5011 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5017 Still best_val_rmse: 0.5011 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5014 Still best_val_rmse: 0.5011 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5013 Still best_val_rmse: 0.5011 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5008 New best_val_rmse: 0.5008\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5008 New best_val_rmse: 0.5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 13:50:46,448]\u001b[0m Trial 10 finished with value: 0.500770628452301 and parameters: {'base_lr': 0.0001049633236379046, 'last_lr': 0.0008637274006839614}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00014813025515643673 last_lr 0.0017270438507141566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3b9d4056fc4490b6c5035c32da446e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.118 New best_val_rmse: 1.118\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8802 New best_val_rmse: 0.8802\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6909 New best_val_rmse: 0.6909\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7579 Still best_val_rmse: 0.6909 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6635 New best_val_rmse: 0.6635\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5883 New best_val_rmse: 0.5883\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5488 New best_val_rmse: 0.5488\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5801 Still best_val_rmse: 0.5488 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5566 Still best_val_rmse: 0.5488 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4997 New best_val_rmse: 0.4997\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.508 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5307 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5204 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5287 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5274 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5155 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5049 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5089 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5006 Still best_val_rmse: 0.4997 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4974 New best_val_rmse: 0.4974\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4948 New best_val_rmse: 0.4948\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4957 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4966 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4962 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4981 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.498 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4962 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4968 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4958 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4955 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4953 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4953 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4953 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4954 Still best_val_rmse: 0.4948 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4954 Still best_val_rmse: 0.4948 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:01:29,429]\u001b[0m Trial 11 finished with value: 0.4947921633720398 and parameters: {'base_lr': 0.00014813025515643673, 'last_lr': 0.0017270438507141566}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00013604630042020016 last_lr 0.0014531988209567372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7274f2a9d9b84999abaa6351946a403f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.149 New best_val_rmse: 1.149\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8815 New best_val_rmse: 0.8815\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6834 New best_val_rmse: 0.6834\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7749 Still best_val_rmse: 0.6834 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6642 New best_val_rmse: 0.6642\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.601 New best_val_rmse: 0.601\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5463 New best_val_rmse: 0.5463\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5481 Still best_val_rmse: 0.5463 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5431 New best_val_rmse: 0.5431\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.507 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5238 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5163 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5218 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5272 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.521 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5049 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5083 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5022 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5002 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4962 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4972 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4974 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "8 steps took 6.09 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4955 New best_val_rmse: 0.4955\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4958 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4986 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4959 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4963 Still best_val_rmse: 0.4955 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4952 New best_val_rmse: 0.4952\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4951 New best_val_rmse: 0.4951\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4951 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4951 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4951 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4951 Still best_val_rmse: 0.4951 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:12:06,762]\u001b[0m Trial 12 finished with value: 0.4950611889362335 and parameters: {'base_lr': 0.00013604630042020016, 'last_lr': 0.0014531988209567372}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 9.730206103250245e-05 last_lr 0.0004146135481763983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e261303f9f2442590b12ed41861a557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.27 New best_val_rmse: 1.27\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9672 New best_val_rmse: 0.9672\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7166 New best_val_rmse: 0.7166\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7281 Still best_val_rmse: 0.7166 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6278 New best_val_rmse: 0.6278\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.599 New best_val_rmse: 0.599\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5435 New best_val_rmse: 0.5435\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5396 New best_val_rmse: 0.5396\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5425 Still best_val_rmse: 0.5396 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5058 New best_val_rmse: 0.5058\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5506 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5353 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5239 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5312 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.544 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5218 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5096 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5073 Still best_val_rmse: 0.5058 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4996 New best_val_rmse: 0.4996\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4996 Still best_val_rmse: 0.4996 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5003 Still best_val_rmse: 0.4996 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5012 Still best_val_rmse: 0.4996 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5019 Still best_val_rmse: 0.4996 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5002 Still best_val_rmse: 0.4996 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4998 Still best_val_rmse: 0.4996 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4995 New best_val_rmse: 0.4995\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4993 New best_val_rmse: 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:22:09,453]\u001b[0m Trial 13 finished with value: 0.4993009865283966 and parameters: {'base_lr': 9.730206103250245e-05, 'last_lr': 0.0004146135481763983}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00023973581827158925 last_lr 0.00013194914165020892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352dca0206e144efaa21184b48de04c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.129 New best_val_rmse: 1.129\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8107 New best_val_rmse: 0.8107\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6572 New best_val_rmse: 0.6572\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6902 Still best_val_rmse: 0.6572 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6238 New best_val_rmse: 0.6238\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5918 New best_val_rmse: 0.5918\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.546 New best_val_rmse: 0.546\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5441 New best_val_rmse: 0.5441\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5807 Still best_val_rmse: 0.5441 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5469 Still best_val_rmse: 0.5441 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5405 New best_val_rmse: 0.5405\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5357 New best_val_rmse: 0.5357\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5291 New best_val_rmse: 0.5291\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5512 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5696 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5207 New best_val_rmse: 0.5207\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5202 New best_val_rmse: 0.5202\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.511 New best_val_rmse: 0.511\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.505 New best_val_rmse: 0.505\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5056 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.505 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5055 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.507 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5076 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5063 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5061 Still best_val_rmse: 0.505 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5061 Still best_val_rmse: 0.505 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:31:40,016]\u001b[0m Trial 14 finished with value: 0.5050088167190552 and parameters: {'base_lr': 0.00023973581827158925, 'last_lr': 0.00013194914165020892}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.291783570455888e-05 last_lr 0.0004053167156653586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f272a89bb84ddbabe1adcd606bb797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.345 New best_val_rmse: 1.345\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.116 New best_val_rmse: 1.116\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9424 New best_val_rmse: 0.9424\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7684 New best_val_rmse: 0.7684\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7033 New best_val_rmse: 0.7033\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6708 New best_val_rmse: 0.6708\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6186 New best_val_rmse: 0.6186\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6031 New best_val_rmse: 0.6031\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6419 Still best_val_rmse: 0.6031 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:35:03,275]\u001b[0m Trial 15 finished with value: 0.6030850410461426 and parameters: {'base_lr': 3.291783570455888e-05, 'last_lr': 0.0004053167156653586}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001807543792223713 last_lr 0.004354964057622347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e5ebcd8084bb6bfb8822f300ba72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.002 New best_val_rmse: 1.002\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7199 New best_val_rmse: 0.7199\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6602 New best_val_rmse: 0.6602\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5976 New best_val_rmse: 0.5976\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5774 New best_val_rmse: 0.5774\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5979 Still best_val_rmse: 0.5774 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6667 Still best_val_rmse: 0.5774 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5863 Still best_val_rmse: 0.5774 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5273 New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 12.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5033 New best_val_rmse: 0.5033\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5671 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.519 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5192 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5255 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5341 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5152 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5138 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.507 Still best_val_rmse: 0.5033 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5029 New best_val_rmse: 0.5029\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4988 New best_val_rmse: 0.4988\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4977 New best_val_rmse: 0.4977\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4994 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5012 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5028 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5004 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4992 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4991 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4992 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4994 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4994 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4993 Still best_val_rmse: 0.4977 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4993 Still best_val_rmse: 0.4977 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:45:13,588]\u001b[0m Trial 16 finished with value: 0.4977419972419739 and parameters: {'base_lr': 0.0001807543792223713, 'last_lr': 0.004354964057622347}. Best is trial 4 with value: 0.491883784532547.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.176324330617398e-05 last_lr 0.0012432581220121835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab060127fec4e95a18d108c08e2d8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.223 New best_val_rmse: 1.223\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9659 New best_val_rmse: 0.9659\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6943 New best_val_rmse: 0.6943\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7109 Still best_val_rmse: 0.6943 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6139 New best_val_rmse: 0.6139\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5907 New best_val_rmse: 0.5907\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5475 New best_val_rmse: 0.5475\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5403 New best_val_rmse: 0.5403\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5342 New best_val_rmse: 0.5342\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4974 New best_val_rmse: 0.4974\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5084 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5314 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5289 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5128 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5228 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5167 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5019 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5019 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5018 Still best_val_rmse: 0.4974 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4955 New best_val_rmse: 0.4955\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4944 New best_val_rmse: 0.4944\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4927 New best_val_rmse: 0.4927\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4923 New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4936 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4925 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4925 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4927 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4922 Still best_val_rmse: 0.4922 (from epoch 2)\n",
      "\n",
      "8 steps took 6.07 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4917 New best_val_rmse: 0.4917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 14:55:56,865]\u001b[0m Trial 17 finished with value: 0.4916570484638214 and parameters: {'base_lr': 8.176324330617398e-05, 'last_lr': 0.0012432581220121835}. Best is trial 17 with value: 0.4916570484638214.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.300164646884342e-05 last_lr 0.0010564751027599343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b077e29064be4d57b29ea1ee47372dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.236 New best_val_rmse: 1.236\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9706 New best_val_rmse: 0.9706\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7057 New best_val_rmse: 0.7057\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7123 Still best_val_rmse: 0.7057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6234 New best_val_rmse: 0.6234\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6027 New best_val_rmse: 0.6027\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5503 New best_val_rmse: 0.5503\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.541 New best_val_rmse: 0.541\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4988 New best_val_rmse: 0.4988\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5026 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5272 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5309 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5162 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5224 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5159 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.505 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5051 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5042 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4974 New best_val_rmse: 0.4974\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4958 New best_val_rmse: 0.4958\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4938 New best_val_rmse: 0.4938\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4936 New best_val_rmse: 0.4936\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4946 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4942 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4946 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4946 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4939 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4938 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.494 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4939 Still best_val_rmse: 0.4936 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4936 New best_val_rmse: 0.4936\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.07 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4934 Still best_val_rmse: 0.4934 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 15:06:41,010]\u001b[0m Trial 18 finished with value: 0.4933856427669525 and parameters: {'base_lr': 8.300164646884342e-05, 'last_lr': 0.0010564751027599343}. Best is trial 17 with value: 0.4916570484638214.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 7.879107116906062e-05 last_lr 0.00041831060497249747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31df1dc7b9604f18b00904739aed8f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.29 New best_val_rmse: 1.29\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9842 New best_val_rmse: 0.9842\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7943 New best_val_rmse: 0.7943\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7376 New best_val_rmse: 0.7376\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6435 New best_val_rmse: 0.6435\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6073 New best_val_rmse: 0.6073\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5596 New best_val_rmse: 0.5596\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5494 New best_val_rmse: 0.5494\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5686 Still best_val_rmse: 0.5494 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5119 New best_val_rmse: 0.5119\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5512 Still best_val_rmse: 0.5119 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5567 Still best_val_rmse: 0.5119 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5217 Still best_val_rmse: 0.5119 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5185 Still best_val_rmse: 0.5119 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5295 Still best_val_rmse: 0.5119 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.507 Still best_val_rmse: 0.5042 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5079 Still best_val_rmse: 0.5042 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5058 Still best_val_rmse: 0.5042 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5001 New best_val_rmse: 0.5001\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4978 New best_val_rmse: 0.4978\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4989 Still best_val_rmse: 0.4978 (from epoch 2)\n",
      "\n",
      "8 steps took 6.07 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4974 New best_val_rmse: 0.4974\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4976 Still best_val_rmse: 0.4974 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4976 Still best_val_rmse: 0.4974 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4972 New best_val_rmse: 0.4972\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4968 New best_val_rmse: 0.4968\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4967 New best_val_rmse: 0.4967\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4965 New best_val_rmse: 0.4965\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4963 New best_val_rmse: 0.4963\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4963 New best_val_rmse: 0.4963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 15:17:07,754]\u001b[0m Trial 19 finished with value: 0.4963444471359253 and parameters: {'base_lr': 7.879107116906062e-05, 'last_lr': 0.00041831060497249747}. Best is trial 17 with value: 0.4916570484638214.\u001b[0m\n",
      "\u001b[32m[I 2021-07-16 15:17:07,756]\u001b[0m A new study created in memory with name: no-name-a742421d-155b-4f4a-a123-2adbd7a1f56e\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.4916570484638214\n",
      " Best params: \n",
      "    base_lr: 8.176324330617398e-05\n",
      "    last_lr: 0.0012432581220121835\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.000471631234884967 last_lr 0.004293828027624209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee56dce247e041778e5b20c065c41b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8874 New best_val_rmse: 0.8874\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.663 New best_val_rmse: 0.663\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7249 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6884 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7479 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.735 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.611 New best_val_rmse: 0.611\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5978 New best_val_rmse: 0.5978\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5598 New best_val_rmse: 0.5598\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5412 New best_val_rmse: 0.5412\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5779 Still best_val_rmse: 0.5412 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5277 New best_val_rmse: 0.5277\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5267 New best_val_rmse: 0.5267\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5269 Still best_val_rmse: 0.5267 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5277 Still best_val_rmse: 0.5151 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5107 New best_val_rmse: 0.5107\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5135 Still best_val_rmse: 0.5107 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5118 Still best_val_rmse: 0.5107 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5035 New best_val_rmse: 0.5035\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5025 New best_val_rmse: 0.5025\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5065 Still best_val_rmse: 0.5025 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5028 Still best_val_rmse: 0.5025 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5019 New best_val_rmse: 0.5019\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5051 Still best_val_rmse: 0.5019 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5065 Still best_val_rmse: 0.5019 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5047 Still best_val_rmse: 0.5019 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 15:26:38,266]\u001b[0m Trial 0 finished with value: 0.5019031763076782 and parameters: {'base_lr': 0.000471631234884967, 'last_lr': 0.004293828027624209}. Best is trial 0 with value: 0.5019031763076782.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0001588419458411717 last_lr 0.0009567157920320694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f66db58ed7649ac85711f01ba15cc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.057 New best_val_rmse: 1.057\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7335 New best_val_rmse: 0.7335\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6864 New best_val_rmse: 0.6864\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6278 New best_val_rmse: 0.6278\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.647 Still best_val_rmse: 0.6278 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5985 New best_val_rmse: 0.5985\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5825 New best_val_rmse: 0.5825\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.592 Still best_val_rmse: 0.5825 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5557 New best_val_rmse: 0.5557\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5885 Still best_val_rmse: 0.5557 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5239 New best_val_rmse: 0.5239\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5128 New best_val_rmse: 0.5128\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5094 New best_val_rmse: 0.5094\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.52 Still best_val_rmse: 0.5094 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5054 New best_val_rmse: 0.5054\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5099 Still best_val_rmse: 0.5054 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4952 New best_val_rmse: 0.4952\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4928 New best_val_rmse: 0.4928\n",
      "\n",
      "8 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5126 Still best_val_rmse: 0.4928 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4992 Still best_val_rmse: 0.4928 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4911 New best_val_rmse: 0.4911\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4926 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4938 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4924 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4917 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4926 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4937 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4931 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4944 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4936 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.493 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4929 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.07 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4928 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4929 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4929 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4929 Still best_val_rmse: 0.4911 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 15:37:28,987]\u001b[0m Trial 1 finished with value: 0.4910915195941925 and parameters: {'base_lr': 0.0001588419458411717, 'last_lr': 0.0009567157920320694}. Best is trial 1 with value: 0.4910915195941925.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 9.170662664079284e-05 last_lr 0.0006441795305290672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9ac9c1de644cbb80f34f3dd6eee044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.163 New best_val_rmse: 1.163\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8978 New best_val_rmse: 0.8978\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6982 New best_val_rmse: 0.6982\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6901 New best_val_rmse: 0.6901\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6022 New best_val_rmse: 0.6022\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6005 New best_val_rmse: 0.6005\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5823 New best_val_rmse: 0.5823\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5595 New best_val_rmse: 0.5595\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.547 New best_val_rmse: 0.547\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.605 Still best_val_rmse: 0.547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5241 New best_val_rmse: 0.5241\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5164 New best_val_rmse: 0.5164\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5074 New best_val_rmse: 0.5074\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5117 Still best_val_rmse: 0.5074 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5095 Still best_val_rmse: 0.5074 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5126 Still best_val_rmse: 0.5074 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5038 New best_val_rmse: 0.5038\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5182 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.497 New best_val_rmse: 0.497\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4942 New best_val_rmse: 0.4942\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.497 Still best_val_rmse: 0.4942 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4944 Still best_val_rmse: 0.4935 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4961 Still best_val_rmse: 0.4935 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.495 Still best_val_rmse: 0.4935 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4938 Still best_val_rmse: 0.4935 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4933 New best_val_rmse: 0.4933\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4937 Still best_val_rmse: 0.4933 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4942 Still best_val_rmse: 0.4933 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4944 Still best_val_rmse: 0.4933 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4942 Still best_val_rmse: 0.4933 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4941 Still best_val_rmse: 0.4933 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4941 Still best_val_rmse: 0.4933 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 15:48:11,002]\u001b[0m Trial 2 finished with value: 0.49325135350227356 and parameters: {'base_lr': 9.170662664079284e-05, 'last_lr': 0.0006441795305290672}. Best is trial 1 with value: 0.4910915195941925.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 6.104770123792814e-05 last_lr 0.0018098034219241532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67e5f3170a44d69a61ca7513c1b7fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.079 New best_val_rmse: 1.079\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7939 New best_val_rmse: 0.7939\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6872 New best_val_rmse: 0.6872\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6566 New best_val_rmse: 0.6566\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5995 New best_val_rmse: 0.5995\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.589 New best_val_rmse: 0.589\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5728 New best_val_rmse: 0.5728\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5697 New best_val_rmse: 0.5697\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5477 New best_val_rmse: 0.5477\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5712 Still best_val_rmse: 0.5477 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5274 New best_val_rmse: 0.5274\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5166 New best_val_rmse: 0.5166\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5158 New best_val_rmse: 0.5158\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5162 Still best_val_rmse: 0.5158 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5112 New best_val_rmse: 0.5112\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5106 New best_val_rmse: 0.5106\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5039 New best_val_rmse: 0.5039\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5028 New best_val_rmse: 0.5028\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5047 Still best_val_rmse: 0.5028 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5014 New best_val_rmse: 0.5014\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.505 Still best_val_rmse: 0.5014 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.504 Still best_val_rmse: 0.5014 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5035 Still best_val_rmse: 0.5014 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5036 Still best_val_rmse: 0.5014 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.504 Still best_val_rmse: 0.5014 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5038 Still best_val_rmse: 0.5014 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 15:57:39,328]\u001b[0m Trial 3 finished with value: 0.5013652443885803 and parameters: {'base_lr': 6.104770123792814e-05, 'last_lr': 0.0018098034219241532}. Best is trial 1 with value: 0.4910915195941925.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00043586247767072087 last_lr 0.00010719695412836729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ea0a1b5fbe4436ba8dfb5c4ef08fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.013 New best_val_rmse: 1.013\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7277 New best_val_rmse: 0.7277\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6241 New best_val_rmse: 0.6241\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5966 New best_val_rmse: 0.5966\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.653 Still best_val_rmse: 0.5966 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5944 New best_val_rmse: 0.5944\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5984 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6022 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6089 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5728 New best_val_rmse: 0.5728\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5338 New best_val_rmse: 0.5338\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5279 New best_val_rmse: 0.5279\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5253 New best_val_rmse: 0.5253\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5309 Still best_val_rmse: 0.5253 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5177 New best_val_rmse: 0.5177\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.524 Still best_val_rmse: 0.5177 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.516 New best_val_rmse: 0.516\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5267 Still best_val_rmse: 0.516 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5142 New best_val_rmse: 0.5142\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5071 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5096 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5085 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5086 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5095 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5105 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5095 Still best_val_rmse: 0.5065 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 16:07:07,978]\u001b[0m Trial 4 finished with value: 0.506541907787323 and parameters: {'base_lr': 0.00043586247767072087, 'last_lr': 0.00010719695412836729}. Best is trial 1 with value: 0.4910915195941925.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00043149275513041085 last_lr 8.652046816842741e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca77d4d0929a4c9f9e1234d401038387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.013 New best_val_rmse: 1.013\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7347 New best_val_rmse: 0.7347\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6569 New best_val_rmse: 0.6569\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5936 New best_val_rmse: 0.5936\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7005 Still best_val_rmse: 0.5936 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6219 Still best_val_rmse: 0.5936 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5783 New best_val_rmse: 0.5783\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6021 Still best_val_rmse: 0.5783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5973 Still best_val_rmse: 0.5783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5489 New best_val_rmse: 0.5489\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5362 New best_val_rmse: 0.5362\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5225 New best_val_rmse: 0.5225\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5267 Still best_val_rmse: 0.5225 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5466 Still best_val_rmse: 0.5225 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5153 New best_val_rmse: 0.5153\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5237 Still best_val_rmse: 0.5153 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5072 New best_val_rmse: 0.5072\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5188 Still best_val_rmse: 0.5072 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5016 New best_val_rmse: 0.5016\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5017 Still best_val_rmse: 0.5016 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5022 Still best_val_rmse: 0.5016 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5004 New best_val_rmse: 0.5004\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.502 Still best_val_rmse: 0.5004 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5041 Still best_val_rmse: 0.5004 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5038 Still best_val_rmse: 0.5004 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5029 Still best_val_rmse: 0.5004 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 16:16:37,959]\u001b[0m Trial 5 finished with value: 0.5003818273544312 and parameters: {'base_lr': 0.00043149275513041085, 'last_lr': 8.652046816842741e-05}. Best is trial 1 with value: 0.4910915195941925.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00010193746973446588 last_lr 0.0048433216768270886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b047b58255d6485e86244c650187a6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9856 New best_val_rmse: 0.9856\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7327 New best_val_rmse: 0.7327\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.71 New best_val_rmse: 0.71\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6677 New best_val_rmse: 0.6677\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5706 New best_val_rmse: 0.5706\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5759 Still best_val_rmse: 0.5706 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6095 Still best_val_rmse: 0.5706 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5792 Still best_val_rmse: 0.5706 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5744 Still best_val_rmse: 0.5706 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5472 New best_val_rmse: 0.5472\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5142 New best_val_rmse: 0.5142\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5114 Still best_val_rmse: 0.4991 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5024 Still best_val_rmse: 0.4991 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4993 Still best_val_rmse: 0.4966 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5019 Still best_val_rmse: 0.4966 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4997 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4947 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4927 Still best_val_rmse: 0.4926 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4916 New best_val_rmse: 0.4916\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.492 Still best_val_rmse: 0.4916 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4923 Still best_val_rmse: 0.4916 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4922 Still best_val_rmse: 0.4916 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4915 New best_val_rmse: 0.4915\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4911 New best_val_rmse: 0.4911\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.491 Still best_val_rmse: 0.491 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 16:27:36,618]\u001b[0m Trial 6 finished with value: 0.49097248911857605 and parameters: {'base_lr': 0.00010193746973446588, 'last_lr': 0.0048433216768270886}. Best is trial 6 with value: 0.49097248911857605.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00024004824734813894 last_lr 0.0005898015737527751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d0198a3aee4e92b228ccfe1861e1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.027 New best_val_rmse: 1.027\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7479 New best_val_rmse: 0.7479\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6303 New best_val_rmse: 0.6303\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6851 Still best_val_rmse: 0.6303 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6777 Still best_val_rmse: 0.6303 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5943 New best_val_rmse: 0.5943\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5737 New best_val_rmse: 0.5737\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.566 New best_val_rmse: 0.566\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5814 Still best_val_rmse: 0.566 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5377 New best_val_rmse: 0.5377\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5259 New best_val_rmse: 0.5259\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5115 New best_val_rmse: 0.5115\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5055 New best_val_rmse: 0.5055\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5219 Still best_val_rmse: 0.5055 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4988 New best_val_rmse: 0.4988\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5005 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4988 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4916 New best_val_rmse: 0.4916\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4925 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5002 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4934 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4888 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4877 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4919 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.494 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4901 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4874 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4872 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4882 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4889 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4899 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4895 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4894 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4893 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4895 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4896 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4892 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.489 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4889 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4888 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4888 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4888 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4888 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4889 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4889 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4889 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.489 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.489 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.489 Still best_val_rmse: 0.4857 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 16:40:20,057]\u001b[0m Trial 7 finished with value: 0.4856576919555664 and parameters: {'base_lr': 0.00024004824734813894, 'last_lr': 0.0005898015737527751}. Best is trial 7 with value: 0.4856576919555664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00030625350108401557 last_lr 0.0005997069317383801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8818944b5544ad7ac65517ab303596e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.011 New best_val_rmse: 1.011\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7295 New best_val_rmse: 0.7295\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6397 New best_val_rmse: 0.6397\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6797 Still best_val_rmse: 0.6397 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6725 Still best_val_rmse: 0.6397 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5986 New best_val_rmse: 0.5986\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5828 New best_val_rmse: 0.5828\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5691 New best_val_rmse: 0.5691\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6011 Still best_val_rmse: 0.5691 (from epoch 0)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5358 New best_val_rmse: 0.5358\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5224 New best_val_rmse: 0.5224\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.507 New best_val_rmse: 0.507\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5157 Still best_val_rmse: 0.507 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4987 New best_val_rmse: 0.4987\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5032 Still best_val_rmse: 0.4987 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5036 Still best_val_rmse: 0.4987 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4944 New best_val_rmse: 0.4944\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5089 Still best_val_rmse: 0.4944 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.507 Still best_val_rmse: 0.4944 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4895 New best_val_rmse: 0.4895\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4944 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4933 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4918 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.49 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4904 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4916 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4918 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4938 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4937 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4924 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4917 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4915 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4915 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4915 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4915 Still best_val_rmse: 0.4895 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 16:51:04,752]\u001b[0m Trial 8 finished with value: 0.48951485753059387 and parameters: {'base_lr': 0.00030625350108401557, 'last_lr': 0.0005997069317383801}. Best is trial 7 with value: 0.4856576919555664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 6.517563273485793e-05 last_lr 0.0012817042987679538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6380c1bca1245d5b1ab37745dc74279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.118 New best_val_rmse: 1.118\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8432 New best_val_rmse: 0.8432\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6786 New best_val_rmse: 0.6786\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6515 New best_val_rmse: 0.6515\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.596 New best_val_rmse: 0.596\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5888 New best_val_rmse: 0.5888\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5771 New best_val_rmse: 0.5771\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5597 New best_val_rmse: 0.5597\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5451 New best_val_rmse: 0.5451\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5864 Still best_val_rmse: 0.5451 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5233 New best_val_rmse: 0.5233\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5149 New best_val_rmse: 0.5149\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5176 Still best_val_rmse: 0.5149 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5162 Still best_val_rmse: 0.5149 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5187 Still best_val_rmse: 0.5149 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5133 New best_val_rmse: 0.5133\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5144 Still best_val_rmse: 0.5133 (from epoch 1)\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5111 New best_val_rmse: 0.5111\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5146 Still best_val_rmse: 0.5111 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5106 New best_val_rmse: 0.5106\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5107 Still best_val_rmse: 0.5106 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5101 New best_val_rmse: 0.5101\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5099 New best_val_rmse: 0.5099\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5099 Still best_val_rmse: 0.5099 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5101 Still best_val_rmse: 0.5099 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5099 Still best_val_rmse: 0.5099 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 17:00:34,119]\u001b[0m Trial 9 finished with value: 0.5098778009414673 and parameters: {'base_lr': 6.517563273485793e-05, 'last_lr': 0.0012817042987679538}. Best is trial 7 with value: 0.4856576919555664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.266078444594387e-05 last_lr 0.0002033942977641447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66bddc174cb4482ae44982595cf0f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.3 New best_val_rmse: 1.3\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.082 New best_val_rmse: 1.082\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9323 New best_val_rmse: 0.9323\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7397 New best_val_rmse: 0.7397\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6561 New best_val_rmse: 0.6561\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6262 New best_val_rmse: 0.6262\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6073 New best_val_rmse: 0.6073\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6183 Still best_val_rmse: 0.6073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5971 New best_val_rmse: 0.5971\n",
      "\n",
      "16 steps took 12.6 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5737 New best_val_rmse: 0.5737\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5663 New best_val_rmse: 0.5663\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.562 New best_val_rmse: 0.562\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5747 Still best_val_rmse: 0.562 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5489 New best_val_rmse: 0.5489\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5591 Still best_val_rmse: 0.5489 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5429 New best_val_rmse: 0.5429\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5365 New best_val_rmse: 0.5365\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5273 New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5226 New best_val_rmse: 0.5226\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5226 Still best_val_rmse: 0.5226 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5221 New best_val_rmse: 0.5221\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5219 New best_val_rmse: 0.5219\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.526 Still best_val_rmse: 0.5219 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5221 Still best_val_rmse: 0.5219 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5213 New best_val_rmse: 0.5213\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5211 New best_val_rmse: 0.5211\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.521 New best_val_rmse: 0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 17:10:03,771]\u001b[0m Trial 10 finished with value: 0.5209943056106567 and parameters: {'base_lr': 3.266078444594387e-05, 'last_lr': 0.0002033942977641447}. Best is trial 7 with value: 0.4856576919555664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002506692176886094 last_lr 0.0002902778174110193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb6edb973104a56b708cd3af86040da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.036 New best_val_rmse: 1.036\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7747 New best_val_rmse: 0.7747\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7542 New best_val_rmse: 0.7542\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6965 New best_val_rmse: 0.6965\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5835 New best_val_rmse: 0.5835\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6147 Still best_val_rmse: 0.5835 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6209 Still best_val_rmse: 0.5835 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5871 Still best_val_rmse: 0.5835 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.582 New best_val_rmse: 0.582\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5352 New best_val_rmse: 0.5352\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5241 New best_val_rmse: 0.5241\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5128 New best_val_rmse: 0.5128\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5018 New best_val_rmse: 0.5018\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5122 Still best_val_rmse: 0.5018 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5029 Still best_val_rmse: 0.5018 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5147 Still best_val_rmse: 0.5018 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4965 New best_val_rmse: 0.4965\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4981 Still best_val_rmse: 0.4965 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5014 Still best_val_rmse: 0.4965 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5113 Still best_val_rmse: 0.4965 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.497 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4948 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4943 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4953 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4948 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4946 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.495 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.494 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4932 New best_val_rmse: 0.4932\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4933 Still best_val_rmse: 0.4932 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4934 Still best_val_rmse: 0.4932 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4934 Still best_val_rmse: 0.4932 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4934 Still best_val_rmse: 0.4932 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 17:20:47,834]\u001b[0m Trial 11 finished with value: 0.49322155117988586 and parameters: {'base_lr': 0.0002506692176886094, 'last_lr': 0.0002902778174110193}. Best is trial 7 with value: 0.4856576919555664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002297546136917806 last_lr 0.00034915806261776055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b753f93b0649688bcc49489fdf4821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.056 New best_val_rmse: 1.056\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8289 New best_val_rmse: 0.8289\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6998 New best_val_rmse: 0.6998\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6867 New best_val_rmse: 0.6867\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5745 New best_val_rmse: 0.5745\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5833 Still best_val_rmse: 0.5745 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6533 Still best_val_rmse: 0.5745 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5828 Still best_val_rmse: 0.5745 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6056 Still best_val_rmse: 0.5745 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5614 New best_val_rmse: 0.5614\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5232 New best_val_rmse: 0.5232\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5047 New best_val_rmse: 0.5047\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5021 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5012 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4961 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5104 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4964 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4978 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5003 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4964 Still best_val_rmse: 0.4937 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4862 New best_val_rmse: 0.4862\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4856 New best_val_rmse: 0.4856\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.485 New best_val_rmse: 0.485\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4864 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4925 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4877 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.49 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4878 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4866 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4871 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4874 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4874 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4865 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4859 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4859 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4861 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4861 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4864 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4865 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4864 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4862 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4862 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4862 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4863 Still best_val_rmse: 0.485 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-16 17:33:39,147]\u001b[0m Trial 12 finished with value: 0.4850253164768219 and parameters: {'base_lr': 0.0002297546136917806, 'last_lr': 0.00034915806261776055}. Best is trial 12 with value: 0.4850253164768219.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00019621961496857723 last_lr 0.00031447979189509236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c916c3d9e64eb588263b09ef8e8d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.078 New best_val_rmse: 1.078\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8251 New best_val_rmse: 0.8251\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.701 New best_val_rmse: 0.701\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6376 New best_val_rmse: 0.6376\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5837 New best_val_rmse: 0.5837\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5747 New best_val_rmse: 0.5747\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5865 Still best_val_rmse: 0.5747 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5816 Still best_val_rmse: 0.5747 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5756 Still best_val_rmse: 0.5747 (from epoch 0)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(3, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
