{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'xlnet-large-cased'\n",
    "    TOKENIZER_PATH = 'xlnet-large-cased'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     DEVICE = \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'xlnet-large-cased'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929301e1-626d-4ba5-9f32-d361769f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenizer.vocab.txt', 'w') as f:\n",
    "    for k, v in tokenizer.vocab.items():\n",
    "        f.write(f'{k}: {v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b946f5-a6f0-4415-911c-e5a7f628f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '______'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "#         tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "#         assert tokenizer.pad_token == pad_token\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69ec8b1-1d38-46f9-af3b-4a34e0c8dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(cfg.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9b003d-a13f-43c9-830e-edecafdec275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 32000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.transformer.mask_emb torch.Size([1, 1, 1024])\n",
      "1 transformer_model.transformer.word_embedding.weight torch.Size([32000, 1024])\n",
      "2 transformer_model.transformer.layer.0.rel_attn.q torch.Size([1024, 16, 64])\n",
      "3 transformer_model.transformer.layer.0.rel_attn.k torch.Size([1024, 16, 64])\n",
      "4 transformer_model.transformer.layer.0.rel_attn.v torch.Size([1024, 16, 64])\n",
      "5 transformer_model.transformer.layer.0.rel_attn.o torch.Size([1024, 16, 64])\n",
      "6 transformer_model.transformer.layer.0.rel_attn.r torch.Size([1024, 16, 64])\n",
      "7 transformer_model.transformer.layer.0.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "8 transformer_model.transformer.layer.0.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "9 transformer_model.transformer.layer.0.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "10 transformer_model.transformer.layer.0.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "11 transformer_model.transformer.layer.0.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "12 transformer_model.transformer.layer.0.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "13 transformer_model.transformer.layer.0.ff.layer_norm.weight torch.Size([1024])\n",
      "14 transformer_model.transformer.layer.0.ff.layer_norm.bias torch.Size([1024])\n",
      "15 transformer_model.transformer.layer.0.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.transformer.layer.0.ff.layer_1.bias torch.Size([4096])\n",
      "17 transformer_model.transformer.layer.0.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "18 transformer_model.transformer.layer.0.ff.layer_2.bias torch.Size([1024])\n",
      "19 transformer_model.transformer.layer.1.rel_attn.q torch.Size([1024, 16, 64])\n",
      "20 transformer_model.transformer.layer.1.rel_attn.k torch.Size([1024, 16, 64])\n",
      "21 transformer_model.transformer.layer.1.rel_attn.v torch.Size([1024, 16, 64])\n",
      "22 transformer_model.transformer.layer.1.rel_attn.o torch.Size([1024, 16, 64])\n",
      "23 transformer_model.transformer.layer.1.rel_attn.r torch.Size([1024, 16, 64])\n",
      "24 transformer_model.transformer.layer.1.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "25 transformer_model.transformer.layer.1.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "26 transformer_model.transformer.layer.1.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "27 transformer_model.transformer.layer.1.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "28 transformer_model.transformer.layer.1.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "29 transformer_model.transformer.layer.1.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "30 transformer_model.transformer.layer.1.ff.layer_norm.weight torch.Size([1024])\n",
      "31 transformer_model.transformer.layer.1.ff.layer_norm.bias torch.Size([1024])\n",
      "32 transformer_model.transformer.layer.1.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "33 transformer_model.transformer.layer.1.ff.layer_1.bias torch.Size([4096])\n",
      "34 transformer_model.transformer.layer.1.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "35 transformer_model.transformer.layer.1.ff.layer_2.bias torch.Size([1024])\n",
      "36 transformer_model.transformer.layer.2.rel_attn.q torch.Size([1024, 16, 64])\n",
      "37 transformer_model.transformer.layer.2.rel_attn.k torch.Size([1024, 16, 64])\n",
      "38 transformer_model.transformer.layer.2.rel_attn.v torch.Size([1024, 16, 64])\n",
      "39 transformer_model.transformer.layer.2.rel_attn.o torch.Size([1024, 16, 64])\n",
      "40 transformer_model.transformer.layer.2.rel_attn.r torch.Size([1024, 16, 64])\n",
      "41 transformer_model.transformer.layer.2.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "42 transformer_model.transformer.layer.2.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "43 transformer_model.transformer.layer.2.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "44 transformer_model.transformer.layer.2.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "45 transformer_model.transformer.layer.2.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "46 transformer_model.transformer.layer.2.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "47 transformer_model.transformer.layer.2.ff.layer_norm.weight torch.Size([1024])\n",
      "48 transformer_model.transformer.layer.2.ff.layer_norm.bias torch.Size([1024])\n",
      "49 transformer_model.transformer.layer.2.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "50 transformer_model.transformer.layer.2.ff.layer_1.bias torch.Size([4096])\n",
      "51 transformer_model.transformer.layer.2.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "52 transformer_model.transformer.layer.2.ff.layer_2.bias torch.Size([1024])\n",
      "53 transformer_model.transformer.layer.3.rel_attn.q torch.Size([1024, 16, 64])\n",
      "54 transformer_model.transformer.layer.3.rel_attn.k torch.Size([1024, 16, 64])\n",
      "55 transformer_model.transformer.layer.3.rel_attn.v torch.Size([1024, 16, 64])\n",
      "56 transformer_model.transformer.layer.3.rel_attn.o torch.Size([1024, 16, 64])\n",
      "57 transformer_model.transformer.layer.3.rel_attn.r torch.Size([1024, 16, 64])\n",
      "58 transformer_model.transformer.layer.3.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "59 transformer_model.transformer.layer.3.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "60 transformer_model.transformer.layer.3.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "61 transformer_model.transformer.layer.3.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "62 transformer_model.transformer.layer.3.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "63 transformer_model.transformer.layer.3.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "64 transformer_model.transformer.layer.3.ff.layer_norm.weight torch.Size([1024])\n",
      "65 transformer_model.transformer.layer.3.ff.layer_norm.bias torch.Size([1024])\n",
      "66 transformer_model.transformer.layer.3.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "67 transformer_model.transformer.layer.3.ff.layer_1.bias torch.Size([4096])\n",
      "68 transformer_model.transformer.layer.3.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "69 transformer_model.transformer.layer.3.ff.layer_2.bias torch.Size([1024])\n",
      "70 transformer_model.transformer.layer.4.rel_attn.q torch.Size([1024, 16, 64])\n",
      "71 transformer_model.transformer.layer.4.rel_attn.k torch.Size([1024, 16, 64])\n",
      "72 transformer_model.transformer.layer.4.rel_attn.v torch.Size([1024, 16, 64])\n",
      "73 transformer_model.transformer.layer.4.rel_attn.o torch.Size([1024, 16, 64])\n",
      "74 transformer_model.transformer.layer.4.rel_attn.r torch.Size([1024, 16, 64])\n",
      "75 transformer_model.transformer.layer.4.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "76 transformer_model.transformer.layer.4.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "77 transformer_model.transformer.layer.4.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "78 transformer_model.transformer.layer.4.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "79 transformer_model.transformer.layer.4.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "80 transformer_model.transformer.layer.4.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "81 transformer_model.transformer.layer.4.ff.layer_norm.weight torch.Size([1024])\n",
      "82 transformer_model.transformer.layer.4.ff.layer_norm.bias torch.Size([1024])\n",
      "83 transformer_model.transformer.layer.4.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "84 transformer_model.transformer.layer.4.ff.layer_1.bias torch.Size([4096])\n",
      "85 transformer_model.transformer.layer.4.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "86 transformer_model.transformer.layer.4.ff.layer_2.bias torch.Size([1024])\n",
      "87 transformer_model.transformer.layer.5.rel_attn.q torch.Size([1024, 16, 64])\n",
      "88 transformer_model.transformer.layer.5.rel_attn.k torch.Size([1024, 16, 64])\n",
      "89 transformer_model.transformer.layer.5.rel_attn.v torch.Size([1024, 16, 64])\n",
      "90 transformer_model.transformer.layer.5.rel_attn.o torch.Size([1024, 16, 64])\n",
      "91 transformer_model.transformer.layer.5.rel_attn.r torch.Size([1024, 16, 64])\n",
      "92 transformer_model.transformer.layer.5.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "93 transformer_model.transformer.layer.5.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "94 transformer_model.transformer.layer.5.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "95 transformer_model.transformer.layer.5.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "96 transformer_model.transformer.layer.5.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "97 transformer_model.transformer.layer.5.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "98 transformer_model.transformer.layer.5.ff.layer_norm.weight torch.Size([1024])\n",
      "99 transformer_model.transformer.layer.5.ff.layer_norm.bias torch.Size([1024])\n",
      "100 transformer_model.transformer.layer.5.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "101 transformer_model.transformer.layer.5.ff.layer_1.bias torch.Size([4096])\n",
      "102 transformer_model.transformer.layer.5.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "103 transformer_model.transformer.layer.5.ff.layer_2.bias torch.Size([1024])\n",
      "104 transformer_model.transformer.layer.6.rel_attn.q torch.Size([1024, 16, 64])\n",
      "105 transformer_model.transformer.layer.6.rel_attn.k torch.Size([1024, 16, 64])\n",
      "106 transformer_model.transformer.layer.6.rel_attn.v torch.Size([1024, 16, 64])\n",
      "107 transformer_model.transformer.layer.6.rel_attn.o torch.Size([1024, 16, 64])\n",
      "108 transformer_model.transformer.layer.6.rel_attn.r torch.Size([1024, 16, 64])\n",
      "109 transformer_model.transformer.layer.6.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "110 transformer_model.transformer.layer.6.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "111 transformer_model.transformer.layer.6.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "112 transformer_model.transformer.layer.6.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "113 transformer_model.transformer.layer.6.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "114 transformer_model.transformer.layer.6.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "115 transformer_model.transformer.layer.6.ff.layer_norm.weight torch.Size([1024])\n",
      "116 transformer_model.transformer.layer.6.ff.layer_norm.bias torch.Size([1024])\n",
      "117 transformer_model.transformer.layer.6.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "118 transformer_model.transformer.layer.6.ff.layer_1.bias torch.Size([4096])\n",
      "119 transformer_model.transformer.layer.6.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "120 transformer_model.transformer.layer.6.ff.layer_2.bias torch.Size([1024])\n",
      "121 transformer_model.transformer.layer.7.rel_attn.q torch.Size([1024, 16, 64])\n",
      "122 transformer_model.transformer.layer.7.rel_attn.k torch.Size([1024, 16, 64])\n",
      "123 transformer_model.transformer.layer.7.rel_attn.v torch.Size([1024, 16, 64])\n",
      "124 transformer_model.transformer.layer.7.rel_attn.o torch.Size([1024, 16, 64])\n",
      "125 transformer_model.transformer.layer.7.rel_attn.r torch.Size([1024, 16, 64])\n",
      "126 transformer_model.transformer.layer.7.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "127 transformer_model.transformer.layer.7.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "128 transformer_model.transformer.layer.7.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "129 transformer_model.transformer.layer.7.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "130 transformer_model.transformer.layer.7.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "131 transformer_model.transformer.layer.7.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "132 transformer_model.transformer.layer.7.ff.layer_norm.weight torch.Size([1024])\n",
      "133 transformer_model.transformer.layer.7.ff.layer_norm.bias torch.Size([1024])\n",
      "134 transformer_model.transformer.layer.7.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "135 transformer_model.transformer.layer.7.ff.layer_1.bias torch.Size([4096])\n",
      "136 transformer_model.transformer.layer.7.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "137 transformer_model.transformer.layer.7.ff.layer_2.bias torch.Size([1024])\n",
      "138 transformer_model.transformer.layer.8.rel_attn.q torch.Size([1024, 16, 64])\n",
      "139 transformer_model.transformer.layer.8.rel_attn.k torch.Size([1024, 16, 64])\n",
      "140 transformer_model.transformer.layer.8.rel_attn.v torch.Size([1024, 16, 64])\n",
      "141 transformer_model.transformer.layer.8.rel_attn.o torch.Size([1024, 16, 64])\n",
      "142 transformer_model.transformer.layer.8.rel_attn.r torch.Size([1024, 16, 64])\n",
      "143 transformer_model.transformer.layer.8.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "144 transformer_model.transformer.layer.8.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "145 transformer_model.transformer.layer.8.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "146 transformer_model.transformer.layer.8.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "147 transformer_model.transformer.layer.8.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "148 transformer_model.transformer.layer.8.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "149 transformer_model.transformer.layer.8.ff.layer_norm.weight torch.Size([1024])\n",
      "150 transformer_model.transformer.layer.8.ff.layer_norm.bias torch.Size([1024])\n",
      "151 transformer_model.transformer.layer.8.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "152 transformer_model.transformer.layer.8.ff.layer_1.bias torch.Size([4096])\n",
      "153 transformer_model.transformer.layer.8.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "154 transformer_model.transformer.layer.8.ff.layer_2.bias torch.Size([1024])\n",
      "155 transformer_model.transformer.layer.9.rel_attn.q torch.Size([1024, 16, 64])\n",
      "156 transformer_model.transformer.layer.9.rel_attn.k torch.Size([1024, 16, 64])\n",
      "157 transformer_model.transformer.layer.9.rel_attn.v torch.Size([1024, 16, 64])\n",
      "158 transformer_model.transformer.layer.9.rel_attn.o torch.Size([1024, 16, 64])\n",
      "159 transformer_model.transformer.layer.9.rel_attn.r torch.Size([1024, 16, 64])\n",
      "160 transformer_model.transformer.layer.9.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "161 transformer_model.transformer.layer.9.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "162 transformer_model.transformer.layer.9.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "163 transformer_model.transformer.layer.9.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "164 transformer_model.transformer.layer.9.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "165 transformer_model.transformer.layer.9.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "166 transformer_model.transformer.layer.9.ff.layer_norm.weight torch.Size([1024])\n",
      "167 transformer_model.transformer.layer.9.ff.layer_norm.bias torch.Size([1024])\n",
      "168 transformer_model.transformer.layer.9.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "169 transformer_model.transformer.layer.9.ff.layer_1.bias torch.Size([4096])\n",
      "170 transformer_model.transformer.layer.9.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "171 transformer_model.transformer.layer.9.ff.layer_2.bias torch.Size([1024])\n",
      "172 transformer_model.transformer.layer.10.rel_attn.q torch.Size([1024, 16, 64])\n",
      "173 transformer_model.transformer.layer.10.rel_attn.k torch.Size([1024, 16, 64])\n",
      "174 transformer_model.transformer.layer.10.rel_attn.v torch.Size([1024, 16, 64])\n",
      "175 transformer_model.transformer.layer.10.rel_attn.o torch.Size([1024, 16, 64])\n",
      "176 transformer_model.transformer.layer.10.rel_attn.r torch.Size([1024, 16, 64])\n",
      "177 transformer_model.transformer.layer.10.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "178 transformer_model.transformer.layer.10.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "179 transformer_model.transformer.layer.10.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "180 transformer_model.transformer.layer.10.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "181 transformer_model.transformer.layer.10.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "182 transformer_model.transformer.layer.10.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "183 transformer_model.transformer.layer.10.ff.layer_norm.weight torch.Size([1024])\n",
      "184 transformer_model.transformer.layer.10.ff.layer_norm.bias torch.Size([1024])\n",
      "185 transformer_model.transformer.layer.10.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "186 transformer_model.transformer.layer.10.ff.layer_1.bias torch.Size([4096])\n",
      "187 transformer_model.transformer.layer.10.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "188 transformer_model.transformer.layer.10.ff.layer_2.bias torch.Size([1024])\n",
      "189 transformer_model.transformer.layer.11.rel_attn.q torch.Size([1024, 16, 64])\n",
      "190 transformer_model.transformer.layer.11.rel_attn.k torch.Size([1024, 16, 64])\n",
      "191 transformer_model.transformer.layer.11.rel_attn.v torch.Size([1024, 16, 64])\n",
      "192 transformer_model.transformer.layer.11.rel_attn.o torch.Size([1024, 16, 64])\n",
      "193 transformer_model.transformer.layer.11.rel_attn.r torch.Size([1024, 16, 64])\n",
      "194 transformer_model.transformer.layer.11.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "195 transformer_model.transformer.layer.11.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "196 transformer_model.transformer.layer.11.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "197 transformer_model.transformer.layer.11.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "198 transformer_model.transformer.layer.11.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "199 transformer_model.transformer.layer.11.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "200 transformer_model.transformer.layer.11.ff.layer_norm.weight torch.Size([1024])\n",
      "201 transformer_model.transformer.layer.11.ff.layer_norm.bias torch.Size([1024])\n",
      "202 transformer_model.transformer.layer.11.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "203 transformer_model.transformer.layer.11.ff.layer_1.bias torch.Size([4096])\n",
      "204 transformer_model.transformer.layer.11.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "205 transformer_model.transformer.layer.11.ff.layer_2.bias torch.Size([1024])\n",
      "206 transformer_model.transformer.layer.12.rel_attn.q torch.Size([1024, 16, 64])\n",
      "207 transformer_model.transformer.layer.12.rel_attn.k torch.Size([1024, 16, 64])\n",
      "208 transformer_model.transformer.layer.12.rel_attn.v torch.Size([1024, 16, 64])\n",
      "209 transformer_model.transformer.layer.12.rel_attn.o torch.Size([1024, 16, 64])\n",
      "210 transformer_model.transformer.layer.12.rel_attn.r torch.Size([1024, 16, 64])\n",
      "211 transformer_model.transformer.layer.12.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "212 transformer_model.transformer.layer.12.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "213 transformer_model.transformer.layer.12.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "214 transformer_model.transformer.layer.12.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "215 transformer_model.transformer.layer.12.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "216 transformer_model.transformer.layer.12.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "217 transformer_model.transformer.layer.12.ff.layer_norm.weight torch.Size([1024])\n",
      "218 transformer_model.transformer.layer.12.ff.layer_norm.bias torch.Size([1024])\n",
      "219 transformer_model.transformer.layer.12.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "220 transformer_model.transformer.layer.12.ff.layer_1.bias torch.Size([4096])\n",
      "221 transformer_model.transformer.layer.12.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "222 transformer_model.transformer.layer.12.ff.layer_2.bias torch.Size([1024])\n",
      "223 transformer_model.transformer.layer.13.rel_attn.q torch.Size([1024, 16, 64])\n",
      "224 transformer_model.transformer.layer.13.rel_attn.k torch.Size([1024, 16, 64])\n",
      "225 transformer_model.transformer.layer.13.rel_attn.v torch.Size([1024, 16, 64])\n",
      "226 transformer_model.transformer.layer.13.rel_attn.o torch.Size([1024, 16, 64])\n",
      "227 transformer_model.transformer.layer.13.rel_attn.r torch.Size([1024, 16, 64])\n",
      "228 transformer_model.transformer.layer.13.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "229 transformer_model.transformer.layer.13.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "230 transformer_model.transformer.layer.13.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "231 transformer_model.transformer.layer.13.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "232 transformer_model.transformer.layer.13.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "233 transformer_model.transformer.layer.13.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "234 transformer_model.transformer.layer.13.ff.layer_norm.weight torch.Size([1024])\n",
      "235 transformer_model.transformer.layer.13.ff.layer_norm.bias torch.Size([1024])\n",
      "236 transformer_model.transformer.layer.13.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "237 transformer_model.transformer.layer.13.ff.layer_1.bias torch.Size([4096])\n",
      "238 transformer_model.transformer.layer.13.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "239 transformer_model.transformer.layer.13.ff.layer_2.bias torch.Size([1024])\n",
      "240 transformer_model.transformer.layer.14.rel_attn.q torch.Size([1024, 16, 64])\n",
      "241 transformer_model.transformer.layer.14.rel_attn.k torch.Size([1024, 16, 64])\n",
      "242 transformer_model.transformer.layer.14.rel_attn.v torch.Size([1024, 16, 64])\n",
      "243 transformer_model.transformer.layer.14.rel_attn.o torch.Size([1024, 16, 64])\n",
      "244 transformer_model.transformer.layer.14.rel_attn.r torch.Size([1024, 16, 64])\n",
      "245 transformer_model.transformer.layer.14.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "246 transformer_model.transformer.layer.14.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "247 transformer_model.transformer.layer.14.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "248 transformer_model.transformer.layer.14.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "249 transformer_model.transformer.layer.14.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "250 transformer_model.transformer.layer.14.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "251 transformer_model.transformer.layer.14.ff.layer_norm.weight torch.Size([1024])\n",
      "252 transformer_model.transformer.layer.14.ff.layer_norm.bias torch.Size([1024])\n",
      "253 transformer_model.transformer.layer.14.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "254 transformer_model.transformer.layer.14.ff.layer_1.bias torch.Size([4096])\n",
      "255 transformer_model.transformer.layer.14.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "256 transformer_model.transformer.layer.14.ff.layer_2.bias torch.Size([1024])\n",
      "257 transformer_model.transformer.layer.15.rel_attn.q torch.Size([1024, 16, 64])\n",
      "258 transformer_model.transformer.layer.15.rel_attn.k torch.Size([1024, 16, 64])\n",
      "259 transformer_model.transformer.layer.15.rel_attn.v torch.Size([1024, 16, 64])\n",
      "260 transformer_model.transformer.layer.15.rel_attn.o torch.Size([1024, 16, 64])\n",
      "261 transformer_model.transformer.layer.15.rel_attn.r torch.Size([1024, 16, 64])\n",
      "262 transformer_model.transformer.layer.15.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "263 transformer_model.transformer.layer.15.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "264 transformer_model.transformer.layer.15.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "265 transformer_model.transformer.layer.15.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "266 transformer_model.transformer.layer.15.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "267 transformer_model.transformer.layer.15.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "268 transformer_model.transformer.layer.15.ff.layer_norm.weight torch.Size([1024])\n",
      "269 transformer_model.transformer.layer.15.ff.layer_norm.bias torch.Size([1024])\n",
      "270 transformer_model.transformer.layer.15.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "271 transformer_model.transformer.layer.15.ff.layer_1.bias torch.Size([4096])\n",
      "272 transformer_model.transformer.layer.15.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "273 transformer_model.transformer.layer.15.ff.layer_2.bias torch.Size([1024])\n",
      "274 transformer_model.transformer.layer.16.rel_attn.q torch.Size([1024, 16, 64])\n",
      "275 transformer_model.transformer.layer.16.rel_attn.k torch.Size([1024, 16, 64])\n",
      "276 transformer_model.transformer.layer.16.rel_attn.v torch.Size([1024, 16, 64])\n",
      "277 transformer_model.transformer.layer.16.rel_attn.o torch.Size([1024, 16, 64])\n",
      "278 transformer_model.transformer.layer.16.rel_attn.r torch.Size([1024, 16, 64])\n",
      "279 transformer_model.transformer.layer.16.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "280 transformer_model.transformer.layer.16.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "281 transformer_model.transformer.layer.16.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "282 transformer_model.transformer.layer.16.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "283 transformer_model.transformer.layer.16.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "284 transformer_model.transformer.layer.16.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "285 transformer_model.transformer.layer.16.ff.layer_norm.weight torch.Size([1024])\n",
      "286 transformer_model.transformer.layer.16.ff.layer_norm.bias torch.Size([1024])\n",
      "287 transformer_model.transformer.layer.16.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "288 transformer_model.transformer.layer.16.ff.layer_1.bias torch.Size([4096])\n",
      "289 transformer_model.transformer.layer.16.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "290 transformer_model.transformer.layer.16.ff.layer_2.bias torch.Size([1024])\n",
      "291 transformer_model.transformer.layer.17.rel_attn.q torch.Size([1024, 16, 64])\n",
      "292 transformer_model.transformer.layer.17.rel_attn.k torch.Size([1024, 16, 64])\n",
      "293 transformer_model.transformer.layer.17.rel_attn.v torch.Size([1024, 16, 64])\n",
      "294 transformer_model.transformer.layer.17.rel_attn.o torch.Size([1024, 16, 64])\n",
      "295 transformer_model.transformer.layer.17.rel_attn.r torch.Size([1024, 16, 64])\n",
      "296 transformer_model.transformer.layer.17.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "297 transformer_model.transformer.layer.17.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "298 transformer_model.transformer.layer.17.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "299 transformer_model.transformer.layer.17.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "300 transformer_model.transformer.layer.17.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "301 transformer_model.transformer.layer.17.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "302 transformer_model.transformer.layer.17.ff.layer_norm.weight torch.Size([1024])\n",
      "303 transformer_model.transformer.layer.17.ff.layer_norm.bias torch.Size([1024])\n",
      "304 transformer_model.transformer.layer.17.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "305 transformer_model.transformer.layer.17.ff.layer_1.bias torch.Size([4096])\n",
      "306 transformer_model.transformer.layer.17.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "307 transformer_model.transformer.layer.17.ff.layer_2.bias torch.Size([1024])\n",
      "308 transformer_model.transformer.layer.18.rel_attn.q torch.Size([1024, 16, 64])\n",
      "309 transformer_model.transformer.layer.18.rel_attn.k torch.Size([1024, 16, 64])\n",
      "310 transformer_model.transformer.layer.18.rel_attn.v torch.Size([1024, 16, 64])\n",
      "311 transformer_model.transformer.layer.18.rel_attn.o torch.Size([1024, 16, 64])\n",
      "312 transformer_model.transformer.layer.18.rel_attn.r torch.Size([1024, 16, 64])\n",
      "313 transformer_model.transformer.layer.18.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "314 transformer_model.transformer.layer.18.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "315 transformer_model.transformer.layer.18.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "316 transformer_model.transformer.layer.18.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "317 transformer_model.transformer.layer.18.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "318 transformer_model.transformer.layer.18.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "319 transformer_model.transformer.layer.18.ff.layer_norm.weight torch.Size([1024])\n",
      "320 transformer_model.transformer.layer.18.ff.layer_norm.bias torch.Size([1024])\n",
      "321 transformer_model.transformer.layer.18.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "322 transformer_model.transformer.layer.18.ff.layer_1.bias torch.Size([4096])\n",
      "323 transformer_model.transformer.layer.18.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "324 transformer_model.transformer.layer.18.ff.layer_2.bias torch.Size([1024])\n",
      "325 transformer_model.transformer.layer.19.rel_attn.q torch.Size([1024, 16, 64])\n",
      "326 transformer_model.transformer.layer.19.rel_attn.k torch.Size([1024, 16, 64])\n",
      "327 transformer_model.transformer.layer.19.rel_attn.v torch.Size([1024, 16, 64])\n",
      "328 transformer_model.transformer.layer.19.rel_attn.o torch.Size([1024, 16, 64])\n",
      "329 transformer_model.transformer.layer.19.rel_attn.r torch.Size([1024, 16, 64])\n",
      "330 transformer_model.transformer.layer.19.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "331 transformer_model.transformer.layer.19.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "332 transformer_model.transformer.layer.19.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "333 transformer_model.transformer.layer.19.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "334 transformer_model.transformer.layer.19.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "335 transformer_model.transformer.layer.19.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "336 transformer_model.transformer.layer.19.ff.layer_norm.weight torch.Size([1024])\n",
      "337 transformer_model.transformer.layer.19.ff.layer_norm.bias torch.Size([1024])\n",
      "338 transformer_model.transformer.layer.19.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "339 transformer_model.transformer.layer.19.ff.layer_1.bias torch.Size([4096])\n",
      "340 transformer_model.transformer.layer.19.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "341 transformer_model.transformer.layer.19.ff.layer_2.bias torch.Size([1024])\n",
      "342 transformer_model.transformer.layer.20.rel_attn.q torch.Size([1024, 16, 64])\n",
      "343 transformer_model.transformer.layer.20.rel_attn.k torch.Size([1024, 16, 64])\n",
      "344 transformer_model.transformer.layer.20.rel_attn.v torch.Size([1024, 16, 64])\n",
      "345 transformer_model.transformer.layer.20.rel_attn.o torch.Size([1024, 16, 64])\n",
      "346 transformer_model.transformer.layer.20.rel_attn.r torch.Size([1024, 16, 64])\n",
      "347 transformer_model.transformer.layer.20.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "348 transformer_model.transformer.layer.20.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "349 transformer_model.transformer.layer.20.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "350 transformer_model.transformer.layer.20.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "351 transformer_model.transformer.layer.20.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "352 transformer_model.transformer.layer.20.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "353 transformer_model.transformer.layer.20.ff.layer_norm.weight torch.Size([1024])\n",
      "354 transformer_model.transformer.layer.20.ff.layer_norm.bias torch.Size([1024])\n",
      "355 transformer_model.transformer.layer.20.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "356 transformer_model.transformer.layer.20.ff.layer_1.bias torch.Size([4096])\n",
      "357 transformer_model.transformer.layer.20.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "358 transformer_model.transformer.layer.20.ff.layer_2.bias torch.Size([1024])\n",
      "359 transformer_model.transformer.layer.21.rel_attn.q torch.Size([1024, 16, 64])\n",
      "360 transformer_model.transformer.layer.21.rel_attn.k torch.Size([1024, 16, 64])\n",
      "361 transformer_model.transformer.layer.21.rel_attn.v torch.Size([1024, 16, 64])\n",
      "362 transformer_model.transformer.layer.21.rel_attn.o torch.Size([1024, 16, 64])\n",
      "363 transformer_model.transformer.layer.21.rel_attn.r torch.Size([1024, 16, 64])\n",
      "364 transformer_model.transformer.layer.21.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "365 transformer_model.transformer.layer.21.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "366 transformer_model.transformer.layer.21.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "367 transformer_model.transformer.layer.21.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "368 transformer_model.transformer.layer.21.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "369 transformer_model.transformer.layer.21.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "370 transformer_model.transformer.layer.21.ff.layer_norm.weight torch.Size([1024])\n",
      "371 transformer_model.transformer.layer.21.ff.layer_norm.bias torch.Size([1024])\n",
      "372 transformer_model.transformer.layer.21.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "373 transformer_model.transformer.layer.21.ff.layer_1.bias torch.Size([4096])\n",
      "374 transformer_model.transformer.layer.21.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "375 transformer_model.transformer.layer.21.ff.layer_2.bias torch.Size([1024])\n",
      "376 transformer_model.transformer.layer.22.rel_attn.q torch.Size([1024, 16, 64])\n",
      "377 transformer_model.transformer.layer.22.rel_attn.k torch.Size([1024, 16, 64])\n",
      "378 transformer_model.transformer.layer.22.rel_attn.v torch.Size([1024, 16, 64])\n",
      "379 transformer_model.transformer.layer.22.rel_attn.o torch.Size([1024, 16, 64])\n",
      "380 transformer_model.transformer.layer.22.rel_attn.r torch.Size([1024, 16, 64])\n",
      "381 transformer_model.transformer.layer.22.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "382 transformer_model.transformer.layer.22.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "383 transformer_model.transformer.layer.22.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "384 transformer_model.transformer.layer.22.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "385 transformer_model.transformer.layer.22.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "386 transformer_model.transformer.layer.22.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "387 transformer_model.transformer.layer.22.ff.layer_norm.weight torch.Size([1024])\n",
      "388 transformer_model.transformer.layer.22.ff.layer_norm.bias torch.Size([1024])\n",
      "389 transformer_model.transformer.layer.22.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "390 transformer_model.transformer.layer.22.ff.layer_1.bias torch.Size([4096])\n",
      "391 transformer_model.transformer.layer.22.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "392 transformer_model.transformer.layer.22.ff.layer_2.bias torch.Size([1024])\n",
      "393 transformer_model.transformer.layer.23.rel_attn.q torch.Size([1024, 16, 64])\n",
      "394 transformer_model.transformer.layer.23.rel_attn.k torch.Size([1024, 16, 64])\n",
      "395 transformer_model.transformer.layer.23.rel_attn.v torch.Size([1024, 16, 64])\n",
      "396 transformer_model.transformer.layer.23.rel_attn.o torch.Size([1024, 16, 64])\n",
      "397 transformer_model.transformer.layer.23.rel_attn.r torch.Size([1024, 16, 64])\n",
      "398 transformer_model.transformer.layer.23.rel_attn.r_r_bias torch.Size([16, 64])\n",
      "399 transformer_model.transformer.layer.23.rel_attn.r_s_bias torch.Size([16, 64])\n",
      "400 transformer_model.transformer.layer.23.rel_attn.r_w_bias torch.Size([16, 64])\n",
      "401 transformer_model.transformer.layer.23.rel_attn.seg_embed torch.Size([2, 16, 64])\n",
      "402 transformer_model.transformer.layer.23.rel_attn.layer_norm.weight torch.Size([1024])\n",
      "403 transformer_model.transformer.layer.23.rel_attn.layer_norm.bias torch.Size([1024])\n",
      "404 transformer_model.transformer.layer.23.ff.layer_norm.weight torch.Size([1024])\n",
      "405 transformer_model.transformer.layer.23.ff.layer_norm.bias torch.Size([1024])\n",
      "406 transformer_model.transformer.layer.23.ff.layer_1.weight torch.Size([4096, 1024])\n",
      "407 transformer_model.transformer.layer.23.ff.layer_1.bias torch.Size([4096])\n",
      "408 transformer_model.transformer.layer.23.ff.layer_2.weight torch.Size([1024, 4096])\n",
      "409 transformer_model.transformer.layer.23.ff.layer_2.bias torch.Size([1024])\n",
      "410 transformer_model.sequence_summary.summary.weight torch.Size([1024, 1024])\n",
      "411 transformer_model.sequence_summary.summary.bias torch.Size([1024])\n",
      "412 transformer_model.logits_proj.weight torch.Size([2, 1024])\n",
      "413 transformer_model.logits_proj.bias torch.Size([2])\n",
      "414 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "415 attention.hidden_layer.bias torch.Size([512])\n",
      "416 attention.final_layer.weight torch.Size([1, 512])\n",
      "417 attention.final_layer.bias torch.Size([1])\n",
      "418 regressor.weight torch.Size([1, 1024])\n",
      "419 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_input_ids = torch.randint(0, 1000, [2, 248])\n",
    "# sample_attention_mask = torch.randint(0, 1000, [2, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca35f5f4-51d1-4000-ad76-ed912daa8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_records = [sample_ds[i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "709e45b4-ac40-4d67-8fd3-45c0f40d8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'target'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_records[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3389bd94-785b-47b5-bc32-1e4e58dd9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.stack([r['input_ids'] for r in sample_records])\n",
    "sample_attention_mask = torch.stack([r['attention_mask'] for r in sample_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04781a7b-218a-41cc-b81f-d2d248e2c7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 248]), torch.Size([2, 248]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape, sample_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66a3b2b4-920e-4dff-bf9f-210d12b9c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "             5,     5,     5,     5,     5,     5,     5,     5,   311,    18,\n",
       "           673,   104,  1061,    22,    18, 24843,    19,    36,  2037,    24,\n",
       "           969,   111,  1318,  2295,     9,  2833,    20,    48,  3831,  2033,\n",
       "            19,    36,    30,    24,  2403,  4525,     9,    32,  1331,    30,\n",
       "          1972,    33,  2789,    13,  7876, 11330,    19,    50,  3514,    31,\n",
       "         13337,    19,    57,    17, 21696,   782,    95,  9717,  3716,    21,\n",
       "          5998,  6834,    23,    19,   115,    24,   525,  2789,   770,     9,\n",
       "            32,  2355, 19163,    21, 29743,    23,    29,    54, 10045,    18,\n",
       "           520,    19,    55,  6407,    68,    33, 10547,    21,    17, 26410,\n",
       "            33,    17,  2853,  2378,    23,    20,  7307,    19,   115,  2789,\n",
       "             9,  1551,  8703,  5903,    54,    72,  9446, 17345,    66,    31,\n",
       "           107,    19,    21, 28037,  6915,    17,   556, 22863,  6699,    40,\n",
       "            18,  6186,     9,   296,   231,   239,    20,    18,   520,    19,\n",
       "            31,    18,  1759,    19,  6699,    24,  1848,  4151,    13,    23,\n",
       "          2160,    17, 12052,     9,   592, 26209,    55,    28, 13104,    19,\n",
       "            65,    28,    18,  2537,    21,    65,    28,    18,  3434,     9,\n",
       "           209,    52,    30,    18,   275,     9,    32,  2537,    55,  3994,\n",
       "            38,    65,   239,    20,    18,   520,    21,    18,  3434,    38,\n",
       "            18,    86,    19,    21,    65,   239,    30,   271,    18,   453,\n",
       "         13311,    19,    21,    18,    86,    18,   334, 13311,     9,  1980,\n",
       "           928,    30,   502,    24,   316,  3864,    59,    63,    55,    22,\n",
       "          1649,    31,  3620,    18, 13311,     9,   122,    74,    47,    72,\n",
       "            48,   955,   918,    19,    57,   231,  1316,  1607,    30, 17029,\n",
       "            22,  3134,  2789, 10816,   202,     9,     4,     3],\n",
       "        [  394,   135,  3073,    92,    19,  3426,     9, 12378,    88,    30,\n",
       "          4385,  5548,    19,    62,   568, 13165,    31, 15527,   111,    33,\n",
       "            24,    17,   694,   677,  1618,    19,  9519,  3235,     9,   147,\n",
       "           603,    22,   371,    18,   863,    18,  4975,    85, 24382,    66,\n",
       "            19,    57,    85,    54,   500,   154,    22,   962,  2123,    22,\n",
       "            18,   424,    20, 23807,    62,   224, 14217,     9,   296,   129,\n",
       "            19,   634,    19,    90,    18,  6014,    30,   896,    95,    19,\n",
       "            85,  3713,    38,    62,   293,  1430,    19,    21,    42,    19,\n",
       "            17,    12,  3273,   203,    19, 15527,   111,    19,    44,   132,\n",
       "           216,     9,    12,    17,    12,  3045,    19,   831,   136,    12,\n",
       "         15527,   111,  9281,    19, 14493,    33,  5648, 11404,     9,    17,\n",
       "            12, 10884,    82,  4756,    19,    35,   569,   102,  5590,   136,\n",
       "          3129,    44,   512,    44,    26,    88,  2910,    82,    12,    17,\n",
       "            12,    96,    26,   189, 14584,  1546,    22,    39,  2910,    19,\n",
       "           157,    94,    53,    19,    12,  1061,  3426,     9, 12378,    88,\n",
       "            19, 31535,   111,     9,    17,    12,    96, 16125,    35,   125,\n",
       "          4177,    22,    47,    44,   216,    19,    57,    35,    64,    26,\n",
       "            46,  4151,    22, 30060,    44,    20,    18,  4975,  1617,     9,\n",
       "           209,    19,    34,    44,   248,    19,    36,    74,    77,   435,\n",
       "           839, 16369,    38,   192,    19,    21,   102,    19, 10713,    19,\n",
       "            35,   232,    35,  1530,    47,    22,   371,    25,     9,    12,\n",
       "            17,    12,  3045,    19,    44, 11166,   831,   136,   201, 13339,\n",
       "          5429,   136,  1134,   195,    44,    41,   136,    12,   209, 15527,\n",
       "           111,  5415,   199,    18,  1324,    21,   675,    62,   831,    24,\n",
       "         11864,    29,   896,    17,    23,  7276,     4,     3]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25dd4b70-f9d7-4c3a-81d6-da03dd8cf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_out = sample_model.transformer_model(sample_input_ids, attention_mask=sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a770a11a-485d-49b5-ae8a-9d5dccc90a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'mems', 'hidden_states'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "841fcfeb-6e75-40e5-8f82-265ed8da72d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, torch.Size([2, 248, 1024]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(internal_out.hidden_states), internal_out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res = sample_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea66f03e-eac6-478c-ab27-042d97ec1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1024]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res[0].shape, sample_res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-33.6125,  20.8116, -37.3507,  ...,  45.8287,   8.7434,  15.5695],\n",
       "        [-10.1358,  -6.9110,   9.7339,  ...,  14.0870, -27.7199,  16.4197],\n",
       "        [ 29.9244, -13.2548,   9.5534,  ...,  -1.6752,  41.7243,   2.7099],\n",
       "        ...,\n",
       "        [ -7.0634,  37.7444,  -5.1741,  ..., -28.5020, -33.9613,  13.2693],\n",
       "        [ -5.2980,   9.7112, -13.2849,  ...,  -2.8648,  -6.6645,  19.3413],\n",
       "        [ 22.5773,  42.2794,   5.8758,  ...,  22.9042,   1.5186,  31.2972]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    attention_param_start = 414\n",
    "    regressor_param_start = 418\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 189\n",
    "    layer_middle_threshold = 325\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "#                 mse.backward()\n",
    "#                 self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f3b704f-b4e5-4b33-a33b-159ba8b5685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: { 'base_lr': 0.0001190683694379101, 'last_lr': 0.00017987585986205585, 'epochs': 4 } Best value: 0.49271923303604126\n",
    "# Fold 1: {'base_lr': 0.00012114635348406963, 'last_lr': 0.0005477206613438486, 'epochs': 4}. Best value:  0.45853328704833984\n",
    "# Fold 2: {'base_lr': 5.24730490640746e-05, 'last_lr': 0.00020041362261812433, 'epochs': 4}   Best value:  0.49088865518569946\n",
    "# Fold 3: {'base_lr': 6.108276630664184e-05, 'last_lr': 0.00011544056953737668, 'epochs': 4}. Best value:  0.4930591881275177\n",
    "# Fold 4: {'base_lr': 0.0001717178883932075, 'last_lr': 0.00042448836147656634, 'epochs': 4}  Best value:  0.48955243825912476\n",
    "# Fold 5: {'base_lr': 0.000135700916847811, 'last_lr': 0.0029640935672153, 'epochs': 4}.      Best value:  0.4688156247138977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 3e-5, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    epochs = 4\n",
    "    schedule_func = trial.suggest_categorical('schedule_func', [get_cosine_with_hard_restarts_schedule_with_warmup, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup])\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr} epochs {epochs}')\n",
    "    print(f'##### Using {schedule_func}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = schedule_func(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler() # fp16\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, \n",
    "                      scheduler = scheduler, num_epochs = epochs)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48210ee3-d9ea-4bab-b852-627f6f75ce0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:09:27,284]\u001b[0m A new study created in memory with name: no-name-f28c07a0-9004-4214-bd53-abc0ddfaed43\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.00026072981385982097 last_lr 0.0025988624450275374 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7fe1f186b280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d679abe0b1468585e07f1df5f57a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.52 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.671 New best_val_rmse: 2.671\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.044 New best_val_rmse: 1.044\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.084 Still best_val_rmse: 1.044 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.206 Still best_val_rmse: 1.044 (from epoch 0)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.33 Still best_val_rmse: 1.044 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.112 Still best_val_rmse: 1.044 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:11:01,214]\u001b[0m Trial 0 finished with value: 1.0441198348999023 and parameters: {'base_lr': 0.00026072981385982097, 'last_lr': 0.0025988624450275374, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7fe1f186b280>}. Best is trial 0 with value: 1.0441198348999023.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00013328109624069146 last_lr 0.0013247307057016205 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45c0ccbb5d04d53a7ccd926665baf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.67 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.373 New best_val_rmse: 1.373\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8198 New best_val_rmse: 0.8198\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7715 New best_val_rmse: 0.7715\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.621 New best_val_rmse: 0.621\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5937 New best_val_rmse: 0.5937\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6484 Still best_val_rmse: 0.5937 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.11 Still best_val_rmse: 0.5937 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7706 Still best_val_rmse: 0.5937 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5872 New best_val_rmse: 0.5872\n",
      "\n",
      "16 steps took 7.33 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.7151 Still best_val_rmse: 0.5872 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5596 New best_val_rmse: 0.5596\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5578 New best_val_rmse: 0.5578\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5774 Still best_val_rmse: 0.5578 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5372 New best_val_rmse: 0.5372\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5451 Still best_val_rmse: 0.5372 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5388 Still best_val_rmse: 0.5372 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5683 Still best_val_rmse: 0.5372 (from epoch 1)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5704 Still best_val_rmse: 0.5372 (from epoch 1)\n",
      "\n",
      "16 steps took 7.28 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.531 New best_val_rmse: 0.531\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5312 Still best_val_rmse: 0.531 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5069 New best_val_rmse: 0.5069\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5224 Still best_val_rmse: 0.5069 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5035 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5024 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5066 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5052 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 7.26 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5049 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5057 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5036 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5096 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5048 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.505 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5079 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5072 Still best_val_rmse: 0.5013 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5169 Still best_val_rmse: 0.5013 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:19:31,200]\u001b[0m Trial 1 finished with value: 0.5012840628623962 and parameters: {'base_lr': 0.00013328109624069146, 'last_lr': 0.0013247307057016205, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>}. Best is trial 1 with value: 0.5012840628623962.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.0001515095861920755 last_lr 0.001985500783768097 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7fe1f186b3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691f5441b47c45cdaefb5b7d701b6d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.59 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.387 New best_val_rmse: 1.387\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.19 New best_val_rmse: 1.19\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.736 New best_val_rmse: 0.736\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.157 Still best_val_rmse: 0.736 (from epoch 0)\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.224 Still best_val_rmse: 0.736 (from epoch 0)\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.1 Still best_val_rmse: 0.736 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:21:05,866]\u001b[0m Trial 2 finished with value: 0.7360088229179382 and parameters: {'base_lr': 0.0001515095861920755, 'last_lr': 0.001985500783768097, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7fe1f186b3a0>}. Best is trial 1 with value: 0.5012840628623962.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 5.0146723276165316e-05 last_lr 0.0010658486417691217 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16816e615c97495bb37221062c4cb56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.72 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.653 New best_val_rmse: 1.653\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9268 New best_val_rmse: 0.9268\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7334 New best_val_rmse: 0.7334\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6508 New best_val_rmse: 0.6508\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9067 Still best_val_rmse: 0.6508 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6018 New best_val_rmse: 0.6018\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5902 New best_val_rmse: 0.5902\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6414 Still best_val_rmse: 0.5902 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6285 Still best_val_rmse: 0.5902 (from epoch 0)\n",
      "\n",
      "16 steps took 7.35 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5575 New best_val_rmse: 0.5575\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5556 New best_val_rmse: 0.5556\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5535 New best_val_rmse: 0.5535\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5864 Still best_val_rmse: 0.5535 (from epoch 1)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5899 Still best_val_rmse: 0.5535 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5672 Still best_val_rmse: 0.5535 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5667 Still best_val_rmse: 0.5535 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5769 Still best_val_rmse: 0.5535 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5865 Still best_val_rmse: 0.5535 (from epoch 1)\n",
      "\n",
      "16 steps took 7.26 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5492 New best_val_rmse: 0.5492\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5423 New best_val_rmse: 0.5423\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5322 New best_val_rmse: 0.5322\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5581 Still best_val_rmse: 0.5322 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5315 New best_val_rmse: 0.5315\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5305 New best_val_rmse: 0.5305\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5329 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.533 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.531 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 7.37 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5309 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5312 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5311 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5344 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5316 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5327 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5359 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5388 Still best_val_rmse: 0.5305 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5491 Still best_val_rmse: 0.5305 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:29:37,669]\u001b[0m Trial 3 finished with value: 0.5305038094520569 and parameters: {'base_lr': 5.0146723276165316e-05, 'last_lr': 0.0010658486417691217, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>}. Best is trial 1 with value: 0.5012840628623962.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.0003178309138188082 last_lr 0.0005171568446463313 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4f81179d5643f9986a1855220c7877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.59 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.328 New best_val_rmse: 1.328\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.557 Still best_val_rmse: 1.328 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.178 New best_val_rmse: 1.178\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.061 New best_val_rmse: 1.061\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.313 Still best_val_rmse: 1.061 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.095 Still best_val_rmse: 1.061 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:31:12,114]\u001b[0m Trial 4 finished with value: 1.0610512495040894 and parameters: {'base_lr': 0.0003178309138188082, 'last_lr': 0.0005171568446463313, 'schedule_func': <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>}. Best is trial 1 with value: 0.5012840628623962.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00021275634427171412 last_lr 0.00021952950481681497 epochs 4\n",
      "##### Using <function get_linear_schedule_with_warmup at 0x7fe1f186b280>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e600e29f55c44464ba1bcb17b33d1742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.67 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.63 New best_val_rmse: 1.63\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.296 New best_val_rmse: 1.296\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7887 New best_val_rmse: 0.7887\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8423 Still best_val_rmse: 0.7887 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6209 New best_val_rmse: 0.6209\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6666 Still best_val_rmse: 0.6209 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6751 Still best_val_rmse: 0.6209 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.615 New best_val_rmse: 0.615\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.614 New best_val_rmse: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:33:30,194]\u001b[0m Trial 5 finished with value: 0.6140443086624146 and parameters: {'base_lr': 0.00021275634427171412, 'last_lr': 0.00021952950481681497, 'schedule_func': <function get_linear_schedule_with_warmup at 0x7fe1f186b280>}. Best is trial 1 with value: 0.5012840628623962.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.000208932441184545 last_lr 0.001504442525654463 epochs 4\n",
      "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7fe1f186b3a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753a3002d6fc42c4b754c5e294963022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.947 New best_val_rmse: 1.947\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.106 New best_val_rmse: 1.106\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.158 Still best_val_rmse: 1.106 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.14 Still best_val_rmse: 1.106 (from epoch 0)\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.393 Still best_val_rmse: 1.106 (from epoch 0)\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.108 Still best_val_rmse: 1.106 (from epoch 0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-27 17:35:04,622]\u001b[0m Trial 6 finished with value: 1.1064022779464722 and parameters: {'base_lr': 0.000208932441184545, 'last_lr': 0.001504442525654463, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7fe1f186b3a0>}. Best is trial 1 with value: 0.5012840628623962.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 3.097867799812087e-05 last_lr 0.00013146474035657642 epochs 4\n",
      "##### Using <function get_cosine_schedule_with_warmup at 0x7fe1f186b310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc8ebe064f547e9b9c9615044c34df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.54 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.011 New best_val_rmse: 2.011\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.112 New best_val_rmse: 1.112\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7378 New best_val_rmse: 0.7378\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6768 New best_val_rmse: 0.6768\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8331 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6172 New best_val_rmse: 0.6172\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6264 Still best_val_rmse: 0.6172 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5771 New best_val_rmse: 0.5771\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5648 New best_val_rmse: 0.5648\n",
      "\n",
      "16 steps took 7.28 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5516 New best_val_rmse: 0.5516\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5675 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5534 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5662 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6003 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5544 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5594 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5581 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5457 New best_val_rmse: 0.5457\n",
      "\n",
      "16 steps took 7.32 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5963 Still best_val_rmse: 0.5457 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5341 New best_val_rmse: 0.5341\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.535 Still best_val_rmse: 0.5341 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5519 Still best_val_rmse: 0.5341 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.531 New best_val_rmse: 0.531\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5317 Still best_val_rmse: 0.531 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5346 Still best_val_rmse: 0.531 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5362 Still best_val_rmse: 0.531 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5333 Still best_val_rmse: 0.531 (from epoch 2)\n",
      "\n",
      "16 steps took 7.28 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5331 Still best_val_rmse: 0.531 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:24:27,045]\u001b[0m A new study created in memory with name: no-name-677fc894-e6f1-489f-bba5-60271810ee28\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 3\n",
      "##### Using base_lr 0.00013674280465279492 last_lr 0.0007463327517034599 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f631978f7d2646378bfbea27c765bde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.41 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.2 New best_val_rmse: 2.2\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8643 New best_val_rmse: 0.8643\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6854 New best_val_rmse: 0.6854\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8669 Still best_val_rmse: 0.6854 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7064 Still best_val_rmse: 0.6854 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6116 New best_val_rmse: 0.6116\n",
      "\n",
      "16 steps took 6.68 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7069 Still best_val_rmse: 0.6116 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6129 Still best_val_rmse: 0.6116 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.82 Still best_val_rmse: 0.6116 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:26:43,001]\u001b[0m Trial 0 finished with value: 0.6116013526916504 and parameters: {'base_lr': 0.00013674280465279492, 'last_lr': 0.0007463327517034599, 'epochs': 5}. Best is trial 0 with value: 0.6116013526916504.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00020254393403725305 last_lr 0.0006604802269671327 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520758c675534dbd8b89d67d11260948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.518 New best_val_rmse: 2.518\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9558 New best_val_rmse: 0.9558\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7423 New best_val_rmse: 0.7423\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.728 New best_val_rmse: 0.728\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7793 Still best_val_rmse: 0.728 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6146 New best_val_rmse: 0.6146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:28:16,635]\u001b[0m Trial 1 finished with value: 0.6145561337471008 and parameters: {'base_lr': 0.00020254393403725305, 'last_lr': 0.0006604802269671327, 'epochs': 3}. Best is trial 0 with value: 0.6116013526916504.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00026969819179850266 last_lr 0.0012131152696478107 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873bd85834064a88ab5f73d36c1c5f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.41 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.31 New best_val_rmse: 2.31\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.284 New best_val_rmse: 1.284\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.207 New best_val_rmse: 1.207\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.097 New best_val_rmse: 1.097\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.031 New best_val_rmse: 1.031\n",
      "\n",
      "16 steps took 6.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:29:50,284]\u001b[0m Trial 2 finished with value: 1.0311695337295532 and parameters: {'base_lr': 0.00026969819179850266, 'last_lr': 0.0012131152696478107, 'epochs': 3}. Best is trial 0 with value: 0.6116013526916504.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 3.536 Still best_val_rmse: 1.031 (from epoch 0)\n",
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 4.244562638646704e-05 last_lr 0.00036484650511324436 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1e06a9e1574dc29804dcb998661363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.567 New best_val_rmse: 1.567\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7649 New best_val_rmse: 0.7649\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6699 New best_val_rmse: 0.6699\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7011 Still best_val_rmse: 0.6699 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6042 New best_val_rmse: 0.6042\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5994 New best_val_rmse: 0.5994\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6721 Still best_val_rmse: 0.5994 (from epoch 0)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5806 New best_val_rmse: 0.5806\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6881 Still best_val_rmse: 0.5806 (from epoch 0)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5791 New best_val_rmse: 0.5791\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5612 New best_val_rmse: 0.5612\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5463 New best_val_rmse: 0.5463\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5589 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5502 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5791 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5757 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.6007 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5752 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 7.27 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5709 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.6292 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5872 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5638 Still best_val_rmse: 0.5463 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5318 New best_val_rmse: 0.5318\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.6382 Still best_val_rmse: 0.5318 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5242 New best_val_rmse: 0.5242\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.6031 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5349 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 7.33 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5329 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5438 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5246 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5357 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5243 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5259 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5304 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5282 Still best_val_rmse: 0.5242 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5315 Still best_val_rmse: 0.5242 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:38:17,159]\u001b[0m Trial 3 finished with value: 0.524225115776062 and parameters: {'base_lr': 4.244562638646704e-05, 'last_lr': 0.00036484650511324436, 'epochs': 4}. Best is trial 3 with value: 0.524225115776062.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0002437699640745575 last_lr 0.0004261877423621473 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d360cb8918494818b26d7a3c74cbd60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.52 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 3.337 New best_val_rmse: 3.337\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.048 New best_val_rmse: 1.048\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.07 Still best_val_rmse: 1.048 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.178 Still best_val_rmse: 1.048 (from epoch 0)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.044 New best_val_rmse: 1.044\n",
      "\n",
      "16 steps took 6.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:39:50,547]\u001b[0m Trial 4 finished with value: 1.0441583395004272 and parameters: {'base_lr': 0.0002437699640745575, 'last_lr': 0.0004261877423621473, 'epochs': 4}. Best is trial 3 with value: 0.524225115776062.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.101 Still best_val_rmse: 1.044 (from epoch 0)\n",
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00012866543806904285 last_lr 0.00294093536575603 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692bb1f662384c6dbaa0fe23a10197c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.881 New best_val_rmse: 1.881\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8812 New best_val_rmse: 0.8812\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7477 New best_val_rmse: 0.7477\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8286 Still best_val_rmse: 0.7477 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6454 New best_val_rmse: 0.6454\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6356 New best_val_rmse: 0.6356\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7481 Still best_val_rmse: 0.6356 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6025 New best_val_rmse: 0.6025\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8019 Still best_val_rmse: 0.6025 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:42:06,608]\u001b[0m Trial 5 finished with value: 0.6025357246398926 and parameters: {'base_lr': 0.00012866543806904285, 'last_lr': 0.00294093536575603, 'epochs': 5}. Best is trial 3 with value: 0.524225115776062.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 9.78622966176048e-05 last_lr 0.000594007307912072 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ab18b474634a6fad2c67894ecc4e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.45 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.297 New best_val_rmse: 1.297\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9661 New best_val_rmse: 0.9661\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6881 New best_val_rmse: 0.6881\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6476 New best_val_rmse: 0.6476\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7009 Still best_val_rmse: 0.6476 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6777 Still best_val_rmse: 0.6476 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8833 Still best_val_rmse: 0.6476 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7291 Still best_val_rmse: 0.6476 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7085 Still best_val_rmse: 0.6476 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:44:23,566]\u001b[0m Trial 6 finished with value: 0.6475713849067688 and parameters: {'base_lr': 9.78622966176048e-05, 'last_lr': 0.000594007307912072, 'epochs': 3}. Best is trial 3 with value: 0.524225115776062.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0003602540833722712 last_lr 0.0018127809391390109 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b56042302b486e90b38932e21f970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.43 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.165 New best_val_rmse: 2.165\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.181 New best_val_rmse: 1.181\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.176 New best_val_rmse: 1.176\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.548 Still best_val_rmse: 1.176 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.042 New best_val_rmse: 1.042\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.07 Still best_val_rmse: 1.042 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:45:56,899]\u001b[0m Trial 7 finished with value: 1.0422102212905884 and parameters: {'base_lr': 0.0003602540833722712, 'last_lr': 0.0018127809391390109, 'epochs': 4}. Best is trial 3 with value: 0.524225115776062.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 4.6415807784583515e-05 last_lr 0.000316814913030945 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb4acf548644048a8f8ec0af9005e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.433 New best_val_rmse: 1.433\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8469 New best_val_rmse: 0.8469\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6655 New best_val_rmse: 0.6655\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6905 Still best_val_rmse: 0.6655 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6058 New best_val_rmse: 0.6058\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.626 Still best_val_rmse: 0.6058 (from epoch 0)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7209 Still best_val_rmse: 0.6058 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5918 New best_val_rmse: 0.5918\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6589 Still best_val_rmse: 0.5918 (from epoch 0)\n",
      "\n",
      "16 steps took 7.14 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5979 Still best_val_rmse: 0.5918 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5621 New best_val_rmse: 0.5621\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5368 New best_val_rmse: 0.5368\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5484 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5575 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5939 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5489 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5535 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6008 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5594 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5987 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5869 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5384 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5476 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.6184 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5482 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5977 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5431 Still best_val_rmse: 0.5368 (from epoch 1)\n",
      "\n",
      "16 steps took 7.12 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.528 New best_val_rmse: 0.528\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5284 Still best_val_rmse: 0.528 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.528 Still best_val_rmse: 0.528 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.531 Still best_val_rmse: 0.528 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.516 New best_val_rmse: 0.516\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5217 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.518 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5187 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5168 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.5212 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.5209 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.5199 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.5194 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.5183 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.5177 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.5174 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.5172 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.5172 Still best_val_rmse: 0.516 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.5172 Still best_val_rmse: 0.516 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:56:35,514]\u001b[0m Trial 8 finished with value: 0.516015350818634 and parameters: {'base_lr': 4.6415807784583515e-05, 'last_lr': 0.000316814913030945, 'epochs': 5}. Best is trial 8 with value: 0.516015350818634.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0001790980718155451 last_lr 9.931083236563293e-05 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255fa380eebd434caa37c97b7983ea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.47 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.762 New best_val_rmse: 1.762\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.101 New best_val_rmse: 1.101\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.059 New best_val_rmse: 1.059\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7186 New best_val_rmse: 0.7186\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.666 New best_val_rmse: 0.666\n",
      "\n",
      "16 steps took 6.6 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 14:58:11,958]\u001b[0m Trial 9 finished with value: 0.6659549474716187 and parameters: {'base_lr': 0.0001790980718155451, 'last_lr': 9.931083236563293e-05, 'epochs': 4}. Best is trial 8 with value: 0.516015350818634.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 0.7002 Still best_val_rmse: 0.666 (from epoch 0)\n",
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.102510831375899e-05 last_lr 0.00014231526067270248 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f12c7da3ccc46379e850aa3750b34b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.836 New best_val_rmse: 1.836\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9155 New best_val_rmse: 0.9155\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8068 New best_val_rmse: 0.8068\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6328 New best_val_rmse: 0.6328\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7069 Still best_val_rmse: 0.6328 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6698 Still best_val_rmse: 0.6328 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.636 Still best_val_rmse: 0.6328 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6255 New best_val_rmse: 0.6255\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6862 Still best_val_rmse: 0.6255 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 15:00:28,294]\u001b[0m Trial 10 finished with value: 0.6255457401275635 and parameters: {'base_lr': 3.102510831375899e-05, 'last_lr': 0.00014231526067270248, 'epochs': 5}. Best is trial 8 with value: 0.516015350818634.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 4.052642811586042e-05 last_lr 0.00022293895409840363 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64ff2dcb0444f3ab13e52e93206e87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.44 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.682 New best_val_rmse: 1.682\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7747 New best_val_rmse: 0.7747\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7187 New best_val_rmse: 0.7187\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6551 New best_val_rmse: 0.6551\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6904 Still best_val_rmse: 0.6551 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7293 Still best_val_rmse: 0.6551 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5945 New best_val_rmse: 0.5945\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5782 New best_val_rmse: 0.5782\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7097 Still best_val_rmse: 0.5782 (from epoch 0)\n",
      "\n",
      "16 steps took 7.22 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5791 Still best_val_rmse: 0.5782 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5859 Still best_val_rmse: 0.5782 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5585 New best_val_rmse: 0.5585\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5647 Still best_val_rmse: 0.5585 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5598 Still best_val_rmse: 0.5585 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6187 Still best_val_rmse: 0.5585 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5621 Still best_val_rmse: 0.5585 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5668 Still best_val_rmse: 0.5585 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5626 Still best_val_rmse: 0.5585 (from epoch 1)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5529 New best_val_rmse: 0.5529\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5932 Still best_val_rmse: 0.5529 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.6051 Still best_val_rmse: 0.5529 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5365 New best_val_rmse: 0.5365\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5495 Still best_val_rmse: 0.5365 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.6507 Still best_val_rmse: 0.5365 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5307 New best_val_rmse: 0.5307\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.6005 Still best_val_rmse: 0.5307 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5343 Still best_val_rmse: 0.5307 (from epoch 2)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5305 New best_val_rmse: 0.5305\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5328 Still best_val_rmse: 0.5305 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.525 New best_val_rmse: 0.525\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5281 Still best_val_rmse: 0.525 (from epoch 3)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5258 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5277 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5239 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5226 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 7.13 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.5251 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.5212 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.522 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.5226 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.5209 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.5208 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.5209 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.5213 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.5211 Still best_val_rmse: 0.5197 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.5211 Still best_val_rmse: 0.5197 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 15:11:05,558]\u001b[0m Trial 11 finished with value: 0.5196667313575745 and parameters: {'base_lr': 4.052642811586042e-05, 'last_lr': 0.00022293895409840363, 'epochs': 5}. Best is trial 8 with value: 0.516015350818634.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.068588484770048e-05 last_lr 0.00021184087471614598 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf7bcdf0c5f47c2be2ff8e4cfdfc388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.49 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6841 New best_val_rmse: 0.6841\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7603 Still best_val_rmse: 0.6841 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8245 Still best_val_rmse: 0.6841 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5825 New best_val_rmse: 0.5825\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5428 New best_val_rmse: 0.5428\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5655 Still best_val_rmse: 0.5428 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5388 New best_val_rmse: 0.5388\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5205 New best_val_rmse: 0.5205\n",
      "\n",
      "16 steps took 7.14 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6897 Still best_val_rmse: 0.5205 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6228 Still best_val_rmse: 0.5205 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5172 New best_val_rmse: 0.5172\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5768 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.527 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5243 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5259 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5615 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5696 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.533 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5727 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.6041 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5148 New best_val_rmse: 0.5148\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5334 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5766 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5125 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5572 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5091 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.503 New best_val_rmse: 0.503\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5005 New best_val_rmse: 0.5005\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5014 Still best_val_rmse: 0.5005 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5032 Still best_val_rmse: 0.5005 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5073 Still best_val_rmse: 0.5005 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5176 Still best_val_rmse: 0.5005 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5006 Still best_val_rmse: 0.5005 (from epoch 3)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.498 New best_val_rmse: 0.498\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4977 New best_val_rmse: 0.4977\n",
      "\n",
      "8 steps took 3.94 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4982 Still best_val_rmse: 0.4969 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.498 Still best_val_rmse: 0.4969 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4981 Still best_val_rmse: 0.4969 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4963 New best_val_rmse: 0.4963\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 40 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.4964 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 56 val_rmse: 0.4968 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.4966 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 72 val_rmse: 0.4973 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4982 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 88 val_rmse: 0.4972 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.4971 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 104 val_rmse: 0.4971 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.497 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 120 val_rmse: 0.4971 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.4971 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 136 val_rmse: 0.4971 Still best_val_rmse: 0.4961 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.497 Still best_val_rmse: 0.4961 (from epoch 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 15:22:51,672]\u001b[0m Trial 12 finished with value: 0.49611586332321167 and parameters: {'base_lr': 6.068588484770048e-05, 'last_lr': 0.00021184087471614598, 'epochs': 5}. Best is trial 12 with value: 0.49611586332321167.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 7.101317396072361e-05 last_lr 0.0002051946814279808 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb2d481fab145d58f19bf229ba5a200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.57 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.146 New best_val_rmse: 1.146\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8374 New best_val_rmse: 0.8374\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7145 New best_val_rmse: 0.7145\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7251 Still best_val_rmse: 0.7145 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6168 New best_val_rmse: 0.6168\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6866 Still best_val_rmse: 0.6168 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6057 New best_val_rmse: 0.6057\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5453 New best_val_rmse: 0.5453\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5674 Still best_val_rmse: 0.5453 (from epoch 0)\n",
      "\n",
      "16 steps took 7.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.589 Still best_val_rmse: 0.5453 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5749 Still best_val_rmse: 0.5453 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5237 New best_val_rmse: 0.5237\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5466 Still best_val_rmse: 0.5237 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5413 Still best_val_rmse: 0.5237 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5493 Still best_val_rmse: 0.5237 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5221 New best_val_rmse: 0.5221\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5369 Still best_val_rmse: 0.5221 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6572 Still best_val_rmse: 0.5221 (from epoch 1)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5412 Still best_val_rmse: 0.5221 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5317 Still best_val_rmse: 0.5221 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.549 Still best_val_rmse: 0.5221 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5138 New best_val_rmse: 0.5138\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5044 New best_val_rmse: 0.5044\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5236 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5422 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5263 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5309 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 7.19 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5117 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5149 Still best_val_rmse: 0.5044 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4985 New best_val_rmse: 0.4985\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4958 New best_val_rmse: 0.4958\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4944 New best_val_rmse: 0.4944\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4958 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4947 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4964 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5061 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5174 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4956 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4967 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4976 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4975 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.82 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.4979 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4957 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4964 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4963 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4958 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 40 val_rmse: 0.4961 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.496 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 56 val_rmse: 0.4952 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.4951 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 72 val_rmse: 0.4965 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4956 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 88 val_rmse: 0.4955 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.4957 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 104 val_rmse: 0.4956 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.4955 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 120 val_rmse: 0.4955 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.4956 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 136 val_rmse: 0.4956 Still best_val_rmse: 0.4944 (from epoch 3)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.4955 Still best_val_rmse: 0.4944 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 15:35:05,685]\u001b[0m Trial 13 finished with value: 0.49442538619041443 and parameters: {'base_lr': 7.101317396072361e-05, 'last_lr': 0.0002051946814279808, 'epochs': 5}. Best is trial 13 with value: 0.49442538619041443.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 7.958738601878582e-05 last_lr 8.250917348407703e-05 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc53a480ae294f208fd17dc9bbb1079c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.47 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.271 New best_val_rmse: 1.271\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7439 New best_val_rmse: 0.7439\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7365 New best_val_rmse: 0.7365\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8225 Still best_val_rmse: 0.7365 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6078 New best_val_rmse: 0.6078\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6447 Still best_val_rmse: 0.6078 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5607 New best_val_rmse: 0.5607\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5597 New best_val_rmse: 0.5597\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5752 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 7.26 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5607 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5699 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5774 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6298 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.564 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5866 Still best_val_rmse: 0.5597 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5241 New best_val_rmse: 0.5241\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5382 Still best_val_rmse: 0.5241 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6035 Still best_val_rmse: 0.5241 (from epoch 1)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5421 Still best_val_rmse: 0.5241 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5822 Still best_val_rmse: 0.5241 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.557 Still best_val_rmse: 0.5241 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5107 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5155 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5378 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5427 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5308 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 7.11 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5095 Still best_val_rmse: 0.5049 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5035 New best_val_rmse: 0.5035\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4989 New best_val_rmse: 0.4989\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5 Still best_val_rmse: 0.4989 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4978 New best_val_rmse: 0.4978\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4977 New best_val_rmse: 0.4977\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5132 Still best_val_rmse: 0.4977 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.521 Still best_val_rmse: 0.4977 (from epoch 3)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5008 Still best_val_rmse: 0.4977 (from epoch 3)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4985 Still best_val_rmse: 0.4977 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4972 New best_val_rmse: 0.4972\n",
      "\n",
      "8 steps took 3.83 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.4983 Still best_val_rmse: 0.4972 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4979 Still best_val_rmse: 0.4972 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4977 Still best_val_rmse: 0.4972 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4967 New best_val_rmse: 0.4967\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 40 val_rmse: 0.4967 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.4976 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 56 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.4968 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 72 val_rmse: 0.4973 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4976 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 88 val_rmse: 0.4974 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.4975 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 104 val_rmse: 0.4976 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.4973 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 120 val_rmse: 0.4971 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.4972 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 136 val_rmse: 0.497 Still best_val_rmse: 0.4966 (from epoch 4)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.4971 Still best_val_rmse: 0.4966 (from epoch 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 15:47:08,171]\u001b[0m Trial 14 finished with value: 0.496579110622406 and parameters: {'base_lr': 7.958738601878582e-05, 'last_lr': 8.250917348407703e-05, 'epochs': 5}. Best is trial 13 with value: 0.49442538619041443.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 7.371486452999384e-05 last_lr 0.00017425449514158598 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e655f91958054351b0003accad313424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.45 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.189 New best_val_rmse: 1.189\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8729 New best_val_rmse: 0.8729\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6903 New best_val_rmse: 0.6903\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7388 Still best_val_rmse: 0.6903 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5962 New best_val_rmse: 0.5962\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6426 Still best_val_rmse: 0.5962 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6349 Still best_val_rmse: 0.5962 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6007 Still best_val_rmse: 0.5962 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5541 New best_val_rmse: 0.5541\n",
      "\n",
      "16 steps took 7.19 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6976 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6161 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5347 New best_val_rmse: 0.5347\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5753 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5576 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5471 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5459 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5571 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6282 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 7.11 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.552 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5376 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5378 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.7371 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.7083 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.177 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.245 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.107 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.041 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 7.05 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 1.046 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 1.134 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 1.045 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 1.041 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 1.074 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 1.043 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 1.05 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 1.052 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 1.053 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 7.09 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 1.045 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 1.052 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 1.048 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 1.044 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 1.045 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 1.041 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 1.041 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 1.042 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 1.042 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 1.042 Still best_val_rmse: 0.5347 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 15:58:01,709]\u001b[0m Trial 15 finished with value: 0.534666895866394 and parameters: {'base_lr': 7.371486452999384e-05, 'last_lr': 0.00017425449514158598, 'epochs': 5}. Best is trial 13 with value: 0.49442538619041443.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.108276630664184e-05 last_lr 0.00011544056953737668 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dbddd881084fd18e0660fb2e41d1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.55 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.048 New best_val_rmse: 1.048\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7279 New best_val_rmse: 0.7279\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7155 New best_val_rmse: 0.7155\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7595 Still best_val_rmse: 0.7155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7031 New best_val_rmse: 0.7031\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5437 New best_val_rmse: 0.5437\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.643 Still best_val_rmse: 0.5437 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5252 New best_val_rmse: 0.5252\n",
      "\n",
      "16 steps took 7.22 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.7084 Still best_val_rmse: 0.5252 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5528 Still best_val_rmse: 0.5252 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5304 Still best_val_rmse: 0.5197 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5186 New best_val_rmse: 0.5186\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5275 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5261 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5407 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.604 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5331 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.6112 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5987 Still best_val_rmse: 0.5186 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5126 New best_val_rmse: 0.5126\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5023 New best_val_rmse: 0.5023\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5722 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5929 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5153 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5374 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5059 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5051 Still best_val_rmse: 0.5023 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4963 New best_val_rmse: 0.4963\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4996 Still best_val_rmse: 0.4963 (from epoch 3)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4971 Still best_val_rmse: 0.4963 (from epoch 3)\n",
      "\n",
      "8 steps took 3.43 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4967 Still best_val_rmse: 0.4963 (from epoch 3)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4962 New best_val_rmse: 0.4962\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.5016 Still best_val_rmse: 0.4962 (from epoch 3)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.5189 Still best_val_rmse: 0.4962 (from epoch 3)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4949 New best_val_rmse: 0.4949\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4954 Still best_val_rmse: 0.4949 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4931 New best_val_rmse: 0.4931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:07:13,880]\u001b[0m Trial 16 finished with value: 0.4930591881275177 and parameters: {'base_lr': 6.108276630664184e-05, 'last_lr': 0.00011544056953737668, 'epochs': 4}. Best is trial 16 with value: 0.4930591881275177.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 5.801665314119398e-05 last_lr 0.00011042592029436529 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c80e00adba049aea205d7a1728c3e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.51 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.139 New best_val_rmse: 1.139\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.759 New best_val_rmse: 0.759\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7591 Still best_val_rmse: 0.759 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7948 Still best_val_rmse: 0.759 (from epoch 0)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.592 New best_val_rmse: 0.592\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5457 New best_val_rmse: 0.5457\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5904 Still best_val_rmse: 0.5457 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5438 New best_val_rmse: 0.5438\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5177 New best_val_rmse: 0.5177\n",
      "\n",
      "16 steps took 7.15 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.7135 Still best_val_rmse: 0.5177 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6268 Still best_val_rmse: 0.5177 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5276 Still best_val_rmse: 0.5177 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5278 Still best_val_rmse: 0.5177 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.523 Still best_val_rmse: 0.5177 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5292 Still best_val_rmse: 0.5177 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5166 New best_val_rmse: 0.5166\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5674 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5822 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 7.15 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5289 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.6219 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.6355 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.513 New best_val_rmse: 0.513\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5142 Still best_val_rmse: 0.513 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5327 Still best_val_rmse: 0.513 (from epoch 2)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5709 Still best_val_rmse: 0.513 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5294 Still best_val_rmse: 0.5117 (from epoch 2)\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5068 New best_val_rmse: 0.5068\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5054 New best_val_rmse: 0.5054\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4981 New best_val_rmse: 0.4981\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.5071 Still best_val_rmse: 0.4981 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4996 Still best_val_rmse: 0.4981 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5044 Still best_val_rmse: 0.4981 (from epoch 3)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5138 Still best_val_rmse: 0.4981 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5137 Still best_val_rmse: 0.4981 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4972 New best_val_rmse: 0.4972\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4964 Still best_val_rmse: 0.4964 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.497 Still best_val_rmse: 0.4964 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:16:02,881]\u001b[0m Trial 17 finished with value: 0.49643710255622864 and parameters: {'base_lr': 5.801665314119398e-05, 'last_lr': 0.00011042592029436529, 'epochs': 4}. Best is trial 16 with value: 0.4930591881275177.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.194618099250861e-05 last_lr 8.128233705048789e-05 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceca8db223274d8eb91f56385803673d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.56 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.848 New best_val_rmse: 1.848\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9074 New best_val_rmse: 0.9074\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8247 New best_val_rmse: 0.8247\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6383 New best_val_rmse: 0.6383\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7207 Still best_val_rmse: 0.6383 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6428 Still best_val_rmse: 0.6383 (from epoch 0)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5909 New best_val_rmse: 0.5909\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6098 Still best_val_rmse: 0.5909 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6287 Still best_val_rmse: 0.5909 (from epoch 0)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6038 Still best_val_rmse: 0.5909 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5552 New best_val_rmse: 0.5552\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5549 New best_val_rmse: 0.5549\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5421 New best_val_rmse: 0.5421\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5759 Still best_val_rmse: 0.5421 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5355 New best_val_rmse: 0.5355\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5443 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5654 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5558 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5399 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5447 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.6104 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5405 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5378 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.6337 Still best_val_rmse: 0.5355 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5314 New best_val_rmse: 0.5314\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5785 Still best_val_rmse: 0.5314 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5294 New best_val_rmse: 0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:22:26,026]\u001b[0m Trial 18 finished with value: 0.5293550491333008 and parameters: {'base_lr': 3.194618099250861e-05, 'last_lr': 8.128233705048789e-05, 'epochs': 3}. Best is trial 16 with value: 0.4930591881275177.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 9.772651821943555e-05 last_lr 0.00013610151307519143 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-07-22 16:22:34,339]\u001b[0m Trial 19 failed because of the following error: ValueError('Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\", line 216, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-53-8b37909b4267>\", line 15, in objective\n",
      "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\", line 445, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1672, in from_pretrained\n",
      "    resolved_vocab_files[file_id] = cached_path(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\", line 1329, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\", line 1552, in get_from_cache\n",
      "    raise ValueError(\n",
      "ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    399\u001b[0m             )\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-8b37909b4267>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSEED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTOKENIZER_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                     resolved_vocab_files[file_id] = cached_path(\n\u001b[0m\u001b[1;32m   1673\u001b[0m                         \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m                         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1550\u001b[0m                     )\n\u001b[1;32m   1551\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1553\u001b[0m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(3, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360e24c-6ca3-4486-b2ba-27d94cb53913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:25:58,833]\u001b[0m A new study created in memory with name: no-name-b64f2479-cc61-4389-a987-26e39eeee01a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 4\n",
      "##### Using base_lr 0.00012455338478817077 last_lr 0.0007911697618105153 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf94715a0744451bb34687680b43c608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.45 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.067 New best_val_rmse: 1.067\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.08 Still best_val_rmse: 1.067 (from epoch 0)\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6595 New best_val_rmse: 0.6595\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8245 Still best_val_rmse: 0.6595 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6526 New best_val_rmse: 0.6526\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6227 New best_val_rmse: 0.6227\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6191 New best_val_rmse: 0.6191\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5593 New best_val_rmse: 0.5593\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5453 New best_val_rmse: 0.5453\n",
      "\n",
      "16 steps took 7.15 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5415 New best_val_rmse: 0.5415\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.54 New best_val_rmse: 0.54\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5531 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5763 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5703 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5801 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5084 New best_val_rmse: 0.5084\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.6587 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.183 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 7.21 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5548 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5588 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5251 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5332 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5124 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5327 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.6128 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5239 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4951 New best_val_rmse: 0.4951\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.5096 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.5027 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.5025 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4971 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4938 New best_val_rmse: 0.4938\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4984 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5014 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4953 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4966 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4957 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4971 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4981 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5002 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4964 Still best_val_rmse: 0.4938 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:35:01,351]\u001b[0m Trial 0 finished with value: 0.4938439726829529 and parameters: {'base_lr': 0.00012455338478817077, 'last_lr': 0.0007911697618105153, 'epochs': 4}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0002489252288528087 last_lr 0.0018404752185335377 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1c7bebee3d43a48736e751544f0cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.59 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 3.833 New best_val_rmse: 3.833\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.265 New best_val_rmse: 1.265\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.18 Still best_val_rmse: 1.045 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.021 New best_val_rmse: 1.021\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.356 Still best_val_rmse: 1.021 (from epoch 0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:36:40,713]\u001b[0m Trial 1 finished with value: 1.0206865072250366 and parameters: {'base_lr': 0.0002489252288528087, 'last_lr': 0.0018404752185335377, 'epochs': 5}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 4\n",
      "##### Using base_lr 9.38235365621146e-05 last_lr 9.008888128281592e-05 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e4904143184298b402d7ad049d2028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.902 New best_val_rmse: 1.902\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9186 New best_val_rmse: 0.9186\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9052 New best_val_rmse: 0.9052\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7886 New best_val_rmse: 0.7886\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6119 New best_val_rmse: 0.6119\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7626 Still best_val_rmse: 0.6119 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6028 New best_val_rmse: 0.6028\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6147 Still best_val_rmse: 0.6028 (from epoch 0)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6376 Still best_val_rmse: 0.6028 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:39:02,587]\u001b[0m Trial 2 finished with value: 0.6027756929397583 and parameters: {'base_lr': 9.38235365621146e-05, 'last_lr': 9.008888128281592e-05, 'epochs': 5}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0003672395297481787 last_lr 0.0013548633017898166 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5322158402ce4108adf1edf2f89f2405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 7.37 New best_val_rmse: 7.37\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.062 New best_val_rmse: 1.062\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.142 Still best_val_rmse: 1.062 (from epoch 0)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.27 Still best_val_rmse: 1.062 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.117 Still best_val_rmse: 1.062 (from epoch 0)\n",
      "\n",
      "16 steps took 6.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:40:35,776]\u001b[0m Trial 3 finished with value: 1.0616463422775269 and parameters: {'base_lr': 0.0003672395297481787, 'last_lr': 0.0013548633017898166, 'epochs': 3}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.151 Still best_val_rmse: 1.062 (from epoch 0)\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.316774517963056e-05 last_lr 0.0007533735404761509 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da7b584843342d4a01f96dfbd308f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.49 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.686 New best_val_rmse: 1.686\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9998 New best_val_rmse: 0.9998\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7348 New best_val_rmse: 0.7348\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.701 New best_val_rmse: 0.701\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6463 New best_val_rmse: 0.6463\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6436 New best_val_rmse: 0.6436\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6069 New best_val_rmse: 0.6069\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.598 New best_val_rmse: 0.598\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6288 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.19 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6033 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6045 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5767 New best_val_rmse: 0.5767\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.568 New best_val_rmse: 0.568\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5443 New best_val_rmse: 0.5443\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5659 Still best_val_rmse: 0.5443 (from epoch 1)\n",
      "\n",
      "16 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5291 New best_val_rmse: 0.5291\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5622 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5457 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5292 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5344 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5331 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.56 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5371 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5418 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5807 Still best_val_rmse: 0.5291 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5135 New best_val_rmse: 0.5135\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.507 New best_val_rmse: 0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:46:58,394]\u001b[0m Trial 4 finished with value: 0.5069844126701355 and parameters: {'base_lr': 8.316774517963056e-05, 'last_lr': 0.0007533735404761509, 'epochs': 3}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.03800437091818e-05 last_lr 0.002631415142328459 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79113ed341e5426fb0ee3179b3af20e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.53 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.245 New best_val_rmse: 1.245\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9155 New best_val_rmse: 0.9155\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 3.788 Still best_val_rmse: 0.9155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.093 Still best_val_rmse: 0.9155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9219 Still best_val_rmse: 0.9155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6371 New best_val_rmse: 0.6371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:48:31,724]\u001b[0m Trial 5 finished with value: 0.6371252536773682 and parameters: {'base_lr': 8.03800437091818e-05, 'last_lr': 0.002631415142328459, 'epochs': 3}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 4\n",
      "##### Using base_lr 6.684301123331321e-05 last_lr 0.0030363691693253004 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342220b80404660b8b392fef71c7990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.52 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.403 New best_val_rmse: 1.403\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8943 New best_val_rmse: 0.8943\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9561 Still best_val_rmse: 0.8943 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6707 New best_val_rmse: 0.6707\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6376 New best_val_rmse: 0.6376\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8012 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7961 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.9327 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6692 Still best_val_rmse: 0.6376 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:50:48,635]\u001b[0m Trial 6 finished with value: 0.6375647187232971 and parameters: {'base_lr': 6.684301123331321e-05, 'last_lr': 0.0030363691693253004, 'epochs': 4}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.880623870249541e-05 last_lr 0.00031391618161442236 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9442a3dcdc254ed6b116eebbe5317fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.53 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.884 New best_val_rmse: 2.884\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9728 New best_val_rmse: 0.9728\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7719 New best_val_rmse: 0.7719\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6433 New best_val_rmse: 0.6433\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6212 New best_val_rmse: 0.6212\n",
      "\n",
      "16 steps took 6.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:52:21,801]\u001b[0m Trial 7 finished with value: 0.5872001051902771 and parameters: {'base_lr': 3.880623870249541e-05, 'last_lr': 0.00031391618161442236, 'epochs': 5}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 0.5872 New best_val_rmse: 0.5872\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0003781921609267425 last_lr 0.0002729467903023369 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d54e19fed364e9ebc35596c7e1ce6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.43 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 7.607 New best_val_rmse: 7.607\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.249 New best_val_rmse: 1.249\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.183 New best_val_rmse: 1.183\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.051 New best_val_rmse: 1.051\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 6.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:53:55,268]\u001b[0m Trial 8 finished with value: 1.0396034717559814 and parameters: {'base_lr': 0.0003781921609267425, 'last_lr': 0.0002729467903023369, 'epochs': 4}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.055 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00030774727212992784 last_lr 0.0029232340532154497 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c187e0354348a19ad451389c7a11d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.52 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 5.963 New best_val_rmse: 5.963\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 2.312 New best_val_rmse: 2.312\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.115 New best_val_rmse: 1.115\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.142 Still best_val_rmse: 1.115 (from epoch 0)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.042 New best_val_rmse: 1.042\n",
      "\n",
      "16 steps took 6.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 16:55:29,041]\u001b[0m Trial 9 finished with value: 1.0418716669082642 and parameters: {'base_lr': 0.00030774727212992784, 'last_lr': 0.0029232340532154497, 'epochs': 3}. Best is trial 0 with value: 0.4938439726829529.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.088 Still best_val_rmse: 1.042 (from epoch 0)\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001717178883932075 last_lr 0.00042448836147656634 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70254c289ad94b9ba1e01c23f1127280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.51 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.608 New best_val_rmse: 1.608\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.143 New best_val_rmse: 1.143\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9745 New best_val_rmse: 0.9745\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.796 New best_val_rmse: 0.796\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6682 New best_val_rmse: 0.6682\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6391 New best_val_rmse: 0.6391\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5981 New best_val_rmse: 0.5981\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5786 New best_val_rmse: 0.5786\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5935 Still best_val_rmse: 0.5786 (from epoch 0)\n",
      "\n",
      "16 steps took 7.21 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5626 New best_val_rmse: 0.5626\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5495 New best_val_rmse: 0.5495\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5507 Still best_val_rmse: 0.5495 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6139 Still best_val_rmse: 0.5495 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.608 Still best_val_rmse: 0.5495 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6785 Still best_val_rmse: 0.5495 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5194 New best_val_rmse: 0.5194\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5962 Still best_val_rmse: 0.5194 (from epoch 1)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6123 Still best_val_rmse: 0.5194 (from epoch 1)\n",
      "\n",
      "16 steps took 7.19 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.659 Still best_val_rmse: 0.5194 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5464 Still best_val_rmse: 0.5194 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5155 New best_val_rmse: 0.5155\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5269 Still best_val_rmse: 0.5155 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5568 Still best_val_rmse: 0.5155 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5095 New best_val_rmse: 0.5095\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5967 Still best_val_rmse: 0.5095 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5008 New best_val_rmse: 0.5008\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5045 Still best_val_rmse: 0.5008 (from epoch 2)\n",
      "\n",
      "16 steps took 7.13 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5144 Still best_val_rmse: 0.5008 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5032 Still best_val_rmse: 0.5008 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5064 Still best_val_rmse: 0.5008 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.513 Still best_val_rmse: 0.4954 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4952 Still best_val_rmse: 0.494 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4951 Still best_val_rmse: 0.494 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5068 Still best_val_rmse: 0.494 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4911 Still best_val_rmse: 0.4896 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4916 Still best_val_rmse: 0.4896 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4955 Still best_val_rmse: 0.4896 (from epoch 3)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4948 Still best_val_rmse: 0.4896 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:04:28,900]\u001b[0m Trial 10 finished with value: 0.48955243825912476 and parameters: {'base_lr': 0.0001717178883932075, 'last_lr': 0.00042448836147656634, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.000186065157334065 last_lr 0.0004248090651725791 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b2bc21112946feb4e9ccf394a86e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.47 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.799 New best_val_rmse: 1.799\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9812 New best_val_rmse: 0.9812\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9166 New best_val_rmse: 0.9166\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8259 New best_val_rmse: 0.8259\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7665 New best_val_rmse: 0.7665\n",
      "\n",
      "16 steps took 6.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:06:02,436]\u001b[0m Trial 11 finished with value: 0.7665433883666992 and parameters: {'base_lr': 0.000186065157334065, 'last_lr': 0.0004248090651725791, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 0.9536 Still best_val_rmse: 0.7665 (from epoch 0)\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00015692949034032473 last_lr 0.00014268196348590442 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5758f76d1844d4aa68f8bdc7733e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.178 New best_val_rmse: 1.178\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8942 New best_val_rmse: 0.8942\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6972 New best_val_rmse: 0.6972\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7543 Still best_val_rmse: 0.6972 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7288 Still best_val_rmse: 0.6972 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.9509 Still best_val_rmse: 0.6972 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6933 New best_val_rmse: 0.6933\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8176 Still best_val_rmse: 0.6933 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7374 Still best_val_rmse: 0.6933 (from epoch 0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:08:18,214]\u001b[0m Trial 12 finished with value: 0.6933364868164062 and parameters: {'base_lr': 0.00015692949034032473, 'last_lr': 0.00014268196348590442, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001732732522059111 last_lr 0.000741969827003011 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87103306539045128c63596757bb1262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.49 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.795 New best_val_rmse: 1.795\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.038 New best_val_rmse: 1.038\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9851 New best_val_rmse: 0.9851\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.35 Still best_val_rmse: 0.9851 (from epoch 0)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.054 Still best_val_rmse: 0.9851 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:09:52,310]\u001b[0m Trial 13 finished with value: 0.9850828051567078 and parameters: {'base_lr': 0.0001732732522059111, 'last_lr': 0.000741969827003011, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.116 Still best_val_rmse: 0.9851 (from epoch 0)\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00012436284032296463 last_lr 0.0011179313245251732 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da8d4dd28c0476893e2927dc366ee07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.042 New best_val_rmse: 1.042\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8783 New best_val_rmse: 0.8783\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7238 New best_val_rmse: 0.7238\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8231 Still best_val_rmse: 0.7238 (from epoch 0)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6483 New best_val_rmse: 0.6483\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7195 Still best_val_rmse: 0.6483 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6436 New best_val_rmse: 0.6436\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7433 Still best_val_rmse: 0.6436 (from epoch 0)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6006 New best_val_rmse: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:12:08,791]\u001b[0m Trial 14 finished with value: 0.6006467938423157 and parameters: {'base_lr': 0.00012436284032296463, 'last_lr': 0.0011179313245251732, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.027865360251286e-05 last_lr 0.00047032769189387546 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58576916da90452ea29c2b1b3eab14b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.785 New best_val_rmse: 2.785\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.004 New best_val_rmse: 1.004\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7524 New best_val_rmse: 0.7524\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6769 New best_val_rmse: 0.6769\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6885 Still best_val_rmse: 0.6769 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:13:42,152]\u001b[0m Trial 15 finished with value: 0.6580143570899963 and parameters: {'base_lr': 4.027865360251286e-05, 'last_lr': 0.00047032769189387546, 'epochs': 5}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 0.658 New best_val_rmse: 0.658\n",
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00022983699940706383 last_lr 0.0001426453208823319 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b404b0d369104ec88e0684db3d003e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.906 New best_val_rmse: 2.906\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.069 New best_val_rmse: 1.069\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8899 New best_val_rmse: 0.8899\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7522 New best_val_rmse: 0.7522\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6326 New best_val_rmse: 0.6326\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6528 Still best_val_rmse: 0.6326 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:15:15,895]\u001b[0m Trial 16 finished with value: 0.6326446533203125 and parameters: {'base_lr': 0.00022983699940706383, 'last_lr': 0.0001426453208823319, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.6335035163877976e-05 last_lr 0.004952718022594586 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e747ef3e55604be9bc0abf8d7f6eaecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.14 New best_val_rmse: 1.14\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8726 New best_val_rmse: 0.8726\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9114 Still best_val_rmse: 0.8726 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7547 New best_val_rmse: 0.7547\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6733 New best_val_rmse: 0.6733\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7172 Still best_val_rmse: 0.6733 (from epoch 0)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7779 Still best_val_rmse: 0.6733 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6899 Still best_val_rmse: 0.6733 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5819 New best_val_rmse: 0.5819\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5634 New best_val_rmse: 0.5634\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5666 Still best_val_rmse: 0.5634 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6364 Still best_val_rmse: 0.5634 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5766 Still best_val_rmse: 0.5634 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5616 New best_val_rmse: 0.5616\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6517 Still best_val_rmse: 0.5616 (from epoch 1)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5384 New best_val_rmse: 0.5384\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5669 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5524 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 7.13 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5309 New best_val_rmse: 0.5309\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5502 Still best_val_rmse: 0.5309 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5173 New best_val_rmse: 0.5173\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5235 Still best_val_rmse: 0.5173 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.524 Still best_val_rmse: 0.5173 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5388 Still best_val_rmse: 0.5173 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5678 Still best_val_rmse: 0.5173 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5213 Still best_val_rmse: 0.5173 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5104 New best_val_rmse: 0.5104\n",
      "\n",
      "16 steps took 7.2 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.511 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5152 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5239 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5206 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.5216 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5184 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5136 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5142 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5128 Still best_val_rmse: 0.5104 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:23:43,960]\u001b[0m Trial 17 finished with value: 0.5104207992553711 and parameters: {'base_lr': 5.6335035163877976e-05, 'last_lr': 0.004952718022594586, 'epochs': 4}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00012067145049904876 last_lr 0.00021114563956794053 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4ff3c3292f4703a6417860d573e80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.53 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.195 New best_val_rmse: 1.195\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.025 New best_val_rmse: 1.025\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7846 New best_val_rmse: 0.7846\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9852 Still best_val_rmse: 0.7846 (from epoch 0)\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6659 New best_val_rmse: 0.6659\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6361 New best_val_rmse: 0.6361\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6051 New best_val_rmse: 0.6051\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6584 Still best_val_rmse: 0.6051 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6044 New best_val_rmse: 0.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:26:00,523]\u001b[0m Trial 18 finished with value: 0.6043904423713684 and parameters: {'base_lr': 0.00012067145049904876, 'last_lr': 0.00021114563956794053, 'epochs': 3}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00012182070076373223 last_lr 0.0006078521862666121 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1fe0b5e1814ed29e98b836b6fe1cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.07 New best_val_rmse: 1.07\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.125 Still best_val_rmse: 1.07 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6927 New best_val_rmse: 0.6927\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7789 Still best_val_rmse: 0.6927 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8107 Still best_val_rmse: 0.6927 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8272 Still best_val_rmse: 0.6927 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6272 New best_val_rmse: 0.6272\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5659 New best_val_rmse: 0.5659\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.54 New best_val_rmse: 0.54\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5606 Still best_val_rmse: 0.54 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5312 New best_val_rmse: 0.5312\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5446 Still best_val_rmse: 0.5312 (from epoch 1)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5288 New best_val_rmse: 0.5288\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.7287 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5467 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5439 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5866 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6292 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 7.13 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.6895 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5399 Still best_val_rmse: 0.5288 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5196 New best_val_rmse: 0.5196\n",
      "\n",
      "16 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5441 Still best_val_rmse: 0.5196 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5193 New best_val_rmse: 0.5193\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5183 New best_val_rmse: 0.5183\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.6602 Still best_val_rmse: 0.5183 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5169 New best_val_rmse: 0.5169\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4957 New best_val_rmse: 0.4957\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.5026 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4965 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4976 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.5031 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.5002 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.5033 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4967 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5018 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4978 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4962 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4966 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5036 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.5015 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4962 Still best_val_rmse: 0.4957 (from epoch 2)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.495 New best_val_rmse: 0.495\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4927 New best_val_rmse: 0.4927\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4928 Still best_val_rmse: 0.4927 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 40 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.493 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 4 batch_num: 56 val_rmse: 0.4931 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.4932 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 72 val_rmse: 0.4932 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4928 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 88 val_rmse: 0.4928 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.4932 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 104 val_rmse: 0.4929 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.4927 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 4 batch_num: 120 val_rmse: 0.4927 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.4927 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 136 val_rmse: 0.4927 Still best_val_rmse: 0.4926 (from epoch 4)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.4926 Still best_val_rmse: 0.4926 (from epoch 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:38:01,904]\u001b[0m Trial 19 finished with value: 0.49262937903404236 and parameters: {'base_lr': 0.00012182070076373223, 'last_lr': 0.0006078521862666121, 'epochs': 5}. Best is trial 10 with value: 0.48955243825912476.\u001b[0m\n",
      "\u001b[32m[I 2021-07-22 17:38:01,909]\u001b[0m A new study created in memory with name: no-name-f1ea1a6e-5e38-4907-acf3-6e239ec55f99\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.48955243825912476\n",
      " Best params: \n",
      "    base_lr: 0.0001717178883932075\n",
      "    last_lr: 0.00042448836147656634\n",
      "    epochs: 4\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002764477472207697 last_lr 0.00021564819034446844 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544f0107f44043b58f1bf18e3ec1cc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.53 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.86 New best_val_rmse: 1.86\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.092 New best_val_rmse: 1.092\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.06 New best_val_rmse: 1.06\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9125 New best_val_rmse: 0.9125\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.102 Still best_val_rmse: 0.9125 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:39:35,458]\u001b[0m Trial 0 finished with value: 0.9125391244888306 and parameters: {'base_lr': 0.0002764477472207697, 'last_lr': 0.00021564819034446844, 'epochs': 3}. Best is trial 0 with value: 0.9125391244888306.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.02 Still best_val_rmse: 0.9125 (from epoch 0)\n",
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00024554936545913427 last_lr 0.0007753895451485905 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3276be82a9cb4c61a6d7b45ea522e92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.715 New best_val_rmse: 1.715\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.003 New best_val_rmse: 1.003\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7058 New best_val_rmse: 0.7058\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7339 Still best_val_rmse: 0.7058 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6272 New best_val_rmse: 0.6272\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8411 Still best_val_rmse: 0.6272 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9506 Still best_val_rmse: 0.6272 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.012 Still best_val_rmse: 0.6272 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7588 Still best_val_rmse: 0.6272 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:41:52,072]\u001b[0m Trial 1 finished with value: 0.6271501779556274 and parameters: {'base_lr': 0.00024554936545913427, 'last_lr': 0.0007753895451485905, 'epochs': 3}. Best is trial 1 with value: 0.6271501779556274.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002748663461976918 last_lr 0.0042910375069491755 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d0dc6307f84e86b5a187fb2bf26d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.728 New best_val_rmse: 1.728\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9309 New best_val_rmse: 0.9309\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9043 New best_val_rmse: 0.9043\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9819 Still best_val_rmse: 0.9043 (from epoch 0)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.066 Still best_val_rmse: 0.9043 (from epoch 0)\n",
      "\n",
      "16 steps took 6.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:43:25,930]\u001b[0m Trial 2 finished with value: 0.904285192489624 and parameters: {'base_lr': 0.0002748663461976918, 'last_lr': 0.0042910375069491755, 'epochs': 4}. Best is trial 1 with value: 0.6271501779556274.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.02 Still best_val_rmse: 0.9043 (from epoch 0)\n",
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 7.652664002075421e-05 last_lr 0.00014979767491809303 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c263d8ab6ade468ba91cb09a89dfcfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8646 New best_val_rmse: 0.8646\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6805 New best_val_rmse: 0.6805\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6246 New best_val_rmse: 0.6246\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7585 Still best_val_rmse: 0.6246 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5748 New best_val_rmse: 0.5748\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7808 Still best_val_rmse: 0.5748 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5781 Still best_val_rmse: 0.5748 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6532 Still best_val_rmse: 0.5748 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5639 New best_val_rmse: 0.5639\n",
      "\n",
      "16 steps took 7.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5591 New best_val_rmse: 0.5591\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6653 Still best_val_rmse: 0.5591 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.573 Still best_val_rmse: 0.5591 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5363 New best_val_rmse: 0.5363\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5687 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5371 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6628 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5251 New best_val_rmse: 0.5251\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5899 Still best_val_rmse: 0.5251 (from epoch 1)\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5335 Still best_val_rmse: 0.5251 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5363 Still best_val_rmse: 0.5251 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5095 New best_val_rmse: 0.5095\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5304 Still best_val_rmse: 0.5095 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5097 Still best_val_rmse: 0.5065 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5011 New best_val_rmse: 0.5011\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.5113 Still best_val_rmse: 0.4917 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4808 New best_val_rmse: 0.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:49:59,020]\u001b[0m Trial 3 finished with value: 0.480750173330307 and parameters: {'base_lr': 7.652664002075421e-05, 'last_lr': 0.00014979767491809303, 'epochs': 3}. Best is trial 3 with value: 0.480750173330307.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00047039367469524384 last_lr 0.0010511399095471956 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbefc246b45c450babfec0fdb90422cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.53 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.18 New best_val_rmse: 1.18\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.429 Still best_val_rmse: 1.18 (from epoch 0)\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.081 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.091 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 6.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 17:51:32,557]\u001b[0m Trial 4 finished with value: 1.0404369831085205 and parameters: {'base_lr': 0.00047039367469524384, 'last_lr': 0.0010511399095471956, 'epochs': 5}. Best is trial 3 with value: 0.480750173330307.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.103 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.000135700916847811 last_lr 0.0029640935672153 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c48cf9dc4c44c9adb041d0cc5f51ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.52 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.176 New best_val_rmse: 1.176\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8045 New best_val_rmse: 0.8045\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6749 New best_val_rmse: 0.6749\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6839 Still best_val_rmse: 0.6749 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6362 New best_val_rmse: 0.6362\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.711 Still best_val_rmse: 0.6362 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6578 Still best_val_rmse: 0.6362 (from epoch 0)\n",
      "\n",
      "16 steps took 6.71 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.773 Still best_val_rmse: 0.6362 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5746 New best_val_rmse: 0.5746\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6094 Still best_val_rmse: 0.5746 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7199 Still best_val_rmse: 0.5746 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5436 New best_val_rmse: 0.5436\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.536 New best_val_rmse: 0.536\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.614 Still best_val_rmse: 0.536 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6357 Still best_val_rmse: 0.536 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5922 Still best_val_rmse: 0.536 (from epoch 1)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.6169 Still best_val_rmse: 0.536 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.676 Still best_val_rmse: 0.536 (from epoch 1)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5808 Still best_val_rmse: 0.536 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.497 New best_val_rmse: 0.497\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5035 Still best_val_rmse: 0.497 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5136 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4845 New best_val_rmse: 0.4845\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5001 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5372 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5027 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4898 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.497 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4989 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4884 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.5197 Still best_val_rmse: 0.4845 (from epoch 2)\n",
      "\n",
      "16 steps took 7.14 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 3)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4783 New best_val_rmse: 0.4783\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4806 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4802 Still best_val_rmse: 0.4783 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4777 Still best_val_rmse: 0.476 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4795 Still best_val_rmse: 0.476 (from epoch 3)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4783 Still best_val_rmse: 0.476 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4768 Still best_val_rmse: 0.476 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4786 Still best_val_rmse: 0.476 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4751 New best_val_rmse: 0.4751\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4757 Still best_val_rmse: 0.4751 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4742 New best_val_rmse: 0.4742\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4712 New best_val_rmse: 0.4712\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4745 Still best_val_rmse: 0.4712 (from epoch 3)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4887 Still best_val_rmse: 0.4712 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.477 Still best_val_rmse: 0.4712 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4807 Still best_val_rmse: 0.4712 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4806 Still best_val_rmse: 0.4712 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4733 Still best_val_rmse: 0.4712 (from epoch 3)\n",
      "\n",
      "2 steps took 0.812 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4709 New best_val_rmse: 0.4709\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4722 Still best_val_rmse: 0.4703 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4738 Still best_val_rmse: 0.4703 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4709 Still best_val_rmse: 0.4703 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4699 New best_val_rmse: 0.4699\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4703 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4707 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4718 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4741 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4782 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4794 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4775 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4772 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.882 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4761 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4748 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4789 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4791 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4779 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4732 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4728 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.473 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4734 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4746 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4752 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4751 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4753 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4747 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4759 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4769 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4759 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4728 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4723 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4715 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.471 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4706 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4699 Still best_val_rmse: 0.4698 (from epoch 3)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.409 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4693 Still best_val_rmse: 0.4693 (from epoch 3)\n",
      "\n",
      "1 steps took 0.413 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4694 Still best_val_rmse: 0.4693 (from epoch 3)\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.406 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4688 New best_val_rmse: 0.4688\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4691 Still best_val_rmse: 0.4688 (from epoch 3)\n",
      "\n",
      "1 steps took 0.255 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 18:07:28,519]\u001b[0m Trial 5 finished with value: 0.4688156247138977 and parameters: {'base_lr': 0.000135700916847811, 'last_lr': 0.0029640935672153, 'epochs': 4}. Best is trial 5 with value: 0.4688156247138977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 batch_num: 147 val_rmse: 0.4695 Still best_val_rmse: 0.4688 (from epoch 3)\n",
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0003816244755730149 last_lr 0.0001760744534229041 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a6739950d04dd09956037a6984d5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.57 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.765 New best_val_rmse: 1.765\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.583 New best_val_rmse: 1.583\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.313 New best_val_rmse: 1.313\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.288 New best_val_rmse: 1.288\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9166 New best_val_rmse: 0.9166\n",
      "\n",
      "16 steps took 6.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 18:09:02,364]\u001b[0m Trial 6 finished with value: 0.9166398644447327 and parameters: {'base_lr': 0.0003816244755730149, 'last_lr': 0.0001760744534229041, 'epochs': 3}. Best is trial 5 with value: 0.4688156247138977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch_num: 96 val_rmse: 1.208 Still best_val_rmse: 0.9166 (from epoch 0)\n",
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 8.995995358073548e-05 last_lr 0.0005913800163279911 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca86aa8577a4ec79a32a8d7b1a939eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.62 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7742 New best_val_rmse: 0.7742\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6992 New best_val_rmse: 0.6992\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6155 New best_val_rmse: 0.6155\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7417 Still best_val_rmse: 0.6155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6845 Still best_val_rmse: 0.6155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.864 Still best_val_rmse: 0.6155 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5769 New best_val_rmse: 0.5769\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8989 Still best_val_rmse: 0.5769 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8375 Still best_val_rmse: 0.5769 (from epoch 0)\n",
      "\n",
      "16 steps took 7.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6572 Still best_val_rmse: 0.5769 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5212 New best_val_rmse: 0.5212\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5406 Still best_val_rmse: 0.5212 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5258 Still best_val_rmse: 0.5212 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5634 Still best_val_rmse: 0.5212 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5529 Still best_val_rmse: 0.5212 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5399 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.582 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 7.15 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5114 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5149 Still best_val_rmse: 0.4918 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4896 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5269 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5111 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5995 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.546 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4935 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4842 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.5037 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "16 steps took 7.25 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.5 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4883 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.5018 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4846 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4745 New best_val_rmse: 0.4745\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4747 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4755 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4772 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4804 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4795 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4803 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4773 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.475 Still best_val_rmse: 0.4745 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4738 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4751 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4747 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4741 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4738 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4751 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.812 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4793 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4805 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4768 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4745 Still best_val_rmse: 0.4736 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4706 New best_val_rmse: 0.4706\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "1 steps took 0.412 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4702 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4719 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4768 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4782 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4741 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4711 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4729 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.477 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.479 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4768 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4768 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4769 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.888 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4748 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4726 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4718 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4714 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4714 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4713 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4709 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4738 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4807 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "4 steps took 2.19 seconds\n",
      "Epoch: 4 batch_num: 1 val_rmse: 0.4811 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 4 batch_num: 5 val_rmse: 0.4734 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 4 batch_num: 7 val_rmse: 0.4733 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 9 val_rmse: 0.4742 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 11 val_rmse: 0.4747 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 13 val_rmse: 0.4751 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 4 batch_num: 15 val_rmse: 0.4753 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 17 val_rmse: 0.4756 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 4 batch_num: 19 val_rmse: 0.4761 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 4 batch_num: 21 val_rmse: 0.4762 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 4 batch_num: 23 val_rmse: 0.4765 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 4 batch_num: 25 val_rmse: 0.4759 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 27 val_rmse: 0.4749 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 29 val_rmse: 0.4739 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 31 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 33 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 35 val_rmse: 0.473 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 4 batch_num: 37 val_rmse: 0.4728 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 39 val_rmse: 0.4726 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 41 val_rmse: 0.4727 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 43 val_rmse: 0.4728 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 45 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 4 batch_num: 47 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 49 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 4 batch_num: 51 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 53 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 55 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 57 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 59 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 4 batch_num: 61 val_rmse: 0.4733 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 4 batch_num: 63 val_rmse: 0.4734 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 4 batch_num: 65 val_rmse: 0.4733 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 67 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 69 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 71 val_rmse: 0.473 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 73 val_rmse: 0.473 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 75 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 77 val_rmse: 0.4734 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 79 val_rmse: 0.4736 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 81 val_rmse: 0.4738 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 83 val_rmse: 0.4737 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 85 val_rmse: 0.4737 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.82 seconds\n",
      "Epoch: 4 batch_num: 87 val_rmse: 0.4737 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 4 batch_num: 89 val_rmse: 0.4735 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 91 val_rmse: 0.4735 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 93 val_rmse: 0.4735 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 95 val_rmse: 0.4733 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 97 val_rmse: 0.4732 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 99 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 101 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.811 seconds\n",
      "Epoch: 4 batch_num: 103 val_rmse: 0.473 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 105 val_rmse: 0.4729 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 107 val_rmse: 0.4729 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 109 val_rmse: 0.4728 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 111 val_rmse: 0.4727 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 113 val_rmse: 0.4727 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 4 batch_num: 115 val_rmse: 0.4727 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 117 val_rmse: 0.4727 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 119 val_rmse: 0.4726 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 121 val_rmse: 0.4726 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 4 batch_num: 123 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 125 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 4 batch_num: 127 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 129 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 131 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 4 batch_num: 133 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 135 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 137 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 139 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 141 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 4 batch_num: 143 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 145 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "2 steps took 0.667 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 18:32:35,727]\u001b[0m Trial 7 finished with value: 0.46997010707855225 and parameters: {'base_lr': 8.995995358073548e-05, 'last_lr': 0.0005913800163279911, 'epochs': 5}. Best is trial 5 with value: 0.4688156247138977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 batch_num: 147 val_rmse: 0.4725 Still best_val_rmse: 0.47 (from epoch 3)\n",
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00012020206263341346 last_lr 0.0005492564740784009 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b89c01c91944c88b5db51480a76cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.58 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.027 New best_val_rmse: 1.027\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8473 New best_val_rmse: 0.8473\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6328 New best_val_rmse: 0.6328\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6024 New best_val_rmse: 0.6024\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5739 New best_val_rmse: 0.5739\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8438 Still best_val_rmse: 0.5739 (from epoch 0)\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6299 Still best_val_rmse: 0.5739 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6472 Still best_val_rmse: 0.5739 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5388 New best_val_rmse: 0.5388\n",
      "\n",
      "16 steps took 7.19 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5286 New best_val_rmse: 0.5286\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7381 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6252 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5514 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5387 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5359 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6916 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5751 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6253 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 7.13 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5524 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5401 Still best_val_rmse: 0.5286 (from epoch 1)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.5085 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4894 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4994 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5293 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5906 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.561 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.545 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5032 Still best_val_rmse: 0.4835 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 18:39:13,829]\u001b[0m Trial 8 finished with value: 0.48350340127944946 and parameters: {'base_lr': 0.00012020206263341346, 'last_lr': 0.0005492564740784009, 'epochs': 3}. Best is trial 5 with value: 0.4688156247138977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.416461739342611e-05 last_lr 0.0021998765313380802 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae12cd11a214838acf762c871c52b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.54 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.098 New best_val_rmse: 1.098\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8311 New best_val_rmse: 0.8311\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7495 New best_val_rmse: 0.7495\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8807 Still best_val_rmse: 0.7495 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7183 New best_val_rmse: 0.7183\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7341 Still best_val_rmse: 0.7183 (from epoch 0)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5591 New best_val_rmse: 0.5591\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6971 Still best_val_rmse: 0.5591 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5719 Still best_val_rmse: 0.5591 (from epoch 0)\n",
      "\n",
      "16 steps took 7.12 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5578 New best_val_rmse: 0.5578\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5869 Still best_val_rmse: 0.5578 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5419 New best_val_rmse: 0.5419\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5358 New best_val_rmse: 0.5358\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5488 Still best_val_rmse: 0.5358 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5657 Still best_val_rmse: 0.5358 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5236 New best_val_rmse: 0.5236\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5334 Still best_val_rmse: 0.5236 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5787 Still best_val_rmse: 0.5236 (from epoch 1)\n",
      "\n",
      "16 steps took 7.13 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5139 New best_val_rmse: 0.5139\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5285 Still best_val_rmse: 0.5139 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5062 New best_val_rmse: 0.5062\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5606 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5576 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5625 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5366 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5159 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5176 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 7.17 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5143 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5185 Still best_val_rmse: 0.5062 (from epoch 2)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5061 New best_val_rmse: 0.5061\n",
      "\n",
      "16 steps took 6.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.5098 Still best_val_rmse: 0.5061 (from epoch 3)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.505 New best_val_rmse: 0.505\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.5044 New best_val_rmse: 0.5044\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.503 New best_val_rmse: 0.503\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5095 Still best_val_rmse: 0.503 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.5117 Still best_val_rmse: 0.503 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 18:47:40,939]\u001b[0m Trial 9 finished with value: 0.503010630607605 and parameters: {'base_lr': 3.416461739342611e-05, 'last_lr': 0.0021998765313380802, 'epochs': 4}. Best is trial 5 with value: 0.4688156247138977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 4.1983500986909504e-05 last_lr 0.004778601048775113 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92ba39892044d8eb0fa8d94cf77f205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.61 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.112 New best_val_rmse: 1.112\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7084 New best_val_rmse: 0.7084\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7748 Still best_val_rmse: 0.7084 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8988 Still best_val_rmse: 0.7084 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7561 Still best_val_rmse: 0.7084 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5986 New best_val_rmse: 0.5986\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6054 Still best_val_rmse: 0.5986 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6619 Still best_val_rmse: 0.5986 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6014 Still best_val_rmse: 0.5986 (from epoch 0)\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6179 Still best_val_rmse: 0.5986 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5504 New best_val_rmse: 0.5504\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5467 New best_val_rmse: 0.5467\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.539 New best_val_rmse: 0.539\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6351 Still best_val_rmse: 0.539 (from epoch 1)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5371 New best_val_rmse: 0.5371\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5271 New best_val_rmse: 0.5271\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5861 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6069 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 7.23 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5119 New best_val_rmse: 0.5119\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5531 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.525 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5504 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5166 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5787 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.543 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.529 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5312 Still best_val_rmse: 0.5119 (from epoch 2)\n",
      "\n",
      "16 steps took 7.14 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.5112 New best_val_rmse: 0.5112\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.5183 Still best_val_rmse: 0.5112 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.5 New best_val_rmse: 0.5\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.499 New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.5094 Still best_val_rmse: 0.499 (from epoch 3)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.5106 Still best_val_rmse: 0.499 (from epoch 3)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.5023 Still best_val_rmse: 0.499 (from epoch 3)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.5039 Still best_val_rmse: 0.499 (from epoch 3)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4979 Still best_val_rmse: 0.4966 (from epoch 3)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4977 Still best_val_rmse: 0.4966 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-22 18:56:23,505]\u001b[0m Trial 10 finished with value: 0.4966193735599518 and parameters: {'base_lr': 4.1983500986909504e-05, 'last_lr': 0.004778601048775113, 'epochs': 4}. Best is trial 5 with value: 0.4688156247138977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00010638643847946303 last_lr 0.00036673227466262124 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85442eabb0424b12a5de64de81b95e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 7.48 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9047 New best_val_rmse: 0.9047\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.114 Still best_val_rmse: 0.9047 (from epoch 0)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.651 New best_val_rmse: 0.651\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6169 New best_val_rmse: 0.6169\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7197 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7152 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8469 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5955 New best_val_rmse: 0.5955\n",
      "\n",
      "16 steps took 6.69 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5545 New best_val_rmse: 0.5545\n",
      "\n",
      "16 steps took 7.18 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5298 New best_val_rmse: 0.5298\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7153 Still best_val_rmse: 0.5298 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5712 Still best_val_rmse: 0.5298 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5219 New best_val_rmse: 0.5219\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5444 Still best_val_rmse: 0.5219 (from epoch 1)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5097 New best_val_rmse: 0.5097\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.591 Still best_val_rmse: 0.5097 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4965 New best_val_rmse: 0.4965\n",
      "\n",
      "8 steps took 3.3 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5287 Still best_val_rmse: 0.4965 (from epoch 1)\n",
      "\n",
      "16 steps took 7.16 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5116 Still best_val_rmse: 0.4965 (from epoch 1)\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5134 Still best_val_rmse: 0.4965 (from epoch 1)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.496 New best_val_rmse: 0.496\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5305 Still best_val_rmse: 0.496 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5086 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "16 steps took 6.6 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4984 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5248 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "16 steps took 6.57 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5035 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.49 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.5099 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4889 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 2.24 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4954 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4899 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4874 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4847 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4913 Still best_val_rmse: 0.4818 (from epoch 3)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4825 Still best_val_rmse: 0.4818 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4855 Still best_val_rmse: 0.4818 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4854 Still best_val_rmse: 0.4814 (from epoch 3)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.482 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4819 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4833 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4822 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4844 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4805 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4799 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4821 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4794 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4805 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4831 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.75 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4858 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4822 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4806 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4823 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4838 Still best_val_rmse: 0.4789 (from epoch 3)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4791 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4787 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4833 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4847 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4797 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4811 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4813 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4825 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4866 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 2.23 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.4803 Still best_val_rmse: 0.4784 (from epoch 3)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 4 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 6 val_rmse: 0.4783 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4785 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 10 val_rmse: 0.4789 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 4 batch_num: 12 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 14 val_rmse: 0.4806 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 4 batch_num: 18 val_rmse: 0.4827 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 22 val_rmse: 0.4861 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 4 batch_num: 26 val_rmse: 0.4823 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 4 batch_num: 30 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.811 seconds\n",
      "Epoch: 4 batch_num: 34 val_rmse: 0.4798 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 36 val_rmse: 0.4804 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.71 seconds\n",
      "Epoch: 4 batch_num: 40 val_rmse: 0.4817 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 44 val_rmse: 0.4809 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.4802 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 52 val_rmse: 0.4805 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 56 val_rmse: 0.4804 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 4 batch_num: 60 val_rmse: 0.4802 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.4798 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 66 val_rmse: 0.4797 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 68 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 70 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 72 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 74 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 76 val_rmse: 0.4793 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 78 val_rmse: 0.4793 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4793 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 82 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 4 batch_num: 84 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 86 val_rmse: 0.4797 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.822 seconds\n",
      "Epoch: 4 batch_num: 88 val_rmse: 0.4797 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 90 val_rmse: 0.4798 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 92 val_rmse: 0.4799 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 4 batch_num: 94 val_rmse: 0.4799 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.4799 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 98 val_rmse: 0.48 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 100 val_rmse: 0.48 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 102 val_rmse: 0.4798 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 104 val_rmse: 0.4797 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 106 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 108 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 110 val_rmse: 0.4796 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.4795 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 4 batch_num: 114 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 116 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 118 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 4 batch_num: 120 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 122 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 4 batch_num: 124 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 4 batch_num: 126 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 4 batch_num: 130 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 132 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 134 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 4 batch_num: 136 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.813 seconds\n",
      "Epoch: 4 batch_num: 138 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.817 seconds\n",
      "Epoch: 4 batch_num: 140 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.818 seconds\n",
      "Epoch: 4 batch_num: 142 val_rmse: 0.4794 Still best_val_rmse: 0.4782 (from epoch 4)\n",
      "\n",
      "2 steps took 0.816 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
