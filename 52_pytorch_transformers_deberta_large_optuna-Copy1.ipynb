{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = 'microsoft/deberta-large'\n",
    "    TOKENIZER_PATH = 'microsoft/deberta-large'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'deberta-large'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "1 transformer_model.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "2 transformer_model.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "3 transformer_model.encoder.layer.0.attention.self.q_bias torch.Size([1024])\n",
      "4 transformer_model.encoder.layer.0.attention.self.v_bias torch.Size([1024])\n",
      "5 transformer_model.encoder.layer.0.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "6 transformer_model.encoder.layer.0.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "7 transformer_model.encoder.layer.0.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.encoder.layer.0.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "9 transformer_model.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "11 transformer_model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "12 transformer_model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "13 transformer_model.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "14 transformer_model.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "15 transformer_model.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "16 transformer_model.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "17 transformer_model.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "18 transformer_model.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "19 transformer_model.encoder.layer.1.attention.self.q_bias torch.Size([1024])\n",
      "20 transformer_model.encoder.layer.1.attention.self.v_bias torch.Size([1024])\n",
      "21 transformer_model.encoder.layer.1.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "22 transformer_model.encoder.layer.1.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "23 transformer_model.encoder.layer.1.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.encoder.layer.1.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "25 transformer_model.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "27 transformer_model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "28 transformer_model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "29 transformer_model.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "30 transformer_model.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "31 transformer_model.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "32 transformer_model.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "33 transformer_model.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "34 transformer_model.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "35 transformer_model.encoder.layer.2.attention.self.q_bias torch.Size([1024])\n",
      "36 transformer_model.encoder.layer.2.attention.self.v_bias torch.Size([1024])\n",
      "37 transformer_model.encoder.layer.2.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "38 transformer_model.encoder.layer.2.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "39 transformer_model.encoder.layer.2.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.encoder.layer.2.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "41 transformer_model.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "43 transformer_model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "44 transformer_model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "45 transformer_model.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "46 transformer_model.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "47 transformer_model.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "48 transformer_model.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "49 transformer_model.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "50 transformer_model.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "51 transformer_model.encoder.layer.3.attention.self.q_bias torch.Size([1024])\n",
      "52 transformer_model.encoder.layer.3.attention.self.v_bias torch.Size([1024])\n",
      "53 transformer_model.encoder.layer.3.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "54 transformer_model.encoder.layer.3.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "55 transformer_model.encoder.layer.3.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.encoder.layer.3.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "57 transformer_model.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "59 transformer_model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "60 transformer_model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "61 transformer_model.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "62 transformer_model.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "63 transformer_model.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "64 transformer_model.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "65 transformer_model.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "66 transformer_model.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "67 transformer_model.encoder.layer.4.attention.self.q_bias torch.Size([1024])\n",
      "68 transformer_model.encoder.layer.4.attention.self.v_bias torch.Size([1024])\n",
      "69 transformer_model.encoder.layer.4.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "70 transformer_model.encoder.layer.4.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "71 transformer_model.encoder.layer.4.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.encoder.layer.4.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "73 transformer_model.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "75 transformer_model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "76 transformer_model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "77 transformer_model.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "78 transformer_model.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "79 transformer_model.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "80 transformer_model.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "81 transformer_model.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "82 transformer_model.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "83 transformer_model.encoder.layer.5.attention.self.q_bias torch.Size([1024])\n",
      "84 transformer_model.encoder.layer.5.attention.self.v_bias torch.Size([1024])\n",
      "85 transformer_model.encoder.layer.5.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "86 transformer_model.encoder.layer.5.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "87 transformer_model.encoder.layer.5.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.encoder.layer.5.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "89 transformer_model.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "91 transformer_model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "92 transformer_model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "93 transformer_model.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "94 transformer_model.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "95 transformer_model.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "96 transformer_model.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "97 transformer_model.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "98 transformer_model.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "99 transformer_model.encoder.layer.6.attention.self.q_bias torch.Size([1024])\n",
      "100 transformer_model.encoder.layer.6.attention.self.v_bias torch.Size([1024])\n",
      "101 transformer_model.encoder.layer.6.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "102 transformer_model.encoder.layer.6.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "103 transformer_model.encoder.layer.6.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.encoder.layer.6.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "105 transformer_model.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "107 transformer_model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "108 transformer_model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "109 transformer_model.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "110 transformer_model.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "111 transformer_model.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "112 transformer_model.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "113 transformer_model.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "114 transformer_model.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "115 transformer_model.encoder.layer.7.attention.self.q_bias torch.Size([1024])\n",
      "116 transformer_model.encoder.layer.7.attention.self.v_bias torch.Size([1024])\n",
      "117 transformer_model.encoder.layer.7.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "118 transformer_model.encoder.layer.7.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "119 transformer_model.encoder.layer.7.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.encoder.layer.7.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "121 transformer_model.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "123 transformer_model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "124 transformer_model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "125 transformer_model.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "126 transformer_model.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "127 transformer_model.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "128 transformer_model.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "129 transformer_model.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "130 transformer_model.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "131 transformer_model.encoder.layer.8.attention.self.q_bias torch.Size([1024])\n",
      "132 transformer_model.encoder.layer.8.attention.self.v_bias torch.Size([1024])\n",
      "133 transformer_model.encoder.layer.8.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "134 transformer_model.encoder.layer.8.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "135 transformer_model.encoder.layer.8.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.encoder.layer.8.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "137 transformer_model.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "139 transformer_model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "140 transformer_model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "141 transformer_model.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "142 transformer_model.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "143 transformer_model.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "144 transformer_model.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "145 transformer_model.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "146 transformer_model.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "147 transformer_model.encoder.layer.9.attention.self.q_bias torch.Size([1024])\n",
      "148 transformer_model.encoder.layer.9.attention.self.v_bias torch.Size([1024])\n",
      "149 transformer_model.encoder.layer.9.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "150 transformer_model.encoder.layer.9.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "151 transformer_model.encoder.layer.9.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.encoder.layer.9.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "153 transformer_model.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "155 transformer_model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "156 transformer_model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "157 transformer_model.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "158 transformer_model.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "159 transformer_model.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "160 transformer_model.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "161 transformer_model.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "162 transformer_model.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "163 transformer_model.encoder.layer.10.attention.self.q_bias torch.Size([1024])\n",
      "164 transformer_model.encoder.layer.10.attention.self.v_bias torch.Size([1024])\n",
      "165 transformer_model.encoder.layer.10.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "166 transformer_model.encoder.layer.10.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "167 transformer_model.encoder.layer.10.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.encoder.layer.10.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "169 transformer_model.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "171 transformer_model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "172 transformer_model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "173 transformer_model.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "174 transformer_model.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "175 transformer_model.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "176 transformer_model.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "177 transformer_model.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "178 transformer_model.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "179 transformer_model.encoder.layer.11.attention.self.q_bias torch.Size([1024])\n",
      "180 transformer_model.encoder.layer.11.attention.self.v_bias torch.Size([1024])\n",
      "181 transformer_model.encoder.layer.11.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "182 transformer_model.encoder.layer.11.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "183 transformer_model.encoder.layer.11.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.encoder.layer.11.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "185 transformer_model.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "187 transformer_model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "188 transformer_model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "189 transformer_model.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "190 transformer_model.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "191 transformer_model.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "192 transformer_model.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "193 transformer_model.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "194 transformer_model.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "195 transformer_model.encoder.layer.12.attention.self.q_bias torch.Size([1024])\n",
      "196 transformer_model.encoder.layer.12.attention.self.v_bias torch.Size([1024])\n",
      "197 transformer_model.encoder.layer.12.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "198 transformer_model.encoder.layer.12.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "199 transformer_model.encoder.layer.12.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.encoder.layer.12.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "201 transformer_model.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "203 transformer_model.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "204 transformer_model.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "205 transformer_model.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "206 transformer_model.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "207 transformer_model.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "208 transformer_model.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "209 transformer_model.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "210 transformer_model.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "211 transformer_model.encoder.layer.13.attention.self.q_bias torch.Size([1024])\n",
      "212 transformer_model.encoder.layer.13.attention.self.v_bias torch.Size([1024])\n",
      "213 transformer_model.encoder.layer.13.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "214 transformer_model.encoder.layer.13.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "215 transformer_model.encoder.layer.13.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.encoder.layer.13.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "217 transformer_model.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "219 transformer_model.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "220 transformer_model.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "221 transformer_model.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "222 transformer_model.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "223 transformer_model.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "224 transformer_model.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "225 transformer_model.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "226 transformer_model.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "227 transformer_model.encoder.layer.14.attention.self.q_bias torch.Size([1024])\n",
      "228 transformer_model.encoder.layer.14.attention.self.v_bias torch.Size([1024])\n",
      "229 transformer_model.encoder.layer.14.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "230 transformer_model.encoder.layer.14.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "231 transformer_model.encoder.layer.14.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.encoder.layer.14.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "233 transformer_model.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "235 transformer_model.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "236 transformer_model.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "237 transformer_model.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "238 transformer_model.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "239 transformer_model.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "240 transformer_model.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "241 transformer_model.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "242 transformer_model.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "243 transformer_model.encoder.layer.15.attention.self.q_bias torch.Size([1024])\n",
      "244 transformer_model.encoder.layer.15.attention.self.v_bias torch.Size([1024])\n",
      "245 transformer_model.encoder.layer.15.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "246 transformer_model.encoder.layer.15.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "247 transformer_model.encoder.layer.15.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.encoder.layer.15.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "249 transformer_model.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "251 transformer_model.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "252 transformer_model.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "253 transformer_model.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "254 transformer_model.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "255 transformer_model.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "256 transformer_model.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "257 transformer_model.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "258 transformer_model.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "259 transformer_model.encoder.layer.16.attention.self.q_bias torch.Size([1024])\n",
      "260 transformer_model.encoder.layer.16.attention.self.v_bias torch.Size([1024])\n",
      "261 transformer_model.encoder.layer.16.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "262 transformer_model.encoder.layer.16.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "263 transformer_model.encoder.layer.16.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.encoder.layer.16.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "265 transformer_model.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "267 transformer_model.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "268 transformer_model.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "269 transformer_model.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "270 transformer_model.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "271 transformer_model.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "272 transformer_model.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "273 transformer_model.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "274 transformer_model.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "275 transformer_model.encoder.layer.17.attention.self.q_bias torch.Size([1024])\n",
      "276 transformer_model.encoder.layer.17.attention.self.v_bias torch.Size([1024])\n",
      "277 transformer_model.encoder.layer.17.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "278 transformer_model.encoder.layer.17.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "279 transformer_model.encoder.layer.17.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.encoder.layer.17.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "281 transformer_model.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "283 transformer_model.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "284 transformer_model.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "285 transformer_model.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "286 transformer_model.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "287 transformer_model.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "288 transformer_model.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "289 transformer_model.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "290 transformer_model.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "291 transformer_model.encoder.layer.18.attention.self.q_bias torch.Size([1024])\n",
      "292 transformer_model.encoder.layer.18.attention.self.v_bias torch.Size([1024])\n",
      "293 transformer_model.encoder.layer.18.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "294 transformer_model.encoder.layer.18.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "295 transformer_model.encoder.layer.18.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.encoder.layer.18.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "297 transformer_model.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "299 transformer_model.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "300 transformer_model.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "301 transformer_model.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "302 transformer_model.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "303 transformer_model.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "304 transformer_model.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "305 transformer_model.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "306 transformer_model.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "307 transformer_model.encoder.layer.19.attention.self.q_bias torch.Size([1024])\n",
      "308 transformer_model.encoder.layer.19.attention.self.v_bias torch.Size([1024])\n",
      "309 transformer_model.encoder.layer.19.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "310 transformer_model.encoder.layer.19.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "311 transformer_model.encoder.layer.19.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.encoder.layer.19.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "313 transformer_model.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "315 transformer_model.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "316 transformer_model.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "317 transformer_model.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "318 transformer_model.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "319 transformer_model.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "320 transformer_model.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "321 transformer_model.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "322 transformer_model.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "323 transformer_model.encoder.layer.20.attention.self.q_bias torch.Size([1024])\n",
      "324 transformer_model.encoder.layer.20.attention.self.v_bias torch.Size([1024])\n",
      "325 transformer_model.encoder.layer.20.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "326 transformer_model.encoder.layer.20.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "327 transformer_model.encoder.layer.20.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.encoder.layer.20.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "329 transformer_model.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "331 transformer_model.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "332 transformer_model.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "333 transformer_model.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "334 transformer_model.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "335 transformer_model.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "336 transformer_model.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "337 transformer_model.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "338 transformer_model.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "339 transformer_model.encoder.layer.21.attention.self.q_bias torch.Size([1024])\n",
      "340 transformer_model.encoder.layer.21.attention.self.v_bias torch.Size([1024])\n",
      "341 transformer_model.encoder.layer.21.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "342 transformer_model.encoder.layer.21.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "343 transformer_model.encoder.layer.21.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.encoder.layer.21.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "345 transformer_model.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "347 transformer_model.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "348 transformer_model.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "349 transformer_model.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "350 transformer_model.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "351 transformer_model.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "352 transformer_model.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "353 transformer_model.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "354 transformer_model.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "355 transformer_model.encoder.layer.22.attention.self.q_bias torch.Size([1024])\n",
      "356 transformer_model.encoder.layer.22.attention.self.v_bias torch.Size([1024])\n",
      "357 transformer_model.encoder.layer.22.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "358 transformer_model.encoder.layer.22.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "359 transformer_model.encoder.layer.22.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.encoder.layer.22.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "361 transformer_model.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "363 transformer_model.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "364 transformer_model.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "365 transformer_model.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "366 transformer_model.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "367 transformer_model.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "368 transformer_model.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "369 transformer_model.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "370 transformer_model.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "371 transformer_model.encoder.layer.23.attention.self.q_bias torch.Size([1024])\n",
      "372 transformer_model.encoder.layer.23.attention.self.v_bias torch.Size([1024])\n",
      "373 transformer_model.encoder.layer.23.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "374 transformer_model.encoder.layer.23.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "375 transformer_model.encoder.layer.23.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.encoder.layer.23.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "377 transformer_model.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "379 transformer_model.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "380 transformer_model.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "381 transformer_model.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "382 transformer_model.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "383 transformer_model.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "384 transformer_model.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "385 transformer_model.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "386 transformer_model.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "387 transformer_model.encoder.rel_embeddings.weight torch.Size([1024, 1024])\n",
      "388 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "389 attention.hidden_layer.bias torch.Size([512])\n",
      "390 attention.final_layer.weight torch.Size([1, 512])\n",
      "391 attention.final_layer.bias torch.Size([1])\n",
      "392 regressor.weight torch.Size([1, 1024])\n",
      "393 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 18.4460,  25.5957, -21.9767,  ...,   3.4511,  14.6272, -67.4450],\n",
       "        [-21.1760,  14.9009, -18.4976,  ...,   4.7277,  36.7557, -29.4302],\n",
       "        [ 61.5393,  -3.9496, -13.8055,  ..., -36.2256,   1.5399,  11.0703],\n",
       "        ...,\n",
       "        [-13.6829,  15.8333,  15.1891,  ...,  -4.2581, -49.4773,  15.2714],\n",
       "        [  5.4886,  -6.1912,   5.1670,  ...,   5.5224,  10.3467,  20.8256],\n",
       "        [-29.5128,  -9.6674, -51.0567,  ...,   2.4404,   0.4094, -15.4870]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 392\n",
    "    attention_param_start = 388\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= 260:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= 132:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6:\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# fold 0: {'base_lr': 4.214048623230046e-05, 'last_lr': 0.00098671139242345}. Best is trial 0 with value: 0.46920305490493774.\n",
    "# fold 1: {'base_lr': 3.4594372607385946e-05, 'last_lr': 0.0005479134338105077}. Best is trial 0 with value: 0.447492390871048\n",
    "# fold 2: {'base_lr': 1.777623134028703e-05, 'last_lr': 0.004132549020616918}. Best is trial 0 with value: 0.46756473183631897\n",
    "# fold 3: {'base_lr': 3.933402254716856e-05, 'last_lr': 0.0018473297738188957}. Best is trial 11 with value: 0.4719877541065216\n",
    "# fold 4: {'base_lr': 1.845975941382356e-05, 'last_lr': 0.0006309278277674714}. Best is trial 15 with value: 0.46920618414878845\n",
    "# fold 5: {'base_lr': 4.430444436442592e-05, 'last_lr': 0.000289231685619846}. Best is trial 6 with value: 0.4629150927066803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 8e-6, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 06:50:57,981]\u001b[0m A new study created in memory with name: no-name-2440677c-11d9-43c3-9e8b-62d883d79864\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd99ba9586e437499ad71c0299eedc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.81 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7108 New best_val_rmse: 0.7108\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6267 New best_val_rmse: 0.6267\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8061 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.11 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 7.95 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.044 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.039 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.048 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 7.95 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.047 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.04 Still best_val_rmse: 0.6267 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 06:54:00,256]\u001b[0m Trial 0 finished with value: 0.6267367601394653 and parameters: {'base_lr': 0.0001236164359060086, 'last_lr': 0.0004392333557443599}. Best is trial 0 with value: 0.6267367601394653.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9b541d59434c56a91a88ba38318280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.29 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8789 New best_val_rmse: 0.8789\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7196 New best_val_rmse: 0.7196\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7295 Still best_val_rmse: 0.7196 (from epoch 0)\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6253 New best_val_rmse: 0.6253\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5938 New best_val_rmse: 0.5938\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6007 Still best_val_rmse: 0.5938 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6401 Still best_val_rmse: 0.5938 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5568 New best_val_rmse: 0.5568\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5465 New best_val_rmse: 0.5465\n",
      "\n",
      "16 steps took 8.31 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5353 New best_val_rmse: 0.5353\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5271 New best_val_rmse: 0.5271\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5597 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5513 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.667 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5084 New best_val_rmse: 0.5084\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5417 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5116 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5107 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 8.41 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5022 New best_val_rmse: 0.5022\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4958 Still best_val_rmse: 0.4954 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4956 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5014 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5032 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4994 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4921 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4922 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4922 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4922 Still best_val_rmse: 0.4919 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4922 Still best_val_rmse: 0.4919 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 07:03:39,633]\u001b[0m Trial 1 finished with value: 0.4918626546859741 and parameters: {'base_lr': 1.745954134846961e-05, 'last_lr': 0.001415690874499708}. Best is trial 1 with value: 0.4918626546859741.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf60fbbbb59e4913a6abdd548bdbc851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.29 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.002 New best_val_rmse: 1.002\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.017 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.157 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.084 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.053 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.047 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 7.91 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.042 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.052 Still best_val_rmse: 1.002 (from epoch 0)\n",
      "\n",
      "16 steps took 7.91 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.053 Still best_val_rmse: 1.002 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 07:06:40,343]\u001b[0m Trial 2 finished with value: 1.001896619796753 and parameters: {'base_lr': 0.0004378776578537877, 'last_lr': 0.003963182774909638}. Best is trial 1 with value: 0.4918626546859741.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5559c79f824a4ca40de85d10347c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.59 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.11 New best_val_rmse: 1.11\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7107 New best_val_rmse: 0.7107\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9303 Still best_val_rmse: 0.7107 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5797 New best_val_rmse: 0.5797\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5575 New best_val_rmse: 0.5575\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6202 Still best_val_rmse: 0.5575 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5301 New best_val_rmse: 0.5301\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5882 Still best_val_rmse: 0.5301 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5964 Still best_val_rmse: 0.5301 (from epoch 0)\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5112 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5224 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.539 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5286 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4998 New best_val_rmse: 0.4998\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5219 Still best_val_rmse: 0.4998 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5073 Still best_val_rmse: 0.4998 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4989 New best_val_rmse: 0.4989\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5211 Still best_val_rmse: 0.4989 (from epoch 1)\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4826 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4854 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4843 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4829 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4796 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4811 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.481 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4819 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4853 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.487 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4811 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4837 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.48 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4794 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4808 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.483 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4827 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4805 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4785 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4785 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4785 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4788 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4797 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4812 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.482 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4808 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4802 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4802 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4803 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4802 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4801 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4799 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4799 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4799 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4798 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4798 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4798 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4798 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4798 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4798 Still best_val_rmse: 0.4784 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 07:21:53,459]\u001b[0m Trial 3 finished with value: 0.47842535376548767 and parameters: {'base_lr': 7.85942885541634e-05, 'last_lr': 0.0004061756428993061}. Best is trial 3 with value: 0.47842535376548767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a96f922b11459f9cda9ca492a732c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.131 New best_val_rmse: 1.131\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7275 New best_val_rmse: 0.7275\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7124 New best_val_rmse: 0.7124\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5925 New best_val_rmse: 0.5925\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5741 New best_val_rmse: 0.5741\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5878 Still best_val_rmse: 0.5741 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5621 New best_val_rmse: 0.5621\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5518 New best_val_rmse: 0.5518\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5432 New best_val_rmse: 0.5432\n",
      "\n",
      "16 steps took 8.43 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5389 New best_val_rmse: 0.5389\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5273 New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5831 Still best_val_rmse: 0.5273 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5646 Still best_val_rmse: 0.5273 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6298 Still best_val_rmse: 0.5273 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.519 New best_val_rmse: 0.519\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5475 Still best_val_rmse: 0.519 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5186 New best_val_rmse: 0.5186\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4987 New best_val_rmse: 0.4987\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.499 Still best_val_rmse: 0.4987 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4983 New best_val_rmse: 0.4983\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4966 New best_val_rmse: 0.4966\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4942 New best_val_rmse: 0.4942\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4955 Still best_val_rmse: 0.4942 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5046 Still best_val_rmse: 0.4942 (from epoch 2)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4998 Still best_val_rmse: 0.4942 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4944 Still best_val_rmse: 0.4942 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5 Still best_val_rmse: 0.4942 (from epoch 2)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4921 Still best_val_rmse: 0.492 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4922 Still best_val_rmse: 0.492 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4925 Still best_val_rmse: 0.492 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4926 Still best_val_rmse: 0.492 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4926 Still best_val_rmse: 0.492 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 07:31:43,707]\u001b[0m Trial 4 finished with value: 0.49199309945106506 and parameters: {'base_lr': 1.0950700468635982e-05, 'last_lr': 0.0007601420973750196}. Best is trial 3 with value: 0.47842535376548767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f0a52d596c4a9c895a7d7a6c83b18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.29 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.046 New best_val_rmse: 1.046\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.806 New best_val_rmse: 0.806\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6806 New best_val_rmse: 0.6806\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6108 New best_val_rmse: 0.6108\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5608 New best_val_rmse: 0.5608\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5411 New best_val_rmse: 0.5411\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5486 Still best_val_rmse: 0.5411 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8226 Still best_val_rmse: 0.5411 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.597 Still best_val_rmse: 0.5411 (from epoch 0)\n",
      "\n",
      "16 steps took 8.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5623 Still best_val_rmse: 0.5411 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5529 Still best_val_rmse: 0.5411 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5073 New best_val_rmse: 0.5073\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.579 Still best_val_rmse: 0.5073 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.543 Still best_val_rmse: 0.5073 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5174 Still best_val_rmse: 0.5073 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5197 Still best_val_rmse: 0.5073 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.509 Still best_val_rmse: 0.5073 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5163 Still best_val_rmse: 0.5073 (from epoch 1)\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4912 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4921 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4922 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4865 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4862 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4863 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4828 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4933 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4851 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4915 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4895 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4929 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4859 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4836 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.482 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4834 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4846 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4823 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4806 New best_val_rmse: 0.4806\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4803 New best_val_rmse: 0.4803\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4804 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4805 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4805 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4806 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4806 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4806 Still best_val_rmse: 0.4803 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 07:43:48,853]\u001b[0m Trial 5 finished with value: 0.480339378118515 and parameters: {'base_lr': 9.561811501893819e-05, 'last_lr': 0.00021196312425399687}. Best is trial 3 with value: 0.47842535376548767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7448ef7b08e5475abdbe361fb860b485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.28 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8781 New best_val_rmse: 0.8781\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6339 New best_val_rmse: 0.6339\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8235 Still best_val_rmse: 0.6339 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5908 New best_val_rmse: 0.5908\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5742 New best_val_rmse: 0.5742\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5961 Still best_val_rmse: 0.5742 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5797 Still best_val_rmse: 0.5742 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5404 New best_val_rmse: 0.5404\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5426 Still best_val_rmse: 0.5404 (from epoch 0)\n",
      "\n",
      "16 steps took 8.44 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5179 New best_val_rmse: 0.5179\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5168 New best_val_rmse: 0.5168\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.57 Still best_val_rmse: 0.5168 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5595 Still best_val_rmse: 0.5168 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6319 Still best_val_rmse: 0.5168 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5126 New best_val_rmse: 0.5126\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5255 Still best_val_rmse: 0.5126 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5014 New best_val_rmse: 0.5014\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5014 Still best_val_rmse: 0.5014 (from epoch 1)\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4813 New best_val_rmse: 0.4813\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4852 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4879 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4866 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4866 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4856 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4855 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4835 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4851 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4846 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.487 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4909 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4841 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4896 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4866 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4847 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4926 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4827 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4827 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4824 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4824 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4824 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4825 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4826 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4826 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4825 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4825 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4826 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4826 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4825 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4825 Still best_val_rmse: 0.4813 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 07:56:24,859]\u001b[0m Trial 6 finished with value: 0.4813295304775238 and parameters: {'base_lr': 2.5713054102170138e-05, 'last_lr': 8.00730383342181e-05}. Best is trial 3 with value: 0.47842535376548767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c748d821390439c8db883ee785377b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.29 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.039 New best_val_rmse: 1.039\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.655 New best_val_rmse: 0.655\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6001 New best_val_rmse: 0.6001\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5673 New best_val_rmse: 0.5673\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5569 New best_val_rmse: 0.5569\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6019 Still best_val_rmse: 0.5569 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5778 Still best_val_rmse: 0.5569 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5456 New best_val_rmse: 0.5456\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5305 New best_val_rmse: 0.5305\n",
      "\n",
      "16 steps took 8.43 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5203 New best_val_rmse: 0.5203\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5116 New best_val_rmse: 0.5116\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5684 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5532 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6248 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5135 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5153 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5004 New best_val_rmse: 0.5004\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5027 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 8.35 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4933 New best_val_rmse: 0.4933\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4854 New best_val_rmse: 0.4854\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4859 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4873 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4858 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4855 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4852 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4876 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4951 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4894 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4842 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4897 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4873 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4837 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4913 Still best_val_rmse: 0.4834 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4819 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4821 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4821 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4823 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4822 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4822 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4823 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4823 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4823 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4824 Still best_val_rmse: 0.4818 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4823 Still best_val_rmse: 0.4818 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 08:09:00,511]\u001b[0m Trial 7 finished with value: 0.4818335473537445 and parameters: {'base_lr': 1.6074093603458293e-05, 'last_lr': 9.773433523491014e-05}. Best is trial 3 with value: 0.47842535376548767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c225ad64426b43809f5f1f75518e49ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.33 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.166 New best_val_rmse: 1.166\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7343 New best_val_rmse: 0.7343\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6814 New best_val_rmse: 0.6814\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5766 New best_val_rmse: 0.5766\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5623 New best_val_rmse: 0.5623\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5733 Still best_val_rmse: 0.5623 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5609 New best_val_rmse: 0.5609\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5414 New best_val_rmse: 0.5414\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5311 New best_val_rmse: 0.5311\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5182 New best_val_rmse: 0.5182\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5303 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5775 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5633 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6027 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5299 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5385 Still best_val_rmse: 0.5182 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.514 New best_val_rmse: 0.514\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4987 New best_val_rmse: 0.4987\n",
      "\n",
      "8 steps took 4.32 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 4.03 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4943 Still best_val_rmse: 0.4937 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4927 New best_val_rmse: 0.4927\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4924 New best_val_rmse: 0.4924\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4861 New best_val_rmse: 0.4861\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4873 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4893 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4971 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.497 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4942 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4876 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4933 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4874 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4855 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4857 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4859 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4856 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4855 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4854 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4855 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4855 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4854 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4854 Still best_val_rmse: 0.4853 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 08:20:55,487]\u001b[0m Trial 8 finished with value: 0.4853384494781494 and parameters: {'base_lr': 1.2285175980980382e-05, 'last_lr': 0.00011493978232590547}. Best is trial 3 with value: 0.47842535376548767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a9bc8884174702916e0ebc2d6b8ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8193 New best_val_rmse: 0.8193\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7549 New best_val_rmse: 0.7549\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7277 New best_val_rmse: 0.7277\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.636 New best_val_rmse: 0.636\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6004 New best_val_rmse: 0.6004\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6 New best_val_rmse: 0.6\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5124 New best_val_rmse: 0.5124\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8435 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5697 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5581 Still best_val_rmse: 0.5124 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5081 New best_val_rmse: 0.5081\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5023 New best_val_rmse: 0.5023\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5422 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5199 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5117 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5228 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.487 New best_val_rmse: 0.487\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4897 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5006 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4939 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4925 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.479 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4828 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4733 New best_val_rmse: 0.4733\n",
      "\n",
      "2 steps took 0.989 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4751 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4754 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4748 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4747 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4794 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4785 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4746 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4749 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4758 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.477 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4829 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4822 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4731 New best_val_rmse: 0.4731\n",
      "\n",
      "2 steps took 0.989 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.473 New best_val_rmse: 0.473\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4743 Still best_val_rmse: 0.473 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4733 Still best_val_rmse: 0.473 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4731 Still best_val_rmse: 0.473 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4733 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4735 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.473 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4729 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4732 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4733 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4738 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4735 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4727 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4727 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4728 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.473 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.473 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4731 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4731 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4732 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4733 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4734 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 08:39:05,473]\u001b[0m Trial 9 finished with value: 0.4723660945892334 and parameters: {'base_lr': 4.93859406656209e-05, 'last_lr': 0.0015750094858236622}. Best is trial 9 with value: 0.4723660945892334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8708f484da274a2798b7810777ebd45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.27 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9189 New best_val_rmse: 0.9189\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.064 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.048 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.04 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 5.768 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 7.83 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.082 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.055 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 7.9 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.042 Still best_val_rmse: 0.9189 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.052 Still best_val_rmse: 0.9189 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 08:42:06,046]\u001b[0m Trial 10 finished with value: 0.9189337491989136 and parameters: {'base_lr': 0.00030188783957741804, 'last_lr': 0.004623364365297586}. Best is trial 9 with value: 0.4723660945892334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4588c81d441d4a358c0f8a1826875f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.26 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8689 New best_val_rmse: 0.8689\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7713 New best_val_rmse: 0.7713\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8985 Still best_val_rmse: 0.7713 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6077 New best_val_rmse: 0.6077\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6338 Still best_val_rmse: 0.6077 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5491 New best_val_rmse: 0.5491\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5093 New best_val_rmse: 0.5093\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7782 Still best_val_rmse: 0.5093 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5216 Still best_val_rmse: 0.5093 (from epoch 0)\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5129 Still best_val_rmse: 0.5093 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5027 New best_val_rmse: 0.5027\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5158 Still best_val_rmse: 0.5027 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5256 Still best_val_rmse: 0.5027 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5179 Still best_val_rmse: 0.5027 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4916 New best_val_rmse: 0.4916\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5204 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.496 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4912 Still best_val_rmse: 0.4847 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4921 Still best_val_rmse: 0.4847 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4892 Still best_val_rmse: 0.4847 (from epoch 1)\n",
      "\n",
      "4 steps took 2.36 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4919 Still best_val_rmse: 0.4847 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4838 New best_val_rmse: 0.4838\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4863 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4809 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4739 New best_val_rmse: 0.4739\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4836 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4768 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4745 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4751 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4745 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4751 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4807 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4728 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4777 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4749 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4748 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4842 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4829 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4732 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4757 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4764 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4731 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4721 New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4722 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.474 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4735 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4725 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4721 New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4725 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4732 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4741 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.473 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.472 New best_val_rmse: 0.472\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4722 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4726 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4727 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4728 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4727 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4728 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4728 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4728 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4727 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4728 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4728 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4729 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4729 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4729 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4729 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.473 Still best_val_rmse: 0.472 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 09:00:34,357]\u001b[0m Trial 11 finished with value: 0.4719877541065216 and parameters: {'base_lr': 3.933402254716856e-05, 'last_lr': 0.0018473297738188957}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58f53edf4ea4c3c99a712f9d052d57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.27 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8707 New best_val_rmse: 0.8707\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7661 New best_val_rmse: 0.7661\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8035 Still best_val_rmse: 0.7661 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6049 New best_val_rmse: 0.6049\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6443 Still best_val_rmse: 0.6049 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5961 New best_val_rmse: 0.5961\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5164 New best_val_rmse: 0.5164\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8776 Still best_val_rmse: 0.5164 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5193 Still best_val_rmse: 0.5164 (from epoch 0)\n",
      "\n",
      "16 steps took 8.42 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5644 Still best_val_rmse: 0.5164 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5154 New best_val_rmse: 0.5154\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5169 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.543 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5147 New best_val_rmse: 0.5147\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5347 Still best_val_rmse: 0.4909 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5016 Still best_val_rmse: 0.4909 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4964 Still best_val_rmse: 0.4846 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4929 Still best_val_rmse: 0.4846 (from epoch 1)\n",
      "\n",
      "8 steps took 4.33 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4746 New best_val_rmse: 0.4746\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4798 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4823 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4887 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4753 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4867 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4864 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4882 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4804 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4847 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4761 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4754 Still best_val_rmse: 0.4746 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4743 New best_val_rmse: 0.4743\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4745 Still best_val_rmse: 0.4736 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4728 New best_val_rmse: 0.4728\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4742 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4734 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4728 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4769 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4786 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4785 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4745 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4732 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4728 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4738 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4734 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4741 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4741 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4746 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4744 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4754 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4762 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4775 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4776 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4768 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4757 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4748 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4743 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.474 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4738 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4734 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4733 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4733 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4734 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4736 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4735 Still best_val_rmse: 0.4728 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 09:19:13,535]\u001b[0m Trial 12 finished with value: 0.47275277972221375 and parameters: {'base_lr': 3.7865492905730714e-05, 'last_lr': 0.0020209346923263974}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87336912231843ca852ca7dbee1f6ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.31 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8109 New best_val_rmse: 0.8109\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7499 New best_val_rmse: 0.7499\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7282 New best_val_rmse: 0.7282\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6398 New best_val_rmse: 0.6398\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5808 New best_val_rmse: 0.5808\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.564 New best_val_rmse: 0.564\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5097 New best_val_rmse: 0.5097\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.869 Still best_val_rmse: 0.5097 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5552 Still best_val_rmse: 0.5097 (from epoch 0)\n",
      "\n",
      "16 steps took 8.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5494 Still best_val_rmse: 0.5097 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5202 Still best_val_rmse: 0.5097 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5001 New best_val_rmse: 0.5001\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5383 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5298 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.51 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5187 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4948 New best_val_rmse: 0.4948\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4864 New best_val_rmse: 0.4864\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4991 Still best_val_rmse: 0.4864 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4948 Still best_val_rmse: 0.4864 (from epoch 1)\n",
      "\n",
      "8 steps took 4.38 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4739 New best_val_rmse: 0.4739\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4754 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4825 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4786 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4872 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4762 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4868 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4858 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4847 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4755 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4804 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4796 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4747 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4741 Still best_val_rmse: 0.4739 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4733 New best_val_rmse: 0.4733\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.475 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4742 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4747 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4744 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4739 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4769 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4776 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4776 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4744 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4736 Still best_val_rmse: 0.4733 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4731 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4731 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4728 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4732 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4733 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4747 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4762 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4782 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4787 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4778 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4761 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4746 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4738 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4737 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 09:38:02,604]\u001b[0m Trial 13 finished with value: 0.47244179248809814 and parameters: {'base_lr': 4.80991294980512e-05, 'last_lr': 0.00235888450338897}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbfccf1805c4ff5a3e2d5ac2e605b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.31 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8965 New best_val_rmse: 0.8965\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7775 New best_val_rmse: 0.7775\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7671 New best_val_rmse: 0.7671\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6421 New best_val_rmse: 0.6421\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6345 New best_val_rmse: 0.6345\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5466 New best_val_rmse: 0.5466\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5212 New best_val_rmse: 0.5212\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5665 Still best_val_rmse: 0.5212 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5554 Still best_val_rmse: 0.5212 (from epoch 0)\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5163 New best_val_rmse: 0.5163\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5025 New best_val_rmse: 0.5025\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5331 Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5167 Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5598 Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4924 New best_val_rmse: 0.4924\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.511 Still best_val_rmse: 0.4924 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 4.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.492 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4944 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5125 Still best_val_rmse: 0.491 (from epoch 1)\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4827 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4809 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4837 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4844 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4861 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4853 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4805 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4853 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.481 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4781 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4799 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4775 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4776 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4849 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.487 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4799 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4778 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4778 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4783 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4782 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4782 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4781 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4783 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4787 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4792 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4803 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4817 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4821 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4794 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4782 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4775 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4772 New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4771 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4772 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4773 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4774 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4775 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4777 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4779 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4779 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4779 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4779 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4777 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4777 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4777 Still best_val_rmse: 0.4771 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 09:55:08,981]\u001b[0m Trial 14 finished with value: 0.477120578289032 and parameters: {'base_lr': 3.375456005933041e-05, 'last_lr': 0.0010378060794420565}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1324eb4e7dcd47319f9d17a29b729515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.33 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.0 New best_val_rmse: 1.0\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6071 New best_val_rmse: 0.6071\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6886 Still best_val_rmse: 0.6071 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7574 Still best_val_rmse: 0.6071 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6176 Still best_val_rmse: 0.6071 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.598 New best_val_rmse: 0.598\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.042 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.053 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.055 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 1.04 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.05 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.041 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.041 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.043 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.04 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.039 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.047 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.95 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.04 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.054 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.039 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.045 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.039 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.044 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.044 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.04 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.04 Still best_val_rmse: 0.598 (from epoch 0)\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.039 Still best_val_rmse: 0.598 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 10:03:44,250]\u001b[0m Trial 15 finished with value: 0.5979675054550171 and parameters: {'base_lr': 0.00014562815724550082, 'last_lr': 0.002862539789708432}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34e76bc68a249bea209efc2fb90d0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.31 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8459 New best_val_rmse: 0.8459\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6719 New best_val_rmse: 0.6719\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7071 Still best_val_rmse: 0.6719 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7028 Still best_val_rmse: 0.6719 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.669 New best_val_rmse: 0.669\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5868 New best_val_rmse: 0.5868\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5136 New best_val_rmse: 0.5136\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8133 Still best_val_rmse: 0.5136 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5237 Still best_val_rmse: 0.5136 (from epoch 0)\n",
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5519 Still best_val_rmse: 0.5136 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5166 Still best_val_rmse: 0.5136 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.512 New best_val_rmse: 0.512\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5454 Still best_val_rmse: 0.512 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5197 Still best_val_rmse: 0.512 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5116 New best_val_rmse: 0.5116\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5179 Still best_val_rmse: 0.5116 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.499 New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4901 New best_val_rmse: 0.4901\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5034 Still best_val_rmse: 0.4901 (from epoch 1)\n",
      "\n",
      "16 steps took 8.34 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4861 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4878 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.483 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4794 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4778 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4767 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4788 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4767 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4779 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4787 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4791 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4794 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4806 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4809 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4782 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4799 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4788 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4773 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4768 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4767 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4773 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4779 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4782 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4781 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4778 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4775 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4769 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4763 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4758 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4754 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4755 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4756 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4759 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4767 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4769 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.477 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4767 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4765 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4767 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4768 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4769 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4769 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4768 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4768 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4768 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4767 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4767 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4766 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4765 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4765 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4765 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4765 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4765 Still best_val_rmse: 0.475 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 10:22:12,498]\u001b[0m Trial 16 finished with value: 0.47497743368148804 and parameters: {'base_lr': 5.949649546990705e-05, 'last_lr': 0.001394817761059422}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf20427de0a44d8bb1ad3b7e3adc63d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.26 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8775 New best_val_rmse: 0.8775\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7235 New best_val_rmse: 0.7235\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8394 Still best_val_rmse: 0.7235 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6569 New best_val_rmse: 0.6569\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5507 New best_val_rmse: 0.5507\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5917 Still best_val_rmse: 0.5507 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5297 New best_val_rmse: 0.5297\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5411 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5638 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.35 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5388 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5323 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5943 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5819 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6367 Still best_val_rmse: 0.5297 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5252 New best_val_rmse: 0.5252\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5337 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5061 New best_val_rmse: 0.5061\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5118 Still best_val_rmse: 0.5061 (from epoch 1)\n",
      "\n",
      "16 steps took 8.41 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4973 New best_val_rmse: 0.4973\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.49 New best_val_rmse: 0.49\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4941 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4925 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4936 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.03 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4915 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4917 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.495 Still best_val_rmse: 0.49 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4895 New best_val_rmse: 0.4895\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4929 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.49 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4977 Still best_val_rmse: 0.4895 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4884 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4882 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4883 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4883 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4882 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4882 New best_val_rmse: 0.4882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 10:33:46,769]\u001b[0m Trial 17 finished with value: 0.488165944814682 and parameters: {'base_lr': 2.6577087600861542e-05, 'last_lr': 0.0006555025937989686}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679ac1c5ea31436ab07a2482a6226c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.841 New best_val_rmse: 1.841\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8765 New best_val_rmse: 0.8765\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6342 New best_val_rmse: 0.6342\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.118 Still best_val_rmse: 0.6342 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.121 Still best_val_rmse: 0.6342 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.04 Still best_val_rmse: 0.6342 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.041 Still best_val_rmse: 0.6342 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.039 Still best_val_rmse: 0.6342 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.036 Still best_val_rmse: 0.6342 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 10:36:49,086]\u001b[0m Trial 18 finished with value: 0.6342186331748962 and parameters: {'base_lr': 0.00018188998981542552, 'last_lr': 0.0032771612277958054}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60e72a5bac8419eb3cf1775bab0a105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.35 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.892 New best_val_rmse: 0.892\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6588 New best_val_rmse: 0.6588\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8821 Still best_val_rmse: 0.6588 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6051 New best_val_rmse: 0.6051\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5817 New best_val_rmse: 0.5817\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6294 Still best_val_rmse: 0.5817 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5552 New best_val_rmse: 0.5552\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8564 Still best_val_rmse: 0.5552 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5483 New best_val_rmse: 0.5483\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.562 Still best_val_rmse: 0.5483 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5133 New best_val_rmse: 0.5133\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4959 New best_val_rmse: 0.4959\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.513 Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5103 Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5493 Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5572 Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5105 Still best_val_rmse: 0.4959 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4957 New best_val_rmse: 0.4957\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5059 Still best_val_rmse: 0.4957 (from epoch 1)\n",
      "\n",
      "16 steps took 8.41 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4764 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4788 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4804 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4806 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4812 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4772 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4829 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4777 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4846 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4802 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4869 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4897 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.483 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.485 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4779 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4795 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4794 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4789 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4785 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4786 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4781 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4775 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.477 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4766 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4761 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4762 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4761 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4761 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4762 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4763 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4765 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4769 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4777 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4783 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4786 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4784 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4782 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4778 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4779 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4781 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4781 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4782 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4781 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4782 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4781 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4781 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.478 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4779 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4779 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4779 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4779 Still best_val_rmse: 0.476 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 10:54:25,550]\u001b[0m Trial 19 finished with value: 0.47603610157966614 and parameters: {'base_lr': 6.340567929223967e-05, 'last_lr': 0.0011594900524503188}. Best is trial 11 with value: 0.4719877541065216.\u001b[0m\n",
      "\u001b[32m[I 2021-07-13 10:54:25,555]\u001b[0m A new study created in memory with name: no-name-81c980ff-72c9-4dca-9827-5098aef4d5af\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.4719877541065216\n",
      " Best params: \n",
      "    base_lr: 3.933402254716856e-05\n",
      "    last_lr: 0.0018473297738188957\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7d49b12645438e9bb9773cc9d7247f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.31 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9356 New best_val_rmse: 0.9356\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6408 New best_val_rmse: 0.6408\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8461 Still best_val_rmse: 0.6408 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6059 New best_val_rmse: 0.6059\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5731 New best_val_rmse: 0.5731\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7197 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5938 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6765 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5783 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5924 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5411 New best_val_rmse: 0.5411\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5212 New best_val_rmse: 0.5212\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5141 New best_val_rmse: 0.5141\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5009 New best_val_rmse: 0.5009\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.508 Still best_val_rmse: 0.5009 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.531 Still best_val_rmse: 0.5009 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.5061 Still best_val_rmse: 0.4853 (from epoch 1)\n",
      "\n",
      "16 steps took 8.32 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.478 New best_val_rmse: 0.478\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4775 New best_val_rmse: 0.4775\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4816 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.479 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4819 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4837 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4866 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4912 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4793 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4779 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4751 New best_val_rmse: 0.4751\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4741 New best_val_rmse: 0.4741\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4731 New best_val_rmse: 0.4731\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4725 New best_val_rmse: 0.4725\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4738 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4767 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4762 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4732 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4719 New best_val_rmse: 0.4719\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4721 Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4733 Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4756 Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4748 Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4728 Still best_val_rmse: 0.4719 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4712 New best_val_rmse: 0.4712\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4713 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4715 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.472 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4729 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4731 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4728 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4721 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4717 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4717 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4719 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4719 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4717 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4719 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4719 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4719 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 11:12:12,676]\u001b[0m Trial 0 finished with value: 0.471155047416687 and parameters: {'base_lr': 3.637249863801786e-05, 'last_lr': 0.0016962627538473951}. Best is trial 0 with value: 0.471155047416687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7533f4e5826749c893044aa6ce19f123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.36 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.6844 New best_val_rmse: 0.6844\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.5989 New best_val_rmse: 0.5989\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9536 Still best_val_rmse: 0.5989 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7987 Still best_val_rmse: 0.5989 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5922 New best_val_rmse: 0.5922\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6743 Still best_val_rmse: 0.5922 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5988 Still best_val_rmse: 0.5922 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8527 Still best_val_rmse: 0.5922 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5598 New best_val_rmse: 0.5598\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5837 Still best_val_rmse: 0.5598 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6229 Still best_val_rmse: 0.5598 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6117 Still best_val_rmse: 0.5598 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5327 New best_val_rmse: 0.5327\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5235 New best_val_rmse: 0.5235\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5066 New best_val_rmse: 0.5066\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5089 Still best_val_rmse: 0.5066 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5331 Still best_val_rmse: 0.5066 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 4.35 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4935 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4849 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4777 New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4792 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4803 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4784 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.479 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4812 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4788 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4784 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4813 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.484 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4785 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4774 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4756 New best_val_rmse: 0.4756\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4746 New best_val_rmse: 0.4746\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4745 New best_val_rmse: 0.4745\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4743 New best_val_rmse: 0.4743\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4742 New best_val_rmse: 0.4742\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4737 New best_val_rmse: 0.4737\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4745 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.476 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4775 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4787 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4783 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4758 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4735 New best_val_rmse: 0.4735\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4732 New best_val_rmse: 0.4732\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4735 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4736 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4731 New best_val_rmse: 0.4731\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4728 New best_val_rmse: 0.4728\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4728 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4731 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4733 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4735 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4736 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4734 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4733 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4733 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4733 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4733 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4734 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4734 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4735 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4735 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4736 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4736 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4737 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4737 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4737 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4737 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4737 Still best_val_rmse: 0.4727 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4737 Still best_val_rmse: 0.4727 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 11:30:23,209]\u001b[0m Trial 1 finished with value: 0.47269728779792786 and parameters: {'base_lr': 6.22889405760658e-05, 'last_lr': 9.249841272963643e-05}. Best is trial 0 with value: 0.471155047416687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cced4db64cf1444f88aa28914ad3ffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.38 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8942 New best_val_rmse: 0.8942\n",
      "\n",
      "16 steps took 7.95 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6449 New best_val_rmse: 0.6449\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8072 Still best_val_rmse: 0.6449 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5804 New best_val_rmse: 0.5804\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7197 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5962 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5877 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7855 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5589 New best_val_rmse: 0.5589\n",
      "\n",
      "16 steps took 8.31 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5516 New best_val_rmse: 0.5516\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6043 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6191 Still best_val_rmse: 0.5516 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5464 New best_val_rmse: 0.5464\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5462 New best_val_rmse: 0.5462\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5016 New best_val_rmse: 0.5016\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5222 Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.533 Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4994 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 4.38 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4893 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4828 New best_val_rmse: 0.4828\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4807 New best_val_rmse: 0.4807\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4797 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4813 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4786 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4812 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4801 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4853 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4849 Still best_val_rmse: 0.4784 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4771 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4776 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4779 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4781 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4785 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4773 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4751 New best_val_rmse: 0.4751\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4755 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4794 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4846 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4799 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4767 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4762 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4801 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4787 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4765 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4755 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4758 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4757 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.476 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.476 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4756 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4751 New best_val_rmse: 0.4751\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4753 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.476 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4764 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4764 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4761 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4758 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4757 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4754 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4753 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4752 Still best_val_rmse: 0.4751 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 11:48:21,012]\u001b[0m Trial 2 finished with value: 0.4750526249408722 and parameters: {'base_lr': 2.906350701126783e-05, 'last_lr': 0.003390354303991374}. Best is trial 0 with value: 0.471155047416687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfeef9823a4548d48eaf57777b2fad47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.41 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7562 New best_val_rmse: 0.7562\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6845 New best_val_rmse: 0.6845\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8432 Still best_val_rmse: 0.6845 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7494 Still best_val_rmse: 0.6845 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6429 New best_val_rmse: 0.6429\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.649 Still best_val_rmse: 0.6429 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.639 New best_val_rmse: 0.639\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7474 Still best_val_rmse: 0.639 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5673 New best_val_rmse: 0.5673\n",
      "\n",
      "16 steps took 8.39 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.538 New best_val_rmse: 0.538\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6314 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6206 Still best_val_rmse: 0.538 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5357 New best_val_rmse: 0.5357\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5276 New best_val_rmse: 0.5276\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5029 New best_val_rmse: 0.5029\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5236 Still best_val_rmse: 0.5029 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5449 Still best_val_rmse: 0.5029 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4968 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 4.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4845 New best_val_rmse: 0.4845\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4792 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4792 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4802 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4791 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4811 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4778 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4796 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4842 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4783 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4805 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4802 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4785 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4774 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4764 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4757 Still best_val_rmse: 0.4753 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4795 Still best_val_rmse: 0.4753 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4846 Still best_val_rmse: 0.4753 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4796 Still best_val_rmse: 0.4753 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.476 Still best_val_rmse: 0.4753 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4745 New best_val_rmse: 0.4745\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4741 New best_val_rmse: 0.4741\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4767 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4779 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4775 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4752 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4753 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4752 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4755 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4756 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4755 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4752 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4748 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.475 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4751 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4751 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4749 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4748 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4749 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4749 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4749 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4749 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4748 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.988 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4748 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4747 Still best_val_rmse: 0.4741 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 12:06:49,618]\u001b[0m Trial 3 finished with value: 0.4741213619709015 and parameters: {'base_lr': 4.5210501000419023e-05, 'last_lr': 0.0001842045235576954}. Best is trial 0 with value: 0.471155047416687.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f36be7f89db495299de6d446a97298a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.32 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8771 New best_val_rmse: 0.8771\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6802 New best_val_rmse: 0.6802\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6318 New best_val_rmse: 0.6318\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5564 New best_val_rmse: 0.5564\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5825 Still best_val_rmse: 0.5564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7171 Still best_val_rmse: 0.5564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5668 Still best_val_rmse: 0.5564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7591 Still best_val_rmse: 0.5564 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5441 New best_val_rmse: 0.5441\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5196 New best_val_rmse: 0.5196\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5817 Still best_val_rmse: 0.5196 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6304 Still best_val_rmse: 0.5196 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5107 New best_val_rmse: 0.5107\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5288 Still best_val_rmse: 0.5107 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4898 New best_val_rmse: 0.4898\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5113 Still best_val_rmse: 0.4898 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.5107 Still best_val_rmse: 0.4898 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4973 Still best_val_rmse: 0.4898 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4845 New best_val_rmse: 0.4845\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4895 Still best_val_rmse: 0.4845 (from epoch 1)\n",
      "\n",
      "4 steps took 2.35 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4844 New best_val_rmse: 0.4844\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4845 Still best_val_rmse: 0.4844 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4969 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4955 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.488 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4824 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4725 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4769 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4803 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4825 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4728 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.472 New best_val_rmse: 0.472\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4729 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4733 Still best_val_rmse: 0.472 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4717 New best_val_rmse: 0.4717\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4711 New best_val_rmse: 0.4711\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4718 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4736 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4731 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4705 New best_val_rmse: 0.4705\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4708 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4707 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4708 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4704 New best_val_rmse: 0.4704\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4707 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4728 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.475 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4737 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4725 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4721 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4718 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4713 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4712 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4712 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4713 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4716 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4717 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4716 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4715 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4714 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4712 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4711 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4711 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.471 Still best_val_rmse: 0.4703 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 12:24:57,399]\u001b[0m Trial 4 finished with value: 0.4703187942504883 and parameters: {'base_lr': 1.4874639372459377e-05, 'last_lr': 0.002410173488190959}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5906f1db23224218ba22a2a4b18889bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9243 New best_val_rmse: 0.9243\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7096 New best_val_rmse: 0.7096\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6871 New best_val_rmse: 0.6871\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5893 New best_val_rmse: 0.5893\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6224 Still best_val_rmse: 0.5893 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6169 Still best_val_rmse: 0.5893 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6461 Still best_val_rmse: 0.5893 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7194 Still best_val_rmse: 0.5893 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5574 New best_val_rmse: 0.5574\n",
      "\n",
      "16 steps took 8.39 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5311 New best_val_rmse: 0.5311\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5202 New best_val_rmse: 0.5202\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5902 Still best_val_rmse: 0.5202 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.499 New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5069 Still best_val_rmse: 0.499 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5012 Still best_val_rmse: 0.499 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5035 Still best_val_rmse: 0.499 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5005 Still best_val_rmse: 0.499 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4878 Still best_val_rmse: 0.4826 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4941 Still best_val_rmse: 0.4826 (from epoch 1)\n",
      "\n",
      "8 steps took 4.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4853 Still best_val_rmse: 0.4826 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4813 New best_val_rmse: 0.4813\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5082 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.5039 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4895 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4816 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4741 New best_val_rmse: 0.4741\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4751 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4762 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.475 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4734 New best_val_rmse: 0.4734\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4736 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4752 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4752 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4736 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4736 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4743 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4743 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4743 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4734 New best_val_rmse: 0.4734\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4732 New best_val_rmse: 0.4732\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.474 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4769 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4794 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4777 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4762 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4758 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4751 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4741 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4738 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4738 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4742 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4745 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4744 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4743 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4742 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.474 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4738 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4736 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4736 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4736 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4736 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4736 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4737 Still best_val_rmse: 0.4732 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 12:41:53,194]\u001b[0m Trial 5 finished with value: 0.47324302792549133 and parameters: {'base_lr': 9.961501774772943e-06, 'last_lr': 0.0020107226677383645}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274870edec8f402e8d6cbf2a23bacd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.41 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.041 New best_val_rmse: 1.041\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7581 New best_val_rmse: 0.7581\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6738 New best_val_rmse: 0.6738\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7111 Still best_val_rmse: 0.6738 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5683 New best_val_rmse: 0.5683\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5784 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5815 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6035 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5559 New best_val_rmse: 0.5559\n",
      "\n",
      "16 steps took 8.31 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.528 New best_val_rmse: 0.528\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5289 Still best_val_rmse: 0.528 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5521 Still best_val_rmse: 0.528 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.556 Still best_val_rmse: 0.528 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5085 New best_val_rmse: 0.5085\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5027 Still best_val_rmse: 0.4969 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 4.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4859 Still best_val_rmse: 0.4834 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4996 Still best_val_rmse: 0.4826 (from epoch 1)\n",
      "\n",
      "8 steps took 4.37 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4809 Still best_val_rmse: 0.4769 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4793 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4905 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4845 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4774 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4735 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4752 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4735 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4716 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4723 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4755 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4779 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.476 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4734 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4775 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4768 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4756 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.474 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4734 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4743 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4746 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4747 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4732 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4729 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4731 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4737 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4744 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4746 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4746 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4743 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4742 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.474 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4736 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4729 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4724 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4721 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4723 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4731 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4734 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4739 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4745 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4741 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4737 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4729 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4721 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4718 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4716 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4716 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4717 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4718 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4717 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4717 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4716 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4716 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4714 New best_val_rmse: 0.4714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 13:01:14,203]\u001b[0m Trial 6 finished with value: 0.47142037749290466 and parameters: {'base_lr': 1.0399554645318021e-05, 'last_lr': 0.00011564456066425258}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb16a1e9bab42b184c6c0eb63352816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.35 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7031 New best_val_rmse: 0.7031\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.733 Still best_val_rmse: 0.7031 (from epoch 0)\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8097 Still best_val_rmse: 0.7031 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7458 Still best_val_rmse: 0.7031 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6738 New best_val_rmse: 0.6738\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7253 Still best_val_rmse: 0.6738 (from epoch 0)\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5863 New best_val_rmse: 0.5863\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7838 Still best_val_rmse: 0.5863 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5812 New best_val_rmse: 0.5812\n",
      "\n",
      "16 steps took 8.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5143 New best_val_rmse: 0.5143\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5556 Still best_val_rmse: 0.5143 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.552 Still best_val_rmse: 0.5143 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5283 Still best_val_rmse: 0.5143 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5433 Still best_val_rmse: 0.5143 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5011 New best_val_rmse: 0.5011\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5307 Still best_val_rmse: 0.5011 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5591 Still best_val_rmse: 0.5011 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4887 New best_val_rmse: 0.4887\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4928 Still best_val_rmse: 0.4887 (from epoch 1)\n",
      "\n",
      "8 steps took 4.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4917 Still best_val_rmse: 0.4887 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4777 New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4784 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4778 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.478 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4777 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4784 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4814 Still best_val_rmse: 0.4777 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4801 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4777 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4809 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4785 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.476 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4783 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4795 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4797 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4795 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4796 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4789 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4775 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4752 Still best_val_rmse: 0.475 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4789 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4851 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4809 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4729 New best_val_rmse: 0.4729\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4731 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4761 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4762 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4748 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.473 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4725 New best_val_rmse: 0.4725\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4728 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4734 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4737 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4736 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4736 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4736 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4735 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4736 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4738 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4739 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4739 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4737 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4735 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4734 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4734 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4735 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4735 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4734 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4734 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4733 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4733 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4733 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4732 Still best_val_rmse: 0.4725 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 13:19:33,625]\u001b[0m Trial 7 finished with value: 0.4724603593349457 and parameters: {'base_lr': 4.7147362319298385e-05, 'last_lr': 0.00022188389800071334}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c4521d1f2c4a70993c319e163c96b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.39 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8232 New best_val_rmse: 0.8232\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6651 New best_val_rmse: 0.6651\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6384 New best_val_rmse: 0.6384\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5698 New best_val_rmse: 0.5698\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6695 Still best_val_rmse: 0.5698 (from epoch 0)\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6021 Still best_val_rmse: 0.5698 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6922 Still best_val_rmse: 0.5698 (from epoch 0)\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6483 Still best_val_rmse: 0.5698 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5467 New best_val_rmse: 0.5467\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5038 New best_val_rmse: 0.5038\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5339 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6217 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5047 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5133 Still best_val_rmse: 0.5038 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4992 New best_val_rmse: 0.4992\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4931 New best_val_rmse: 0.4931\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5009 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5354 Still best_val_rmse: 0.4931 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4923 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "8 steps took 4.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4855 New best_val_rmse: 0.4855\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4891 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4798 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4922 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4888 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4741 New best_val_rmse: 0.4741\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4788 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4785 Still best_val_rmse: 0.4741 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4737 New best_val_rmse: 0.4737\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4769 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4837 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.473 New best_val_rmse: 0.473\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4761 Still best_val_rmse: 0.473 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4818 Still best_val_rmse: 0.473 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4747 Still best_val_rmse: 0.473 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4757 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4772 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4758 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4728 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4718 New best_val_rmse: 0.4718\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4732 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4754 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4741 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4734 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4728 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4729 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4732 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4732 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4735 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4734 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4731 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4733 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4737 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4739 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4739 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4735 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4731 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4729 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4732 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4731 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4729 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4728 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4728 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4727 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4727 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4728 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4728 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4729 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4729 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.473 Still best_val_rmse: 0.4718 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 13:37:52,280]\u001b[0m Trial 8 finished with value: 0.47184598445892334 and parameters: {'base_lr': 1.970149129585611e-05, 'last_lr': 0.00100252784120404}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497489dadb944a6fbf9c3929b8aa8b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.26 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.666 New best_val_rmse: 1.666\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7666 New best_val_rmse: 0.7666\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7597 New best_val_rmse: 0.7597\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6991 New best_val_rmse: 0.6991\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6907 New best_val_rmse: 0.6907\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 4.2 Still best_val_rmse: 0.6907 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.148 Still best_val_rmse: 0.6907 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.034 Still best_val_rmse: 0.6907 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.105 Still best_val_rmse: 0.6907 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 13:40:54,104]\u001b[0m Trial 9 finished with value: 0.690746009349823 and parameters: {'base_lr': 0.0003317799849554229, 'last_lr': 0.0003281776713604505}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dc65736fb34bf39ea081e42e86c277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.36 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.007 New best_val_rmse: 1.007\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6504 New best_val_rmse: 0.6504\n",
      "\n",
      "16 steps took 7.97 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.044 Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.038 Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.05 Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.093 Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.048 Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.038 Still best_val_rmse: 0.6504 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.037 Still best_val_rmse: 0.6504 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 13:43:55,451]\u001b[0m Trial 10 finished with value: 0.6503852605819702 and parameters: {'base_lr': 0.0001803181764505108, 'last_lr': 0.003692075608241321}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ea60df3524496aaf594fa0fb13abd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.37 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.829 New best_val_rmse: 0.829\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6708 New best_val_rmse: 0.6708\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6308 New best_val_rmse: 0.6308\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5644 New best_val_rmse: 0.5644\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5571 New best_val_rmse: 0.5571\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6138 Still best_val_rmse: 0.5571 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5799 Still best_val_rmse: 0.5571 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7296 Still best_val_rmse: 0.5571 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5496 New best_val_rmse: 0.5496\n",
      "\n",
      "16 steps took 8.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5224 New best_val_rmse: 0.5224\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5698 Still best_val_rmse: 0.5224 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6234 Still best_val_rmse: 0.5224 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5121 New best_val_rmse: 0.5121\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5316 Still best_val_rmse: 0.5121 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4918 New best_val_rmse: 0.4918\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5001 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5183 Still best_val_rmse: 0.4918 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4892 Still best_val_rmse: 0.4837 (from epoch 1)\n",
      "\n",
      "4 steps took 2.34 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4853 Still best_val_rmse: 0.4837 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4863 Still best_val_rmse: 0.4837 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4848 Still best_val_rmse: 0.4837 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4821 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4793 New best_val_rmse: 0.4793\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4861 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4826 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4744 New best_val_rmse: 0.4744\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4766 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4761 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.474 New best_val_rmse: 0.474\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4774 Still best_val_rmse: 0.474 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4836 Still best_val_rmse: 0.474 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4734 New best_val_rmse: 0.4734\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4777 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4857 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4761 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4736 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4774 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4783 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4767 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4735 Still best_val_rmse: 0.4734 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4746 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4777 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4749 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4733 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4729 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4739 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.474 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4739 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4732 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4728 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4735 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4742 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4742 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4736 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.473 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4727 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4726 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4724 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4724 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4724 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4724 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4725 Still best_val_rmse: 0.4724 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 14:02:46,556]\u001b[0m Trial 11 finished with value: 0.4723910689353943 and parameters: {'base_lr': 1.8341141319343864e-05, 'last_lr': 0.0011650766745043514}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7c1aec77c4672a1fa65144a0d8013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.38 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9417 New best_val_rmse: 0.9417\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7311 New best_val_rmse: 0.7311\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7455 Still best_val_rmse: 0.7311 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7113 New best_val_rmse: 0.7113\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8049 Still best_val_rmse: 0.7113 (from epoch 0)\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6505 New best_val_rmse: 0.6505\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6253 New best_val_rmse: 0.6253\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6892 Still best_val_rmse: 0.6253 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.664 Still best_val_rmse: 0.6253 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 14:05:48,709]\u001b[0m Trial 12 finished with value: 0.6253228783607483 and parameters: {'base_lr': 9.549879790379279e-05, 'last_lr': 0.0019663403905362067}. Best is trial 4 with value: 0.4703187942504883.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7743ae7a7bab460b91ab6b9becc06d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.41 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8501 New best_val_rmse: 0.8501\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6586 New best_val_rmse: 0.6586\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6735 Still best_val_rmse: 0.6586 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5731 New best_val_rmse: 0.5731\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.598 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6531 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5963 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7161 Still best_val_rmse: 0.5731 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.556 New best_val_rmse: 0.556\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5045 New best_val_rmse: 0.5045\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.517 Still best_val_rmse: 0.5045 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5986 Still best_val_rmse: 0.5045 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5037 New best_val_rmse: 0.5037\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5275 Still best_val_rmse: 0.5037 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5038 Still best_val_rmse: 0.4935 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4913 New best_val_rmse: 0.4913\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5129 Still best_val_rmse: 0.4913 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.488 Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "4 steps took 2.32 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4813 New best_val_rmse: 0.4813\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4834 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4848 Still best_val_rmse: 0.4813 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4776 Still best_val_rmse: 0.4774 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4761 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4814 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4729 New best_val_rmse: 0.4729\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4771 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4779 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4716 New best_val_rmse: 0.4716\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4726 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4718 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4745 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4776 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.475 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4716 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4749 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4793 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4765 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4721 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4707 New best_val_rmse: 0.4707\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4736 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4742 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4732 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4704 New best_val_rmse: 0.4704\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4706 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4737 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4732 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4716 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4706 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4719 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4718 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4717 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4713 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4714 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4723 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4732 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4735 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4729 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.472 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4714 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4709 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4709 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.01 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4708 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4708 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4709 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4709 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.471 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.471 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4711 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 14:25:31,690]\u001b[0m Trial 13 finished with value: 0.469761461019516 and parameters: {'base_lr': 1.755971846813911e-05, 'last_lr': 0.0006141892845557757}. Best is trial 13 with value: 0.469761461019516.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4712 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff611205396a4dacbf8d9a0b7135737c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.35 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.052 New best_val_rmse: 1.052\n",
      "\n",
      "16 steps took 7.96 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8371 New best_val_rmse: 0.8371\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6503 New best_val_rmse: 0.6503\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6529 Still best_val_rmse: 0.6503 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5979 New best_val_rmse: 0.5979\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5914 New best_val_rmse: 0.5914\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6094 Still best_val_rmse: 0.5914 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5812 New best_val_rmse: 0.5812\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5705 New best_val_rmse: 0.5705\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5354 New best_val_rmse: 0.5354\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5317 New best_val_rmse: 0.5317\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5386 Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5349 Still best_val_rmse: 0.5317 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5247 New best_val_rmse: 0.5247\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5033 New best_val_rmse: 0.5033\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4942 New best_val_rmse: 0.4942\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5008 Still best_val_rmse: 0.4942 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.496 Still best_val_rmse: 0.4942 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 4.32 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4926 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4943 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4873 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4828 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4835 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4825 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4807 New best_val_rmse: 0.4807\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4814 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4802 New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4801 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4809 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4848 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4832 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4804 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 1.99 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4817 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4817 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4814 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4815 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4813 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4807 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.481 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4817 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4817 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4815 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4809 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4807 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4807 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4807 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4806 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4806 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4806 Still best_val_rmse: 0.48 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 14:39:01,594]\u001b[0m Trial 14 finished with value: 0.47996050119400024 and parameters: {'base_lr': 8.736957650525945e-06, 'last_lr': 0.0004694326964204411}. Best is trial 13 with value: 0.469761461019516.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037d6278c477437da30e4a7b61a61061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.34 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8399 New best_val_rmse: 0.8399\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6516 New best_val_rmse: 0.6516\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7146 Still best_val_rmse: 0.6516 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5865 New best_val_rmse: 0.5865\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6733 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6069 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6322 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7055 Still best_val_rmse: 0.5865 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5523 New best_val_rmse: 0.5523\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5052 New best_val_rmse: 0.5052\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5293 Still best_val_rmse: 0.5052 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6059 Still best_val_rmse: 0.5052 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5022 New best_val_rmse: 0.5022\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5225 Still best_val_rmse: 0.5022 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4943 Still best_val_rmse: 0.492 (from epoch 1)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4911 New best_val_rmse: 0.4911\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4937 Still best_val_rmse: 0.4911 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5206 Still best_val_rmse: 0.4911 (from epoch 1)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4865 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 2.33 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4833 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.485 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4823 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4762 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4832 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4757 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4701 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4714 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.475 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4776 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4783 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4753 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4717 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4698 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4699 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.47 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4703 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.471 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4699 Still best_val_rmse: 0.4698 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4698 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4712 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4737 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4751 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4732 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4697 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4692 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4694 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4695 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4695 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4696 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4697 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4698 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4701 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4716 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4715 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4709 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4707 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4708 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4708 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4707 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4708 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4707 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4708 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.471 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.471 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4709 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4709 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4707 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4706 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4706 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 0.821 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 15:00:27,732]\u001b[0m Trial 15 finished with value: 0.46920618414878845 and parameters: {'base_lr': 1.845975941382356e-05, 'last_lr': 0.0006309278277674714}. Best is trial 15 with value: 0.46920618414878845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7f31ec8ff54048935397edd3af7a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.34 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8327 New best_val_rmse: 0.8327\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6397 New best_val_rmse: 0.6397\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7712 Still best_val_rmse: 0.6397 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5924 New best_val_rmse: 0.5924\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5603 New best_val_rmse: 0.5603\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7454 Still best_val_rmse: 0.5603 (from epoch 0)\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5352 New best_val_rmse: 0.5352\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6057 Still best_val_rmse: 0.5352 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5539 Still best_val_rmse: 0.5352 (from epoch 0)\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5806 Still best_val_rmse: 0.5352 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5532 Still best_val_rmse: 0.5352 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5484 Still best_val_rmse: 0.5352 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.487 New best_val_rmse: 0.487\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4844 New best_val_rmse: 0.4844\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5152 Still best_val_rmse: 0.4844 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4902 Still best_val_rmse: 0.4844 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4955 Still best_val_rmse: 0.4844 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4867 Still best_val_rmse: 0.4844 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.5106 Still best_val_rmse: 0.4844 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4856 Still best_val_rmse: 0.4844 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4783 New best_val_rmse: 0.4783\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4817 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4858 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "4 steps took 2.33 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4906 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4883 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4865 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4903 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4865 Still best_val_rmse: 0.4783 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4763 New best_val_rmse: 0.4763\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4767 Still best_val_rmse: 0.4763 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4758 New best_val_rmse: 0.4758\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4788 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4883 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4765 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4798 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4931 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4768 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4784 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4825 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4771 Still best_val_rmse: 0.4758 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4742 New best_val_rmse: 0.4742\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4752 Still best_val_rmse: 0.4742 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4764 Still best_val_rmse: 0.4742 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4758 Still best_val_rmse: 0.4742 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.474 New best_val_rmse: 0.474\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4733 New best_val_rmse: 0.4733\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.472 New best_val_rmse: 0.472\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4719 New best_val_rmse: 0.4719\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4717 New best_val_rmse: 0.4717\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4718 Still best_val_rmse: 0.4717 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4711 New best_val_rmse: 0.4711\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.471 New best_val_rmse: 0.471\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4709 New best_val_rmse: 0.4709\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4709 New best_val_rmse: 0.4709\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.471 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4711 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.471 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4714 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4713 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4709 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4704 New best_val_rmse: 0.4704\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4699 New best_val_rmse: 0.4699\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4699 New best_val_rmse: 0.4699\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4699 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.47 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.47 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.47 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.47 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 15:19:28,960]\u001b[0m Trial 16 finished with value: 0.4699382781982422 and parameters: {'base_lr': 2.4833933515823515e-05, 'last_lr': 0.0007117616210852127}. Best is trial 15 with value: 0.46920618414878845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f3ad72569641dbafcdf99fd275eaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.34 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.052 New best_val_rmse: 1.052\n",
      "\n",
      "16 steps took 7.98 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6398 New best_val_rmse: 0.6398\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9464 Still best_val_rmse: 0.6398 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7123 Still best_val_rmse: 0.6398 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6794 Still best_val_rmse: 0.6398 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6726 Still best_val_rmse: 0.6398 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7121 Still best_val_rmse: 0.6398 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6967 Still best_val_rmse: 0.6398 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6503 Still best_val_rmse: 0.6398 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 15:22:32,226]\u001b[0m Trial 17 finished with value: 0.6398378014564514 and parameters: {'base_lr': 0.00011914266709867103, 'last_lr': 0.0005277625982160621}. Best is trial 15 with value: 0.46920618414878845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edffd25dab354ce991e83d659e38c93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.37 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9714 New best_val_rmse: 0.9714\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7257 New best_val_rmse: 0.7257\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6301 New best_val_rmse: 0.6301\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6015 New best_val_rmse: 0.6015\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5762 New best_val_rmse: 0.5762\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5876 Still best_val_rmse: 0.5762 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6596 Still best_val_rmse: 0.5762 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6537 Still best_val_rmse: 0.5762 (from epoch 0)\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5619 New best_val_rmse: 0.5619\n",
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5171 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6113 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5015 New best_val_rmse: 0.5015\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5178 Still best_val_rmse: 0.5015 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4934 New best_val_rmse: 0.4934\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5087 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4987 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "8 steps took 4.03 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5022 Still best_val_rmse: 0.4934 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4897 New best_val_rmse: 0.4897\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4968 Still best_val_rmse: 0.4897 (from epoch 1)\n",
      "\n",
      "8 steps took 4.36 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4791 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4772 New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4822 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4794 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4754 Still best_val_rmse: 0.4738 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4772 Still best_val_rmse: 0.4738 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4729 New best_val_rmse: 0.4729\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4725 New best_val_rmse: 0.4725\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4724 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4762 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4773 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4726 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.472 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4753 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4764 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4746 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4719 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.472 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4727 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4734 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4723 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4711 New best_val_rmse: 0.4711\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4713 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4733 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4728 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4728 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4721 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4714 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4714 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4716 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4719 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.472 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4715 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4715 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4719 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4722 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4722 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4719 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4715 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4713 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4714 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4714 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4713 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4712 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4712 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4712 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4712 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4712 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4713 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4714 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4714 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4715 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4716 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4716 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4717 Still best_val_rmse: 0.4711 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 15:41:54,521]\u001b[0m Trial 18 finished with value: 0.4710596203804016 and parameters: {'base_lr': 1.1878531343620975e-05, 'last_lr': 0.00035396203468292405}. Best is trial 15 with value: 0.46920618414878845.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c0c7d2b7fb478c8cd235e6dc8eb010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.29 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8745 New best_val_rmse: 0.8745\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6961 New best_val_rmse: 0.6961\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6701 New best_val_rmse: 0.6701\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5687 New best_val_rmse: 0.5687\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6342 Still best_val_rmse: 0.5687 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6058 Still best_val_rmse: 0.5687 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6632 Still best_val_rmse: 0.5687 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.671 Still best_val_rmse: 0.5687 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5496 New best_val_rmse: 0.5496\n",
      "\n",
      "16 steps took 8.38 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5156 New best_val_rmse: 0.5156\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5284 Still best_val_rmse: 0.5156 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6273 Still best_val_rmse: 0.5156 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.505 New best_val_rmse: 0.505\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5226 Still best_val_rmse: 0.505 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4932 New best_val_rmse: 0.4932\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5031 Still best_val_rmse: 0.4932 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5025 Still best_val_rmse: 0.4932 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4918 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "8 steps took 4.29 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4897 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4922 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4936 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4916 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "8 steps took 4.03 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4828 New best_val_rmse: 0.4828\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4872 Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4845 Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4759 New best_val_rmse: 0.4759\n",
      "\n",
      "2 steps took 0.991 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.479 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.481 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4772 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4768 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4795 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4787 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4783 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4775 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4776 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4779 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4783 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4783 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4776 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4776 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4779 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4783 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4784 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4781 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4776 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4774 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4774 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4775 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4771 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.477 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.477 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4769 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4769 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4769 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.477 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4771 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4771 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.989 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4772 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4773 Still best_val_rmse: 0.4757 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 15:59:01,034]\u001b[0m Trial 19 finished with value: 0.4756593704223633 and parameters: {'base_lr': 1.5255093672945908e-05, 'last_lr': 0.0008108752119000499}. Best is trial 15 with value: 0.46920618414878845.\u001b[0m\n",
      "\u001b[32m[I 2021-07-13 15:59:01,037]\u001b[0m A new study created in memory with name: no-name-a893385a-6b86-4d58-8c06-3611fd571506\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.46920618414878845\n",
      " Best params: \n",
      "    base_lr: 1.845975941382356e-05\n",
      "    last_lr: 0.0006309278277674714\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0861014bd41248f4bca5cd02b6fb03e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.28 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8557 New best_val_rmse: 0.8557\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6837 New best_val_rmse: 0.6837\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6163 New best_val_rmse: 0.6163\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.566 New best_val_rmse: 0.566\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5588 New best_val_rmse: 0.5588\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6468 Still best_val_rmse: 0.5588 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5589 Still best_val_rmse: 0.5588 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.578 Still best_val_rmse: 0.5588 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5499 New best_val_rmse: 0.5499\n",
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5383 New best_val_rmse: 0.5383\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.585 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5727 Still best_val_rmse: 0.5057 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4839 New best_val_rmse: 0.4839\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4801 New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4872 Still best_val_rmse: 0.4801 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4862 Still best_val_rmse: 0.4801 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4879 Still best_val_rmse: 0.4801 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 1 batch_num: 98 val_rmse: 0.4798 Still best_val_rmse: 0.4791 (from epoch 1)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 1 batch_num: 102 val_rmse: 0.5083 Still best_val_rmse: 0.4786 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.4787 Still best_val_rmse: 0.4786 (from epoch 1)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4756 New best_val_rmse: 0.4756\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4745 New best_val_rmse: 0.4745\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.4744 New best_val_rmse: 0.4744\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4747 Still best_val_rmse: 0.4744 (from epoch 1)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4758 Still best_val_rmse: 0.4744 (from epoch 1)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4741 New best_val_rmse: 0.4741\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.477 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 1.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4752 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4764 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4783 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4789 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 1.32 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4749 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4744 Still best_val_rmse: 0.4741 (from epoch 1)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.471 New best_val_rmse: 0.471\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4729 Still best_val_rmse: 0.471 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4827 Still best_val_rmse: 0.471 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4813 Still best_val_rmse: 0.471 (from epoch 2)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4674 New best_val_rmse: 0.4674\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.467 Still best_val_rmse: 0.4669 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4675 Still best_val_rmse: 0.4669 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.468 Still best_val_rmse: 0.4669 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4685 Still best_val_rmse: 0.4669 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4677 Still best_val_rmse: 0.4669 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4653 New best_val_rmse: 0.4653\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4652 New best_val_rmse: 0.4652\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.465 New best_val_rmse: 0.465\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.465 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4651 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4651 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4651 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4651 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4654 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4661 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4666 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4669 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4675 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4679 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4693 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4715 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4766 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.479 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4794 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.475 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4699 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.469 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4692 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.47 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4709 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4721 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4711 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4692 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4695 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4707 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.475 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4807 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4747 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4713 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4692 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4685 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4681 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.468 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4679 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4678 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.467 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4667 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4663 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4661 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.466 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4661 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4661 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4662 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4663 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4663 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4664 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4664 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4664 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4664 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4665 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4665 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4666 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4667 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4667 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4669 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.467 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4674 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4673 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4672 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.508 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.317 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 16:30:18,907]\u001b[0m Trial 0 finished with value: 0.465007483959198 and parameters: {'base_lr': 9.312465436613192e-06, 'last_lr': 0.0029042699025681466}. Best is trial 0 with value: 0.465007483959198.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4671 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2860812fd1e4c7fb020af6ccf84b04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.36 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9079 New best_val_rmse: 0.9079\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8916 New best_val_rmse: 0.8916\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6242 New best_val_rmse: 0.6242\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7972 Still best_val_rmse: 0.6242 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6644 Still best_val_rmse: 0.6242 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7782 Still best_val_rmse: 0.6242 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6638 Still best_val_rmse: 0.6242 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7407 Still best_val_rmse: 0.6242 (from epoch 0)\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7381 Still best_val_rmse: 0.6242 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 16:33:20,817]\u001b[0m Trial 1 finished with value: 0.6241759061813354 and parameters: {'base_lr': 0.00016555675424943537, 'last_lr': 0.0005712885346356431}. Best is trial 0 with value: 0.465007483959198.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfae3b975da4c9aa318689d4dcea899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.32 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7645 New best_val_rmse: 0.7645\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6515 New best_val_rmse: 0.6515\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6742 Still best_val_rmse: 0.6515 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.594 New best_val_rmse: 0.594\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5645 New best_val_rmse: 0.5645\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5809 Still best_val_rmse: 0.5645 (from epoch 0)\n",
      "\n",
      "16 steps took 8.07 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5695 Still best_val_rmse: 0.5645 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.611 Still best_val_rmse: 0.5645 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 8.36 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5037 New best_val_rmse: 0.5037\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.51 Still best_val_rmse: 0.5037 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4962 New best_val_rmse: 0.4962\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.59 Still best_val_rmse: 0.4962 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 72 val_rmse: 0.4943 Still best_val_rmse: 0.4836 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 82 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4853 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4865 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4737 New best_val_rmse: 0.4737\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 1 batch_num: 94 val_rmse: 0.4967 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 102 val_rmse: 0.534 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.5168 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4913 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.475 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4908 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "8 steps took 4.36 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4839 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5121 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5085 Still best_val_rmse: 0.4737 (from epoch 1)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4729 New best_val_rmse: 0.4729\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4713 New best_val_rmse: 0.4713\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4641 New best_val_rmse: 0.4641\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4648 Still best_val_rmse: 0.4641 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4674 Still best_val_rmse: 0.4641 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4705 Still best_val_rmse: 0.4641 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4742 Still best_val_rmse: 0.4641 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4683 Still best_val_rmse: 0.4641 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4649 Still best_val_rmse: 0.4641 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4637 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4654 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4674 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4685 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4678 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4663 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4661 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4658 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4655 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4659 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4663 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4667 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4676 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4681 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4678 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4676 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4671 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4668 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4664 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.466 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4657 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4652 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.465 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.465 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4651 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4651 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4652 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4655 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4658 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4661 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4664 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4665 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4666 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4663 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4664 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4666 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.467 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4671 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.467 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4671 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4671 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4671 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4667 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4664 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4661 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4658 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4653 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4648 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4642 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4638 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4631 New best_val_rmse: 0.4631\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4631 New best_val_rmse: 0.4631\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4632 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4633 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4634 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4635 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4636 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4637 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4638 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4638 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4638 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4637 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4637 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4636 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4635 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4634 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4634 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4633 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4632 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4632 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4632 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "1 steps took 0.317 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 17:00:19,615]\u001b[0m Trial 2 finished with value: 0.46307051181793213 and parameters: {'base_lr': 3.3743909778302605e-05, 'last_lr': 0.0005121870072732148}. Best is trial 2 with value: 0.46307051181793213.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4631 Still best_val_rmse: 0.4631 (from epoch 2)\n",
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a164ac3ca1432c9ed059a142e5242f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.23 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.008 New best_val_rmse: 1.008\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.346 Still best_val_rmse: 1.008 (from epoch 0)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.067 Still best_val_rmse: 1.008 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8383 New best_val_rmse: 0.8383\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.903 Still best_val_rmse: 0.8383 (from epoch 0)\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.025 Still best_val_rmse: 0.8383 (from epoch 0)\n",
      "\n",
      "16 steps took 7.92 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.017 Still best_val_rmse: 0.8383 (from epoch 0)\n",
      "\n",
      "16 steps took 7.91 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.032 Still best_val_rmse: 0.8383 (from epoch 0)\n",
      "\n",
      "16 steps took 7.95 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.02 Still best_val_rmse: 0.8383 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 17:03:21,235]\u001b[0m Trial 3 finished with value: 0.8382678031921387 and parameters: {'base_lr': 0.00033442959232821184, 'last_lr': 0.0003110821446659171}. Best is trial 2 with value: 0.46307051181793213.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0b94e5e08748d5a15883974608f139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.34 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7787 New best_val_rmse: 0.7787\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6617 New best_val_rmse: 0.6617\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6232 New best_val_rmse: 0.6232\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6554 Still best_val_rmse: 0.6232 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.613 New best_val_rmse: 0.613\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.565 New best_val_rmse: 0.565\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5412 New best_val_rmse: 0.5412\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6958 Still best_val_rmse: 0.5412 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.609 Still best_val_rmse: 0.5412 (from epoch 0)\n",
      "\n",
      "16 steps took 8.33 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5406 New best_val_rmse: 0.5406\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5232 New best_val_rmse: 0.5232\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5533 Still best_val_rmse: 0.5232 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5505 Still best_val_rmse: 0.5232 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5018 New best_val_rmse: 0.5018\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5004 Still best_val_rmse: 0.4961 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.5087 Still best_val_rmse: 0.4784 (from epoch 1)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.473 New best_val_rmse: 0.473\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4735 Still best_val_rmse: 0.473 (from epoch 1)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4702 New best_val_rmse: 0.4702\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4702 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4725 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4746 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 1.33 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4717 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4757 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4726 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4733 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4954 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4821 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4801 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4824 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4835 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.473 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4708 Still best_val_rmse: 0.4693 (from epoch 1)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4675 New best_val_rmse: 0.4675\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4666 New best_val_rmse: 0.4666\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4667 Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.467 Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4689 Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4706 Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4721 Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4669 Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4663 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4675 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4692 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4697 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4687 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4665 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4646 New best_val_rmse: 0.4646\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.464 New best_val_rmse: 0.464\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4644 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.465 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4662 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4675 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4679 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4674 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4674 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4676 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4681 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4688 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4695 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4699 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4703 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4696 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4685 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4675 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4666 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.466 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4657 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4657 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4658 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.466 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4662 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4664 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4664 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4666 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4668 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4671 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4674 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4679 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4686 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.469 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4696 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4703 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4709 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "2 steps took 0.99 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4697 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.501 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4687 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4683 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4684 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4685 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.469 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.489 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4689 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4688 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4688 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4685 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.468 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4672 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4665 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.466 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4656 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4652 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4648 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4644 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4641 Still best_val_rmse: 0.464 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4638 New best_val_rmse: 0.4638\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4637 New best_val_rmse: 0.4637\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4636 New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4636 New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4636 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4637 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4637 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4637 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4639 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.464 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.464 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4641 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4641 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4641 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4641 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4641 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.464 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.464 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4639 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4639 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4639 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "1 steps took 0.315 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 17:32:13,569]\u001b[0m Trial 4 finished with value: 0.4635846018791199 and parameters: {'base_lr': 2.967895929001887e-05, 'last_lr': 0.0030633417185138667}. Best is trial 2 with value: 0.46307051181793213.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4638 Still best_val_rmse: 0.4636 (from epoch 2)\n",
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced3e3ad5e214a62bf5d22edc3a1dcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.27 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9965 New best_val_rmse: 0.9965\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.213 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.093 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.076 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 7.93 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.036 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.02 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.044 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.019 Still best_val_rmse: 0.9965 (from epoch 0)\n",
      "\n",
      "16 steps took 7.94 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.026 Still best_val_rmse: 0.9965 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 17:35:14,695]\u001b[0m Trial 5 finished with value: 0.9964724779129028 and parameters: {'base_lr': 0.0004485841895433537, 'last_lr': 0.0003495453706911383}. Best is trial 2 with value: 0.46307051181793213.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9850239bfe04372a642469f3bab0391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.32 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8116 New best_val_rmse: 0.8116\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6713 New best_val_rmse: 0.6713\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6413 New best_val_rmse: 0.6413\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5976 New best_val_rmse: 0.5976\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6084 Still best_val_rmse: 0.5976 (from epoch 0)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5541 New best_val_rmse: 0.5541\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6146 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5835 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5376 New best_val_rmse: 0.5376\n",
      "\n",
      "16 steps took 8.37 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5577 Still best_val_rmse: 0.5376 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5408 Still best_val_rmse: 0.5376 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5109 New best_val_rmse: 0.5109\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.503 New best_val_rmse: 0.503\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4742 New best_val_rmse: 0.4742\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 94 val_rmse: 0.4978 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 102 val_rmse: 0.551 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.5324 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4917 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4763 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4937 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "8 steps took 4.33 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4836 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4881 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4735 New best_val_rmse: 0.4735\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4745 Still best_val_rmse: 0.4735 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.471 Still best_val_rmse: 0.4672 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.482 Still best_val_rmse: 0.4672 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.485 Still best_val_rmse: 0.4672 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4746 Still best_val_rmse: 0.4672 (from epoch 2)\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4747 Still best_val_rmse: 0.4672 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4705 Still best_val_rmse: 0.4672 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.465 New best_val_rmse: 0.465\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4657 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4693 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4742 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4773 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.992 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4758 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4705 Still best_val_rmse: 0.465 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4649 New best_val_rmse: 0.4649\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4642 New best_val_rmse: 0.4642\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4643 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4649 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4656 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4662 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4657 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.465 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4648 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4657 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4674 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.47 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "2 steps took 0.999 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4737 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4744 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4718 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4697 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4681 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4671 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4662 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4649 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4643 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4643 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4645 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4646 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4642 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4638 New best_val_rmse: 0.4638\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4633 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.464 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4648 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4654 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4662 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4672 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4675 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4677 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4675 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.467 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4659 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4647 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4639 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4635 Still best_val_rmse: 0.4632 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.463 New best_val_rmse: 0.463\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4629 New best_val_rmse: 0.4629\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4629 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4629 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4631 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4634 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4638 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4639 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.464 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4645 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4646 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4648 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.465 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4652 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4652 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4652 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.465 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4649 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4648 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4647 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4648 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4648 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4649 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.465 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4651 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4651 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4652 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4652 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4652 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.499 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4653 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4655 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "1 steps took 0.317 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 18:03:14,933]\u001b[0m Trial 6 finished with value: 0.4629150927066803 and parameters: {'base_lr': 4.430444436442592e-05, 'last_lr': 0.000289231685619846}. Best is trial 6 with value: 0.4629150927066803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4654 Still best_val_rmse: 0.4629 (from epoch 2)\n",
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f629e726797f41b48fce4827fffa8547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.33 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7818 New best_val_rmse: 0.7818\n",
      "\n",
      "16 steps took 7.99 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7 New best_val_rmse: 0.7\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6136 New best_val_rmse: 0.6136\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.845 Still best_val_rmse: 0.6136 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6006 New best_val_rmse: 0.6006\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7325 Still best_val_rmse: 0.6006 (from epoch 0)\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6362 Still best_val_rmse: 0.6006 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6959 Still best_val_rmse: 0.6006 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7211 Still best_val_rmse: 0.6006 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 18:06:17,718]\u001b[0m Trial 7 finished with value: 0.6005736589431763 and parameters: {'base_lr': 9.6674926540802e-05, 'last_lr': 0.00010231346059528046}. Best is trial 6 with value: 0.4629150927066803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bfdfb0534240d0a8522a949703be17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8044 New best_val_rmse: 0.8044\n",
      "\n",
      "16 steps took 8.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6903 New best_val_rmse: 0.6903\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6092 New best_val_rmse: 0.6092\n",
      "\n",
      "16 steps took 8.02 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5856 New best_val_rmse: 0.5856\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5966 Still best_val_rmse: 0.5856 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6428 Still best_val_rmse: 0.5856 (from epoch 0)\n",
      "\n",
      "16 steps took 8.01 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5922 Still best_val_rmse: 0.5856 (from epoch 0)\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.607 Still best_val_rmse: 0.5856 (from epoch 0)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5705 New best_val_rmse: 0.5705\n",
      "\n",
      "16 steps took 8.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5374 New best_val_rmse: 0.5374\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.514 New best_val_rmse: 0.514\n",
      "\n",
      "16 steps took 8.06 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5985 Still best_val_rmse: 0.514 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6032 Still best_val_rmse: 0.514 (from epoch 1)\n",
      "\n",
      "16 steps took 8.05 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.495 New best_val_rmse: 0.495\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4956 Still best_val_rmse: 0.4943 (from epoch 1)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4863 New best_val_rmse: 0.4863\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.4941 Still best_val_rmse: 0.4863 (from epoch 1)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.5083 Still best_val_rmse: 0.4863 (from epoch 1)\n",
      "\n",
      "16 steps took 8.04 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 2.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4815 Still best_val_rmse: 0.4795 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4833 Still best_val_rmse: 0.4795 (from epoch 1)\n",
      "\n",
      "4 steps took 2.33 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 1)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4778 New best_val_rmse: 0.4778\n",
      "\n",
      "2 steps took 0.996 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4755 New best_val_rmse: 0.4755\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4744 New best_val_rmse: 0.4744\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4823 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4942 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "8 steps took 4.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4746 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4763 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.502 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4913 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "8 steps took 4.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4717 New best_val_rmse: 0.4717\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4735 Still best_val_rmse: 0.4717 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4757 Still best_val_rmse: 0.4717 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4724 Still best_val_rmse: 0.4695 (from epoch 2)\n",
      "\n",
      "2 steps took 0.995 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4839 Still best_val_rmse: 0.4695 (from epoch 2)\n",
      "\n",
      "4 steps took 2.0 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4958 Still best_val_rmse: 0.4695 (from epoch 2)\n",
      "\n",
      "8 steps took 4.01 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4678 New best_val_rmse: 0.4678\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4681 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4683 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.498 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4689 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4687 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4682 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4678 New best_val_rmse: 0.4678\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4675 New best_val_rmse: 0.4675\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4674 New best_val_rmse: 0.4674\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4674 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4677 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.468 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4687 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4694 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4699 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.49 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4703 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 0.998 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4704 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 0.994 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4705 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 0.993 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4709 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4709 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 0.997 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4711 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4707 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "2 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4698 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4692 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4687 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.491 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4683 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.468 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4678 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4677 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4676 Still best_val_rmse: 0.4674 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4671 New best_val_rmse: 0.4671\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.467 New best_val_rmse: 0.467\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.467 New best_val_rmse: 0.467\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.467 New best_val_rmse: 0.467\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4668 New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4668 New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4667 New best_val_rmse: 0.4667\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4667 New best_val_rmse: 0.4667\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4668 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4668 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4668 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4669 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.467 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.467 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4671 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4671 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.494 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4672 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4672 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.492 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4672 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.495 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.488 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.496 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.493 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.497 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.316 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-13 18:29:06,335]\u001b[0m Trial 8 finished with value: 0.466734915971756 and parameters: {'base_lr': 9.146832246693641e-06, 'last_lr': 0.004898014673940014}. Best is trial 6 with value: 0.4629150927066803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 batch_num: 147 val_rmse: 0.4673 Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "##### Using fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'config', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41032d52087648c48342c4d359bb69a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 9.26 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.005 New best_val_rmse: 1.005\n",
      "\n",
      "16 steps took 8.03 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    399\u001b[0m             )\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-892f8897e58e>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mrmse_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4de684114aa8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mlast_eval_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mval_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch} batch_num: {batch_num}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"val_rmse: {val_rmse:0.4} \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4cfcef8fcc16>\u001b[0m in \u001b[0;36meval_mse\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmse_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmse_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(3, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
