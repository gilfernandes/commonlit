{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 4\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    ROBERTA_PATH = 't5-large'\n",
    "    TOKENIZER_PATH = 't5-large'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 't5-large'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.ROBERTA_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = T5EncoderModel.from_pretrained(cfg.ROBERTA_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.shared.weight torch.Size([32128, 1024])\n",
      "1 transformer_model.encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "2 transformer_model.encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "3 transformer_model.encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "4 transformer_model.encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "5 transformer_model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 16])\n",
      "6 transformer_model.encoder.block.0.layer.0.layer_norm.weight torch.Size([1024])\n",
      "7 transformer_model.encoder.block.0.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "8 transformer_model.encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "9 transformer_model.encoder.block.0.layer.1.layer_norm.weight torch.Size([1024])\n",
      "10 transformer_model.encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "11 transformer_model.encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "12 transformer_model.encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "13 transformer_model.encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "14 transformer_model.encoder.block.1.layer.0.layer_norm.weight torch.Size([1024])\n",
      "15 transformer_model.encoder.block.1.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "17 transformer_model.encoder.block.1.layer.1.layer_norm.weight torch.Size([1024])\n",
      "18 transformer_model.encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "19 transformer_model.encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "20 transformer_model.encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "21 transformer_model.encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "22 transformer_model.encoder.block.2.layer.0.layer_norm.weight torch.Size([1024])\n",
      "23 transformer_model.encoder.block.2.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "24 transformer_model.encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "25 transformer_model.encoder.block.2.layer.1.layer_norm.weight torch.Size([1024])\n",
      "26 transformer_model.encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "27 transformer_model.encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "28 transformer_model.encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "29 transformer_model.encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "30 transformer_model.encoder.block.3.layer.0.layer_norm.weight torch.Size([1024])\n",
      "31 transformer_model.encoder.block.3.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "32 transformer_model.encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "33 transformer_model.encoder.block.3.layer.1.layer_norm.weight torch.Size([1024])\n",
      "34 transformer_model.encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "35 transformer_model.encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "36 transformer_model.encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "37 transformer_model.encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "38 transformer_model.encoder.block.4.layer.0.layer_norm.weight torch.Size([1024])\n",
      "39 transformer_model.encoder.block.4.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "40 transformer_model.encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "41 transformer_model.encoder.block.4.layer.1.layer_norm.weight torch.Size([1024])\n",
      "42 transformer_model.encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "43 transformer_model.encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "44 transformer_model.encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "45 transformer_model.encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "46 transformer_model.encoder.block.5.layer.0.layer_norm.weight torch.Size([1024])\n",
      "47 transformer_model.encoder.block.5.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "48 transformer_model.encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "49 transformer_model.encoder.block.5.layer.1.layer_norm.weight torch.Size([1024])\n",
      "50 transformer_model.encoder.block.6.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "51 transformer_model.encoder.block.6.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "52 transformer_model.encoder.block.6.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "53 transformer_model.encoder.block.6.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "54 transformer_model.encoder.block.6.layer.0.layer_norm.weight torch.Size([1024])\n",
      "55 transformer_model.encoder.block.6.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "56 transformer_model.encoder.block.6.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "57 transformer_model.encoder.block.6.layer.1.layer_norm.weight torch.Size([1024])\n",
      "58 transformer_model.encoder.block.7.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "59 transformer_model.encoder.block.7.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "60 transformer_model.encoder.block.7.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "61 transformer_model.encoder.block.7.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "62 transformer_model.encoder.block.7.layer.0.layer_norm.weight torch.Size([1024])\n",
      "63 transformer_model.encoder.block.7.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "64 transformer_model.encoder.block.7.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "65 transformer_model.encoder.block.7.layer.1.layer_norm.weight torch.Size([1024])\n",
      "66 transformer_model.encoder.block.8.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "67 transformer_model.encoder.block.8.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "68 transformer_model.encoder.block.8.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "69 transformer_model.encoder.block.8.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "70 transformer_model.encoder.block.8.layer.0.layer_norm.weight torch.Size([1024])\n",
      "71 transformer_model.encoder.block.8.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "72 transformer_model.encoder.block.8.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "73 transformer_model.encoder.block.8.layer.1.layer_norm.weight torch.Size([1024])\n",
      "74 transformer_model.encoder.block.9.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "75 transformer_model.encoder.block.9.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "76 transformer_model.encoder.block.9.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "77 transformer_model.encoder.block.9.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "78 transformer_model.encoder.block.9.layer.0.layer_norm.weight torch.Size([1024])\n",
      "79 transformer_model.encoder.block.9.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "80 transformer_model.encoder.block.9.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "81 transformer_model.encoder.block.9.layer.1.layer_norm.weight torch.Size([1024])\n",
      "82 transformer_model.encoder.block.10.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "83 transformer_model.encoder.block.10.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "84 transformer_model.encoder.block.10.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "85 transformer_model.encoder.block.10.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "86 transformer_model.encoder.block.10.layer.0.layer_norm.weight torch.Size([1024])\n",
      "87 transformer_model.encoder.block.10.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "88 transformer_model.encoder.block.10.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "89 transformer_model.encoder.block.10.layer.1.layer_norm.weight torch.Size([1024])\n",
      "90 transformer_model.encoder.block.11.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "91 transformer_model.encoder.block.11.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "92 transformer_model.encoder.block.11.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "93 transformer_model.encoder.block.11.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "94 transformer_model.encoder.block.11.layer.0.layer_norm.weight torch.Size([1024])\n",
      "95 transformer_model.encoder.block.11.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "96 transformer_model.encoder.block.11.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "97 transformer_model.encoder.block.11.layer.1.layer_norm.weight torch.Size([1024])\n",
      "98 transformer_model.encoder.block.12.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "99 transformer_model.encoder.block.12.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "100 transformer_model.encoder.block.12.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "101 transformer_model.encoder.block.12.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "102 transformer_model.encoder.block.12.layer.0.layer_norm.weight torch.Size([1024])\n",
      "103 transformer_model.encoder.block.12.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "104 transformer_model.encoder.block.12.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "105 transformer_model.encoder.block.12.layer.1.layer_norm.weight torch.Size([1024])\n",
      "106 transformer_model.encoder.block.13.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "107 transformer_model.encoder.block.13.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "108 transformer_model.encoder.block.13.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "109 transformer_model.encoder.block.13.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "110 transformer_model.encoder.block.13.layer.0.layer_norm.weight torch.Size([1024])\n",
      "111 transformer_model.encoder.block.13.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "112 transformer_model.encoder.block.13.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "113 transformer_model.encoder.block.13.layer.1.layer_norm.weight torch.Size([1024])\n",
      "114 transformer_model.encoder.block.14.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "115 transformer_model.encoder.block.14.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "116 transformer_model.encoder.block.14.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "117 transformer_model.encoder.block.14.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "118 transformer_model.encoder.block.14.layer.0.layer_norm.weight torch.Size([1024])\n",
      "119 transformer_model.encoder.block.14.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "120 transformer_model.encoder.block.14.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "121 transformer_model.encoder.block.14.layer.1.layer_norm.weight torch.Size([1024])\n",
      "122 transformer_model.encoder.block.15.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "123 transformer_model.encoder.block.15.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "124 transformer_model.encoder.block.15.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "125 transformer_model.encoder.block.15.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "126 transformer_model.encoder.block.15.layer.0.layer_norm.weight torch.Size([1024])\n",
      "127 transformer_model.encoder.block.15.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "128 transformer_model.encoder.block.15.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "129 transformer_model.encoder.block.15.layer.1.layer_norm.weight torch.Size([1024])\n",
      "130 transformer_model.encoder.block.16.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "131 transformer_model.encoder.block.16.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "132 transformer_model.encoder.block.16.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "133 transformer_model.encoder.block.16.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "134 transformer_model.encoder.block.16.layer.0.layer_norm.weight torch.Size([1024])\n",
      "135 transformer_model.encoder.block.16.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "136 transformer_model.encoder.block.16.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "137 transformer_model.encoder.block.16.layer.1.layer_norm.weight torch.Size([1024])\n",
      "138 transformer_model.encoder.block.17.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "139 transformer_model.encoder.block.17.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "140 transformer_model.encoder.block.17.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "141 transformer_model.encoder.block.17.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "142 transformer_model.encoder.block.17.layer.0.layer_norm.weight torch.Size([1024])\n",
      "143 transformer_model.encoder.block.17.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "144 transformer_model.encoder.block.17.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "145 transformer_model.encoder.block.17.layer.1.layer_norm.weight torch.Size([1024])\n",
      "146 transformer_model.encoder.block.18.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "147 transformer_model.encoder.block.18.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "148 transformer_model.encoder.block.18.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "149 transformer_model.encoder.block.18.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "150 transformer_model.encoder.block.18.layer.0.layer_norm.weight torch.Size([1024])\n",
      "151 transformer_model.encoder.block.18.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "152 transformer_model.encoder.block.18.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "153 transformer_model.encoder.block.18.layer.1.layer_norm.weight torch.Size([1024])\n",
      "154 transformer_model.encoder.block.19.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "155 transformer_model.encoder.block.19.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "156 transformer_model.encoder.block.19.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "157 transformer_model.encoder.block.19.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "158 transformer_model.encoder.block.19.layer.0.layer_norm.weight torch.Size([1024])\n",
      "159 transformer_model.encoder.block.19.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "160 transformer_model.encoder.block.19.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "161 transformer_model.encoder.block.19.layer.1.layer_norm.weight torch.Size([1024])\n",
      "162 transformer_model.encoder.block.20.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "163 transformer_model.encoder.block.20.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "164 transformer_model.encoder.block.20.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "165 transformer_model.encoder.block.20.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "166 transformer_model.encoder.block.20.layer.0.layer_norm.weight torch.Size([1024])\n",
      "167 transformer_model.encoder.block.20.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "168 transformer_model.encoder.block.20.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "169 transformer_model.encoder.block.20.layer.1.layer_norm.weight torch.Size([1024])\n",
      "170 transformer_model.encoder.block.21.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "171 transformer_model.encoder.block.21.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "172 transformer_model.encoder.block.21.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "173 transformer_model.encoder.block.21.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "174 transformer_model.encoder.block.21.layer.0.layer_norm.weight torch.Size([1024])\n",
      "175 transformer_model.encoder.block.21.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "176 transformer_model.encoder.block.21.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "177 transformer_model.encoder.block.21.layer.1.layer_norm.weight torch.Size([1024])\n",
      "178 transformer_model.encoder.block.22.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "179 transformer_model.encoder.block.22.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "180 transformer_model.encoder.block.22.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "181 transformer_model.encoder.block.22.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "182 transformer_model.encoder.block.22.layer.0.layer_norm.weight torch.Size([1024])\n",
      "183 transformer_model.encoder.block.22.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "184 transformer_model.encoder.block.22.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "185 transformer_model.encoder.block.22.layer.1.layer_norm.weight torch.Size([1024])\n",
      "186 transformer_model.encoder.block.23.layer.0.SelfAttention.q.weight torch.Size([1024, 1024])\n",
      "187 transformer_model.encoder.block.23.layer.0.SelfAttention.k.weight torch.Size([1024, 1024])\n",
      "188 transformer_model.encoder.block.23.layer.0.SelfAttention.v.weight torch.Size([1024, 1024])\n",
      "189 transformer_model.encoder.block.23.layer.0.SelfAttention.o.weight torch.Size([1024, 1024])\n",
      "190 transformer_model.encoder.block.23.layer.0.layer_norm.weight torch.Size([1024])\n",
      "191 transformer_model.encoder.block.23.layer.1.DenseReluDense.wi.weight torch.Size([4096, 1024])\n",
      "192 transformer_model.encoder.block.23.layer.1.DenseReluDense.wo.weight torch.Size([1024, 4096])\n",
      "193 transformer_model.encoder.block.23.layer.1.layer_norm.weight torch.Size([1024])\n",
      "194 transformer_model.encoder.final_layer_norm.weight torch.Size([1024])\n",
      "195 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "196 attention.hidden_layer.bias torch.Size([512])\n",
      "197 attention.final_layer.weight torch.Size([1, 512])\n",
      "198 attention.final_layer.bias torch.Size([1])\n",
      "199 regressor.weight torch.Size([1, 1024])\n",
      "200 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 34.8250, -39.8186, -31.5651,  ...,   3.9555,   3.5706,  26.8950],\n",
       "        [-13.0407,   0.9347,  -1.1054,  ...,  -5.6926,  32.7766,  -8.8879],\n",
       "        [ 43.8069,  17.7453,  11.5391,  ..., -21.7446,  34.9114,  -6.1605],\n",
       "        ...,\n",
       "        [-14.2535, -12.2508, -15.4470,  ..., -40.4770,  25.5900,  66.0776],\n",
       "        [ 18.7770, -12.6678,  -9.4641,  ...,  26.9359, -33.2451,  -3.8131],\n",
       "        [-10.4062,  -9.2380,   1.4874,  ...,  11.0226, -21.4092,  33.4435]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 199\n",
    "    attention_param_start = 195\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= 130:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= 82:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    def train_loss_backward(self, input_ids, attention_mask, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.scaler == None:\n",
    "            pred, _ = self.model(input_ids, attention_mask)\n",
    "            mse = self.mse_loss(pred.flatten(), target)\n",
    "            mse.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "        else:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                mse = self.mse_loss(pred.flatten(), target)\n",
    "            self.scaler.scale(mse).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.train_loss_backward(input_ids, attention_mask, target)\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(base_lr, last_lr, fold = 0):\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "#     scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c9d397f-aa6e-4823-a4fd-b8625c580972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: {'base_lr': 0.00013575061062518292, 'last_lr': 0.0027390926762560675} Best value:  0.48893508315086365\n",
    "# Fold 1: {'base_lr': 6.433162302000639e-05, 'last_lr': 0.0025302612125878217}. Best is trial 0 with value: 0.4527459144592285\n",
    "# Fold 2: {'base_lr': 0.00012105407461535033, 'last_lr': 0.00012780642309774768}. Best is trial 4 with value: 0.476582378149032\n",
    "# Fold 3: {'base_lr': 0.00016420220823284873, 'last_lr': 0.004783602075813355}. Best is trial 13 with value: 0.4700598418712616\n",
    "# Fold 4: {'base_lr': 8.176324330617398e-05, 'last_lr': 0.0012432581220121835}. Best is trial 17 with value: 0.4916570484638214\n",
    "# Fold 5: {'base_lr': 0.0002297546136917806, 'last_lr': 0.00034915806261776055}. Best is trial 12 with value: 0.4850253164768219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fbc2b14-ea34-48b6-82e9-10809a6d9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [\n",
    "    {'base_lr': 0.00013575061062518292, 'last_lr': 0.0027390926762560675},\n",
    "    {'base_lr': 6.433162302000639e-05, 'last_lr': 0.0025302612125878217},\n",
    "    {'base_lr': 0.00012105407461535033, 'last_lr': 0.00012780642309774768},\n",
    "    {'base_lr': 0.00016420220823284873, 'last_lr': 0.004783602075813355},\n",
    "    {'base_lr': 8.176324330617398e-05, 'last_lr': 0.0012432581220121835}, \n",
    "    {'base_lr': 0.0002297546136917806, 'last_lr': 0.00034915806261776055}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.00013575061062518292 last_lr 0.0027390926762560675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc39c3b2bf44f84ad67689deaa12a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.114 New best_val_rmse: 1.114\n",
      "\n",
      "16 steps took 11.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7997 New best_val_rmse: 0.7997\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6515 New best_val_rmse: 0.6515\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6193 New best_val_rmse: 0.6193\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6318 Still best_val_rmse: 0.6193 (from epoch 0)\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6026 New best_val_rmse: 0.6026\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5812 New best_val_rmse: 0.5812\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5631 New best_val_rmse: 0.5631\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6505 Still best_val_rmse: 0.5631 (from epoch 0)\n",
      "\n",
      "16 steps took 12.5 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5374 New best_val_rmse: 0.5374\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5321 New best_val_rmse: 0.5321\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5036 New best_val_rmse: 0.5036\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.53 Still best_val_rmse: 0.5036 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5101 Still best_val_rmse: 0.5036 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5326 Still best_val_rmse: 0.5036 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5101 Still best_val_rmse: 0.5036 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5198 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5209 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5039 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.497 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5027 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5048 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5021 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4992 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.5016 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4981 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4954 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4968 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4983 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4994 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.38 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4994 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4967 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.495 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4938 New best_val_rmse: 0.4938\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.494 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4961 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4945 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4943 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4952 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4951 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4946 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4944 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4948 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4949 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4951 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4952 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4952 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4952 Still best_val_rmse: 0.4938 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.49384191632270813\n",
      "##### Using fold 1\n",
      "##### Using base_lr 6.433162302000639e-05 last_lr 0.0025302612125878217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0908abb68c46f9ae33b5d76e575bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.08 New best_val_rmse: 1.08\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8312 New best_val_rmse: 0.8312\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7014 New best_val_rmse: 0.7014\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6252 New best_val_rmse: 0.6252\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6035 New best_val_rmse: 0.6035\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.586 New best_val_rmse: 0.586\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5444 New best_val_rmse: 0.5444\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5685 Still best_val_rmse: 0.5444 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 1 batch_num: 0 val_rmse: 0.4978 Still best_val_rmse: 0.4883 (from epoch 0)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 8 val_rmse: 0.4985 Still best_val_rmse: 0.4883 (from epoch 0)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 16 val_rmse: 0.4942 Still best_val_rmse: 0.4883 (from epoch 0)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 24 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 26 val_rmse: 0.4823 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 30 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 32 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 1 batch_num: 33 val_rmse: 0.4674 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 1 batch_num: 34 val_rmse: 0.4682 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "1 steps took 0.726 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.4686 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.4688 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 1 batch_num: 37 val_rmse: 0.4694 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 1 batch_num: 38 val_rmse: 0.4709 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 40 val_rmse: 0.4757 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 42 val_rmse: 0.4775 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.478 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 46 val_rmse: 0.4771 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 48 val_rmse: 0.4713 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 50 val_rmse: 0.4717 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.4782 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 54 val_rmse: 0.4821 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 1 batch_num: 58 val_rmse: 0.4795 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4795 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 62 val_rmse: 0.4812 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 66 val_rmse: 0.4778 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.4781 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 70 val_rmse: 0.4828 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 74 val_rmse: 0.4815 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 78 val_rmse: 0.4784 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4784 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 1 batch_num: 82 val_rmse: 0.4887 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 86 val_rmse: 0.4852 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 1 batch_num: 90 val_rmse: 0.4984 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 98 val_rmse: 0.5201 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 114 val_rmse: 0.4701 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4765 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.4716 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.4741 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.4793 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4875 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4834 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.471 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4707 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4676 Still best_val_rmse: 0.4657 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.4638 New best_val_rmse: 0.4638\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4615 New best_val_rmse: 0.4615\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 1 batch_num: 139 val_rmse: 0.4624 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4665 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4703 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 143 val_rmse: 0.4771 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 145 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.462 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.427 seconds\n",
      "Epoch: 1 batch_num: 147 val_rmse: 0.464 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 1.46 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.469 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 1 val_rmse: 0.4734 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 3 val_rmse: 0.4692 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4643 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 5 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4673 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.4697 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4713 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.467 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 2 batch_num: 11 val_rmse: 0.4658 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4658 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 13 val_rmse: 0.4658 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4663 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4671 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4676 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4687 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4689 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4687 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4691 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4693 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4695 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4691 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4684 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4678 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4675 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4673 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4672 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4674 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4674 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4671 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4672 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4672 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.467 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4669 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4668 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4667 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4668 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4672 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4677 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4673 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4666 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4661 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.466 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4661 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.466 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4659 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4661 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4657 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4664 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4668 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.467 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4672 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4685 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4695 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4689 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.468 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.467 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4657 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.726 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.726 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.466 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4667 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4679 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4681 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4678 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4666 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4644 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4655 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4663 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4668 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4676 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4682 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4689 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4694 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4701 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4708 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4685 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4674 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4671 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.468 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4718 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4761 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4752 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4712 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4685 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4675 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4664 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4663 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4668 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4672 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4678 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4679 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4673 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4665 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4657 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4641 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4638 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.464 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4659 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4669 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4678 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.727 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4679 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.468 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4682 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4687 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.727 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4687 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4687 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4689 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4691 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4692 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4687 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4678 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4673 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.467 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4668 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4667 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4664 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4661 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4657 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4657 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4662 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4665 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4663 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4665 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4664 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4661 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4652 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 1.47 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4644 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4643 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4643 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4643 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4643 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4643 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4655 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4652 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4652 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4644 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4644 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4644 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4644 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4645 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4646 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4652 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4653 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4655 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.755 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4656 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4655 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4654 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4653 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4653 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4652 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4652 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4651 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.465 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4647 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.737 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4648 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.73 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.74 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.729 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.739 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.732 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.736 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.735 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.727 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.728 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.734 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.731 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.733 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.738 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "1 steps took 0.432 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 1)\n",
      "\n",
      "Final RMSE: 0.4615400433540344\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00012105407461535033 last_lr 0.00012780642309774768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e1893265004d9c9fdc1d73d34a83eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.3 New best_val_rmse: 1.3\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.013 New best_val_rmse: 1.013\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7137 New best_val_rmse: 0.7137\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7536 Still best_val_rmse: 0.7137 (from epoch 0)\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6186 New best_val_rmse: 0.6186\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5987 New best_val_rmse: 0.5987\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6573 Still best_val_rmse: 0.5987 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5885 New best_val_rmse: 0.5885\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5478 New best_val_rmse: 0.5478\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5279 New best_val_rmse: 0.5279\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5342 Still best_val_rmse: 0.5279 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5118 Still best_val_rmse: 0.5042 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5144 Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5227 Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4963 Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4972 Still best_val_rmse: 0.494 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.485 New best_val_rmse: 0.485\n",
      "\n",
      "4 steps took 3.38 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4952 Still best_val_rmse: 0.485 (from epoch 1)\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5108 Still best_val_rmse: 0.485 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4965 Still best_val_rmse: 0.485 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4957 Still best_val_rmse: 0.485 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4986 Still best_val_rmse: 0.485 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4884 Still best_val_rmse: 0.485 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4872 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4818 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4841 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4856 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4866 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4895 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4892 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4876 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4877 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.484 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4834 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4828 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4828 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4857 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4878 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4846 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4819 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4836 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4834 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4862 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4873 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.486 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4847 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4838 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4835 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4825 Still best_val_rmse: 0.4817 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4814 Still best_val_rmse: 0.4809 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.481 Still best_val_rmse: 0.4809 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4804 New best_val_rmse: 0.4804\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4798 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4801 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4802 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4801 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4799 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4798 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4798 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4798 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4798 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4799 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.48 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4801 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4801 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4802 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4803 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4804 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4805 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4806 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4806 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4807 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4808 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4808 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4808 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4808 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.47974342107772827\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00016420220823284873 last_lr 0.004783602075813355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc7db35b351462eb6a130e9ed47be12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9745 New best_val_rmse: 0.9745\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7092 New best_val_rmse: 0.7092\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6529 New best_val_rmse: 0.6529\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6999 Still best_val_rmse: 0.6529 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6003 New best_val_rmse: 0.6003\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5593 Still best_val_rmse: 0.5197 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.55 Still best_val_rmse: 0.5197 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5593 Still best_val_rmse: 0.5197 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5654 Still best_val_rmse: 0.5197 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5217 Still best_val_rmse: 0.5197 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5277 Still best_val_rmse: 0.5197 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4886 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.4912 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4941 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.489 Still best_val_rmse: 0.4848 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.534 Still best_val_rmse: 0.4836 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4937 Still best_val_rmse: 0.4836 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4883 Still best_val_rmse: 0.4836 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4751 New best_val_rmse: 0.4751\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 1.9 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4742 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4751 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4752 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4794 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4897 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.486 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4853 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4864 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4879 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.14 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4883 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4851 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4843 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4843 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4867 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4835 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4907 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4883 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4787 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4885 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4956 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "8 steps took 6.11 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4948 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.484 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4827 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4811 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4816 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4768 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4758 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4757 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4759 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4757 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4762 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4773 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4781 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4765 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4771 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4783 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4794 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4786 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4783 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4775 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4779 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4787 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4788 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4795 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4805 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.481 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4796 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4792 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4789 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4784 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4784 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4787 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4788 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4788 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4785 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4781 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4779 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4777 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4776 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4775 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4773 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4774 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4776 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4778 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4777 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4777 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4776 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4776 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4777 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4778 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4781 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.47 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4783 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4783 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4783 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4784 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4785 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4784 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4784 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4783 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4782 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4781 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4781 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4781 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "2 steps took 1.48 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.478 Still best_val_rmse: 0.4736 (from epoch 1)\n",
      "\n",
      "Final RMSE: 0.4736339747905731\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.176324330617398e-05 last_lr 0.0012432581220121835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607a86f0f8d245d386bab10727cac85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.223 New best_val_rmse: 1.223\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9659 New best_val_rmse: 0.9659\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6943 New best_val_rmse: 0.6943\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7113 Still best_val_rmse: 0.6943 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6151 New best_val_rmse: 0.6151\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5904 New best_val_rmse: 0.5904\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5465 New best_val_rmse: 0.5465\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.542 New best_val_rmse: 0.542\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5324 New best_val_rmse: 0.5324\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4988 New best_val_rmse: 0.4988\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.509 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5288 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5169 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5077 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5192 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5104 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5022 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5048 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5015 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4913 New best_val_rmse: 0.4913\n",
      "\n",
      "8 steps took 5.96 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4933 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4962 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4946 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4975 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4951 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4938 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4935 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4946 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.493 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4916 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4922 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4913 New best_val_rmse: 0.4913\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4924 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "8 steps took 6.37 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4901 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4893 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4895 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4895 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4893 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4894 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4895 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4893 Still best_val_rmse: 0.4891 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4886 New best_val_rmse: 0.4886\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4884 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4885 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4886 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4888 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4888 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4888 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4888 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4888 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4888 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.98 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "4 steps took 2.99 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 3)\n",
      "\n",
      "Final RMSE: 0.4883658289909363\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002297546136917806 last_lr 0.00034915806261776055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e4f66f0e784b3692c4781cc7c6565c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.056 New best_val_rmse: 1.056\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8289 New best_val_rmse: 0.8289\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6989 New best_val_rmse: 0.6989\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.677 New best_val_rmse: 0.677\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5826 New best_val_rmse: 0.5826\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5872 Still best_val_rmse: 0.5826 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6534 Still best_val_rmse: 0.5826 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5857 Still best_val_rmse: 0.5826 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6036 Still best_val_rmse: 0.5826 (from epoch 0)\n",
      "\n",
      "16 steps took 12.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5479 New best_val_rmse: 0.5479\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5146 New best_val_rmse: 0.5146\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4903 New best_val_rmse: 0.4903\n",
      "\n",
      "8 steps took 5.96 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5072 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5153 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5042 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5001 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5317 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "16 steps took 12.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4977 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4938 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4945 Still best_val_rmse: 0.4903 (from epoch 1)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4899 New best_val_rmse: 0.4899\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4933 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4954 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.97 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4961 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4985 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.5017 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4989 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.5075 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4961 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4981 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.5002 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4979 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4949 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.4 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4952 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4966 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4971 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4969 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4963 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4964 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4951 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4942 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4937 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4938 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4942 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4946 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4948 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4947 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4947 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.98 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4946 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 5.99 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4946 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4946 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.0 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4946 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "Final RMSE: 0.48991984128952026\n",
      "CPU times: user 2h 47min 10s, sys: 20min 38s, total: 3h 7min 49s\n",
      "Wall time: 2h 30min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rmse_values = []\n",
    "for i in range(len(list(splits))):\n",
    "    fold = i\n",
    "    lrs = lr_list[fold]\n",
    "    rmse_val = train_fold(lrs['base_lr'], lrs['last_lr'], fold=fold)\n",
    "    print(f'Final RMSE: {rmse_val}')\n",
    "    rmse_values.append(rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b28ebe4-316a-47e7-b20e-64a20ee4c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean RMSE values: 0.48117420077323914'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'mean RMSE values: {np.mean(np.array(rmse_values))}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-large and are newly initialized: ['encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 s, sys: 8.05 s, total: 28.6 s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beadd59d3a004527994ef8c9173140e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9199566dad734bb9a99297f00aef1fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.2415086947333281\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.2216085863410393\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.22009991871818577\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.1682507285438524\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.22258471061984836\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.23731845792659875\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8ccb690c1845b7b59c7a17ff1323e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b802f8d2ed324501836ce1115973424f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.24705024155731803\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.2846898785276363\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.24225463809179723\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.2546013494043661\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.2693135819058382\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.26921631713111804\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3a43bc49c44f7b8261e9c528be99fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d125efceecb4083beba206bc4b3b84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.21582619151293175\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.21511829136970892\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.19165811391410964\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.21890566878879789\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.22949254710767267\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.23732083073771526\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15113f2dd3934ca2934c223d123a44b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452c980929d541369e1aa172c737531d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.2585554361619335\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.2742133021747491\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2681325740426326\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.23288886927630803\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.2556749320895782\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.2521655987835131\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5465b493ed463286d3938ffe4fabac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51729996884d569d29de50cf6f19f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.23735370363725433\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.2260231656925247\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.17705968841756026\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.18456155158433835\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.23833255880485676\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.23580790900008441\n",
      "FINAL RMSE score 0.23425293455323987\n",
      "CPU times: user 7min 36s, sys: 4.37 s, total: 7min 40s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0004000474161001688],\n",
       " [-2.7107279494187878e-05],\n",
       " [0.0019157176886315172],\n",
       " [-0.0002821996557550001],\n",
       " [0.001758119471241923]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20334918, 0.19425093, 0.20345766, 0.1951579 , 0.20378434])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.8712663489484819)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27755795, -0.58138935, -0.44272449, -2.08837953, -1.85758621,\n",
       "       -1.34266514,  0.49023989])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.08821985317649383, -0.017643970635298767)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.365778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.669609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.530944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.176599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.945806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.430885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.402020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.365778\n",
       "1  f0953f0a5 -0.669609\n",
       "2  0df072751 -0.530944\n",
       "3  04caf4e0c -2.176599\n",
       "4  0e63f8bea -1.945806\n",
       "5  12537fe78 -1.430885\n",
       "6  965e592c0  0.402020"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-large'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/t5-large/best')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/t5-large_1'),\n",
       " PosixPath('/home/commonlit/models/t5-large_2'),\n",
       " PosixPath('/home/commonlit/models/t5-large_3'),\n",
       " PosixPath('/home/commonlit/models/t5-large_4'),\n",
       " PosixPath('/home/commonlit/models/t5-large_5'),\n",
       " PosixPath('/home/commonlit/models/t5-large_6')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        special_tokens = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/special_tokens_map.json'))\n",
    "        assert special_tokens.exists()\n",
    "        copyfile(special_tokens, tokenizer_path/'special_tokens_map')\n",
    "        \n",
    "        spiece_model = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/spiece.model'))\n",
    "        assert spiece_model.exists()\n",
    "        copyfile(spiece_model, tokenizer_path/'spiece.model')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/t5-large/best_models.zip'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip  dataset-metadata.json  lm.zip\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/t5-large.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1M\t/home/commonlit/models/t5-large/best/tokenizer-1\n",
      "2.1M\t/home/commonlit/models/t5-large/best/tokenizer-2\n",
      "2.1M\t/home/commonlit/models/t5-large/best/tokenizer-3\n",
      "2.1M\t/home/commonlit/models/t5-large/best/tokenizer-4\n",
      "2.1M\t/home/commonlit/models/t5-large/best/tokenizer-5\n",
      "2.1M\t/home/commonlit/models/t5-large/best/tokenizer-6\n",
      "7.6G\t/home/commonlit/models/t5-large/best\n",
      "6.8G\t/home/commonlit/models/t5-large/best_models.zip\n",
      "4.0K\t/home/commonlit/models/t5-large/dataset-metadata.json\n",
      "1.3G\t/home/commonlit/models/t5-large/lm\n",
      "1.2G\t/home/commonlit/models/t5-large/lm.zip\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/t5-large/lm.zip'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/t5-large/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-t5-large\",\n",
      "  \"id\": \"gilfernandes/commonlit-t5-large\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.77G/6.77G [11:26<00:00, 10.6MB/s]\n",
      "Upload successful: best_models.zip (7GB)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.12G/1.12G [01:59<00:00, 10.1MB/s]\n",
      "Upload successful: lm.zip (1GB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-t5-large\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.77G/6.77G [11:07<00:00, 10.9MB/s]\n",
      "Upload successful: best_models.zip (7GB)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.12G/1.12G [01:56<00:00, 10.3MB/s]\n",
      "Upload successful: lm.zip (1GB)\n",
      "Dataset version is being created. Please check progress at /api/v1/datasets/status/gilfernandes/commonlit-t5-large\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with spiece.model\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
