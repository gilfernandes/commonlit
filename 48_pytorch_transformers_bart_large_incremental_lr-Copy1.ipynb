{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'facebook/bart-large'\n",
    "    TOKENIZER_PATH = 'facebook/bart-large'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'bart-large'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "current_layer = ''\n",
    "base_lr = 5e-5\n",
    "layer_map = {}\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('.layer') > -1):\n",
    "        layer_name = re.sub(r'.+\\.(layer\\.\\d+).+', r'\\1', name)\n",
    "        if(current_layer != layer_name):\n",
    "            current_layer = layer_name\n",
    "            layer_map[i] = current_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1e6a36-c30d-4040-a252-ef66a0d37b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_lrs = []\n",
    "for layer in layer_map.keys():\n",
    "    if (layer < 170):\n",
    "        layer_lrs.append((layer, base_lr / 5.0))\n",
    "    elif (layer < 334):\n",
    "        layer_lrs.append((layer, base_lr))\n",
    "    else:\n",
    "        layer_lrs.append((layer, base_lr * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe38610c-252d-4496-b271-6a164f1eba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1e-05),\n",
       " (3, 1e-05),\n",
       " (4, 1e-05),\n",
       " (5, 1e-05),\n",
       " (6, 1e-05),\n",
       " (7, 1e-05),\n",
       " (8, 1e-05),\n",
       " (9, 1e-05),\n",
       " (10, 1e-05),\n",
       " (11, 1e-05),\n",
       " (12, 1e-05),\n",
       " (13, 1e-05),\n",
       " (14, 1e-05),\n",
       " (15, 1e-05),\n",
       " (16, 1e-05),\n",
       " (17, 1e-05),\n",
       " (18, 1e-05),\n",
       " (19, 1e-05),\n",
       " (20, 1e-05),\n",
       " (21, 1e-05),\n",
       " (22, 1e-05),\n",
       " (23, 1e-05),\n",
       " (24, 1e-05),\n",
       " (25, 1e-05),\n",
       " (26, 1e-05),\n",
       " (27, 1e-05),\n",
       " (28, 1e-05),\n",
       " (29, 1e-05),\n",
       " (30, 1e-05),\n",
       " (31, 1e-05),\n",
       " (32, 1e-05),\n",
       " (33, 1e-05),\n",
       " (34, 1e-05),\n",
       " (35, 1e-05),\n",
       " (36, 1e-05),\n",
       " (37, 1e-05),\n",
       " (38, 1e-05),\n",
       " (39, 1e-05),\n",
       " (40, 1e-05),\n",
       " (41, 1e-05),\n",
       " (42, 1e-05),\n",
       " (43, 1e-05),\n",
       " (44, 1e-05),\n",
       " (45, 1e-05),\n",
       " (46, 1e-05),\n",
       " (47, 1e-05),\n",
       " (48, 1e-05),\n",
       " (49, 1e-05),\n",
       " (50, 1e-05),\n",
       " (51, 1e-05),\n",
       " (52, 1e-05),\n",
       " (53, 1e-05),\n",
       " (54, 1e-05),\n",
       " (55, 1e-05),\n",
       " (56, 1e-05),\n",
       " (57, 1e-05),\n",
       " (58, 1e-05),\n",
       " (59, 1e-05),\n",
       " (60, 1e-05),\n",
       " (61, 1e-05),\n",
       " (62, 1e-05),\n",
       " (63, 1e-05),\n",
       " (64, 1e-05),\n",
       " (65, 1e-05),\n",
       " (66, 1e-05),\n",
       " (67, 1e-05),\n",
       " (68, 1e-05),\n",
       " (69, 1e-05),\n",
       " (70, 1e-05),\n",
       " (71, 1e-05),\n",
       " (72, 1e-05),\n",
       " (73, 1e-05),\n",
       " (74, 1e-05),\n",
       " (75, 1e-05),\n",
       " (76, 1e-05),\n",
       " (77, 1e-05),\n",
       " (78, 1e-05),\n",
       " (79, 1e-05),\n",
       " (80, 1e-05),\n",
       " (81, 1e-05),\n",
       " (82, 1e-05),\n",
       " (83, 1e-05),\n",
       " (84, 1e-05),\n",
       " (85, 1e-05),\n",
       " (86, 1e-05),\n",
       " (87, 1e-05),\n",
       " (88, 1e-05),\n",
       " (89, 1e-05),\n",
       " (90, 1e-05),\n",
       " (91, 1e-05),\n",
       " (92, 1e-05),\n",
       " (93, 1e-05),\n",
       " (94, 1e-05),\n",
       " (95, 1e-05),\n",
       " (96, 1e-05),\n",
       " (97, 1e-05),\n",
       " (98, 1e-05),\n",
       " (99, 1e-05),\n",
       " (100, 1e-05),\n",
       " (101, 1e-05),\n",
       " (102, 1e-05),\n",
       " (103, 1e-05),\n",
       " (104, 1e-05),\n",
       " (105, 1e-05),\n",
       " (106, 1e-05),\n",
       " (107, 1e-05),\n",
       " (108, 1e-05),\n",
       " (109, 1e-05),\n",
       " (110, 1e-05),\n",
       " (111, 1e-05),\n",
       " (112, 1e-05),\n",
       " (113, 1e-05),\n",
       " (114, 1e-05),\n",
       " (115, 1e-05),\n",
       " (116, 1e-05),\n",
       " (117, 1e-05),\n",
       " (118, 1e-05),\n",
       " (119, 1e-05),\n",
       " (120, 1e-05),\n",
       " (121, 1e-05),\n",
       " (122, 1e-05),\n",
       " (123, 1e-05),\n",
       " (124, 1e-05),\n",
       " (125, 1e-05),\n",
       " (126, 1e-05),\n",
       " (127, 1e-05),\n",
       " (128, 1e-05),\n",
       " (129, 1e-05),\n",
       " (130, 1e-05),\n",
       " (131, 1e-05),\n",
       " (132, 1e-05),\n",
       " (133, 1e-05),\n",
       " (134, 1e-05),\n",
       " (135, 1e-05),\n",
       " (136, 1e-05),\n",
       " (137, 1e-05),\n",
       " (138, 1e-05),\n",
       " (139, 1e-05),\n",
       " (140, 1e-05),\n",
       " (141, 1e-05),\n",
       " (142, 1e-05),\n",
       " (143, 1e-05),\n",
       " (144, 1e-05),\n",
       " (145, 1e-05),\n",
       " (146, 1e-05),\n",
       " (147, 1e-05),\n",
       " (148, 1e-05),\n",
       " (149, 1e-05),\n",
       " (150, 1e-05),\n",
       " (151, 1e-05),\n",
       " (152, 1e-05),\n",
       " (153, 1e-05),\n",
       " (154, 1e-05),\n",
       " (155, 1e-05),\n",
       " (156, 1e-05),\n",
       " (157, 1e-05),\n",
       " (158, 1e-05),\n",
       " (159, 1e-05),\n",
       " (160, 1e-05),\n",
       " (161, 1e-05),\n",
       " (162, 1e-05),\n",
       " (163, 1e-05),\n",
       " (164, 1e-05),\n",
       " (165, 1e-05),\n",
       " (166, 1e-05),\n",
       " (167, 1e-05),\n",
       " (168, 1e-05),\n",
       " (169, 1e-05),\n",
       " (170, 5e-05),\n",
       " (171, 5e-05),\n",
       " (172, 5e-05),\n",
       " (173, 5e-05),\n",
       " (174, 5e-05),\n",
       " (175, 5e-05),\n",
       " (176, 5e-05),\n",
       " (177, 5e-05),\n",
       " (178, 5e-05),\n",
       " (179, 5e-05),\n",
       " (180, 5e-05),\n",
       " (181, 5e-05),\n",
       " (182, 5e-05),\n",
       " (183, 5e-05),\n",
       " (184, 5e-05),\n",
       " (185, 5e-05),\n",
       " (186, 5e-05),\n",
       " (187, 5e-05),\n",
       " (188, 5e-05),\n",
       " (189, 5e-05),\n",
       " (190, 5e-05),\n",
       " (191, 5e-05),\n",
       " (192, 5e-05),\n",
       " (193, 5e-05),\n",
       " (194, 5e-05),\n",
       " (195, 5e-05),\n",
       " (197, 5e-05),\n",
       " (198, 5e-05),\n",
       " (199, 5e-05),\n",
       " (200, 5e-05),\n",
       " (201, 5e-05),\n",
       " (202, 5e-05),\n",
       " (203, 5e-05),\n",
       " (204, 5e-05),\n",
       " (205, 5e-05),\n",
       " (206, 5e-05),\n",
       " (207, 5e-05),\n",
       " (208, 5e-05),\n",
       " (209, 5e-05),\n",
       " (210, 5e-05),\n",
       " (211, 5e-05),\n",
       " (212, 5e-05),\n",
       " (213, 5e-05),\n",
       " (214, 5e-05),\n",
       " (215, 5e-05),\n",
       " (216, 5e-05),\n",
       " (217, 5e-05),\n",
       " (218, 5e-05),\n",
       " (219, 5e-05),\n",
       " (220, 5e-05),\n",
       " (221, 5e-05),\n",
       " (222, 5e-05),\n",
       " (223, 5e-05),\n",
       " (224, 5e-05),\n",
       " (225, 5e-05),\n",
       " (226, 5e-05),\n",
       " (227, 5e-05),\n",
       " (228, 5e-05),\n",
       " (229, 5e-05),\n",
       " (230, 5e-05),\n",
       " (231, 5e-05),\n",
       " (232, 5e-05),\n",
       " (233, 5e-05),\n",
       " (234, 5e-05),\n",
       " (235, 5e-05),\n",
       " (236, 5e-05),\n",
       " (237, 5e-05),\n",
       " (238, 5e-05),\n",
       " (239, 5e-05),\n",
       " (240, 5e-05),\n",
       " (241, 5e-05),\n",
       " (242, 5e-05),\n",
       " (243, 5e-05),\n",
       " (244, 5e-05),\n",
       " (245, 5e-05),\n",
       " (246, 5e-05),\n",
       " (247, 5e-05),\n",
       " (248, 5e-05),\n",
       " (249, 5e-05),\n",
       " (250, 5e-05),\n",
       " (251, 5e-05),\n",
       " (252, 5e-05),\n",
       " (253, 5e-05),\n",
       " (254, 5e-05),\n",
       " (255, 5e-05),\n",
       " (256, 5e-05),\n",
       " (257, 5e-05),\n",
       " (258, 5e-05),\n",
       " (259, 5e-05),\n",
       " (260, 5e-05),\n",
       " (261, 5e-05),\n",
       " (262, 5e-05),\n",
       " (263, 5e-05),\n",
       " (264, 5e-05),\n",
       " (265, 5e-05),\n",
       " (266, 5e-05),\n",
       " (267, 5e-05),\n",
       " (268, 5e-05),\n",
       " (269, 5e-05),\n",
       " (270, 5e-05),\n",
       " (271, 5e-05),\n",
       " (272, 5e-05),\n",
       " (273, 5e-05),\n",
       " (274, 5e-05),\n",
       " (275, 5e-05),\n",
       " (276, 5e-05),\n",
       " (277, 5e-05),\n",
       " (278, 5e-05),\n",
       " (279, 5e-05),\n",
       " (280, 5e-05),\n",
       " (281, 5e-05),\n",
       " (282, 5e-05),\n",
       " (283, 5e-05),\n",
       " (284, 5e-05),\n",
       " (285, 5e-05),\n",
       " (286, 5e-05),\n",
       " (287, 5e-05),\n",
       " (288, 5e-05),\n",
       " (289, 5e-05),\n",
       " (290, 5e-05),\n",
       " (291, 5e-05),\n",
       " (292, 5e-05),\n",
       " (293, 5e-05),\n",
       " (294, 5e-05),\n",
       " (295, 5e-05),\n",
       " (296, 5e-05),\n",
       " (297, 5e-05),\n",
       " (298, 5e-05),\n",
       " (299, 5e-05),\n",
       " (300, 5e-05),\n",
       " (301, 5e-05),\n",
       " (302, 5e-05),\n",
       " (303, 5e-05),\n",
       " (304, 5e-05),\n",
       " (305, 5e-05),\n",
       " (306, 5e-05),\n",
       " (307, 5e-05),\n",
       " (308, 5e-05),\n",
       " (309, 5e-05),\n",
       " (310, 5e-05),\n",
       " (311, 5e-05),\n",
       " (312, 5e-05),\n",
       " (313, 5e-05),\n",
       " (314, 5e-05),\n",
       " (315, 5e-05),\n",
       " (316, 5e-05),\n",
       " (317, 5e-05),\n",
       " (318, 5e-05),\n",
       " (319, 5e-05),\n",
       " (320, 5e-05),\n",
       " (321, 5e-05),\n",
       " (322, 5e-05),\n",
       " (323, 5e-05),\n",
       " (324, 5e-05),\n",
       " (325, 5e-05),\n",
       " (326, 5e-05),\n",
       " (327, 5e-05),\n",
       " (328, 5e-05),\n",
       " (329, 5e-05),\n",
       " (330, 5e-05),\n",
       " (331, 5e-05),\n",
       " (332, 5e-05),\n",
       " (333, 5e-05),\n",
       " (334, 0.001),\n",
       " (335, 0.001),\n",
       " (336, 0.001),\n",
       " (337, 0.001),\n",
       " (338, 0.001),\n",
       " (339, 0.001),\n",
       " (340, 0.001),\n",
       " (341, 0.001),\n",
       " (342, 0.001),\n",
       " (343, 0.001),\n",
       " (344, 0.001),\n",
       " (345, 0.001),\n",
       " (346, 0.001),\n",
       " (347, 0.001),\n",
       " (348, 0.001),\n",
       " (349, 0.001),\n",
       " (350, 0.001),\n",
       " (351, 0.001),\n",
       " (352, 0.001),\n",
       " (353, 0.001),\n",
       " (354, 0.001),\n",
       " (355, 0.001),\n",
       " (356, 0.001),\n",
       " (357, 0.001),\n",
       " (358, 0.001),\n",
       " (359, 0.001),\n",
       " (360, 0.001),\n",
       " (361, 0.001),\n",
       " (362, 0.001),\n",
       " (363, 0.001),\n",
       " (364, 0.001),\n",
       " (365, 0.001),\n",
       " (366, 0.001),\n",
       " (367, 0.001),\n",
       " (368, 0.001),\n",
       " (369, 0.001),\n",
       " (370, 0.001),\n",
       " (371, 0.001),\n",
       " (372, 0.001),\n",
       " (373, 0.001),\n",
       " (374, 0.001),\n",
       " (375, 0.001),\n",
       " (376, 0.001),\n",
       " (377, 0.001),\n",
       " (378, 0.001),\n",
       " (379, 0.001),\n",
       " (380, 0.001),\n",
       " (381, 0.001),\n",
       " (382, 0.001),\n",
       " (383, 0.001),\n",
       " (384, 0.001),\n",
       " (385, 0.001),\n",
       " (386, 0.001),\n",
       " (387, 0.001),\n",
       " (388, 0.001),\n",
       " (389, 0.001),\n",
       " (390, 0.001),\n",
       " (391, 0.001),\n",
       " (392, 0.001),\n",
       " (393, 0.001),\n",
       " (394, 0.001),\n",
       " (395, 0.001),\n",
       " (396, 0.001),\n",
       " (397, 0.001),\n",
       " (398, 0.001),\n",
       " (399, 0.001),\n",
       " (400, 0.001),\n",
       " (401, 0.001),\n",
       " (402, 0.001),\n",
       " (403, 0.001),\n",
       " (404, 0.001),\n",
       " (405, 0.001),\n",
       " (406, 0.001),\n",
       " (407, 0.001),\n",
       " (408, 0.001),\n",
       " (409, 0.001),\n",
       " (410, 0.001),\n",
       " (411, 0.001),\n",
       " (412, 0.001),\n",
       " (413, 0.001),\n",
       " (414, 0.001),\n",
       " (415, 0.001),\n",
       " (416, 0.001),\n",
       " (417, 0.001),\n",
       " (418, 0.001),\n",
       " (419, 0.001),\n",
       " (420, 0.001),\n",
       " (421, 0.001),\n",
       " (422, 0.001),\n",
       " (423, 0.001),\n",
       " (424, 0.001),\n",
       " (425, 0.001),\n",
       " (426, 0.001),\n",
       " (427, 0.001),\n",
       " (428, 0.001),\n",
       " (429, 0.001),\n",
       " (430, 0.001),\n",
       " (431, 0.001),\n",
       " (432, 0.001),\n",
       " (433, 0.001),\n",
       " (434, 0.001),\n",
       " (435, 0.001),\n",
       " (436, 0.001),\n",
       " (437, 0.001),\n",
       " (438, 0.001),\n",
       " (439, 0.001),\n",
       " (440, 0.001),\n",
       " (441, 0.001),\n",
       " (442, 0.001),\n",
       " (443, 0.001),\n",
       " (444, 0.001),\n",
       " (445, 0.001),\n",
       " (446, 0.001),\n",
       " (447, 0.001),\n",
       " (448, 0.001),\n",
       " (449, 0.001),\n",
       " (450, 0.001),\n",
       " (451, 0.001),\n",
       " (452, 0.001),\n",
       " (453, 0.001),\n",
       " (454, 0.001),\n",
       " (455, 0.001),\n",
       " (456, 0.001),\n",
       " (457, 0.001),\n",
       " (458, 0.001),\n",
       " (459, 0.001),\n",
       " (460, 0.001),\n",
       " (461, 0.001),\n",
       " (462, 0.001),\n",
       " (463, 0.001),\n",
       " (464, 0.001),\n",
       " (465, 0.001),\n",
       " (466, 0.001),\n",
       " (467, 0.001),\n",
       " (468, 0.001),\n",
       " (469, 0.001),\n",
       " (470, 0.001),\n",
       " (471, 0.001),\n",
       " (472, 0.001),\n",
       " (473, 0.001),\n",
       " (474, 0.001),\n",
       " (475, 0.001),\n",
       " (476, 0.001),\n",
       " (477, 0.001),\n",
       " (478, 0.001),\n",
       " (479, 0.001),\n",
       " (480, 0.001),\n",
       " (481, 0.001),\n",
       " (482, 0.001),\n",
       " (483, 0.001),\n",
       " (484, 0.001),\n",
       " (485, 0.001),\n",
       " (486, 0.001),\n",
       " (487, 0.001),\n",
       " (488, 0.001),\n",
       " (489, 0.001),\n",
       " (490, 0.001),\n",
       " (491, 0.001),\n",
       " (492, 0.001),\n",
       " (493, 0.001),\n",
       " (494, 0.001),\n",
       " (495, 0.001),\n",
       " (496, 0.001),\n",
       " (497, 0.001),\n",
       " (498, 0.001),\n",
       " (499, 0.001),\n",
       " (500, 0.001),\n",
       " (501, 0.001),\n",
       " (502, 0.001),\n",
       " (503, 0.001),\n",
       " (504, 0.001),\n",
       " (505, 0.001),\n",
       " (506, 0.001),\n",
       " (507, 0.001),\n",
       " (508, 0.001),\n",
       " (509, 0.001),\n",
       " (510, 0.001)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b53258c0-ca17-4c62-b63c-b58481fae790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr_for_layer(layer_num, layer_lrs):\n",
    "    lr = layer_lrs[0][1]\n",
    "    for layer_lr in layer_lrs:\n",
    "        if layer_num >= layer_lr[0]:\n",
    "            lr = layer_lr[1]\n",
    "        else:\n",
    "            break\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.shared.weight torch.Size([50265, 1024])\n",
      "1 transformer_model.encoder.embed_positions.weight torch.Size([1026, 1024])\n",
      "2 transformer_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "3 transformer_model.encoder.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "4 transformer_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "5 transformer_model.encoder.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "6 transformer_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "7 transformer_model.encoder.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "8 transformer_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "9 transformer_model.encoder.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "10 transformer_model.encoder.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "11 transformer_model.encoder.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "12 transformer_model.encoder.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "13 transformer_model.encoder.layers.0.fc1.bias torch.Size([4096])\n",
      "14 transformer_model.encoder.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "15 transformer_model.encoder.layers.0.fc2.bias torch.Size([1024])\n",
      "16 transformer_model.encoder.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "17 transformer_model.encoder.layers.0.final_layer_norm.bias torch.Size([1024])\n",
      "18 transformer_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "19 transformer_model.encoder.layers.1.self_attn.k_proj.bias torch.Size([1024])\n",
      "20 transformer_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "21 transformer_model.encoder.layers.1.self_attn.v_proj.bias torch.Size([1024])\n",
      "22 transformer_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "23 transformer_model.encoder.layers.1.self_attn.q_proj.bias torch.Size([1024])\n",
      "24 transformer_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "25 transformer_model.encoder.layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "26 transformer_model.encoder.layers.1.self_attn_layer_norm.weight torch.Size([1024])\n",
      "27 transformer_model.encoder.layers.1.self_attn_layer_norm.bias torch.Size([1024])\n",
      "28 transformer_model.encoder.layers.1.fc1.weight torch.Size([4096, 1024])\n",
      "29 transformer_model.encoder.layers.1.fc1.bias torch.Size([4096])\n",
      "30 transformer_model.encoder.layers.1.fc2.weight torch.Size([1024, 4096])\n",
      "31 transformer_model.encoder.layers.1.fc2.bias torch.Size([1024])\n",
      "32 transformer_model.encoder.layers.1.final_layer_norm.weight torch.Size([1024])\n",
      "33 transformer_model.encoder.layers.1.final_layer_norm.bias torch.Size([1024])\n",
      "34 transformer_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "35 transformer_model.encoder.layers.2.self_attn.k_proj.bias torch.Size([1024])\n",
      "36 transformer_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "37 transformer_model.encoder.layers.2.self_attn.v_proj.bias torch.Size([1024])\n",
      "38 transformer_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "39 transformer_model.encoder.layers.2.self_attn.q_proj.bias torch.Size([1024])\n",
      "40 transformer_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "41 transformer_model.encoder.layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "42 transformer_model.encoder.layers.2.self_attn_layer_norm.weight torch.Size([1024])\n",
      "43 transformer_model.encoder.layers.2.self_attn_layer_norm.bias torch.Size([1024])\n",
      "44 transformer_model.encoder.layers.2.fc1.weight torch.Size([4096, 1024])\n",
      "45 transformer_model.encoder.layers.2.fc1.bias torch.Size([4096])\n",
      "46 transformer_model.encoder.layers.2.fc2.weight torch.Size([1024, 4096])\n",
      "47 transformer_model.encoder.layers.2.fc2.bias torch.Size([1024])\n",
      "48 transformer_model.encoder.layers.2.final_layer_norm.weight torch.Size([1024])\n",
      "49 transformer_model.encoder.layers.2.final_layer_norm.bias torch.Size([1024])\n",
      "50 transformer_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "51 transformer_model.encoder.layers.3.self_attn.k_proj.bias torch.Size([1024])\n",
      "52 transformer_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "53 transformer_model.encoder.layers.3.self_attn.v_proj.bias torch.Size([1024])\n",
      "54 transformer_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "55 transformer_model.encoder.layers.3.self_attn.q_proj.bias torch.Size([1024])\n",
      "56 transformer_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "57 transformer_model.encoder.layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "58 transformer_model.encoder.layers.3.self_attn_layer_norm.weight torch.Size([1024])\n",
      "59 transformer_model.encoder.layers.3.self_attn_layer_norm.bias torch.Size([1024])\n",
      "60 transformer_model.encoder.layers.3.fc1.weight torch.Size([4096, 1024])\n",
      "61 transformer_model.encoder.layers.3.fc1.bias torch.Size([4096])\n",
      "62 transformer_model.encoder.layers.3.fc2.weight torch.Size([1024, 4096])\n",
      "63 transformer_model.encoder.layers.3.fc2.bias torch.Size([1024])\n",
      "64 transformer_model.encoder.layers.3.final_layer_norm.weight torch.Size([1024])\n",
      "65 transformer_model.encoder.layers.3.final_layer_norm.bias torch.Size([1024])\n",
      "66 transformer_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "67 transformer_model.encoder.layers.4.self_attn.k_proj.bias torch.Size([1024])\n",
      "68 transformer_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "69 transformer_model.encoder.layers.4.self_attn.v_proj.bias torch.Size([1024])\n",
      "70 transformer_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "71 transformer_model.encoder.layers.4.self_attn.q_proj.bias torch.Size([1024])\n",
      "72 transformer_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "73 transformer_model.encoder.layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "74 transformer_model.encoder.layers.4.self_attn_layer_norm.weight torch.Size([1024])\n",
      "75 transformer_model.encoder.layers.4.self_attn_layer_norm.bias torch.Size([1024])\n",
      "76 transformer_model.encoder.layers.4.fc1.weight torch.Size([4096, 1024])\n",
      "77 transformer_model.encoder.layers.4.fc1.bias torch.Size([4096])\n",
      "78 transformer_model.encoder.layers.4.fc2.weight torch.Size([1024, 4096])\n",
      "79 transformer_model.encoder.layers.4.fc2.bias torch.Size([1024])\n",
      "80 transformer_model.encoder.layers.4.final_layer_norm.weight torch.Size([1024])\n",
      "81 transformer_model.encoder.layers.4.final_layer_norm.bias torch.Size([1024])\n",
      "82 transformer_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "83 transformer_model.encoder.layers.5.self_attn.k_proj.bias torch.Size([1024])\n",
      "84 transformer_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "85 transformer_model.encoder.layers.5.self_attn.v_proj.bias torch.Size([1024])\n",
      "86 transformer_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "87 transformer_model.encoder.layers.5.self_attn.q_proj.bias torch.Size([1024])\n",
      "88 transformer_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "89 transformer_model.encoder.layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "90 transformer_model.encoder.layers.5.self_attn_layer_norm.weight torch.Size([1024])\n",
      "91 transformer_model.encoder.layers.5.self_attn_layer_norm.bias torch.Size([1024])\n",
      "92 transformer_model.encoder.layers.5.fc1.weight torch.Size([4096, 1024])\n",
      "93 transformer_model.encoder.layers.5.fc1.bias torch.Size([4096])\n",
      "94 transformer_model.encoder.layers.5.fc2.weight torch.Size([1024, 4096])\n",
      "95 transformer_model.encoder.layers.5.fc2.bias torch.Size([1024])\n",
      "96 transformer_model.encoder.layers.5.final_layer_norm.weight torch.Size([1024])\n",
      "97 transformer_model.encoder.layers.5.final_layer_norm.bias torch.Size([1024])\n",
      "98 transformer_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "99 transformer_model.encoder.layers.6.self_attn.k_proj.bias torch.Size([1024])\n",
      "100 transformer_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "101 transformer_model.encoder.layers.6.self_attn.v_proj.bias torch.Size([1024])\n",
      "102 transformer_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "103 transformer_model.encoder.layers.6.self_attn.q_proj.bias torch.Size([1024])\n",
      "104 transformer_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "105 transformer_model.encoder.layers.6.self_attn.out_proj.bias torch.Size([1024])\n",
      "106 transformer_model.encoder.layers.6.self_attn_layer_norm.weight torch.Size([1024])\n",
      "107 transformer_model.encoder.layers.6.self_attn_layer_norm.bias torch.Size([1024])\n",
      "108 transformer_model.encoder.layers.6.fc1.weight torch.Size([4096, 1024])\n",
      "109 transformer_model.encoder.layers.6.fc1.bias torch.Size([4096])\n",
      "110 transformer_model.encoder.layers.6.fc2.weight torch.Size([1024, 4096])\n",
      "111 transformer_model.encoder.layers.6.fc2.bias torch.Size([1024])\n",
      "112 transformer_model.encoder.layers.6.final_layer_norm.weight torch.Size([1024])\n",
      "113 transformer_model.encoder.layers.6.final_layer_norm.bias torch.Size([1024])\n",
      "114 transformer_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "115 transformer_model.encoder.layers.7.self_attn.k_proj.bias torch.Size([1024])\n",
      "116 transformer_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "117 transformer_model.encoder.layers.7.self_attn.v_proj.bias torch.Size([1024])\n",
      "118 transformer_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "119 transformer_model.encoder.layers.7.self_attn.q_proj.bias torch.Size([1024])\n",
      "120 transformer_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "121 transformer_model.encoder.layers.7.self_attn.out_proj.bias torch.Size([1024])\n",
      "122 transformer_model.encoder.layers.7.self_attn_layer_norm.weight torch.Size([1024])\n",
      "123 transformer_model.encoder.layers.7.self_attn_layer_norm.bias torch.Size([1024])\n",
      "124 transformer_model.encoder.layers.7.fc1.weight torch.Size([4096, 1024])\n",
      "125 transformer_model.encoder.layers.7.fc1.bias torch.Size([4096])\n",
      "126 transformer_model.encoder.layers.7.fc2.weight torch.Size([1024, 4096])\n",
      "127 transformer_model.encoder.layers.7.fc2.bias torch.Size([1024])\n",
      "128 transformer_model.encoder.layers.7.final_layer_norm.weight torch.Size([1024])\n",
      "129 transformer_model.encoder.layers.7.final_layer_norm.bias torch.Size([1024])\n",
      "130 transformer_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "131 transformer_model.encoder.layers.8.self_attn.k_proj.bias torch.Size([1024])\n",
      "132 transformer_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "133 transformer_model.encoder.layers.8.self_attn.v_proj.bias torch.Size([1024])\n",
      "134 transformer_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "135 transformer_model.encoder.layers.8.self_attn.q_proj.bias torch.Size([1024])\n",
      "136 transformer_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "137 transformer_model.encoder.layers.8.self_attn.out_proj.bias torch.Size([1024])\n",
      "138 transformer_model.encoder.layers.8.self_attn_layer_norm.weight torch.Size([1024])\n",
      "139 transformer_model.encoder.layers.8.self_attn_layer_norm.bias torch.Size([1024])\n",
      "140 transformer_model.encoder.layers.8.fc1.weight torch.Size([4096, 1024])\n",
      "141 transformer_model.encoder.layers.8.fc1.bias torch.Size([4096])\n",
      "142 transformer_model.encoder.layers.8.fc2.weight torch.Size([1024, 4096])\n",
      "143 transformer_model.encoder.layers.8.fc2.bias torch.Size([1024])\n",
      "144 transformer_model.encoder.layers.8.final_layer_norm.weight torch.Size([1024])\n",
      "145 transformer_model.encoder.layers.8.final_layer_norm.bias torch.Size([1024])\n",
      "146 transformer_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "147 transformer_model.encoder.layers.9.self_attn.k_proj.bias torch.Size([1024])\n",
      "148 transformer_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "149 transformer_model.encoder.layers.9.self_attn.v_proj.bias torch.Size([1024])\n",
      "150 transformer_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "151 transformer_model.encoder.layers.9.self_attn.q_proj.bias torch.Size([1024])\n",
      "152 transformer_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "153 transformer_model.encoder.layers.9.self_attn.out_proj.bias torch.Size([1024])\n",
      "154 transformer_model.encoder.layers.9.self_attn_layer_norm.weight torch.Size([1024])\n",
      "155 transformer_model.encoder.layers.9.self_attn_layer_norm.bias torch.Size([1024])\n",
      "156 transformer_model.encoder.layers.9.fc1.weight torch.Size([4096, 1024])\n",
      "157 transformer_model.encoder.layers.9.fc1.bias torch.Size([4096])\n",
      "158 transformer_model.encoder.layers.9.fc2.weight torch.Size([1024, 4096])\n",
      "159 transformer_model.encoder.layers.9.fc2.bias torch.Size([1024])\n",
      "160 transformer_model.encoder.layers.9.final_layer_norm.weight torch.Size([1024])\n",
      "161 transformer_model.encoder.layers.9.final_layer_norm.bias torch.Size([1024])\n",
      "162 transformer_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "163 transformer_model.encoder.layers.10.self_attn.k_proj.bias torch.Size([1024])\n",
      "164 transformer_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "165 transformer_model.encoder.layers.10.self_attn.v_proj.bias torch.Size([1024])\n",
      "166 transformer_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "167 transformer_model.encoder.layers.10.self_attn.q_proj.bias torch.Size([1024])\n",
      "168 transformer_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "169 transformer_model.encoder.layers.10.self_attn.out_proj.bias torch.Size([1024])\n",
      "170 transformer_model.encoder.layers.10.self_attn_layer_norm.weight torch.Size([1024])\n",
      "171 transformer_model.encoder.layers.10.self_attn_layer_norm.bias torch.Size([1024])\n",
      "172 transformer_model.encoder.layers.10.fc1.weight torch.Size([4096, 1024])\n",
      "173 transformer_model.encoder.layers.10.fc1.bias torch.Size([4096])\n",
      "174 transformer_model.encoder.layers.10.fc2.weight torch.Size([1024, 4096])\n",
      "175 transformer_model.encoder.layers.10.fc2.bias torch.Size([1024])\n",
      "176 transformer_model.encoder.layers.10.final_layer_norm.weight torch.Size([1024])\n",
      "177 transformer_model.encoder.layers.10.final_layer_norm.bias torch.Size([1024])\n",
      "178 transformer_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "179 transformer_model.encoder.layers.11.self_attn.k_proj.bias torch.Size([1024])\n",
      "180 transformer_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "181 transformer_model.encoder.layers.11.self_attn.v_proj.bias torch.Size([1024])\n",
      "182 transformer_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "183 transformer_model.encoder.layers.11.self_attn.q_proj.bias torch.Size([1024])\n",
      "184 transformer_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "185 transformer_model.encoder.layers.11.self_attn.out_proj.bias torch.Size([1024])\n",
      "186 transformer_model.encoder.layers.11.self_attn_layer_norm.weight torch.Size([1024])\n",
      "187 transformer_model.encoder.layers.11.self_attn_layer_norm.bias torch.Size([1024])\n",
      "188 transformer_model.encoder.layers.11.fc1.weight torch.Size([4096, 1024])\n",
      "189 transformer_model.encoder.layers.11.fc1.bias torch.Size([4096])\n",
      "190 transformer_model.encoder.layers.11.fc2.weight torch.Size([1024, 4096])\n",
      "191 transformer_model.encoder.layers.11.fc2.bias torch.Size([1024])\n",
      "192 transformer_model.encoder.layers.11.final_layer_norm.weight torch.Size([1024])\n",
      "193 transformer_model.encoder.layers.11.final_layer_norm.bias torch.Size([1024])\n",
      "194 transformer_model.encoder.layernorm_embedding.weight torch.Size([1024])\n",
      "195 transformer_model.encoder.layernorm_embedding.bias torch.Size([1024])\n",
      "196 transformer_model.decoder.embed_positions.weight torch.Size([1026, 1024])\n",
      "197 transformer_model.decoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "198 transformer_model.decoder.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "199 transformer_model.decoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.decoder.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "201 transformer_model.decoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.decoder.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "203 transformer_model.decoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "204 transformer_model.decoder.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "205 transformer_model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "206 transformer_model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "207 transformer_model.decoder.layers.0.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "208 transformer_model.decoder.layers.0.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "209 transformer_model.decoder.layers.0.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "210 transformer_model.decoder.layers.0.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "211 transformer_model.decoder.layers.0.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "212 transformer_model.decoder.layers.0.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "213 transformer_model.decoder.layers.0.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "214 transformer_model.decoder.layers.0.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "215 transformer_model.decoder.layers.0.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "216 transformer_model.decoder.layers.0.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "217 transformer_model.decoder.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "218 transformer_model.decoder.layers.0.fc1.bias torch.Size([4096])\n",
      "219 transformer_model.decoder.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "220 transformer_model.decoder.layers.0.fc2.bias torch.Size([1024])\n",
      "221 transformer_model.decoder.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "222 transformer_model.decoder.layers.0.final_layer_norm.bias torch.Size([1024])\n",
      "223 transformer_model.decoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "224 transformer_model.decoder.layers.1.self_attn.k_proj.bias torch.Size([1024])\n",
      "225 transformer_model.decoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "226 transformer_model.decoder.layers.1.self_attn.v_proj.bias torch.Size([1024])\n",
      "227 transformer_model.decoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "228 transformer_model.decoder.layers.1.self_attn.q_proj.bias torch.Size([1024])\n",
      "229 transformer_model.decoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "230 transformer_model.decoder.layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "231 transformer_model.decoder.layers.1.self_attn_layer_norm.weight torch.Size([1024])\n",
      "232 transformer_model.decoder.layers.1.self_attn_layer_norm.bias torch.Size([1024])\n",
      "233 transformer_model.decoder.layers.1.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.decoder.layers.1.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "235 transformer_model.decoder.layers.1.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "236 transformer_model.decoder.layers.1.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "237 transformer_model.decoder.layers.1.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "238 transformer_model.decoder.layers.1.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "239 transformer_model.decoder.layers.1.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "240 transformer_model.decoder.layers.1.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "241 transformer_model.decoder.layers.1.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "242 transformer_model.decoder.layers.1.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "243 transformer_model.decoder.layers.1.fc1.weight torch.Size([4096, 1024])\n",
      "244 transformer_model.decoder.layers.1.fc1.bias torch.Size([4096])\n",
      "245 transformer_model.decoder.layers.1.fc2.weight torch.Size([1024, 4096])\n",
      "246 transformer_model.decoder.layers.1.fc2.bias torch.Size([1024])\n",
      "247 transformer_model.decoder.layers.1.final_layer_norm.weight torch.Size([1024])\n",
      "248 transformer_model.decoder.layers.1.final_layer_norm.bias torch.Size([1024])\n",
      "249 transformer_model.decoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.decoder.layers.2.self_attn.k_proj.bias torch.Size([1024])\n",
      "251 transformer_model.decoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "252 transformer_model.decoder.layers.2.self_attn.v_proj.bias torch.Size([1024])\n",
      "253 transformer_model.decoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "254 transformer_model.decoder.layers.2.self_attn.q_proj.bias torch.Size([1024])\n",
      "255 transformer_model.decoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "256 transformer_model.decoder.layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "257 transformer_model.decoder.layers.2.self_attn_layer_norm.weight torch.Size([1024])\n",
      "258 transformer_model.decoder.layers.2.self_attn_layer_norm.bias torch.Size([1024])\n",
      "259 transformer_model.decoder.layers.2.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "260 transformer_model.decoder.layers.2.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "261 transformer_model.decoder.layers.2.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "262 transformer_model.decoder.layers.2.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "263 transformer_model.decoder.layers.2.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.decoder.layers.2.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "265 transformer_model.decoder.layers.2.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.decoder.layers.2.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "267 transformer_model.decoder.layers.2.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "268 transformer_model.decoder.layers.2.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "269 transformer_model.decoder.layers.2.fc1.weight torch.Size([4096, 1024])\n",
      "270 transformer_model.decoder.layers.2.fc1.bias torch.Size([4096])\n",
      "271 transformer_model.decoder.layers.2.fc2.weight torch.Size([1024, 4096])\n",
      "272 transformer_model.decoder.layers.2.fc2.bias torch.Size([1024])\n",
      "273 transformer_model.decoder.layers.2.final_layer_norm.weight torch.Size([1024])\n",
      "274 transformer_model.decoder.layers.2.final_layer_norm.bias torch.Size([1024])\n",
      "275 transformer_model.decoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "276 transformer_model.decoder.layers.3.self_attn.k_proj.bias torch.Size([1024])\n",
      "277 transformer_model.decoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "278 transformer_model.decoder.layers.3.self_attn.v_proj.bias torch.Size([1024])\n",
      "279 transformer_model.decoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.decoder.layers.3.self_attn.q_proj.bias torch.Size([1024])\n",
      "281 transformer_model.decoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.decoder.layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "283 transformer_model.decoder.layers.3.self_attn_layer_norm.weight torch.Size([1024])\n",
      "284 transformer_model.decoder.layers.3.self_attn_layer_norm.bias torch.Size([1024])\n",
      "285 transformer_model.decoder.layers.3.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "286 transformer_model.decoder.layers.3.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "287 transformer_model.decoder.layers.3.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "288 transformer_model.decoder.layers.3.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "289 transformer_model.decoder.layers.3.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "290 transformer_model.decoder.layers.3.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "291 transformer_model.decoder.layers.3.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "292 transformer_model.decoder.layers.3.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "293 transformer_model.decoder.layers.3.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "294 transformer_model.decoder.layers.3.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "295 transformer_model.decoder.layers.3.fc1.weight torch.Size([4096, 1024])\n",
      "296 transformer_model.decoder.layers.3.fc1.bias torch.Size([4096])\n",
      "297 transformer_model.decoder.layers.3.fc2.weight torch.Size([1024, 4096])\n",
      "298 transformer_model.decoder.layers.3.fc2.bias torch.Size([1024])\n",
      "299 transformer_model.decoder.layers.3.final_layer_norm.weight torch.Size([1024])\n",
      "300 transformer_model.decoder.layers.3.final_layer_norm.bias torch.Size([1024])\n",
      "301 transformer_model.decoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "302 transformer_model.decoder.layers.4.self_attn.k_proj.bias torch.Size([1024])\n",
      "303 transformer_model.decoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "304 transformer_model.decoder.layers.4.self_attn.v_proj.bias torch.Size([1024])\n",
      "305 transformer_model.decoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "306 transformer_model.decoder.layers.4.self_attn.q_proj.bias torch.Size([1024])\n",
      "307 transformer_model.decoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "308 transformer_model.decoder.layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "309 transformer_model.decoder.layers.4.self_attn_layer_norm.weight torch.Size([1024])\n",
      "310 transformer_model.decoder.layers.4.self_attn_layer_norm.bias torch.Size([1024])\n",
      "311 transformer_model.decoder.layers.4.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.decoder.layers.4.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "313 transformer_model.decoder.layers.4.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.decoder.layers.4.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "315 transformer_model.decoder.layers.4.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "316 transformer_model.decoder.layers.4.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "317 transformer_model.decoder.layers.4.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "318 transformer_model.decoder.layers.4.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "319 transformer_model.decoder.layers.4.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "320 transformer_model.decoder.layers.4.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "321 transformer_model.decoder.layers.4.fc1.weight torch.Size([4096, 1024])\n",
      "322 transformer_model.decoder.layers.4.fc1.bias torch.Size([4096])\n",
      "323 transformer_model.decoder.layers.4.fc2.weight torch.Size([1024, 4096])\n",
      "324 transformer_model.decoder.layers.4.fc2.bias torch.Size([1024])\n",
      "325 transformer_model.decoder.layers.4.final_layer_norm.weight torch.Size([1024])\n",
      "326 transformer_model.decoder.layers.4.final_layer_norm.bias torch.Size([1024])\n",
      "327 transformer_model.decoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.decoder.layers.5.self_attn.k_proj.bias torch.Size([1024])\n",
      "329 transformer_model.decoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.decoder.layers.5.self_attn.v_proj.bias torch.Size([1024])\n",
      "331 transformer_model.decoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "332 transformer_model.decoder.layers.5.self_attn.q_proj.bias torch.Size([1024])\n",
      "333 transformer_model.decoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "334 transformer_model.decoder.layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "335 transformer_model.decoder.layers.5.self_attn_layer_norm.weight torch.Size([1024])\n",
      "336 transformer_model.decoder.layers.5.self_attn_layer_norm.bias torch.Size([1024])\n",
      "337 transformer_model.decoder.layers.5.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "338 transformer_model.decoder.layers.5.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "339 transformer_model.decoder.layers.5.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "340 transformer_model.decoder.layers.5.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "341 transformer_model.decoder.layers.5.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "342 transformer_model.decoder.layers.5.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "343 transformer_model.decoder.layers.5.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.decoder.layers.5.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "345 transformer_model.decoder.layers.5.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "346 transformer_model.decoder.layers.5.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "347 transformer_model.decoder.layers.5.fc1.weight torch.Size([4096, 1024])\n",
      "348 transformer_model.decoder.layers.5.fc1.bias torch.Size([4096])\n",
      "349 transformer_model.decoder.layers.5.fc2.weight torch.Size([1024, 4096])\n",
      "350 transformer_model.decoder.layers.5.fc2.bias torch.Size([1024])\n",
      "351 transformer_model.decoder.layers.5.final_layer_norm.weight torch.Size([1024])\n",
      "352 transformer_model.decoder.layers.5.final_layer_norm.bias torch.Size([1024])\n",
      "353 transformer_model.decoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "354 transformer_model.decoder.layers.6.self_attn.k_proj.bias torch.Size([1024])\n",
      "355 transformer_model.decoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "356 transformer_model.decoder.layers.6.self_attn.v_proj.bias torch.Size([1024])\n",
      "357 transformer_model.decoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "358 transformer_model.decoder.layers.6.self_attn.q_proj.bias torch.Size([1024])\n",
      "359 transformer_model.decoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.decoder.layers.6.self_attn.out_proj.bias torch.Size([1024])\n",
      "361 transformer_model.decoder.layers.6.self_attn_layer_norm.weight torch.Size([1024])\n",
      "362 transformer_model.decoder.layers.6.self_attn_layer_norm.bias torch.Size([1024])\n",
      "363 transformer_model.decoder.layers.6.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "364 transformer_model.decoder.layers.6.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "365 transformer_model.decoder.layers.6.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "366 transformer_model.decoder.layers.6.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "367 transformer_model.decoder.layers.6.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "368 transformer_model.decoder.layers.6.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "369 transformer_model.decoder.layers.6.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "370 transformer_model.decoder.layers.6.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "371 transformer_model.decoder.layers.6.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "372 transformer_model.decoder.layers.6.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "373 transformer_model.decoder.layers.6.fc1.weight torch.Size([4096, 1024])\n",
      "374 transformer_model.decoder.layers.6.fc1.bias torch.Size([4096])\n",
      "375 transformer_model.decoder.layers.6.fc2.weight torch.Size([1024, 4096])\n",
      "376 transformer_model.decoder.layers.6.fc2.bias torch.Size([1024])\n",
      "377 transformer_model.decoder.layers.6.final_layer_norm.weight torch.Size([1024])\n",
      "378 transformer_model.decoder.layers.6.final_layer_norm.bias torch.Size([1024])\n",
      "379 transformer_model.decoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "380 transformer_model.decoder.layers.7.self_attn.k_proj.bias torch.Size([1024])\n",
      "381 transformer_model.decoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "382 transformer_model.decoder.layers.7.self_attn.v_proj.bias torch.Size([1024])\n",
      "383 transformer_model.decoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "384 transformer_model.decoder.layers.7.self_attn.q_proj.bias torch.Size([1024])\n",
      "385 transformer_model.decoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "386 transformer_model.decoder.layers.7.self_attn.out_proj.bias torch.Size([1024])\n",
      "387 transformer_model.decoder.layers.7.self_attn_layer_norm.weight torch.Size([1024])\n",
      "388 transformer_model.decoder.layers.7.self_attn_layer_norm.bias torch.Size([1024])\n",
      "389 transformer_model.decoder.layers.7.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "390 transformer_model.decoder.layers.7.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "391 transformer_model.decoder.layers.7.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "392 transformer_model.decoder.layers.7.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "393 transformer_model.decoder.layers.7.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "394 transformer_model.decoder.layers.7.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "395 transformer_model.decoder.layers.7.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "396 transformer_model.decoder.layers.7.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "397 transformer_model.decoder.layers.7.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "398 transformer_model.decoder.layers.7.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "399 transformer_model.decoder.layers.7.fc1.weight torch.Size([4096, 1024])\n",
      "400 transformer_model.decoder.layers.7.fc1.bias torch.Size([4096])\n",
      "401 transformer_model.decoder.layers.7.fc2.weight torch.Size([1024, 4096])\n",
      "402 transformer_model.decoder.layers.7.fc2.bias torch.Size([1024])\n",
      "403 transformer_model.decoder.layers.7.final_layer_norm.weight torch.Size([1024])\n",
      "404 transformer_model.decoder.layers.7.final_layer_norm.bias torch.Size([1024])\n",
      "405 transformer_model.decoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "406 transformer_model.decoder.layers.8.self_attn.k_proj.bias torch.Size([1024])\n",
      "407 transformer_model.decoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "408 transformer_model.decoder.layers.8.self_attn.v_proj.bias torch.Size([1024])\n",
      "409 transformer_model.decoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "410 transformer_model.decoder.layers.8.self_attn.q_proj.bias torch.Size([1024])\n",
      "411 transformer_model.decoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "412 transformer_model.decoder.layers.8.self_attn.out_proj.bias torch.Size([1024])\n",
      "413 transformer_model.decoder.layers.8.self_attn_layer_norm.weight torch.Size([1024])\n",
      "414 transformer_model.decoder.layers.8.self_attn_layer_norm.bias torch.Size([1024])\n",
      "415 transformer_model.decoder.layers.8.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "416 transformer_model.decoder.layers.8.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "417 transformer_model.decoder.layers.8.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "418 transformer_model.decoder.layers.8.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "419 transformer_model.decoder.layers.8.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "420 transformer_model.decoder.layers.8.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "421 transformer_model.decoder.layers.8.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "422 transformer_model.decoder.layers.8.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "423 transformer_model.decoder.layers.8.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "424 transformer_model.decoder.layers.8.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "425 transformer_model.decoder.layers.8.fc1.weight torch.Size([4096, 1024])\n",
      "426 transformer_model.decoder.layers.8.fc1.bias torch.Size([4096])\n",
      "427 transformer_model.decoder.layers.8.fc2.weight torch.Size([1024, 4096])\n",
      "428 transformer_model.decoder.layers.8.fc2.bias torch.Size([1024])\n",
      "429 transformer_model.decoder.layers.8.final_layer_norm.weight torch.Size([1024])\n",
      "430 transformer_model.decoder.layers.8.final_layer_norm.bias torch.Size([1024])\n",
      "431 transformer_model.decoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "432 transformer_model.decoder.layers.9.self_attn.k_proj.bias torch.Size([1024])\n",
      "433 transformer_model.decoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "434 transformer_model.decoder.layers.9.self_attn.v_proj.bias torch.Size([1024])\n",
      "435 transformer_model.decoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "436 transformer_model.decoder.layers.9.self_attn.q_proj.bias torch.Size([1024])\n",
      "437 transformer_model.decoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "438 transformer_model.decoder.layers.9.self_attn.out_proj.bias torch.Size([1024])\n",
      "439 transformer_model.decoder.layers.9.self_attn_layer_norm.weight torch.Size([1024])\n",
      "440 transformer_model.decoder.layers.9.self_attn_layer_norm.bias torch.Size([1024])\n",
      "441 transformer_model.decoder.layers.9.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "442 transformer_model.decoder.layers.9.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "443 transformer_model.decoder.layers.9.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "444 transformer_model.decoder.layers.9.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "445 transformer_model.decoder.layers.9.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "446 transformer_model.decoder.layers.9.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "447 transformer_model.decoder.layers.9.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "448 transformer_model.decoder.layers.9.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "449 transformer_model.decoder.layers.9.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "450 transformer_model.decoder.layers.9.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "451 transformer_model.decoder.layers.9.fc1.weight torch.Size([4096, 1024])\n",
      "452 transformer_model.decoder.layers.9.fc1.bias torch.Size([4096])\n",
      "453 transformer_model.decoder.layers.9.fc2.weight torch.Size([1024, 4096])\n",
      "454 transformer_model.decoder.layers.9.fc2.bias torch.Size([1024])\n",
      "455 transformer_model.decoder.layers.9.final_layer_norm.weight torch.Size([1024])\n",
      "456 transformer_model.decoder.layers.9.final_layer_norm.bias torch.Size([1024])\n",
      "457 transformer_model.decoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "458 transformer_model.decoder.layers.10.self_attn.k_proj.bias torch.Size([1024])\n",
      "459 transformer_model.decoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "460 transformer_model.decoder.layers.10.self_attn.v_proj.bias torch.Size([1024])\n",
      "461 transformer_model.decoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "462 transformer_model.decoder.layers.10.self_attn.q_proj.bias torch.Size([1024])\n",
      "463 transformer_model.decoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "464 transformer_model.decoder.layers.10.self_attn.out_proj.bias torch.Size([1024])\n",
      "465 transformer_model.decoder.layers.10.self_attn_layer_norm.weight torch.Size([1024])\n",
      "466 transformer_model.decoder.layers.10.self_attn_layer_norm.bias torch.Size([1024])\n",
      "467 transformer_model.decoder.layers.10.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "468 transformer_model.decoder.layers.10.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "469 transformer_model.decoder.layers.10.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "470 transformer_model.decoder.layers.10.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "471 transformer_model.decoder.layers.10.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "472 transformer_model.decoder.layers.10.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "473 transformer_model.decoder.layers.10.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "474 transformer_model.decoder.layers.10.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "475 transformer_model.decoder.layers.10.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "476 transformer_model.decoder.layers.10.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "477 transformer_model.decoder.layers.10.fc1.weight torch.Size([4096, 1024])\n",
      "478 transformer_model.decoder.layers.10.fc1.bias torch.Size([4096])\n",
      "479 transformer_model.decoder.layers.10.fc2.weight torch.Size([1024, 4096])\n",
      "480 transformer_model.decoder.layers.10.fc2.bias torch.Size([1024])\n",
      "481 transformer_model.decoder.layers.10.final_layer_norm.weight torch.Size([1024])\n",
      "482 transformer_model.decoder.layers.10.final_layer_norm.bias torch.Size([1024])\n",
      "483 transformer_model.decoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "484 transformer_model.decoder.layers.11.self_attn.k_proj.bias torch.Size([1024])\n",
      "485 transformer_model.decoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "486 transformer_model.decoder.layers.11.self_attn.v_proj.bias torch.Size([1024])\n",
      "487 transformer_model.decoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "488 transformer_model.decoder.layers.11.self_attn.q_proj.bias torch.Size([1024])\n",
      "489 transformer_model.decoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "490 transformer_model.decoder.layers.11.self_attn.out_proj.bias torch.Size([1024])\n",
      "491 transformer_model.decoder.layers.11.self_attn_layer_norm.weight torch.Size([1024])\n",
      "492 transformer_model.decoder.layers.11.self_attn_layer_norm.bias torch.Size([1024])\n",
      "493 transformer_model.decoder.layers.11.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "494 transformer_model.decoder.layers.11.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "495 transformer_model.decoder.layers.11.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "496 transformer_model.decoder.layers.11.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "497 transformer_model.decoder.layers.11.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "498 transformer_model.decoder.layers.11.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "499 transformer_model.decoder.layers.11.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "500 transformer_model.decoder.layers.11.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "501 transformer_model.decoder.layers.11.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "502 transformer_model.decoder.layers.11.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "503 transformer_model.decoder.layers.11.fc1.weight torch.Size([4096, 1024])\n",
      "504 transformer_model.decoder.layers.11.fc1.bias torch.Size([4096])\n",
      "505 transformer_model.decoder.layers.11.fc2.weight torch.Size([1024, 4096])\n",
      "506 transformer_model.decoder.layers.11.fc2.bias torch.Size([1024])\n",
      "507 transformer_model.decoder.layers.11.final_layer_norm.weight torch.Size([1024])\n",
      "508 transformer_model.decoder.layers.11.final_layer_norm.bias torch.Size([1024])\n",
      "509 transformer_model.decoder.layernorm_embedding.weight torch.Size([1024])\n",
      "510 transformer_model.decoder.layernorm_embedding.bias torch.Size([1024])\n",
      "511 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "512 attention.hidden_layer.bias torch.Size([512])\n",
      "513 attention.final_layer.weight torch.Size([1, 512])\n",
      "514 attention.final_layer.bias torch.Size([1])\n",
      "515 regressor.weight torch.Size([1, 1024])\n",
      "516 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model(sample_input_ids, sample_attention_mask)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-21.0677, -42.9958,  14.3378,  ...,  -4.5223,   0.5046,  37.1187],\n",
       "        [  6.4391,   0.1978,   8.0326,  ..., -29.4494,   5.1051,   7.0165],\n",
       "        [ 40.7368, -11.5052,  24.5615,  ...,  15.3304, -21.9535, -11.8778],\n",
       "        ...,\n",
       "        [ 19.7794,   3.5911, -28.0920,  ..., -44.1329,  44.9046, -11.6836],\n",
       "        [ 46.3422,  -7.2867, -12.5700,  ..., -15.5036, -19.0379,  16.4539],\n",
       "        [ 15.3006, -20.9010, -11.6092,  ..., -15.8121,  52.2668, -24.7503]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 515\n",
    "    attention_start = 511\n",
    "    roberta_parameters = named_parameters[:attention_start]\n",
    "    attention_parameters = named_parameters[attention_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group, \"lr\": base_lr * 20})\n",
    "    parameters.append({\"params\": regressor_group, \"lr\": base_lr * 20})\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = find_lr_for_layer(layer_num, layer_lrs)\n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                \n",
    "                mse.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, self.model_path)\n",
    "                    start = time.time()\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = kfold.split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f21d9414c6748b590f2effebed5619b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b969e39b7f4f1cb25949421c650fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8931 New best_val_rmse: 0.8931\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6515 New best_val_rmse: 0.6515\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7985 Still best_val_rmse: 0.6515 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7719 Still best_val_rmse: 0.6515 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7072 Still best_val_rmse: 0.6515 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.678 Still best_val_rmse: 0.6515 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5962 New best_val_rmse: 0.5962\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5374 New best_val_rmse: 0.5374\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5383 Still best_val_rmse: 0.5374 (from epoch 0)\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5861 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5284 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5676 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5172 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 14.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5468 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5827 Still best_val_rmse: 0.5117 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 1.81 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.484 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.5119 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.5212 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "16 steps took 14.6 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.5079 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4946 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "8 steps took 7.22 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4907 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "8 steps took 7.22 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4953 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "8 steps took 7.22 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4843 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4952 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "8 steps took 7.21 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4858 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4889 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4896 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4854 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4857 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4861 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4853 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4886 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4899 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4878 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4839 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4836 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4841 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4838 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4835 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4833 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4833 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4835 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4835 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4836 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4836 Still best_val_rmse: 0.4774 (from epoch 1)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d751cff2c1043e499ab4483990c39aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7084 New best_val_rmse: 0.7084\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6344 New best_val_rmse: 0.6344\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7007 Still best_val_rmse: 0.6344 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5596 New best_val_rmse: 0.5596\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5558 New best_val_rmse: 0.5558\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6971 Still best_val_rmse: 0.5558 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5417 New best_val_rmse: 0.5417\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7419 Still best_val_rmse: 0.5417 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5798 Still best_val_rmse: 0.5417 (from epoch 0)\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5198 New best_val_rmse: 0.5198\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5854 Still best_val_rmse: 0.5198 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5037 New best_val_rmse: 0.5037\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5021 New best_val_rmse: 0.5021\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5384 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5667 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5217 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 1 batch_num: 126 val_rmse: 0.5323 Still best_val_rmse: 0.4796 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4823 Still best_val_rmse: 0.4748 (from epoch 1)\n",
      "\n",
      "4 steps took 3.91 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5027 Still best_val_rmse: 0.4748 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4838 Still best_val_rmse: 0.4748 (from epoch 1)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4811 Still best_val_rmse: 0.4748 (from epoch 1)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4723 New best_val_rmse: 0.4723\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4728 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4741 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4808 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4898 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4759 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4772 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4813 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4744 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.483 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4887 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4773 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.81 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4745 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4741 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4741 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4751 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4755 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4751 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4748 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4748 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4748 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4746 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4744 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4744 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4743 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4744 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.81 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4746 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4747 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4749 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.475 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4748 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4747 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4747 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4747 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4745 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4742 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.474 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.474 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4738 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4737 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4737 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4737 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.474 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.474 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.81 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.474 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.474 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4738 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4739 Still best_val_rmse: 0.4723 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f2232e94914db9beded204b9a5d393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9779 New best_val_rmse: 0.9779\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7781 New best_val_rmse: 0.7781\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6573 New best_val_rmse: 0.6573\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6245 New best_val_rmse: 0.6245\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6699 Still best_val_rmse: 0.6245 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7512 Still best_val_rmse: 0.6245 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6194 New best_val_rmse: 0.6194\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7121 Still best_val_rmse: 0.6194 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5345 New best_val_rmse: 0.5345\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.504 New best_val_rmse: 0.504\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5328 Still best_val_rmse: 0.504 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5039 New best_val_rmse: 0.5039\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5175 Still best_val_rmse: 0.5039 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5054 Still best_val_rmse: 0.5039 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5351 Still best_val_rmse: 0.5039 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4998 New best_val_rmse: 0.4998\n",
      "\n",
      "8 steps took 7.21 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.53 Still best_val_rmse: 0.4964 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4988 Still best_val_rmse: 0.4964 (from epoch 1)\n",
      "\n",
      "8 steps took 7.49 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5137 Still best_val_rmse: 0.4964 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5015 Still best_val_rmse: 0.4964 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4891 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4856 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4918 Still best_val_rmse: 0.4824 (from epoch 2)\n",
      "\n",
      "8 steps took 7.19 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4859 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.81 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4798 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4815 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4818 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4824 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4831 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4831 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4848 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4848 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4835 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.48 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4794 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4794 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4794 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4794 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce94b04a441c4765be0b3ea9f4091b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7563 New best_val_rmse: 0.7563\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6404 New best_val_rmse: 0.6404\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.004 Still best_val_rmse: 0.6404 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7435 Still best_val_rmse: 0.6404 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6564 Still best_val_rmse: 0.6404 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6377 New best_val_rmse: 0.6377\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5479 New best_val_rmse: 0.5479\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5184 New best_val_rmse: 0.5184\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5743 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6278 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5583 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5311 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5662 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5284 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5231 Still best_val_rmse: 0.5184 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5055 New best_val_rmse: 0.5055\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.515 Still best_val_rmse: 0.5055 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4954 New best_val_rmse: 0.4954\n",
      "\n",
      "8 steps took 7.5 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5035 Still best_val_rmse: 0.4954 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4963 Still best_val_rmse: 0.4943 (from epoch 2)\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4968 Still best_val_rmse: 0.4943 (from epoch 2)\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4932 New best_val_rmse: 0.4932\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4927 New best_val_rmse: 0.4927\n",
      "\n",
      "8 steps took 7.17 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4937 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4968 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4939 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.19 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4928 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4951 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4938 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4937 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4938 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4938 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4939 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4939 Still best_val_rmse: 0.4927 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10ee55f677d400282bcd8a55d26ea79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.283 New best_val_rmse: 1.283\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7121 New best_val_rmse: 0.7121\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6553 New best_val_rmse: 0.6553\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6183 New best_val_rmse: 0.6183\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.623 Still best_val_rmse: 0.6183 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5669 New best_val_rmse: 0.5669\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6269 Still best_val_rmse: 0.5669 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7732 Still best_val_rmse: 0.5669 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5977 Still best_val_rmse: 0.5669 (from epoch 0)\n",
      "\n",
      "16 steps took 14.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.609 Still best_val_rmse: 0.5378 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5589 Still best_val_rmse: 0.5378 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.549 Still best_val_rmse: 0.5378 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5198 New best_val_rmse: 0.5198\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.496 New best_val_rmse: 0.496\n",
      "\n",
      "8 steps took 7.19 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5027 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4973 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4966 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "8 steps took 7.21 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5231 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4998 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "8 steps took 7.21 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4968 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "8 steps took 7.21 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.506 Still best_val_rmse: 0.496 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4906 New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 7.21 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4904 New best_val_rmse: 0.4904\n",
      "\n",
      "8 steps took 7.19 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4915 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 7.18 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4911 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 7.19 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4898 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4887 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.489 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4891 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.49 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.492 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 7.22 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.49 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4903 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 7.23 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4909 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 7.22 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4897 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4895 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4896 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4896 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4896 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4896 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.61 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4896 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4896 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13537093e094df695725e68fa492ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7622 New best_val_rmse: 0.7622\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.785 Still best_val_rmse: 0.7622 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6317 New best_val_rmse: 0.6317\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6095 New best_val_rmse: 0.6095\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5888 New best_val_rmse: 0.5888\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5495 New best_val_rmse: 0.5495\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5211 New best_val_rmse: 0.5211\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7054 Still best_val_rmse: 0.5211 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5248 Still best_val_rmse: 0.5211 (from epoch 0)\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5044 New best_val_rmse: 0.5044\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5119 Still best_val_rmse: 0.5044 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5077 Still best_val_rmse: 0.5044 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4996 New best_val_rmse: 0.4996\n",
      "\n",
      "8 steps took 7.19 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 1 batch_num: 72 val_rmse: 0.5246 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.5046 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.5071 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.5098 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5048 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 14.7 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4905 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4844 New best_val_rmse: 0.4844\n",
      "\n",
      "4 steps took 3.6 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5228 Still best_val_rmse: 0.4844 (from epoch 2)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4884 Still best_val_rmse: 0.4844 (from epoch 2)\n",
      "\n",
      "4 steps took 3.59 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4911 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "8 steps took 7.17 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4785 New best_val_rmse: 0.4785\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4787 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4783 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4777 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4767 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4772 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4793 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.479 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.477 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4762 Still best_val_rmse: 0.4761 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4755 New best_val_rmse: 0.4755\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4772 Still best_val_rmse: 0.4755 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4803 Still best_val_rmse: 0.4755 (from epoch 2)\n",
      "\n",
      "4 steps took 3.58 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4846 Still best_val_rmse: 0.4755 (from epoch 2)\n",
      "\n",
      "4 steps took 3.58 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4759 Still best_val_rmse: 0.4755 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4769 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4779 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4792 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4794 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4782 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4768 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4749 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4749 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4748 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4749 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4751 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4754 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4757 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4758 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4757 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4757 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4755 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4754 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4753 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4753 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.79 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.8 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4752 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "\n",
      "\n",
      "Performance estimates:\n",
      "[tensor(0.4774), tensor(0.4723), tensor(0.4794), tensor(0.4927), tensor(0.4884), tensor(0.4748)]\n",
      "Mean: 0.48080757\n",
      "CPU times: user 1h 43min 26s, sys: 11min 16s, total: 1h 54min 43s\n",
      "Wall time: 1h 40min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list_val_rmse = []\n",
    "\n",
    "pbar = tqdm(enumerate(splits), total=cfg.NUM_FOLDS, position=0, leave=True)\n",
    "for fold, (train_indices, val_indices) in pbar:\n",
    "    pbar.set_description(f'Fold {fold}')\n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "        \n",
    "    optimizer = create_optimizer(model)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    \n",
    "    trainer = Trainer(model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    list_val_rmse.append(trainer.train())\n",
    "    \n",
    "    tokenizer.save_pretrained(str(model_path.parent))\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    if cfg.DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print(\"\\nPerformance estimates:\")\n",
    "print(list_val_rmse)\n",
    "print(\"Mean:\", np.array(list_val_rmse).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Model 2\n",
      "Model 3\n",
      "Model 4\n",
      "Model 5\n",
      "Model 6\n",
      "CPU times: user 24.9 s, sys: 6.41 s, total: 31.3 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = BartTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7235db72ae34d0292c4b46ac47b4de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04b196ab57c4ac892b7a230280b77fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.37325547229341144\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3453324134543273\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.32340817929201776\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3204405526229345\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3333327884678928\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.3635039683803656\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa7003a4337487997d4528e98f3951a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a67b9e89064e149b39fa19d8252d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.29010519810262403\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.3004871401415004\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2570034726737771\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.28763543759620486\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.2990042489319806\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.30301432244041115\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be997dfa9c214de190d754230cf0e2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e10d5ef20a4c2ba7a2a3c0a74d73d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.31286316098675426\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.30026259230902386\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.2755827957132132\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.29837352871199657\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3043165327157811\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.30457660838784817\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8724334f9903423eae29bd2399eebd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014799d95f3046038b0a8097d9d50023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.35345980165197854\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.34255884712955575\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.31107739678635\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.3140189767969981\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.33835151304788896\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.35750849503225257\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf96ce2582f42f48de2e8aed5b94087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8be64bdbe794bfc8b2bae3035e54f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2360,) (473,)\n",
      "rmse_score 0.319893525050458\n",
      "Fold 1 (2361,) (472,)\n",
      "rmse_score 0.2997590314969981\n",
      "Fold 2 (2361,) (472,)\n",
      "rmse_score 0.26195076450751587\n",
      "Fold 3 (2361,) (472,)\n",
      "rmse_score 0.2603595665123617\n",
      "Fold 4 (2361,) (472,)\n",
      "rmse_score 0.3039593548003887\n",
      "Fold 5 (2361,) (472,)\n",
      "rmse_score 0.30584878947201594\n",
      "FINAL RMSE score 0.31204148251689423\n",
      "CPU times: user 7min 52s, sys: 6.32 s, total: 7min 58s\n",
      "Wall time: 5min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0004524387870961925],\n",
       " [-0.0012236217380374081],\n",
       " [-0.0014261383614178824],\n",
       " [-0.0022271296833980864],\n",
       " [0.0007525520826139716]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19500535, 0.20360527, 0.20203695, 0.19613497, 0.20321746])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9596573929279916, -0.8822176723123903)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29396289, -0.54543004, -0.45002452, -2.10472559, -1.79757158,\n",
       "       -1.30941135,  0.32487005])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.07733511993582376, -0.015467023987164753)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.371298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.622765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.527360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.182061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.874907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.386746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.247535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.371298\n",
       "1  f0953f0a5 -0.622765\n",
       "2  0df072751 -0.527360\n",
       "3  04caf4e0c -2.182061\n",
       "4  0e63f8bea -1.874907\n",
       "5  12537fe78 -1.386746\n",
       "6  965e592c0  0.247535"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bart-large'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/bart-large/best')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/commonlit/models/bart-large_1'),\n",
       " PosixPath('/home/commonlit/models/bart-large_2'),\n",
       " PosixPath('/home/commonlit/models/bart-large_3'),\n",
       " PosixPath('/home/commonlit/models/bart-large_4'),\n",
       " PosixPath('/home/commonlit/models/bart-large_5'),\n",
       " PosixPath('/home/commonlit/models/bart-large_6')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/bart-large/best_models.zip'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/commonlit/models/bart-large.yaml': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3M\t/home/commonlit/models/bart-large/best/tokenizer-1\n",
      "1.3M\t/home/commonlit/models/bart-large/best/tokenizer-2\n",
      "1.3M\t/home/commonlit/models/bart-large/best/tokenizer-3\n",
      "1.3M\t/home/commonlit/models/bart-large/best/tokenizer-4\n",
      "1.3M\t/home/commonlit/models/bart-large/best/tokenizer-5\n",
      "1.3M\t/home/commonlit/models/bart-large/best/tokenizer-6\n",
      "9.2G\t/home/commonlit/models/bart-large/best\n",
      "8.4G\t/home/commonlit/models/bart-large/best_models.zip\n",
      "1.6G\t/home/commonlit/models/bart-large/lm\n"
     ]
    }
   ],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/bart-large/lm.zip'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/bart-large/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-bart-large-light\",\n",
      "  \"id\": \"gilfernandes/commonlit-bart-large-light\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.31G/8.31G [14:19<00:00, 10.4MB/s]\n",
      "Upload successful: best_models.zip (8GB)\n",
      "Starting upload for file lm.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.38G/1.38G [03:05<00:00, 7.99MB/s]\n",
      "Upload successful: lm.zip (1GB)\n",
      "400 - Bad Request\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
