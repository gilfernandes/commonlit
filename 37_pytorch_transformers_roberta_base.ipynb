{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gc, sys, time, collections, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Optional, Union, Any, List, Tuple\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as D\n",
    "from torch.utils.data.dataset import Dataset, IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertModel\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertModel\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "from transformers.trainer import logging\n",
    "from transformers.file_utils import is_torch_tpu_available, is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import find_batch_size, nested_concat, nested_numpify, nested_truncate, nested_detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b25934-3c16-4cb0-ab3b-6d95223432f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonlit_lm\t\t       the_huge_hunter.txt\n",
      "commonlit_lm.zip\t       the_twin_stars.txt\n",
      "commonlitreadabilityprize.zip  thumbelina\n",
      "data_enhancements.ipynb        train-orig.csv\n",
      "mapping_the_oceans.txt\t       train.csv\n",
      "pecks_uncle_ike.txt\t       train_duo.csv\n",
      "sample_submission.csv\t       train_enhancements.csv\n",
      "sports_in_adolescence.txt      understood_betsy.txt\n",
      "stories_for_little_boys.txt    why_the_swallows_tail_is_forked.txt\n",
      "test.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patty concluded to move very slowly, thinking ...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patty concluded to move very slowly, thinking ...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>At last the game was concluded, as Roger Farri...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>There are heaps of different things that can s...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>But we will also talk about how getting a conc...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>The brain goes through a lot of changes during...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>During adolescence, the white matter begins to...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>During adolescence, the prefrontal cortex (PFC...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2928 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal    license  \\\n",
       "0     c12129c31                                                NaN        NaN   \n",
       "1     c12129c31                                                NaN        NaN   \n",
       "2     c12129c31                                                NaN        NaN   \n",
       "3     c12129c31                                                NaN        NaN   \n",
       "4     c12129c31                                                NaN        NaN   \n",
       "...         ...                                                ...        ...   \n",
       "2923  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2924  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2925  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2926  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2927  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "\n",
       "                                                excerpt    target  \\\n",
       "0     When the young people returned to the ballroom... -0.340259   \n",
       "1     When the young people returned to the ballroom... -0.340259   \n",
       "2     Patty concluded to move very slowly, thinking ... -0.340259   \n",
       "3     Patty concluded to move very slowly, thinking ... -0.340259   \n",
       "4     At last the game was concluded, as Roger Farri... -0.340259   \n",
       "...                                                 ...       ...   \n",
       "2923  There are heaps of different things that can s... -0.451736   \n",
       "2924  But we will also talk about how getting a conc... -0.451736   \n",
       "2925  The brain goes through a lot of changes during... -0.451736   \n",
       "2926  During adolescence, the white matter begins to... -0.451736   \n",
       "2927  During adolescence, the prefrontal cortex (PFC... -0.451736   \n",
       "\n",
       "      standard_error  \n",
       "0           0.464009  \n",
       "1           0.464009  \n",
       "2           0.464009  \n",
       "3           0.464009  \n",
       "4           0.464009  \n",
       "...              ...  \n",
       "2923        0.469100  \n",
       "2924        0.469100  \n",
       "2925        0.469100  \n",
       "2926        0.469100  \n",
       "2927        0.469100  \n",
       "\n",
       "[2928 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efffba57-a6ab-4210-8391-5f2fccb3fd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>21ea485fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A little within the wood there was a fair cast...</td>\n",
       "      <td>-1.302688</td>\n",
       "      <td>0.450399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>a04741371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The king dwelt for many months in Nottingham, ...</td>\n",
       "      <td>-0.714009</td>\n",
       "      <td>0.506864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>5cb5ab998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When they drew near Nottingham, all the people...</td>\n",
       "      <td>-1.541347</td>\n",
       "      <td>0.478166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>622f6215e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>About this time there was living in Nottingham...</td>\n",
       "      <td>-2.054284</td>\n",
       "      <td>0.538084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id url_legal license  \\\n",
       "231  21ea485fb       NaN     NaN   \n",
       "233  a04741371       NaN     NaN   \n",
       "234  5cb5ab998       NaN     NaN   \n",
       "235  622f6215e       NaN     NaN   \n",
       "\n",
       "                                               excerpt    target  \\\n",
       "231  A little within the wood there was a fair cast... -1.302688   \n",
       "233  The king dwelt for many months in Nottingham, ... -0.714009   \n",
       "234  When they drew near Nottingham, all the people... -1.541347   \n",
       "235  About this time there was living in Nottingham... -2.054284   \n",
       "\n",
       "     standard_error  \n",
       "231        0.450399  \n",
       "233        0.506864  \n",
       "234        0.478166  \n",
       "235        0.538084  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['excerpt'].str.contains('Robin Hood')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13ba276-1696-40d2-b05f-8f5f31e79c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A little within the wood there was a fair castle, with a double moat, and surrounded by stout walls. Here dwelt that noble knight, Sir Richard Lee, to whom Robin Hood had lent the four hundred pounds to redeem his land.\\nHe saw the little company of outlaws fighting their way along, so he hastened to call them to come and take shelter in his castle.\\n\"Welcome art thou, Robin Hood! Welcome!\" he cried, as he led them in. \"Much I thank thee for thy comfort and courtesy and great kindness to me in the forest. There is no man in the world I love so much as thee. For all the proud Sheriff of Nottingham, here thou shalt be safe!—Shut the gates, and draw the bridge, and let no man come in!\" he shouted to his retainers. \"Arm you well; make ready; guard the walls! One thing, Robin, I promise thee: here shalt thou stay for twelve days as my guest, to sup, and eat, and dine.\"\\nSwiftly and readily tables were laid and cloths spread, and Robin Hood and his merry men sat down to a good meal.',\n",
       "       'A little within the wood there was an imposing castle, with a double dig, and surrounded by stout walls. Here dwelt that phantastic knight, Sir William Jones, to whom Trevor Blanford had lent the twenty hundred dollars to redeem his land.\\nHe saw the little company of small criminals fighting their way along, so he hastened to call them to come and take shelter in his castle.\\n\"Welcome art thou, Trevor Blanford! Welcome!\" he cried, as he led them in. \"Much I thank thee for thy comfort and sympathy and great kindness to me in the forest. There is no man in the world I love so much as thee. For all the proud Sheriff of Kendall, here thou shalt be safe!—Shut the gates, and draw the bridge, and let no man come in!\" he shouted to his retainers. \"Arm you well; prepare yourselves; guard the fortress! One thing, Trevor, I promise thee: here shalt thou stay for fifteen days as my guest, to sup, and eat, and dine.\"\\nSwiftly and readily tables were laid and cloths spread, and Trevor Blanford and his jolly men sat down to a good meal.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['id'] == '21ea485fb']['excerpt'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5b0bd8-527e-4b89-9082-f0780c9ae28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1305   -3.351956\n",
       "1306   -3.351956\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['id'] == '0bf29d257']['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2928.000000\n",
       "mean       -0.935530\n",
       "std         1.040718\n",
       "min        -3.676268\n",
       "25%        -1.681407\n",
       "50%        -0.875757\n",
       "75%        -0.159258\n",
       "max         1.711390\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e568bce3-f458-4471-986b-9e993ebe2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44eaabcc-b0aa-4f11-ae01-79ac86b2f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9355302017117486, 1.040717901616821)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_mean, train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a331cf16-59f9-492f-b753-96c72e69cc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.928000e+03\n",
       "mean    -2.426717e-17\n",
       "std      1.000000e+00\n",
       "min     -2.633507e+00\n",
       "25%     -7.166943e-01\n",
       "50%      5.743453e-02\n",
       "75%      7.459011e-01\n",
       "max      2.543360e+00\n",
       "Name: normalized_target, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['normalized_target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a80edc32-5e12-4487-98ba-f73d5256e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['normalized_target_check'] = train_df['normalized_target'] * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1433f5d3-98d1-419d-9635-71b379fa214f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>normalized_target</th>\n",
       "      <th>normalized_target_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patty concluded to move very slowly, thinking ...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patty concluded to move very slowly, thinking ...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>At last the game was concluded, as Roger Farri...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>0.595894</td>\n",
       "      <td>-0.315372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>0.341507</td>\n",
       "      <td>-0.580118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>-0.113848</td>\n",
       "      <td>-1.054013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>-0.113848</td>\n",
       "      <td>-1.054013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>1.136454</td>\n",
       "      <td>0.247197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>f9bf357fe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hal and Chester found ample time to take an in...</td>\n",
       "      <td>-0.861809</td>\n",
       "      <td>0.480936</td>\n",
       "      <td>0.070837</td>\n",
       "      <td>-0.861809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eaf8e7355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hal Paine and Chester Crawford were typical Am...</td>\n",
       "      <td>-1.759061</td>\n",
       "      <td>0.476507</td>\n",
       "      <td>-0.791311</td>\n",
       "      <td>-1.759061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>eaf8e7355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As related in the third book of this series, \"...</td>\n",
       "      <td>-1.759061</td>\n",
       "      <td>0.476507</td>\n",
       "      <td>-0.791311</td>\n",
       "      <td>-1.759061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0a43a07f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>On the twenty-second of February, 1916, an aut...</td>\n",
       "      <td>-0.952325</td>\n",
       "      <td>0.498116</td>\n",
       "      <td>-0.016137</td>\n",
       "      <td>-0.952325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>f7eff7419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The boys left the capitol and made their way d...</td>\n",
       "      <td>-0.371641</td>\n",
       "      <td>0.463710</td>\n",
       "      <td>0.541827</td>\n",
       "      <td>-0.371641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>d96e6dbcd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One day he had gone beyond any point which he ...</td>\n",
       "      <td>-1.238432</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>-0.291051</td>\n",
       "      <td>-1.238432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>c57b50918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was believed by the principal men of Virgin...</td>\n",
       "      <td>-3.081337</td>\n",
       "      <td>0.553260</td>\n",
       "      <td>-2.061853</td>\n",
       "      <td>-3.081337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8d8a2667f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Pedrarias was seventy-two years old. He w...</td>\n",
       "      <td>-2.145248</td>\n",
       "      <td>0.518351</td>\n",
       "      <td>-1.162388</td>\n",
       "      <td>-2.145248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a0d4cd896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Emperor walked nervously up and down the l...</td>\n",
       "      <td>-1.400318</td>\n",
       "      <td>0.494799</td>\n",
       "      <td>-0.446603</td>\n",
       "      <td>-1.400318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0963b8cf9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The clock in a nearby church struck the hour o...</td>\n",
       "      <td>-0.495299</td>\n",
       "      <td>0.471694</td>\n",
       "      <td>0.423008</td>\n",
       "      <td>-0.495299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>587502a70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aunt Abigail was gone, Eleanor was gone. The r...</td>\n",
       "      <td>0.245806</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>1.135116</td>\n",
       "      <td>0.245806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>af79954c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So off went Lionel to be made a King. He had n...</td>\n",
       "      <td>-0.188186</td>\n",
       "      <td>0.481518</td>\n",
       "      <td>0.718105</td>\n",
       "      <td>-0.188186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7656dea91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Her name was Simpkins, and her cottage was jus...</td>\n",
       "      <td>-1.286039</td>\n",
       "      <td>0.470564</td>\n",
       "      <td>-0.336795</td>\n",
       "      <td>-1.286039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>60ecc9777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The children had explored the gardens and the ...</td>\n",
       "      <td>-0.617733</td>\n",
       "      <td>0.496645</td>\n",
       "      <td>0.305363</td>\n",
       "      <td>-0.617733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>057f69731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a wet day, so none of the plans for see...</td>\n",
       "      <td>-1.126248</td>\n",
       "      <td>0.477028</td>\n",
       "      <td>-0.183256</td>\n",
       "      <td>-1.126248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5e7c0b55b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Father had been away in the country for three ...</td>\n",
       "      <td>-1.009999</td>\n",
       "      <td>0.496148</td>\n",
       "      <td>-0.071555</td>\n",
       "      <td>-1.009999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>a9ebe5d33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Then the man took off his hat and walked away,...</td>\n",
       "      <td>0.281178</td>\n",
       "      <td>0.510595</td>\n",
       "      <td>1.169105</td>\n",
       "      <td>0.281178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>47e98a5c8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>At that moment the queen came out through the ...</td>\n",
       "      <td>0.278203</td>\n",
       "      <td>0.484045</td>\n",
       "      <td>1.166246</td>\n",
       "      <td>0.278203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7285ba024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Were you ever on the coast of Maine? If so, yo...</td>\n",
       "      <td>-1.566220</td>\n",
       "      <td>0.485318</td>\n",
       "      <td>-0.606014</td>\n",
       "      <td>-1.566220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>66f0a9ff1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If nature afflicts in one direction she freque...</td>\n",
       "      <td>-2.386485</td>\n",
       "      <td>0.526079</td>\n",
       "      <td>-1.394186</td>\n",
       "      <td>-2.386485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id url_legal license  \\\n",
       "0   c12129c31       NaN     NaN   \n",
       "1   c12129c31       NaN     NaN   \n",
       "2   c12129c31       NaN     NaN   \n",
       "3   c12129c31       NaN     NaN   \n",
       "4   c12129c31       NaN     NaN   \n",
       "5   85aa80a4c       NaN     NaN   \n",
       "6   b69ac6792       NaN     NaN   \n",
       "7   dd1000b26       NaN     NaN   \n",
       "8   dd1000b26       NaN     NaN   \n",
       "9   37c1b32fb       NaN     NaN   \n",
       "10  f9bf357fe       NaN     NaN   \n",
       "11  eaf8e7355       NaN     NaN   \n",
       "12  eaf8e7355       NaN     NaN   \n",
       "13  0a43a07f1       NaN     NaN   \n",
       "14  f7eff7419       NaN     NaN   \n",
       "15  d96e6dbcd       NaN     NaN   \n",
       "16  c57b50918       NaN     NaN   \n",
       "17  8d8a2667f       NaN     NaN   \n",
       "18  a0d4cd896       NaN     NaN   \n",
       "19  0963b8cf9       NaN     NaN   \n",
       "20  587502a70       NaN     NaN   \n",
       "21  af79954c5       NaN     NaN   \n",
       "22  7656dea91       NaN     NaN   \n",
       "23  60ecc9777       NaN     NaN   \n",
       "24  057f69731       NaN     NaN   \n",
       "25  5e7c0b55b       NaN     NaN   \n",
       "26  a9ebe5d33       NaN     NaN   \n",
       "27  47e98a5c8       NaN     NaN   \n",
       "28  7285ba024       NaN     NaN   \n",
       "29  66f0a9ff1       NaN     NaN   \n",
       "\n",
       "                                              excerpt    target  \\\n",
       "0   When the young people returned to the ballroom... -0.340259   \n",
       "1   When the young people returned to the ballroom... -0.340259   \n",
       "2   Patty concluded to move very slowly, thinking ... -0.340259   \n",
       "3   Patty concluded to move very slowly, thinking ... -0.340259   \n",
       "4   At last the game was concluded, as Roger Farri... -0.340259   \n",
       "5   All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "6   As Roger had predicted, the snow departed as q... -0.580118   \n",
       "7   And outside before the palace a great garden w... -1.054013   \n",
       "8   And outside before the palace a great garden w... -1.054013   \n",
       "9   Once upon a time there were Three Bears who li...  0.247197   \n",
       "10  Hal and Chester found ample time to take an in... -0.861809   \n",
       "11  Hal Paine and Chester Crawford were typical Am... -1.759061   \n",
       "12  As related in the third book of this series, \"... -1.759061   \n",
       "13  On the twenty-second of February, 1916, an aut... -0.952325   \n",
       "14  The boys left the capitol and made their way d... -0.371641   \n",
       "15  One day he had gone beyond any point which he ... -1.238432   \n",
       "16  It was believed by the principal men of Virgin... -3.081337   \n",
       "17  This Pedrarias was seventy-two years old. He w... -2.145248   \n",
       "18  The Emperor walked nervously up and down the l... -1.400318   \n",
       "19  The clock in a nearby church struck the hour o... -0.495299   \n",
       "20  Aunt Abigail was gone, Eleanor was gone. The r...  0.245806   \n",
       "21  So off went Lionel to be made a King. He had n... -0.188186   \n",
       "22  Her name was Simpkins, and her cottage was jus... -1.286039   \n",
       "23  The children had explored the gardens and the ... -0.617733   \n",
       "24  It was a wet day, so none of the plans for see... -1.126248   \n",
       "25  Father had been away in the country for three ... -1.009999   \n",
       "26  Then the man took off his hat and walked away,...  0.281178   \n",
       "27  At that moment the queen came out through the ...  0.278203   \n",
       "28  Were you ever on the coast of Maine? If so, yo... -1.566220   \n",
       "29  If nature afflicts in one direction she freque... -2.386485   \n",
       "\n",
       "    standard_error  normalized_target  normalized_target_check  \n",
       "0         0.464009           0.571981                -0.340259  \n",
       "1         0.464009           0.571981                -0.340259  \n",
       "2         0.464009           0.571981                -0.340259  \n",
       "3         0.464009           0.571981                -0.340259  \n",
       "4         0.464009           0.571981                -0.340259  \n",
       "5         0.480805           0.595894                -0.315372  \n",
       "6         0.476676           0.341507                -0.580118  \n",
       "7         0.450007          -0.113848                -1.054013  \n",
       "8         0.450007          -0.113848                -1.054013  \n",
       "9         0.510845           1.136454                 0.247197  \n",
       "10        0.480936           0.070837                -0.861809  \n",
       "11        0.476507          -0.791311                -1.759061  \n",
       "12        0.476507          -0.791311                -1.759061  \n",
       "13        0.498116          -0.016137                -0.952325  \n",
       "14        0.463710           0.541827                -0.371641  \n",
       "15        0.465900          -0.291051                -1.238432  \n",
       "16        0.553260          -2.061853                -3.081337  \n",
       "17        0.518351          -1.162388                -2.145248  \n",
       "18        0.494799          -0.446603                -1.400318  \n",
       "19        0.471694           0.423008                -0.495299  \n",
       "20        0.491793           1.135116                 0.245806  \n",
       "21        0.481518           0.718105                -0.188186  \n",
       "22        0.470564          -0.336795                -1.286039  \n",
       "23        0.496645           0.305363                -0.617733  \n",
       "24        0.477028          -0.183256                -1.126248  \n",
       "25        0.496148          -0.071555                -1.009999  \n",
       "26        0.510595           1.169105                 0.281178  \n",
       "27        0.484045           1.166246                 0.278203  \n",
       "28        0.485318          -0.606014                -1.566220  \n",
       "29        0.526079          -1.394186                -2.386485  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5cd025a-a37c-4aaf-a192-ac278264a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b62eb3-cedd-4629-a726-173588d6a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b48a945-271e-4c18-afac-e0a4c9194e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(train_df['target'],bins=100,kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01155bd0-8460-404e-8b2b-f8e72d5ef04d",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11427cac-5fa2-4449-bc11-c8b870fe3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53766f20-5598-499a-b1a6-0a6b19060132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    batch_size = 16\n",
    "    max_len = 256\n",
    "    num_workers = 4\n",
    "    epochs = 16\n",
    "    pretrained_transformers_model = f'roberta-base'\n",
    "    target_field = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3933dfcb-16b7-4691-b8ac-f1878b1cc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CONFIG()\n",
    "cfg.model_name = 'roberta-base'\n",
    "cfg.num_folds = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_target_std = float(train_target_std)\n",
    "cfg.train_target_mean = float(train_target_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d7236c3-198b-440a-9f7b-fd714be1a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert Path(cfg.pretrained_transformers_model).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7bb8579-7374-48dd-9847-672dc57eff20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'roberta-base',\n",
       " 'num_folds': 12,\n",
       " 'train_target_std': 1.040717901616821,\n",
       " 'train_target_mean': -0.9355302017117486}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17671f1f-4785-459f-98ec-ebcaacda6c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/commonlit/models/facebook’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir {MODELS_PATH}/facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce25d430-59b5-4d91-936f-589dc022dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELS_PATH}/{cfg.model_name}.yaml', 'w') as file:\n",
    "    documents = yaml.dump(vars(cfg), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Prepare Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_df[cfg.target_field].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbbc9a60-4c04-447a-9ddc-e5125688f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_bins = int(np.floor(np.log2(len(train_df))) + 1)\n",
    "# train_df.loc[:, 'bins'] = pd.cut(train_df[cfg.target_field], bins=num_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df):\n",
    "    num_bins = int(np.floor(np.log2(len(train_df))) + 1)\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df[cfg.target_field], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = add_bins(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0769b07-e9ea-42b7-95a6-5becf82dd824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.411708</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.969369</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.518891</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.106393</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.652995</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.201392</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.745424</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.309740</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.131842</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.567749</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.978923</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.399764</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target      \n",
       "          mean count\n",
       "bins                \n",
       "0    -3.411708    44\n",
       "1    -2.969369    79\n",
       "2    -2.518891   182\n",
       "3    -2.106393   269\n",
       "4    -1.652995   368\n",
       "5    -1.201392   420\n",
       "6    -0.745424   492\n",
       "7    -0.309740   427\n",
       "8     0.131842   317\n",
       "9     0.567749   224\n",
       "10    0.978923    83\n",
       "11    1.399764    23"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[[cfg.target_field, 'bins']].groupby(['bins']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=cfg.num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kfold(train_df):\n",
    "    for i, (t_, v_) in enumerate(kf.split(X=train_df, y=train_df.bins.values)):\n",
    "        train_df.loc[v_, 'kfold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kfold(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b311fb49-dfdc-43e7-a89a-182b731c879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['kfold'] = train_df['kfold'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de386c06-bb90-4034-b290-cd03cb61cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('bins', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>normalized_target</th>\n",
       "      <th>normalized_target_check</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patty concluded to move very slowly, thinking ...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patty concluded to move very slowly, thinking ...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>At last the game was concluded, as Roger Farri...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.571981</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>There are heaps of different things that can s...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.464866</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>But we will also talk about how getting a conc...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.464866</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>The brain goes through a lot of changes during...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.464866</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>During adolescence, the white matter begins to...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.464866</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>e48f075f9</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>During adolescence, the prefrontal cortex (PFC...</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.464866</td>\n",
       "      <td>-0.451736</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2928 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal    license  \\\n",
       "0     c12129c31                                                NaN        NaN   \n",
       "1     c12129c31                                                NaN        NaN   \n",
       "2     c12129c31                                                NaN        NaN   \n",
       "3     c12129c31                                                NaN        NaN   \n",
       "4     c12129c31                                                NaN        NaN   \n",
       "...         ...                                                ...        ...   \n",
       "2923  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2924  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2925  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2926  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "2927  e48f075f9  https://kids.frontiersin.org/article/10.3389/f...  CC BY 4.0   \n",
       "\n",
       "                                                excerpt    target  \\\n",
       "0     When the young people returned to the ballroom... -0.340259   \n",
       "1     When the young people returned to the ballroom... -0.340259   \n",
       "2     Patty concluded to move very slowly, thinking ... -0.340259   \n",
       "3     Patty concluded to move very slowly, thinking ... -0.340259   \n",
       "4     At last the game was concluded, as Roger Farri... -0.340259   \n",
       "...                                                 ...       ...   \n",
       "2923  There are heaps of different things that can s... -0.451736   \n",
       "2924  But we will also talk about how getting a conc... -0.451736   \n",
       "2925  The brain goes through a lot of changes during... -0.451736   \n",
       "2926  During adolescence, the white matter begins to... -0.451736   \n",
       "2927  During adolescence, the prefrontal cortex (PFC... -0.451736   \n",
       "\n",
       "      standard_error  normalized_target  normalized_target_check  kfold  \n",
       "0           0.464009           0.571981                -0.340259      0  \n",
       "1           0.464009           0.571981                -0.340259      0  \n",
       "2           0.464009           0.571981                -0.340259      0  \n",
       "3           0.464009           0.571981                -0.340259      0  \n",
       "4           0.464009           0.571981                -0.340259      0  \n",
       "...              ...                ...                      ...    ...  \n",
       "2923        0.469100           0.464866                -0.451736     11  \n",
       "2924        0.469100           0.464866                -0.451736     11  \n",
       "2925        0.469100           0.464866                -0.451736     11  \n",
       "2926        0.469100           0.464866                -0.451736     11  \n",
       "2927        0.469100           0.464866                -0.451736     11  \n",
       "\n",
       "[2928 rows x 9 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kfold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.920794</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.940242</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.920961</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.910262</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.941331</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.936685</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.959726</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.944420</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.947685</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.939967</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.943887</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.920404</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target      \n",
       "           mean count\n",
       "kfold                \n",
       "0     -0.920794   244\n",
       "1     -0.940242   244\n",
       "2     -0.920961   244\n",
       "3     -0.910262   244\n",
       "4     -0.941331   244\n",
       "5     -0.936685   244\n",
       "6     -0.959726   244\n",
       "7     -0.944420   244\n",
       "8     -0.947685   244\n",
       "9     -0.939967   244\n",
       "10    -0.943887   244\n",
       "11    -0.920404   244"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[[cfg.target_field, 'kfold']].groupby(['kfold']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9, 11, 8, 6, 4, 5, 10, 3, 1, 7, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_list = list(range(num_bins))\n",
    "random.shuffle(bin_list)\n",
    "bin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8f50740-d161-4f65-a762-169977d95a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.426716993716189e-17"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['normalized_target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fd3a4-9d7b-42be-b31c-2f6c6c15a3a0",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d57cfc8c-551e-4f67-91f8-5aec7002d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "def rmse_score_2(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18e1c48e-da7e-4a1e-b91e-c9c5a262a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10)\n",
    "b = np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "883bd02c-7ab8-4fdc-bb6e-42e6c98a0935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5932371878578023, 0.5932371878578023)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(a, b), rmse_score_2(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96bf65-b75e-468d-83e2-1d60792baebd",
   "metadata": {},
   "source": [
    "### Prepare train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e601b365-a123-4a1d-bd31-3e6aeeee2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(fold = [1]):\n",
    "    valid_df = train_df[train_df['kfold'].isin(fold)]\n",
    "    valid_text = valid_df['excerpt'].values\n",
    "    valid_target = valid_df[cfg.target_field].values\n",
    "    training_df = train_df[~train_df['kfold'].isin(fold)]\n",
    "    train_text = training_df['excerpt'].values\n",
    "    train_target = training_df[cfg.target_field].values\n",
    "    return train_text, train_target, valid_text, valid_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8575c418-6c76-409c-9ce2-f907a92764d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2684, 244)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text, train_target, valid_text, valid_target = create_split([0])\n",
    "len(train_text), len(valid_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c638-0a6f-4b0b-baab-9422908e0343",
   "metadata": {},
   "source": [
    "### Prepare Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34addbcd-c989-475e-9c61-72b2e3ddf8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "# Save the tokenizer so that you can download the files and move it to a Kaggle dataset.\n",
    "# tokenizer.save_pretrained(cfg.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10cea73f-5dd2-42f2-bc8d-454190f84655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer(train_df['excerpt'].values[0],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=cfg.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                return_token_type_ids=True)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"].squeeze())\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6caf9132-a5d5-460f-8584-5f22a87abac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1ee2410-2ce9-46c8-9e93-9aeb14bf6bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eab4041e-0b2d-41d6-94a3-6e3646e3427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d093a87-1edd-4e0b-8545-5e97a91a7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, target, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.target = target\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return InputFeatures(input_ids=convert_to_list(encode['input_ids']),\n",
    "                      attention_mask=convert_to_list(encode['attention_mask']),\n",
    "                      label=torch.tensor(self.target[idx]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a038b280-5d95-4a08-b3c6-da445770f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target):\n",
    "    train_ds = CommonLitDataset(train_text, train_target, tokenizer, cfg.max_len)\n",
    "    valid_ds = CommonLitDataset(valid_text, valid_target, tokenizer, cfg.max_len)\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13ae453b-84a5-4a13-a2fc-00e5562a95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode.keys(), target.shape, encode['input_ids'].shape, encode['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddf1a2cf-19d7-42c1-ac39-83a8bb2655bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode['input_ids'][0].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed362b-cd1d-4d1a-b343-6ecddef7e324",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "863d53eb-7874-4bef-8b6a-d542749f7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# You can use a Transformer model of your choice.\n",
    "# transformer_model = DistilBertModel.from_pretrained(cfg.pretrained_transformers_model)\n",
    "transformer_model = AutoModel.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2551c8b-5ac7-4af0-bb2f-854c7fc45559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(transformer_out)['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "814e739b-f7a1-405c-b13c-31e6b93dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean(transformer_out.last_hidden_state, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecf50196-0a69-4db8-bc17-b967f89911f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_layer = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "132b7585-1f6e-4482-9b92-9a7260618888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "403ef922-6ede-4fc1-83a1-fe12b85a517b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.7.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f2dbb7c-e9b8-47af-a124-32c5a5448ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5021054-9f67-404b-9a03-a34db81e49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class CommonLitModel(PreTrainedModel):\n",
    "    def __init__(self):\n",
    "        super(PreTrainedModel, self).__init__()\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_transformers_model)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.config = AutoConfig.from_pretrained(cfg.pretrained_transformers_model)\n",
    "        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
    "        self.out = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer_model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), \n",
    "                                                 output_hidden_states=True)\n",
    "        x = transformer_out.pooler_output\n",
    "#         x = transformer_out.last_hidden_state[:, 0, :] # N, C, X\n",
    "#         x = torch.mean(transformer_out.hidden_states[-2], axis=1)\n",
    "#         x2 = torch.mean(transformer_out.encoder_hidden_states[-2], axis=1)\n",
    "#         x = torch.cat([x, x2], axis=1)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n",
    "        \"\"\"\n",
    "        For models that inherit from :class:`~transformers.PreTrainedModel`, uses that method to compute the number of\n",
    "        floating point operations for every backward + forward pass. If using another model, either implement such a\n",
    "        method in the model or subclass and override this method.\n",
    "        Args:\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "        Returns:\n",
    "            :obj:`int`: The number of floating-point operations.\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68bf7bb6-efa1-438e-8f61-a60f46df79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d402996c-e965-4047-a39c-68a2c465280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict.token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87795a72-d4cc-4481-a07c-28cbbdf32458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict.token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a04bb94-1364-4709-bc76-878d6d9ced4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = transformer_model.cuda()\n",
    "sample_out = transformer_model(encoded_dict.input_ids.cuda(), encoded_dict.attention_mask.cuda(), encoded_dict.token_type_ids.cuda(), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e942e16-3653-4a19-b765-f3ef11189391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e210ebf4-4ada-4a98-aa85-fb77e052e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = torch.mean(sample_out.hidden_states[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3aba55a-5465-4871-99ef-1f5a9c51d00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([sample_x, sample_x], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34428a20-76bf-4dcd-ad14-c008f82c7123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out.hidden_states[-1].shape, sample_out.hidden_states[-1][:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0504d936-bebe-4fbd-90b5-b5b990501033",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bfcf10f5-20f7-4f97-9a17-35c63ec4915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7265d3b2-f7cc-48aa-9f2b-66987f10182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.attention_mask.unsqueeze(0).shape, encoded_dict.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b825b57-a244-4a0c-be59-102e9c8f2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_out = transformer_model(encode.input_ids.unsqueeze(0).cuda(), encode.attention_mask.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d826f6b-79c8-447d-9d78-df79b99e6655",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68a48686-ddb0-4046-ab70-40e16117bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8fa4e6a-6a87-4860-b33a-075c5e693a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "\n",
    "def loss_fct(yhat, y):\n",
    "    return criterion(yhat, y) * 0.7 + 0.3 * kl_loss(yhat, y)\n",
    "\n",
    "loss_fct = nn.MSELoss()\n",
    "\n",
    "def loss_fct(yhat, y):\n",
    "    return torch.sqrt(criterion(yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5fed3ae-50e7-4c4e-b364-588fa921782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'learning_rate': 3.737865823546993e-05, 'num_train_epochs': 16, 'seed': 39, 'per_device_train_batch_size': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a861b0e-1207-454d-81fa-4a684a153d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_args(fold):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{fold}\"),\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_first_step=True,\n",
    "        save_steps=40000,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_total_limit = 3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='rmse',\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=3e-05,\n",
    "        report_to=None,\n",
    "        seed=39\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb7ea1ed-e72f-43fd-aa2c-b1645480f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    return {'rmse': rmse_score_2(logits, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aab5a7e5-3ab5-4896-bc96-e43091994174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "325a9194-570e-4c1b-83c7-eb0fe7723de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "826b6f17-dd24-4be8-8644-4e4e40712f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class CommonLitTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        input_ids = inputs.pop(\"input_ids\")\n",
    "        attention_mask = inputs.pop(\"attention_mask\")\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs\n",
    "        loss = loss_fct(logits.flatten(),\n",
    "                        labels.float().flatten())\n",
    "        zero_cat = torch.zeros([1, 1]).to(outputs.device)\n",
    "        return (loss, torch.cat([zero_cat, outputs])) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "004a6849-c4d7-40e9-98e9-eacd947f550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/commonlit/models/{cfg.model_name.replace('/', '_')}-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a8d4f-8fb7-4519-8c81-aeef2cd444cd",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e813ece-98c4-447e-972e-97757ed50b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(i = -1):\n",
    "    if i == -1:\n",
    "        return CommonLitModel()\n",
    "    else:\n",
    "        inference_model = CommonLitModel()\n",
    "        inference_model = inference_model.cuda()\n",
    "        inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{i}/pytorch_model.bin\")))\n",
    "        inference_model.eval();\n",
    "        return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c828c520-9d42-4630-b37a-2df0b5c9b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "def gen_model_init(i = -1):\n",
    "    return gen_model(-1)\n",
    "\n",
    "def gen_model_improve(i):\n",
    "    return gen_model(i)\n",
    "\n",
    "def do_train(gen_model=gen_model_init, learning_rate=None):\n",
    "    bin_step = 1\n",
    "    bestmodels = []\n",
    "    eval_rmses = []\n",
    "    for i in range(0, cfg.num_folds, bin_step):\n",
    "        train_bins = bin_list[i:i+bin_step]\n",
    "        print('train_bins', f'{i}: {train_bins}')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "        train_text, train_target, valid_text, valid_target = create_split([i])\n",
    "        train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)\n",
    "        training_args = create_training_args(i)\n",
    "        if learning_rate is not None:\n",
    "            training_args.learning_rate = learning_rate\n",
    "        model = gen_model(i)\n",
    "    #     wandb.init(project=f\"commonlit_{cfg.model_name.replace('/', '_')}\")\n",
    "        trainer = CommonLitTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=valid_ds,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=12)]\n",
    "        )\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        print('training_args.output_dir', training_args.output_dir)\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "        result = trainer.evaluate()\n",
    "        bestmodels.append(trainer.state.best_model_checkpoint)\n",
    "        print('best_model_checkpoint', trainer.state.best_model_checkpoint)\n",
    "        print('result', result)\n",
    "        eval_rmses.append(result['eval_rmse'])\n",
    "    return eval_rmses, bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3822354-7ee5-4f5f-93e6-f357a0157811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 0: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:49, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.914100</td>\n",
       "      <td>0.783099</td>\n",
       "      <td>0.798601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653900</td>\n",
       "      <td>0.662302</td>\n",
       "      <td>0.680518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.649469</td>\n",
       "      <td>0.662514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.483600</td>\n",
       "      <td>0.640348</td>\n",
       "      <td>0.656115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.639594</td>\n",
       "      <td>0.653866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.709039</td>\n",
       "      <td>0.725457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.360600</td>\n",
       "      <td>0.768635</td>\n",
       "      <td>0.783268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.711626</td>\n",
       "      <td>0.723635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.660652</td>\n",
       "      <td>0.674778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.661585</td>\n",
       "      <td>0.674555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.685644</td>\n",
       "      <td>0.698263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.657447</td>\n",
       "      <td>0.673037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.655755</td>\n",
       "      <td>0.672410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>0.686438</td>\n",
       "      <td>0.700434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>0.682571</td>\n",
       "      <td>0.696838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>0.662965</td>\n",
       "      <td>0.678509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-0/checkpoint-840\n",
      "result {'eval_loss': 0.6395938396453857, 'eval_rmse': 0.6538658142089844, 'eval_runtime': 1.4454, 'eval_samples_per_second': 168.809, 'eval_steps_per_second': 11.069, 'epoch': 16.0}\n",
      "train_bins 1: [9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:50, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>0.696837</td>\n",
       "      <td>0.716605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.582641</td>\n",
       "      <td>0.614510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.515499</td>\n",
       "      <td>0.531748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.604783</td>\n",
       "      <td>0.641203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.497670</td>\n",
       "      <td>0.522545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.685448</td>\n",
       "      <td>0.708860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.677514</td>\n",
       "      <td>0.697435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.653652</td>\n",
       "      <td>0.675654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.314700</td>\n",
       "      <td>0.489038</td>\n",
       "      <td>0.516884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.524722</td>\n",
       "      <td>0.548319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.510119</td>\n",
       "      <td>0.533385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.524905</td>\n",
       "      <td>0.551637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.563583</td>\n",
       "      <td>0.590292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.609251</td>\n",
       "      <td>0.633341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.577665</td>\n",
       "      <td>0.604403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.572291</td>\n",
       "      <td>0.597053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-1/checkpoint-1512\n",
      "result {'eval_loss': 0.48903775215148926, 'eval_rmse': 0.5168837308883667, 'eval_runtime': 1.4592, 'eval_samples_per_second': 167.216, 'eval_steps_per_second': 10.965, 'epoch': 16.0}\n",
      "train_bins 2: [11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:47, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.901100</td>\n",
       "      <td>0.619712</td>\n",
       "      <td>0.659472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.563385</td>\n",
       "      <td>0.579703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.551700</td>\n",
       "      <td>0.572268</td>\n",
       "      <td>0.586149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.599923</td>\n",
       "      <td>0.620753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.601946</td>\n",
       "      <td>0.611416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>0.514894</td>\n",
       "      <td>0.520751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.533624</td>\n",
       "      <td>0.543581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>0.638751</td>\n",
       "      <td>0.658202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.310100</td>\n",
       "      <td>0.600987</td>\n",
       "      <td>0.611240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.556865</td>\n",
       "      <td>0.567837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.541885</td>\n",
       "      <td>0.557175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.544799</td>\n",
       "      <td>0.566480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.639559</td>\n",
       "      <td>0.658637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>0.623736</td>\n",
       "      <td>0.641298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>0.634171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>0.590434</td>\n",
       "      <td>0.606285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-2/checkpoint-1008\n",
      "result {'eval_loss': 0.5148942470550537, 'eval_rmse': 0.520750880241394, 'eval_runtime': 1.4491, 'eval_samples_per_second': 168.381, 'eval_steps_per_second': 11.041, 'epoch': 16.0}\n",
      "train_bins 3: [8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2352' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2352/2688 06:48 < 00:58, 5.75 it/s, Epoch 14/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>0.755362</td>\n",
       "      <td>0.768908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.669200</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.500602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.668116</td>\n",
       "      <td>0.678135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>0.649762</td>\n",
       "      <td>0.662287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.652977</td>\n",
       "      <td>0.665426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.636978</td>\n",
       "      <td>0.648136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.569811</td>\n",
       "      <td>0.579801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.632376</td>\n",
       "      <td>0.643560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.599397</td>\n",
       "      <td>0.606179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290100</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.626560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.617061</td>\n",
       "      <td>0.625101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.611594</td>\n",
       "      <td>0.624178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.671350</td>\n",
       "      <td>0.680347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.646009</td>\n",
       "      <td>0.656787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-3/checkpoint-336\n",
      "result {'eval_loss': 0.493867963552475, 'eval_rmse': 0.5006024241447449, 'eval_runtime': 1.4445, 'eval_samples_per_second': 168.918, 'eval_steps_per_second': 11.077, 'epoch': 14.0}\n",
      "train_bins 4: [6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2352' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2352/2688 06:51 < 00:58, 5.70 it/s, Epoch 14/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.631077</td>\n",
       "      <td>0.653934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704300</td>\n",
       "      <td>0.509745</td>\n",
       "      <td>0.526025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.586258</td>\n",
       "      <td>0.619398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.668995</td>\n",
       "      <td>0.692499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.780694</td>\n",
       "      <td>0.803894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.579172</td>\n",
       "      <td>0.606405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.579536</td>\n",
       "      <td>0.592989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.617457</td>\n",
       "      <td>0.657420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.670062</td>\n",
       "      <td>0.690371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.585793</td>\n",
       "      <td>0.606382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.577926</td>\n",
       "      <td>0.605855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.513007</td>\n",
       "      <td>0.534599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.579795</td>\n",
       "      <td>0.600631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.560257</td>\n",
       "      <td>0.586607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-4/checkpoint-336\n",
      "result {'eval_loss': 0.5097450017929077, 'eval_rmse': 0.5260251760482788, 'eval_runtime': 1.4432, 'eval_samples_per_second': 169.073, 'eval_steps_per_second': 11.087, 'epoch': 14.0}\n",
      "train_bins 5: [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:53, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.508510</td>\n",
       "      <td>0.521170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.529543</td>\n",
       "      <td>0.542721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>0.546981</td>\n",
       "      <td>0.558148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>0.739402</td>\n",
       "      <td>0.755798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.475547</td>\n",
       "      <td>0.490931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.593569</td>\n",
       "      <td>0.604542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.374300</td>\n",
       "      <td>0.598697</td>\n",
       "      <td>0.610919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.567580</td>\n",
       "      <td>0.583042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.537262</td>\n",
       "      <td>0.551503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.555071</td>\n",
       "      <td>0.570471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.574171</td>\n",
       "      <td>0.587339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.621003</td>\n",
       "      <td>0.636556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.246900</td>\n",
       "      <td>0.557644</td>\n",
       "      <td>0.572282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.579438</td>\n",
       "      <td>0.594577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.536099</td>\n",
       "      <td>0.550703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.573667</td>\n",
       "      <td>0.588657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-5/checkpoint-840\n",
      "result {'eval_loss': 0.4755469560623169, 'eval_rmse': 0.4909307658672333, 'eval_runtime': 1.442, 'eval_samples_per_second': 169.211, 'eval_steps_per_second': 11.096, 'epoch': 16.0}\n",
      "train_bins 6: [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:52, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>0.515831</td>\n",
       "      <td>0.524690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.435935</td>\n",
       "      <td>0.444653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>0.704219</td>\n",
       "      <td>0.715867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>0.421089</td>\n",
       "      <td>0.428051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.492205</td>\n",
       "      <td>0.508689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>0.717893</td>\n",
       "      <td>0.728449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>0.653499</td>\n",
       "      <td>0.671749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.412826</td>\n",
       "      <td>0.420046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.556254</td>\n",
       "      <td>0.568975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.513007</td>\n",
       "      <td>0.523051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.475999</td>\n",
       "      <td>0.488616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.474559</td>\n",
       "      <td>0.489193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.461005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>0.590555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.561299</td>\n",
       "      <td>0.572599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.530247</td>\n",
       "      <td>0.539310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-6/checkpoint-1344\n",
      "result {'eval_loss': 0.4128262996673584, 'eval_rmse': 0.42004555463790894, 'eval_runtime': 1.429, 'eval_samples_per_second': 170.751, 'eval_steps_per_second': 11.197, 'epoch': 16.0}\n",
      "train_bins 7: [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:46, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>0.518108</td>\n",
       "      <td>0.526193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.501834</td>\n",
       "      <td>0.511486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.533923</td>\n",
       "      <td>0.551732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.675056</td>\n",
       "      <td>0.704275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.502743</td>\n",
       "      <td>0.512640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.490751</td>\n",
       "      <td>0.502486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.530235</td>\n",
       "      <td>0.540028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.455109</td>\n",
       "      <td>0.461411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.630165</td>\n",
       "      <td>0.652777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.297700</td>\n",
       "      <td>0.485656</td>\n",
       "      <td>0.496338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.562921</td>\n",
       "      <td>0.582099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.513548</td>\n",
       "      <td>0.526338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.522437</td>\n",
       "      <td>0.536894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.544648</td>\n",
       "      <td>0.561063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.540869</td>\n",
       "      <td>0.556473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.534760</td>\n",
       "      <td>0.551501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-7/checkpoint-1344\n",
      "result {'eval_loss': 0.45510876178741455, 'eval_rmse': 0.46141064167022705, 'eval_runtime': 1.4226, 'eval_samples_per_second': 171.515, 'eval_steps_per_second': 11.247, 'epoch': 16.0}\n",
      "train_bins 8: [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:45, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.655521</td>\n",
       "      <td>0.677440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.779725</td>\n",
       "      <td>0.810609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.617225</td>\n",
       "      <td>0.637055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.694399</td>\n",
       "      <td>0.717925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.642365</td>\n",
       "      <td>0.660624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.568373</td>\n",
       "      <td>0.580233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.601907</td>\n",
       "      <td>0.624047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.530129</td>\n",
       "      <td>0.539511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.307300</td>\n",
       "      <td>0.720043</td>\n",
       "      <td>0.736207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>0.635441</td>\n",
       "      <td>0.651832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.603692</td>\n",
       "      <td>0.620955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.586989</td>\n",
       "      <td>0.606038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.579780</td>\n",
       "      <td>0.594851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.648304</td>\n",
       "      <td>0.663477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.630541</td>\n",
       "      <td>0.644940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.605701</td>\n",
       "      <td>0.620750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-8/checkpoint-1344\n",
      "result {'eval_loss': 0.5301288962364197, 'eval_rmse': 0.5395112633705139, 'eval_runtime': 1.431, 'eval_samples_per_second': 170.505, 'eval_steps_per_second': 11.181, 'epoch': 16.0}\n",
      "train_bins 9: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:47, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>0.631764</td>\n",
       "      <td>0.652696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>0.519062</td>\n",
       "      <td>0.537026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.670105</td>\n",
       "      <td>0.692232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.876582</td>\n",
       "      <td>0.895946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.437400</td>\n",
       "      <td>0.622790</td>\n",
       "      <td>0.639756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.595366</td>\n",
       "      <td>0.609558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>0.516892</td>\n",
       "      <td>0.532436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.610252</td>\n",
       "      <td>0.621853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.664714</td>\n",
       "      <td>0.681275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.579866</td>\n",
       "      <td>0.598279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.567505</td>\n",
       "      <td>0.584137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.596547</td>\n",
       "      <td>0.612289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.566806</td>\n",
       "      <td>0.581365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.637081</td>\n",
       "      <td>0.652160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.619568</td>\n",
       "      <td>0.633345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.590594</td>\n",
       "      <td>0.604356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-9/checkpoint-1176\n",
      "result {'eval_loss': 0.5168922543525696, 'eval_rmse': 0.5324362516403198, 'eval_runtime': 1.4529, 'eval_samples_per_second': 167.943, 'eval_steps_per_second': 11.013, 'epoch': 16.0}\n",
      "train_bins 10: [7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:40, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.927600</td>\n",
       "      <td>0.608548</td>\n",
       "      <td>0.632314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.585069</td>\n",
       "      <td>0.603464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.557300</td>\n",
       "      <td>0.686529</td>\n",
       "      <td>0.704827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.476100</td>\n",
       "      <td>0.866658</td>\n",
       "      <td>0.881530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.617176</td>\n",
       "      <td>0.629894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.710159</td>\n",
       "      <td>0.723888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.609124</td>\n",
       "      <td>0.625945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.330700</td>\n",
       "      <td>0.665829</td>\n",
       "      <td>0.680830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.640432</td>\n",
       "      <td>0.652223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.557164</td>\n",
       "      <td>0.574196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.716374</td>\n",
       "      <td>0.729557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.650382</td>\n",
       "      <td>0.664809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.646957</td>\n",
       "      <td>0.661255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.690188</td>\n",
       "      <td>0.703019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.691098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.662933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-10/checkpoint-1680\n",
      "result {'eval_loss': 0.557164192199707, 'eval_rmse': 0.5741955637931824, 'eval_runtime': 1.4407, 'eval_samples_per_second': 169.358, 'eval_steps_per_second': 11.105, 'epoch': 16.0}\n",
      "train_bins 11: [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2352' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2352/2688 06:53 < 00:59, 5.68 it/s, Epoch 14/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.873800</td>\n",
       "      <td>0.752297</td>\n",
       "      <td>0.795913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.624824</td>\n",
       "      <td>0.661478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565900</td>\n",
       "      <td>0.916131</td>\n",
       "      <td>0.966771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.763025</td>\n",
       "      <td>0.805956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>0.704032</td>\n",
       "      <td>0.742202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.646626</td>\n",
       "      <td>0.688187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.740197</td>\n",
       "      <td>0.774591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.677872</td>\n",
       "      <td>0.717093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.305400</td>\n",
       "      <td>0.682106</td>\n",
       "      <td>0.718801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.688208</td>\n",
       "      <td>0.731060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.668568</td>\n",
       "      <td>0.708417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.659566</td>\n",
       "      <td>0.702057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.238500</td>\n",
       "      <td>0.627167</td>\n",
       "      <td>0.668265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.239400</td>\n",
       "      <td>0.700538</td>\n",
       "      <td>0.739081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-11/checkpoint-336\n",
      "result {'eval_loss': 0.6248243451118469, 'eval_rmse': 0.6614784002304077, 'eval_runtime': 1.431, 'eval_samples_per_second': 170.514, 'eval_steps_per_second': 11.181, 'epoch': 14.0}\n",
      "CPU times: user 1h 28min 1s, sys: 3min 54s, total: 1h 31min 56s\n",
      "Wall time: 1h 33min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "        \n",
    "eval_rmses, bestmodels = do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5290ec66-8db4-4047-8fea-8d8f609c3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mean best RSME losses', 0.5331780388951302)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Mean best RSME losses', np.array(eval_rmses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107a8d7-48eb-4171-8244-9aa927ceace9",
   "metadata": {},
   "source": [
    "#### Start second training with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7a5a24c2-0c49-4ad9-b3c3-fa2032e58449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "088368e8-11a8-44b7-9216-e414c5684767",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = add_bins(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8c1ba32-8641-4820-8fcd-847ac1f81cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kfold(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6fa22ff7-bdf5-4f56-b90d-6023c0b432a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "662ceb25-2892-4c06-863c-e591533b63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 0: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>0.522977</td>\n",
       "      <td>0.543480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.754966</td>\n",
       "      <td>0.762678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.563913</td>\n",
       "      <td>0.582870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.552371</td>\n",
       "      <td>0.571678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>0.658795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.690956</td>\n",
       "      <td>0.701031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.247300</td>\n",
       "      <td>0.703624</td>\n",
       "      <td>0.715453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.667485</td>\n",
       "      <td>0.679388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-0/checkpoint-163\n",
      "result {'eval_loss': 0.5229774117469788, 'eval_rmse': 0.5434800982475281, 'eval_runtime': 1.3847, 'eval_samples_per_second': 171.162, 'eval_steps_per_second': 10.833, 'epoch': 8.0}\n",
      "train_bins 1: [9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:48, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.473580</td>\n",
       "      <td>0.498504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.330700</td>\n",
       "      <td>0.529719</td>\n",
       "      <td>0.562338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.549730</td>\n",
       "      <td>0.571306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.640558</td>\n",
       "      <td>0.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.502951</td>\n",
       "      <td>0.529475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.538344</td>\n",
       "      <td>0.565779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.545292</td>\n",
       "      <td>0.569818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.509592</td>\n",
       "      <td>0.533912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-1/checkpoint-163\n",
      "result {'eval_loss': 0.47358036041259766, 'eval_rmse': 0.4985039234161377, 'eval_runtime': 1.4015, 'eval_samples_per_second': 169.105, 'eval_steps_per_second': 10.703, 'epoch': 8.0}\n",
      "train_bins 2: [11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:46, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.549305</td>\n",
       "      <td>0.568551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.376300</td>\n",
       "      <td>0.599214</td>\n",
       "      <td>0.612313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.523953</td>\n",
       "      <td>0.536107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.314800</td>\n",
       "      <td>0.572687</td>\n",
       "      <td>0.579696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.566389</td>\n",
       "      <td>0.578749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.541769</td>\n",
       "      <td>0.552875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.546921</td>\n",
       "      <td>0.560044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.564568</td>\n",
       "      <td>0.577242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-2/checkpoint-489\n",
      "result {'eval_loss': 0.5239534378051758, 'eval_rmse': 0.536106526851654, 'eval_runtime': 1.4162, 'eval_samples_per_second': 166.637, 'eval_steps_per_second': 10.591, 'epoch': 8.0}\n",
      "train_bins 3: [8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:47, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.577200</td>\n",
       "      <td>0.503331</td>\n",
       "      <td>0.511954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.694826</td>\n",
       "      <td>0.709438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.757652</td>\n",
       "      <td>0.773194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.737878</td>\n",
       "      <td>0.749882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.338000</td>\n",
       "      <td>0.629589</td>\n",
       "      <td>0.644119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>0.695096</td>\n",
       "      <td>0.709610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.678992</td>\n",
       "      <td>0.694074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.650315</td>\n",
       "      <td>0.666505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-3/checkpoint-163\n",
      "result {'eval_loss': 0.5033307671546936, 'eval_rmse': 0.5119540095329285, 'eval_runtime': 1.4238, 'eval_samples_per_second': 165.751, 'eval_steps_per_second': 10.535, 'epoch': 8.0}\n",
      "train_bins 4: [6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:47, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.590800</td>\n",
       "      <td>0.486388</td>\n",
       "      <td>0.510742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.510316</td>\n",
       "      <td>0.525549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.648338</td>\n",
       "      <td>0.663378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.636245</td>\n",
       "      <td>0.653675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.581768</td>\n",
       "      <td>0.593057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.559250</td>\n",
       "      <td>0.576540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.610105</td>\n",
       "      <td>0.627931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.605554</td>\n",
       "      <td>0.621856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-4/checkpoint-163\n",
      "result {'eval_loss': 0.4863879084587097, 'eval_rmse': 0.5107418894767761, 'eval_runtime': 1.3932, 'eval_samples_per_second': 169.393, 'eval_steps_per_second': 10.767, 'epoch': 8.0}\n",
      "train_bins 5: [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:47, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.528727</td>\n",
       "      <td>0.543604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.485964</td>\n",
       "      <td>0.497805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.361400</td>\n",
       "      <td>0.531036</td>\n",
       "      <td>0.538890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.697538</td>\n",
       "      <td>0.713022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.550998</td>\n",
       "      <td>0.562577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.569660</td>\n",
       "      <td>0.579689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.258700</td>\n",
       "      <td>0.528722</td>\n",
       "      <td>0.539592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>0.532976</td>\n",
       "      <td>0.544313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-5/checkpoint-326\n",
      "result {'eval_loss': 0.4859643578529358, 'eval_rmse': 0.4978049397468567, 'eval_runtime': 1.3903, 'eval_samples_per_second': 169.75, 'eval_steps_per_second': 10.789, 'epoch': 8.0}\n",
      "train_bins 6: [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:49, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.517132</td>\n",
       "      <td>0.526285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.358400</td>\n",
       "      <td>0.578978</td>\n",
       "      <td>0.595760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.474261</td>\n",
       "      <td>0.484535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.288200</td>\n",
       "      <td>0.405784</td>\n",
       "      <td>0.415258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.521520</td>\n",
       "      <td>0.529875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.582171</td>\n",
       "      <td>0.590609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.229100</td>\n",
       "      <td>0.484047</td>\n",
       "      <td>0.493384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.509246</td>\n",
       "      <td>0.518092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-6/checkpoint-652\n",
      "result {'eval_loss': 0.4057837724685669, 'eval_rmse': 0.4152580201625824, 'eval_runtime': 1.3887, 'eval_samples_per_second': 169.946, 'eval_steps_per_second': 10.802, 'epoch': 8.0}\n",
      "train_bins 7: [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.500593</td>\n",
       "      <td>0.518480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.487802</td>\n",
       "      <td>0.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.512401</td>\n",
       "      <td>0.535028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>0.478206</td>\n",
       "      <td>0.492231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.508465</td>\n",
       "      <td>0.528987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.517445</td>\n",
       "      <td>0.539636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.447713</td>\n",
       "      <td>0.464520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.464478</td>\n",
       "      <td>0.482437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-7/checkpoint-1141\n",
      "result {'eval_loss': 0.4477129280567169, 'eval_rmse': 0.464519739151001, 'eval_runtime': 1.3944, 'eval_samples_per_second': 169.248, 'eval_steps_per_second': 10.757, 'epoch': 8.0}\n",
      "train_bins 8: [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:49, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.638225</td>\n",
       "      <td>0.650452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.760838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.639924</td>\n",
       "      <td>0.651299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.628228</td>\n",
       "      <td>0.645571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.609503</td>\n",
       "      <td>0.622467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.645910</td>\n",
       "      <td>0.658022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.561472</td>\n",
       "      <td>0.574048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.209900</td>\n",
       "      <td>0.577287</td>\n",
       "      <td>0.589218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-8/checkpoint-1141\n",
      "result {'eval_loss': 0.5614721179008484, 'eval_rmse': 0.574047863483429, 'eval_runtime': 1.388, 'eval_samples_per_second': 170.024, 'eval_steps_per_second': 10.807, 'epoch': 8.0}\n",
      "train_bins 9: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:49, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.650294</td>\n",
       "      <td>0.660222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.592750</td>\n",
       "      <td>0.600809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.328300</td>\n",
       "      <td>0.602631</td>\n",
       "      <td>0.608450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.297700</td>\n",
       "      <td>0.508406</td>\n",
       "      <td>0.518287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.603166</td>\n",
       "      <td>0.609404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.590606</td>\n",
       "      <td>0.598157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.530152</td>\n",
       "      <td>0.542943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.553011</td>\n",
       "      <td>0.563121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-9/checkpoint-652\n",
      "result {'eval_loss': 0.5084058046340942, 'eval_rmse': 0.5182868242263794, 'eval_runtime': 1.3941, 'eval_samples_per_second': 169.28, 'eval_steps_per_second': 10.759, 'epoch': 8.0}\n",
      "train_bins 10: [7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:52, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.633788</td>\n",
       "      <td>0.645978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.757189</td>\n",
       "      <td>0.767589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.621921</td>\n",
       "      <td>0.630763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.576387</td>\n",
       "      <td>0.582375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.732506</td>\n",
       "      <td>0.739805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.634169</td>\n",
       "      <td>0.640750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.596224</td>\n",
       "      <td>0.603828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.624023</td>\n",
       "      <td>0.630861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-10/checkpoint-652\n",
      "result {'eval_loss': 0.5763869285583496, 'eval_rmse': 0.5823750495910645, 'eval_runtime': 1.4202, 'eval_samples_per_second': 166.173, 'eval_steps_per_second': 10.562, 'epoch': 8.0}\n",
      "train_bins 11: [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1304' max='1304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1304/1304 03:49, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.697373</td>\n",
       "      <td>0.708539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.889491</td>\n",
       "      <td>0.904940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.594718</td>\n",
       "      <td>0.607297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.382100</td>\n",
       "      <td>0.550512</td>\n",
       "      <td>0.564813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.353200</td>\n",
       "      <td>0.646537</td>\n",
       "      <td>0.657917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.630301</td>\n",
       "      <td>0.642318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.607167</td>\n",
       "      <td>0.617659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.642823</td>\n",
       "      <td>0.654329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/roberta-base-11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/roberta-base-11/checkpoint-652\n",
      "result {'eval_loss': 0.5505121350288391, 'eval_rmse': 0.5648131966590881, 'eval_runtime': 1.3853, 'eval_samples_per_second': 170.355, 'eval_steps_per_second': 10.828, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "eval_rmses, bestmodels = do_train(gen_model = gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "459dd2d4-6b7c-4246-bf0b-52fa9c72a1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mean best RSME losses', 0.5181576733787855)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Mean best RSME losses', np.array(eval_rmses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.14 s, sys: 4.01 s, total: 12.1 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(cfg.model_offset, cfg.model_limit):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{i}/pytorch_model.bin\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = []\n",
    "for i in range(cfg.model_offset, cfg.model_limit):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}-{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "            cls_embeddings.extend(output[0][:,0,:].detach().cpu().numpy())\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.max_len)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.batch_size,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff1179674ab4c54b0db80ef5c56db54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5996a80b444d8bb493b2faa35e4eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2267,) (567,)\n",
      "rmse_score 0.38228116391920103\n",
      "Fold 1 (2267,) (567,)\n",
      "rmse_score 0.23020199988842074\n",
      "Fold 2 (2267,) (567,)\n",
      "rmse_score 0.22338789291918779\n",
      "Fold 3 (2267,) (567,)\n",
      "rmse_score 0.2399314797731374\n",
      "Fold 4 (2268,) (566,)\n",
      "rmse_score 0.22071101339221674\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8456b23e21454e8573e70f3f792075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438f502946c842f6b1e2cdf74ac8729f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2267,) (567,)\n",
      "rmse_score 0.34758976393733726\n",
      "Fold 1 (2267,) (567,)\n",
      "rmse_score 0.19552462700366963\n",
      "Fold 2 (2267,) (567,)\n",
      "rmse_score 0.19921897850478987\n",
      "Fold 3 (2267,) (567,)\n",
      "rmse_score 0.2048869909194589\n",
      "Fold 4 (2268,) (566,)\n",
      "rmse_score 0.19419248376969586\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d022050e034ca793dbf1a92f42bfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3631dc47024746b693ab5519c66e0fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2267,) (567,)\n",
      "rmse_score 0.2604898437943273\n",
      "Fold 1 (2267,) (567,)\n",
      "rmse_score 0.29842629214808036\n",
      "Fold 2 (2267,) (567,)\n",
      "rmse_score 0.175767002813484\n",
      "Fold 3 (2267,) (567,)\n",
      "rmse_score 0.18322758180620996\n",
      "Fold 4 (2268,) (566,)\n",
      "rmse_score 0.17827547527059692\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9001837f9a47a6b4c4ff17ea610d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c608fc342e6c4df0947b9af71b30cb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2267,) (567,)\n",
      "rmse_score 0.32201410839922245\n",
      "Fold 1 (2267,) (567,)\n",
      "rmse_score 0.39336643864894527\n",
      "Fold 2 (2267,) (567,)\n",
      "rmse_score 0.31771496750462347\n",
      "Fold 3 (2267,) (567,)\n",
      "rmse_score 0.32954612900610075\n",
      "Fold 4 (2268,) (566,)\n",
      "rmse_score 0.33896694550293915\n",
      "Model 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd0d478a5284c308d4dfea444c0a83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8407365b04345bfa6b0454bddc351a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2267,) (567,)\n",
      "rmse_score 0.3411381539224017\n",
      "Fold 1 (2267,) (567,)\n",
      "rmse_score 0.3860309058597993\n",
      "Fold 2 (2267,) (567,)\n",
      "rmse_score 0.3341383211995144\n",
      "Fold 3 (2267,) (567,)\n",
      "rmse_score 0.3524072524854283\n",
      "Fold 4 (2268,) (566,)\n",
      "rmse_score 0.34426913658633396\n",
      "Model 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a082720dec4c08a4153d9927a6f943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33f64dc53824ceaa475b48e3b02c97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel rbf\n",
      "Fold 0 (2267,) (567,)\n",
      "rmse_score 0.20868946918382705\n",
      "Fold 1 (2267,) (567,)\n",
      "rmse_score 0.21438294176152012\n",
      "Fold 2 (2267,) (567,)\n",
      "rmse_score 0.3134344222808848\n",
      "Fold 3 (2267,) (567,)\n",
      "rmse_score 0.21281279226366587\n",
      "Fold 4 (2268,) (566,)\n",
      "rmse_score 0.22427716371996695\n",
      "FINAL RMSE score 0.272243391272833\n",
      "CPU times: user 2min 21s, sys: 2.22 s, total: 2min 23s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model.transformer_model if hasattr(inference_model, 'transformer_model') else inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    kfold = StratifiedKFold(n_splits=cfg.n_folds)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.005451741849667065],\n",
       " [-0.0026813585056343963],\n",
       " [-0.00458860183462211],\n",
       " [-0.004192325458655511],\n",
       " [0.0005734713322523101],\n",
       " [0.0011197194164974775]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16825112, 0.17204921, 0.17315671, 0.15833119, 0.15695068,\n",
       "       0.17126109])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9593187699947071, -0.9172533003008131)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4081141 , -0.49169398, -0.55743103, -2.42654871, -1.76727267,\n",
       "       -1.1058365 ,  0.33528881])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0419461725766761, -0.00699102876277935)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.450060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.533640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.599377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.468495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.809219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.147783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.293343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.450060\n",
       "1  f0953f0a5 -0.533640\n",
       "2  0df072751 -0.599377\n",
       "3  04caf4e0c -2.468495\n",
       "4  0e63f8bea -1.809219\n",
       "5  12537fe78 -1.147783\n",
       "6  965e592c0  0.293343"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/commonlit/models/roberta-base/best')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th model\n",
      "Processing 1th model\n",
      "Processing 2th model\n",
      "Processing 3th model\n",
      "Processing 4th model\n",
      "Processing 5th model\n",
      "Processing 6th model\n",
      "Processing 7th model\n",
      "Processing 8th model\n",
      "Processing 9th model\n",
      "Processing 10th model\n",
      "Processing 11th model\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    best_model_file = f'{best_model}/pytorch_model.bin'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/tokenizer.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        config_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}-{i}/config.json'))\n",
    "        assert config_json.exists()\n",
    "        copyfile(config_json, tokenizer_path/'config.json')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/best_models.zip'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best  best_models.zip  dataset-metadata.json  lm.zip  roberta-base.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/roberta-base/lm.zip'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: /home/commonlit/models/roberta-base/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"INSERT_TITLE_HERE\",\n",
      "  \"id\": \"gilfernandes/INSERT_SLUG_HERE\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"commonlit-roberta-base-extended\",\n",
      "  \"id\": \"gilfernandes/commonlit-roberta-base-extended\",\n",
      "  \"licenses\": [\n",
      "    {\n",
      "      \"name\": \"CC0-1.0\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-extended').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-extended')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file best_models.zip\n",
      "100%|██████████████████████████████████████| 4.95G/4.95G [08:56<00:00, 9.90MB/s]\n",
      "Upload successful: best_models.zip (5GB)\n",
      "Starting upload for file roberta-base.yaml\n",
      "100%|███████████████████████████████████████████| 114/114 [00:07<00:00, 15.4B/s]\n",
      "Upload successful: roberta-base.yaml (114B)\n",
      "Starting upload for file lm.zip\n",
      "100%|████████████████████████████████████████| 421M/421M [00:44<00:00, 9.89MB/s]\n",
      "Upload successful: lm.zip (421MB)\n",
      "Your private Dataset is being created. Please check progress at /api/v1/datasets/status//gilfernandes/commonlit-roberta-base-extended\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with pooled output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
