{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'microsoft/deberta-xlarge'\n",
    "    TOKENIZER_PATH = 'microsoft/deberta-xlarge'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'deberta-xlarge'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929301e1-626d-4ba5-9f32-d361769f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenizer.vocab.txt', 'w') as f:\n",
    "    for k, v in tokenizer.vocab.items():\n",
    "        f.write(f'{k}: {v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b946f5-a6f0-4415-911c-e5a7f628f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '______'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "#         tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "#         assert tokenizer.pad_token == pad_token\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69ec8b1-1d38-46f9-af3b-4a34e0c8dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(cfg.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9b003d-a13f-43c9-830e-edecafdec275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50265, 50265)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.deberta.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "1 transformer_model.deberta.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "2 transformer_model.deberta.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "3 transformer_model.deberta.encoder.layer.0.attention.self.q_bias torch.Size([1024])\n",
      "4 transformer_model.deberta.encoder.layer.0.attention.self.v_bias torch.Size([1024])\n",
      "5 transformer_model.deberta.encoder.layer.0.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "6 transformer_model.deberta.encoder.layer.0.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "7 transformer_model.deberta.encoder.layer.0.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.deberta.encoder.layer.0.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "9 transformer_model.deberta.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.deberta.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "11 transformer_model.deberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "12 transformer_model.deberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "13 transformer_model.deberta.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "14 transformer_model.deberta.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "15 transformer_model.deberta.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "16 transformer_model.deberta.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "17 transformer_model.deberta.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "18 transformer_model.deberta.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "19 transformer_model.deberta.encoder.layer.1.attention.self.q_bias torch.Size([1024])\n",
      "20 transformer_model.deberta.encoder.layer.1.attention.self.v_bias torch.Size([1024])\n",
      "21 transformer_model.deberta.encoder.layer.1.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "22 transformer_model.deberta.encoder.layer.1.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "23 transformer_model.deberta.encoder.layer.1.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.deberta.encoder.layer.1.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "25 transformer_model.deberta.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.deberta.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "27 transformer_model.deberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "28 transformer_model.deberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "29 transformer_model.deberta.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "30 transformer_model.deberta.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "31 transformer_model.deberta.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "32 transformer_model.deberta.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "33 transformer_model.deberta.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "34 transformer_model.deberta.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "35 transformer_model.deberta.encoder.layer.2.attention.self.q_bias torch.Size([1024])\n",
      "36 transformer_model.deberta.encoder.layer.2.attention.self.v_bias torch.Size([1024])\n",
      "37 transformer_model.deberta.encoder.layer.2.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "38 transformer_model.deberta.encoder.layer.2.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "39 transformer_model.deberta.encoder.layer.2.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.deberta.encoder.layer.2.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "41 transformer_model.deberta.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.deberta.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "43 transformer_model.deberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "44 transformer_model.deberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "45 transformer_model.deberta.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "46 transformer_model.deberta.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "47 transformer_model.deberta.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "48 transformer_model.deberta.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "49 transformer_model.deberta.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "50 transformer_model.deberta.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "51 transformer_model.deberta.encoder.layer.3.attention.self.q_bias torch.Size([1024])\n",
      "52 transformer_model.deberta.encoder.layer.3.attention.self.v_bias torch.Size([1024])\n",
      "53 transformer_model.deberta.encoder.layer.3.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "54 transformer_model.deberta.encoder.layer.3.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "55 transformer_model.deberta.encoder.layer.3.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.deberta.encoder.layer.3.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "57 transformer_model.deberta.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.deberta.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "59 transformer_model.deberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "60 transformer_model.deberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "61 transformer_model.deberta.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "62 transformer_model.deberta.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "63 transformer_model.deberta.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "64 transformer_model.deberta.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "65 transformer_model.deberta.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "66 transformer_model.deberta.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "67 transformer_model.deberta.encoder.layer.4.attention.self.q_bias torch.Size([1024])\n",
      "68 transformer_model.deberta.encoder.layer.4.attention.self.v_bias torch.Size([1024])\n",
      "69 transformer_model.deberta.encoder.layer.4.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "70 transformer_model.deberta.encoder.layer.4.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "71 transformer_model.deberta.encoder.layer.4.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.deberta.encoder.layer.4.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "73 transformer_model.deberta.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.deberta.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "75 transformer_model.deberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "76 transformer_model.deberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "77 transformer_model.deberta.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "78 transformer_model.deberta.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "79 transformer_model.deberta.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "80 transformer_model.deberta.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "81 transformer_model.deberta.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "82 transformer_model.deberta.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "83 transformer_model.deberta.encoder.layer.5.attention.self.q_bias torch.Size([1024])\n",
      "84 transformer_model.deberta.encoder.layer.5.attention.self.v_bias torch.Size([1024])\n",
      "85 transformer_model.deberta.encoder.layer.5.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "86 transformer_model.deberta.encoder.layer.5.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "87 transformer_model.deberta.encoder.layer.5.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.deberta.encoder.layer.5.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "89 transformer_model.deberta.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.deberta.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "91 transformer_model.deberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "92 transformer_model.deberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "93 transformer_model.deberta.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "94 transformer_model.deberta.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "95 transformer_model.deberta.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "96 transformer_model.deberta.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "97 transformer_model.deberta.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "98 transformer_model.deberta.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "99 transformer_model.deberta.encoder.layer.6.attention.self.q_bias torch.Size([1024])\n",
      "100 transformer_model.deberta.encoder.layer.6.attention.self.v_bias torch.Size([1024])\n",
      "101 transformer_model.deberta.encoder.layer.6.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "102 transformer_model.deberta.encoder.layer.6.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "103 transformer_model.deberta.encoder.layer.6.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.deberta.encoder.layer.6.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "105 transformer_model.deberta.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.deberta.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "107 transformer_model.deberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "108 transformer_model.deberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "109 transformer_model.deberta.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "110 transformer_model.deberta.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "111 transformer_model.deberta.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "112 transformer_model.deberta.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "113 transformer_model.deberta.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "114 transformer_model.deberta.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "115 transformer_model.deberta.encoder.layer.7.attention.self.q_bias torch.Size([1024])\n",
      "116 transformer_model.deberta.encoder.layer.7.attention.self.v_bias torch.Size([1024])\n",
      "117 transformer_model.deberta.encoder.layer.7.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "118 transformer_model.deberta.encoder.layer.7.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "119 transformer_model.deberta.encoder.layer.7.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.deberta.encoder.layer.7.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "121 transformer_model.deberta.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.deberta.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "123 transformer_model.deberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "124 transformer_model.deberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "125 transformer_model.deberta.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "126 transformer_model.deberta.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "127 transformer_model.deberta.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "128 transformer_model.deberta.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "129 transformer_model.deberta.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "130 transformer_model.deberta.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "131 transformer_model.deberta.encoder.layer.8.attention.self.q_bias torch.Size([1024])\n",
      "132 transformer_model.deberta.encoder.layer.8.attention.self.v_bias torch.Size([1024])\n",
      "133 transformer_model.deberta.encoder.layer.8.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "134 transformer_model.deberta.encoder.layer.8.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "135 transformer_model.deberta.encoder.layer.8.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.deberta.encoder.layer.8.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "137 transformer_model.deberta.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.deberta.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "139 transformer_model.deberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "140 transformer_model.deberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "141 transformer_model.deberta.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "142 transformer_model.deberta.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "143 transformer_model.deberta.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "144 transformer_model.deberta.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "145 transformer_model.deberta.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "146 transformer_model.deberta.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "147 transformer_model.deberta.encoder.layer.9.attention.self.q_bias torch.Size([1024])\n",
      "148 transformer_model.deberta.encoder.layer.9.attention.self.v_bias torch.Size([1024])\n",
      "149 transformer_model.deberta.encoder.layer.9.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "150 transformer_model.deberta.encoder.layer.9.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "151 transformer_model.deberta.encoder.layer.9.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.deberta.encoder.layer.9.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "153 transformer_model.deberta.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.deberta.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "155 transformer_model.deberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "156 transformer_model.deberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "157 transformer_model.deberta.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "158 transformer_model.deberta.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "159 transformer_model.deberta.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "160 transformer_model.deberta.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "161 transformer_model.deberta.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "162 transformer_model.deberta.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "163 transformer_model.deberta.encoder.layer.10.attention.self.q_bias torch.Size([1024])\n",
      "164 transformer_model.deberta.encoder.layer.10.attention.self.v_bias torch.Size([1024])\n",
      "165 transformer_model.deberta.encoder.layer.10.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "166 transformer_model.deberta.encoder.layer.10.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "167 transformer_model.deberta.encoder.layer.10.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.deberta.encoder.layer.10.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "169 transformer_model.deberta.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.deberta.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "171 transformer_model.deberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "172 transformer_model.deberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "173 transformer_model.deberta.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "174 transformer_model.deberta.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "175 transformer_model.deberta.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "176 transformer_model.deberta.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "177 transformer_model.deberta.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "178 transformer_model.deberta.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "179 transformer_model.deberta.encoder.layer.11.attention.self.q_bias torch.Size([1024])\n",
      "180 transformer_model.deberta.encoder.layer.11.attention.self.v_bias torch.Size([1024])\n",
      "181 transformer_model.deberta.encoder.layer.11.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "182 transformer_model.deberta.encoder.layer.11.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "183 transformer_model.deberta.encoder.layer.11.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.deberta.encoder.layer.11.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "185 transformer_model.deberta.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.deberta.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "187 transformer_model.deberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "188 transformer_model.deberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "189 transformer_model.deberta.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "190 transformer_model.deberta.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "191 transformer_model.deberta.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "192 transformer_model.deberta.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "193 transformer_model.deberta.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "194 transformer_model.deberta.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "195 transformer_model.deberta.encoder.layer.12.attention.self.q_bias torch.Size([1024])\n",
      "196 transformer_model.deberta.encoder.layer.12.attention.self.v_bias torch.Size([1024])\n",
      "197 transformer_model.deberta.encoder.layer.12.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "198 transformer_model.deberta.encoder.layer.12.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "199 transformer_model.deberta.encoder.layer.12.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.deberta.encoder.layer.12.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "201 transformer_model.deberta.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.deberta.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "203 transformer_model.deberta.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "204 transformer_model.deberta.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "205 transformer_model.deberta.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "206 transformer_model.deberta.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "207 transformer_model.deberta.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "208 transformer_model.deberta.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "209 transformer_model.deberta.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "210 transformer_model.deberta.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "211 transformer_model.deberta.encoder.layer.13.attention.self.q_bias torch.Size([1024])\n",
      "212 transformer_model.deberta.encoder.layer.13.attention.self.v_bias torch.Size([1024])\n",
      "213 transformer_model.deberta.encoder.layer.13.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "214 transformer_model.deberta.encoder.layer.13.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "215 transformer_model.deberta.encoder.layer.13.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.deberta.encoder.layer.13.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "217 transformer_model.deberta.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.deberta.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "219 transformer_model.deberta.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "220 transformer_model.deberta.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "221 transformer_model.deberta.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "222 transformer_model.deberta.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "223 transformer_model.deberta.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "224 transformer_model.deberta.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "225 transformer_model.deberta.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "226 transformer_model.deberta.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "227 transformer_model.deberta.encoder.layer.14.attention.self.q_bias torch.Size([1024])\n",
      "228 transformer_model.deberta.encoder.layer.14.attention.self.v_bias torch.Size([1024])\n",
      "229 transformer_model.deberta.encoder.layer.14.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "230 transformer_model.deberta.encoder.layer.14.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "231 transformer_model.deberta.encoder.layer.14.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.deberta.encoder.layer.14.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "233 transformer_model.deberta.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.deberta.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "235 transformer_model.deberta.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "236 transformer_model.deberta.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "237 transformer_model.deberta.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "238 transformer_model.deberta.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "239 transformer_model.deberta.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "240 transformer_model.deberta.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "241 transformer_model.deberta.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "242 transformer_model.deberta.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "243 transformer_model.deberta.encoder.layer.15.attention.self.q_bias torch.Size([1024])\n",
      "244 transformer_model.deberta.encoder.layer.15.attention.self.v_bias torch.Size([1024])\n",
      "245 transformer_model.deberta.encoder.layer.15.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "246 transformer_model.deberta.encoder.layer.15.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "247 transformer_model.deberta.encoder.layer.15.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.deberta.encoder.layer.15.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "249 transformer_model.deberta.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.deberta.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "251 transformer_model.deberta.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "252 transformer_model.deberta.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "253 transformer_model.deberta.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "254 transformer_model.deberta.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "255 transformer_model.deberta.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "256 transformer_model.deberta.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "257 transformer_model.deberta.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "258 transformer_model.deberta.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "259 transformer_model.deberta.encoder.layer.16.attention.self.q_bias torch.Size([1024])\n",
      "260 transformer_model.deberta.encoder.layer.16.attention.self.v_bias torch.Size([1024])\n",
      "261 transformer_model.deberta.encoder.layer.16.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "262 transformer_model.deberta.encoder.layer.16.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "263 transformer_model.deberta.encoder.layer.16.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.deberta.encoder.layer.16.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "265 transformer_model.deberta.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.deberta.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "267 transformer_model.deberta.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "268 transformer_model.deberta.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "269 transformer_model.deberta.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "270 transformer_model.deberta.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "271 transformer_model.deberta.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "272 transformer_model.deberta.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "273 transformer_model.deberta.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "274 transformer_model.deberta.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "275 transformer_model.deberta.encoder.layer.17.attention.self.q_bias torch.Size([1024])\n",
      "276 transformer_model.deberta.encoder.layer.17.attention.self.v_bias torch.Size([1024])\n",
      "277 transformer_model.deberta.encoder.layer.17.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "278 transformer_model.deberta.encoder.layer.17.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "279 transformer_model.deberta.encoder.layer.17.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.deberta.encoder.layer.17.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "281 transformer_model.deberta.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.deberta.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "283 transformer_model.deberta.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "284 transformer_model.deberta.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "285 transformer_model.deberta.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "286 transformer_model.deberta.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "287 transformer_model.deberta.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "288 transformer_model.deberta.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "289 transformer_model.deberta.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "290 transformer_model.deberta.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "291 transformer_model.deberta.encoder.layer.18.attention.self.q_bias torch.Size([1024])\n",
      "292 transformer_model.deberta.encoder.layer.18.attention.self.v_bias torch.Size([1024])\n",
      "293 transformer_model.deberta.encoder.layer.18.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "294 transformer_model.deberta.encoder.layer.18.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "295 transformer_model.deberta.encoder.layer.18.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.deberta.encoder.layer.18.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "297 transformer_model.deberta.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.deberta.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "299 transformer_model.deberta.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "300 transformer_model.deberta.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "301 transformer_model.deberta.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "302 transformer_model.deberta.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "303 transformer_model.deberta.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "304 transformer_model.deberta.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "305 transformer_model.deberta.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "306 transformer_model.deberta.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "307 transformer_model.deberta.encoder.layer.19.attention.self.q_bias torch.Size([1024])\n",
      "308 transformer_model.deberta.encoder.layer.19.attention.self.v_bias torch.Size([1024])\n",
      "309 transformer_model.deberta.encoder.layer.19.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "310 transformer_model.deberta.encoder.layer.19.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "311 transformer_model.deberta.encoder.layer.19.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.deberta.encoder.layer.19.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "313 transformer_model.deberta.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.deberta.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "315 transformer_model.deberta.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "316 transformer_model.deberta.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "317 transformer_model.deberta.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "318 transformer_model.deberta.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "319 transformer_model.deberta.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "320 transformer_model.deberta.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "321 transformer_model.deberta.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "322 transformer_model.deberta.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "323 transformer_model.deberta.encoder.layer.20.attention.self.q_bias torch.Size([1024])\n",
      "324 transformer_model.deberta.encoder.layer.20.attention.self.v_bias torch.Size([1024])\n",
      "325 transformer_model.deberta.encoder.layer.20.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "326 transformer_model.deberta.encoder.layer.20.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "327 transformer_model.deberta.encoder.layer.20.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.deberta.encoder.layer.20.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "329 transformer_model.deberta.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.deberta.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "331 transformer_model.deberta.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "332 transformer_model.deberta.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "333 transformer_model.deberta.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "334 transformer_model.deberta.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "335 transformer_model.deberta.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "336 transformer_model.deberta.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "337 transformer_model.deberta.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "338 transformer_model.deberta.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "339 transformer_model.deberta.encoder.layer.21.attention.self.q_bias torch.Size([1024])\n",
      "340 transformer_model.deberta.encoder.layer.21.attention.self.v_bias torch.Size([1024])\n",
      "341 transformer_model.deberta.encoder.layer.21.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "342 transformer_model.deberta.encoder.layer.21.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "343 transformer_model.deberta.encoder.layer.21.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.deberta.encoder.layer.21.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "345 transformer_model.deberta.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.deberta.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "347 transformer_model.deberta.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "348 transformer_model.deberta.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "349 transformer_model.deberta.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "350 transformer_model.deberta.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "351 transformer_model.deberta.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "352 transformer_model.deberta.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "353 transformer_model.deberta.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "354 transformer_model.deberta.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "355 transformer_model.deberta.encoder.layer.22.attention.self.q_bias torch.Size([1024])\n",
      "356 transformer_model.deberta.encoder.layer.22.attention.self.v_bias torch.Size([1024])\n",
      "357 transformer_model.deberta.encoder.layer.22.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "358 transformer_model.deberta.encoder.layer.22.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "359 transformer_model.deberta.encoder.layer.22.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.deberta.encoder.layer.22.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "361 transformer_model.deberta.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.deberta.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "363 transformer_model.deberta.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "364 transformer_model.deberta.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "365 transformer_model.deberta.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "366 transformer_model.deberta.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "367 transformer_model.deberta.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "368 transformer_model.deberta.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "369 transformer_model.deberta.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "370 transformer_model.deberta.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "371 transformer_model.deberta.encoder.layer.23.attention.self.q_bias torch.Size([1024])\n",
      "372 transformer_model.deberta.encoder.layer.23.attention.self.v_bias torch.Size([1024])\n",
      "373 transformer_model.deberta.encoder.layer.23.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "374 transformer_model.deberta.encoder.layer.23.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "375 transformer_model.deberta.encoder.layer.23.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.deberta.encoder.layer.23.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "377 transformer_model.deberta.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.deberta.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "379 transformer_model.deberta.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "380 transformer_model.deberta.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "381 transformer_model.deberta.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "382 transformer_model.deberta.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "383 transformer_model.deberta.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "384 transformer_model.deberta.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "385 transformer_model.deberta.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "386 transformer_model.deberta.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "387 transformer_model.deberta.encoder.layer.24.attention.self.q_bias torch.Size([1024])\n",
      "388 transformer_model.deberta.encoder.layer.24.attention.self.v_bias torch.Size([1024])\n",
      "389 transformer_model.deberta.encoder.layer.24.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "390 transformer_model.deberta.encoder.layer.24.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "391 transformer_model.deberta.encoder.layer.24.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "392 transformer_model.deberta.encoder.layer.24.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "393 transformer_model.deberta.encoder.layer.24.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "394 transformer_model.deberta.encoder.layer.24.attention.output.dense.bias torch.Size([1024])\n",
      "395 transformer_model.deberta.encoder.layer.24.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "396 transformer_model.deberta.encoder.layer.24.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "397 transformer_model.deberta.encoder.layer.24.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "398 transformer_model.deberta.encoder.layer.24.intermediate.dense.bias torch.Size([4096])\n",
      "399 transformer_model.deberta.encoder.layer.24.output.dense.weight torch.Size([1024, 4096])\n",
      "400 transformer_model.deberta.encoder.layer.24.output.dense.bias torch.Size([1024])\n",
      "401 transformer_model.deberta.encoder.layer.24.output.LayerNorm.weight torch.Size([1024])\n",
      "402 transformer_model.deberta.encoder.layer.24.output.LayerNorm.bias torch.Size([1024])\n",
      "403 transformer_model.deberta.encoder.layer.25.attention.self.q_bias torch.Size([1024])\n",
      "404 transformer_model.deberta.encoder.layer.25.attention.self.v_bias torch.Size([1024])\n",
      "405 transformer_model.deberta.encoder.layer.25.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "406 transformer_model.deberta.encoder.layer.25.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "407 transformer_model.deberta.encoder.layer.25.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "408 transformer_model.deberta.encoder.layer.25.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "409 transformer_model.deberta.encoder.layer.25.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "410 transformer_model.deberta.encoder.layer.25.attention.output.dense.bias torch.Size([1024])\n",
      "411 transformer_model.deberta.encoder.layer.25.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "412 transformer_model.deberta.encoder.layer.25.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "413 transformer_model.deberta.encoder.layer.25.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "414 transformer_model.deberta.encoder.layer.25.intermediate.dense.bias torch.Size([4096])\n",
      "415 transformer_model.deberta.encoder.layer.25.output.dense.weight torch.Size([1024, 4096])\n",
      "416 transformer_model.deberta.encoder.layer.25.output.dense.bias torch.Size([1024])\n",
      "417 transformer_model.deberta.encoder.layer.25.output.LayerNorm.weight torch.Size([1024])\n",
      "418 transformer_model.deberta.encoder.layer.25.output.LayerNorm.bias torch.Size([1024])\n",
      "419 transformer_model.deberta.encoder.layer.26.attention.self.q_bias torch.Size([1024])\n",
      "420 transformer_model.deberta.encoder.layer.26.attention.self.v_bias torch.Size([1024])\n",
      "421 transformer_model.deberta.encoder.layer.26.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "422 transformer_model.deberta.encoder.layer.26.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "423 transformer_model.deberta.encoder.layer.26.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "424 transformer_model.deberta.encoder.layer.26.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "425 transformer_model.deberta.encoder.layer.26.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "426 transformer_model.deberta.encoder.layer.26.attention.output.dense.bias torch.Size([1024])\n",
      "427 transformer_model.deberta.encoder.layer.26.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "428 transformer_model.deberta.encoder.layer.26.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "429 transformer_model.deberta.encoder.layer.26.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "430 transformer_model.deberta.encoder.layer.26.intermediate.dense.bias torch.Size([4096])\n",
      "431 transformer_model.deberta.encoder.layer.26.output.dense.weight torch.Size([1024, 4096])\n",
      "432 transformer_model.deberta.encoder.layer.26.output.dense.bias torch.Size([1024])\n",
      "433 transformer_model.deberta.encoder.layer.26.output.LayerNorm.weight torch.Size([1024])\n",
      "434 transformer_model.deberta.encoder.layer.26.output.LayerNorm.bias torch.Size([1024])\n",
      "435 transformer_model.deberta.encoder.layer.27.attention.self.q_bias torch.Size([1024])\n",
      "436 transformer_model.deberta.encoder.layer.27.attention.self.v_bias torch.Size([1024])\n",
      "437 transformer_model.deberta.encoder.layer.27.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "438 transformer_model.deberta.encoder.layer.27.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "439 transformer_model.deberta.encoder.layer.27.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "440 transformer_model.deberta.encoder.layer.27.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "441 transformer_model.deberta.encoder.layer.27.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "442 transformer_model.deberta.encoder.layer.27.attention.output.dense.bias torch.Size([1024])\n",
      "443 transformer_model.deberta.encoder.layer.27.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "444 transformer_model.deberta.encoder.layer.27.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "445 transformer_model.deberta.encoder.layer.27.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "446 transformer_model.deberta.encoder.layer.27.intermediate.dense.bias torch.Size([4096])\n",
      "447 transformer_model.deberta.encoder.layer.27.output.dense.weight torch.Size([1024, 4096])\n",
      "448 transformer_model.deberta.encoder.layer.27.output.dense.bias torch.Size([1024])\n",
      "449 transformer_model.deberta.encoder.layer.27.output.LayerNorm.weight torch.Size([1024])\n",
      "450 transformer_model.deberta.encoder.layer.27.output.LayerNorm.bias torch.Size([1024])\n",
      "451 transformer_model.deberta.encoder.layer.28.attention.self.q_bias torch.Size([1024])\n",
      "452 transformer_model.deberta.encoder.layer.28.attention.self.v_bias torch.Size([1024])\n",
      "453 transformer_model.deberta.encoder.layer.28.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "454 transformer_model.deberta.encoder.layer.28.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "455 transformer_model.deberta.encoder.layer.28.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "456 transformer_model.deberta.encoder.layer.28.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "457 transformer_model.deberta.encoder.layer.28.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "458 transformer_model.deberta.encoder.layer.28.attention.output.dense.bias torch.Size([1024])\n",
      "459 transformer_model.deberta.encoder.layer.28.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "460 transformer_model.deberta.encoder.layer.28.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "461 transformer_model.deberta.encoder.layer.28.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "462 transformer_model.deberta.encoder.layer.28.intermediate.dense.bias torch.Size([4096])\n",
      "463 transformer_model.deberta.encoder.layer.28.output.dense.weight torch.Size([1024, 4096])\n",
      "464 transformer_model.deberta.encoder.layer.28.output.dense.bias torch.Size([1024])\n",
      "465 transformer_model.deberta.encoder.layer.28.output.LayerNorm.weight torch.Size([1024])\n",
      "466 transformer_model.deberta.encoder.layer.28.output.LayerNorm.bias torch.Size([1024])\n",
      "467 transformer_model.deberta.encoder.layer.29.attention.self.q_bias torch.Size([1024])\n",
      "468 transformer_model.deberta.encoder.layer.29.attention.self.v_bias torch.Size([1024])\n",
      "469 transformer_model.deberta.encoder.layer.29.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "470 transformer_model.deberta.encoder.layer.29.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "471 transformer_model.deberta.encoder.layer.29.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "472 transformer_model.deberta.encoder.layer.29.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "473 transformer_model.deberta.encoder.layer.29.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "474 transformer_model.deberta.encoder.layer.29.attention.output.dense.bias torch.Size([1024])\n",
      "475 transformer_model.deberta.encoder.layer.29.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "476 transformer_model.deberta.encoder.layer.29.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "477 transformer_model.deberta.encoder.layer.29.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "478 transformer_model.deberta.encoder.layer.29.intermediate.dense.bias torch.Size([4096])\n",
      "479 transformer_model.deberta.encoder.layer.29.output.dense.weight torch.Size([1024, 4096])\n",
      "480 transformer_model.deberta.encoder.layer.29.output.dense.bias torch.Size([1024])\n",
      "481 transformer_model.deberta.encoder.layer.29.output.LayerNorm.weight torch.Size([1024])\n",
      "482 transformer_model.deberta.encoder.layer.29.output.LayerNorm.bias torch.Size([1024])\n",
      "483 transformer_model.deberta.encoder.layer.30.attention.self.q_bias torch.Size([1024])\n",
      "484 transformer_model.deberta.encoder.layer.30.attention.self.v_bias torch.Size([1024])\n",
      "485 transformer_model.deberta.encoder.layer.30.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "486 transformer_model.deberta.encoder.layer.30.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "487 transformer_model.deberta.encoder.layer.30.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "488 transformer_model.deberta.encoder.layer.30.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "489 transformer_model.deberta.encoder.layer.30.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "490 transformer_model.deberta.encoder.layer.30.attention.output.dense.bias torch.Size([1024])\n",
      "491 transformer_model.deberta.encoder.layer.30.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "492 transformer_model.deberta.encoder.layer.30.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "493 transformer_model.deberta.encoder.layer.30.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "494 transformer_model.deberta.encoder.layer.30.intermediate.dense.bias torch.Size([4096])\n",
      "495 transformer_model.deberta.encoder.layer.30.output.dense.weight torch.Size([1024, 4096])\n",
      "496 transformer_model.deberta.encoder.layer.30.output.dense.bias torch.Size([1024])\n",
      "497 transformer_model.deberta.encoder.layer.30.output.LayerNorm.weight torch.Size([1024])\n",
      "498 transformer_model.deberta.encoder.layer.30.output.LayerNorm.bias torch.Size([1024])\n",
      "499 transformer_model.deberta.encoder.layer.31.attention.self.q_bias torch.Size([1024])\n",
      "500 transformer_model.deberta.encoder.layer.31.attention.self.v_bias torch.Size([1024])\n",
      "501 transformer_model.deberta.encoder.layer.31.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "502 transformer_model.deberta.encoder.layer.31.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "503 transformer_model.deberta.encoder.layer.31.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "504 transformer_model.deberta.encoder.layer.31.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "505 transformer_model.deberta.encoder.layer.31.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "506 transformer_model.deberta.encoder.layer.31.attention.output.dense.bias torch.Size([1024])\n",
      "507 transformer_model.deberta.encoder.layer.31.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "508 transformer_model.deberta.encoder.layer.31.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "509 transformer_model.deberta.encoder.layer.31.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "510 transformer_model.deberta.encoder.layer.31.intermediate.dense.bias torch.Size([4096])\n",
      "511 transformer_model.deberta.encoder.layer.31.output.dense.weight torch.Size([1024, 4096])\n",
      "512 transformer_model.deberta.encoder.layer.31.output.dense.bias torch.Size([1024])\n",
      "513 transformer_model.deberta.encoder.layer.31.output.LayerNorm.weight torch.Size([1024])\n",
      "514 transformer_model.deberta.encoder.layer.31.output.LayerNorm.bias torch.Size([1024])\n",
      "515 transformer_model.deberta.encoder.layer.32.attention.self.q_bias torch.Size([1024])\n",
      "516 transformer_model.deberta.encoder.layer.32.attention.self.v_bias torch.Size([1024])\n",
      "517 transformer_model.deberta.encoder.layer.32.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "518 transformer_model.deberta.encoder.layer.32.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "519 transformer_model.deberta.encoder.layer.32.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "520 transformer_model.deberta.encoder.layer.32.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "521 transformer_model.deberta.encoder.layer.32.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "522 transformer_model.deberta.encoder.layer.32.attention.output.dense.bias torch.Size([1024])\n",
      "523 transformer_model.deberta.encoder.layer.32.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "524 transformer_model.deberta.encoder.layer.32.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "525 transformer_model.deberta.encoder.layer.32.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "526 transformer_model.deberta.encoder.layer.32.intermediate.dense.bias torch.Size([4096])\n",
      "527 transformer_model.deberta.encoder.layer.32.output.dense.weight torch.Size([1024, 4096])\n",
      "528 transformer_model.deberta.encoder.layer.32.output.dense.bias torch.Size([1024])\n",
      "529 transformer_model.deberta.encoder.layer.32.output.LayerNorm.weight torch.Size([1024])\n",
      "530 transformer_model.deberta.encoder.layer.32.output.LayerNorm.bias torch.Size([1024])\n",
      "531 transformer_model.deberta.encoder.layer.33.attention.self.q_bias torch.Size([1024])\n",
      "532 transformer_model.deberta.encoder.layer.33.attention.self.v_bias torch.Size([1024])\n",
      "533 transformer_model.deberta.encoder.layer.33.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "534 transformer_model.deberta.encoder.layer.33.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "535 transformer_model.deberta.encoder.layer.33.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "536 transformer_model.deberta.encoder.layer.33.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "537 transformer_model.deberta.encoder.layer.33.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "538 transformer_model.deberta.encoder.layer.33.attention.output.dense.bias torch.Size([1024])\n",
      "539 transformer_model.deberta.encoder.layer.33.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "540 transformer_model.deberta.encoder.layer.33.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "541 transformer_model.deberta.encoder.layer.33.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "542 transformer_model.deberta.encoder.layer.33.intermediate.dense.bias torch.Size([4096])\n",
      "543 transformer_model.deberta.encoder.layer.33.output.dense.weight torch.Size([1024, 4096])\n",
      "544 transformer_model.deberta.encoder.layer.33.output.dense.bias torch.Size([1024])\n",
      "545 transformer_model.deberta.encoder.layer.33.output.LayerNorm.weight torch.Size([1024])\n",
      "546 transformer_model.deberta.encoder.layer.33.output.LayerNorm.bias torch.Size([1024])\n",
      "547 transformer_model.deberta.encoder.layer.34.attention.self.q_bias torch.Size([1024])\n",
      "548 transformer_model.deberta.encoder.layer.34.attention.self.v_bias torch.Size([1024])\n",
      "549 transformer_model.deberta.encoder.layer.34.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "550 transformer_model.deberta.encoder.layer.34.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "551 transformer_model.deberta.encoder.layer.34.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "552 transformer_model.deberta.encoder.layer.34.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "553 transformer_model.deberta.encoder.layer.34.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "554 transformer_model.deberta.encoder.layer.34.attention.output.dense.bias torch.Size([1024])\n",
      "555 transformer_model.deberta.encoder.layer.34.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "556 transformer_model.deberta.encoder.layer.34.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "557 transformer_model.deberta.encoder.layer.34.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "558 transformer_model.deberta.encoder.layer.34.intermediate.dense.bias torch.Size([4096])\n",
      "559 transformer_model.deberta.encoder.layer.34.output.dense.weight torch.Size([1024, 4096])\n",
      "560 transformer_model.deberta.encoder.layer.34.output.dense.bias torch.Size([1024])\n",
      "561 transformer_model.deberta.encoder.layer.34.output.LayerNorm.weight torch.Size([1024])\n",
      "562 transformer_model.deberta.encoder.layer.34.output.LayerNorm.bias torch.Size([1024])\n",
      "563 transformer_model.deberta.encoder.layer.35.attention.self.q_bias torch.Size([1024])\n",
      "564 transformer_model.deberta.encoder.layer.35.attention.self.v_bias torch.Size([1024])\n",
      "565 transformer_model.deberta.encoder.layer.35.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "566 transformer_model.deberta.encoder.layer.35.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "567 transformer_model.deberta.encoder.layer.35.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "568 transformer_model.deberta.encoder.layer.35.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "569 transformer_model.deberta.encoder.layer.35.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "570 transformer_model.deberta.encoder.layer.35.attention.output.dense.bias torch.Size([1024])\n",
      "571 transformer_model.deberta.encoder.layer.35.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "572 transformer_model.deberta.encoder.layer.35.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "573 transformer_model.deberta.encoder.layer.35.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "574 transformer_model.deberta.encoder.layer.35.intermediate.dense.bias torch.Size([4096])\n",
      "575 transformer_model.deberta.encoder.layer.35.output.dense.weight torch.Size([1024, 4096])\n",
      "576 transformer_model.deberta.encoder.layer.35.output.dense.bias torch.Size([1024])\n",
      "577 transformer_model.deberta.encoder.layer.35.output.LayerNorm.weight torch.Size([1024])\n",
      "578 transformer_model.deberta.encoder.layer.35.output.LayerNorm.bias torch.Size([1024])\n",
      "579 transformer_model.deberta.encoder.layer.36.attention.self.q_bias torch.Size([1024])\n",
      "580 transformer_model.deberta.encoder.layer.36.attention.self.v_bias torch.Size([1024])\n",
      "581 transformer_model.deberta.encoder.layer.36.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "582 transformer_model.deberta.encoder.layer.36.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "583 transformer_model.deberta.encoder.layer.36.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "584 transformer_model.deberta.encoder.layer.36.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "585 transformer_model.deberta.encoder.layer.36.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "586 transformer_model.deberta.encoder.layer.36.attention.output.dense.bias torch.Size([1024])\n",
      "587 transformer_model.deberta.encoder.layer.36.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "588 transformer_model.deberta.encoder.layer.36.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "589 transformer_model.deberta.encoder.layer.36.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "590 transformer_model.deberta.encoder.layer.36.intermediate.dense.bias torch.Size([4096])\n",
      "591 transformer_model.deberta.encoder.layer.36.output.dense.weight torch.Size([1024, 4096])\n",
      "592 transformer_model.deberta.encoder.layer.36.output.dense.bias torch.Size([1024])\n",
      "593 transformer_model.deberta.encoder.layer.36.output.LayerNorm.weight torch.Size([1024])\n",
      "594 transformer_model.deberta.encoder.layer.36.output.LayerNorm.bias torch.Size([1024])\n",
      "595 transformer_model.deberta.encoder.layer.37.attention.self.q_bias torch.Size([1024])\n",
      "596 transformer_model.deberta.encoder.layer.37.attention.self.v_bias torch.Size([1024])\n",
      "597 transformer_model.deberta.encoder.layer.37.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "598 transformer_model.deberta.encoder.layer.37.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "599 transformer_model.deberta.encoder.layer.37.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "600 transformer_model.deberta.encoder.layer.37.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "601 transformer_model.deberta.encoder.layer.37.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "602 transformer_model.deberta.encoder.layer.37.attention.output.dense.bias torch.Size([1024])\n",
      "603 transformer_model.deberta.encoder.layer.37.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "604 transformer_model.deberta.encoder.layer.37.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "605 transformer_model.deberta.encoder.layer.37.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "606 transformer_model.deberta.encoder.layer.37.intermediate.dense.bias torch.Size([4096])\n",
      "607 transformer_model.deberta.encoder.layer.37.output.dense.weight torch.Size([1024, 4096])\n",
      "608 transformer_model.deberta.encoder.layer.37.output.dense.bias torch.Size([1024])\n",
      "609 transformer_model.deberta.encoder.layer.37.output.LayerNorm.weight torch.Size([1024])\n",
      "610 transformer_model.deberta.encoder.layer.37.output.LayerNorm.bias torch.Size([1024])\n",
      "611 transformer_model.deberta.encoder.layer.38.attention.self.q_bias torch.Size([1024])\n",
      "612 transformer_model.deberta.encoder.layer.38.attention.self.v_bias torch.Size([1024])\n",
      "613 transformer_model.deberta.encoder.layer.38.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "614 transformer_model.deberta.encoder.layer.38.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "615 transformer_model.deberta.encoder.layer.38.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "616 transformer_model.deberta.encoder.layer.38.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "617 transformer_model.deberta.encoder.layer.38.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "618 transformer_model.deberta.encoder.layer.38.attention.output.dense.bias torch.Size([1024])\n",
      "619 transformer_model.deberta.encoder.layer.38.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "620 transformer_model.deberta.encoder.layer.38.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "621 transformer_model.deberta.encoder.layer.38.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "622 transformer_model.deberta.encoder.layer.38.intermediate.dense.bias torch.Size([4096])\n",
      "623 transformer_model.deberta.encoder.layer.38.output.dense.weight torch.Size([1024, 4096])\n",
      "624 transformer_model.deberta.encoder.layer.38.output.dense.bias torch.Size([1024])\n",
      "625 transformer_model.deberta.encoder.layer.38.output.LayerNorm.weight torch.Size([1024])\n",
      "626 transformer_model.deberta.encoder.layer.38.output.LayerNorm.bias torch.Size([1024])\n",
      "627 transformer_model.deberta.encoder.layer.39.attention.self.q_bias torch.Size([1024])\n",
      "628 transformer_model.deberta.encoder.layer.39.attention.self.v_bias torch.Size([1024])\n",
      "629 transformer_model.deberta.encoder.layer.39.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "630 transformer_model.deberta.encoder.layer.39.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "631 transformer_model.deberta.encoder.layer.39.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "632 transformer_model.deberta.encoder.layer.39.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "633 transformer_model.deberta.encoder.layer.39.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "634 transformer_model.deberta.encoder.layer.39.attention.output.dense.bias torch.Size([1024])\n",
      "635 transformer_model.deberta.encoder.layer.39.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "636 transformer_model.deberta.encoder.layer.39.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "637 transformer_model.deberta.encoder.layer.39.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "638 transformer_model.deberta.encoder.layer.39.intermediate.dense.bias torch.Size([4096])\n",
      "639 transformer_model.deberta.encoder.layer.39.output.dense.weight torch.Size([1024, 4096])\n",
      "640 transformer_model.deberta.encoder.layer.39.output.dense.bias torch.Size([1024])\n",
      "641 transformer_model.deberta.encoder.layer.39.output.LayerNorm.weight torch.Size([1024])\n",
      "642 transformer_model.deberta.encoder.layer.39.output.LayerNorm.bias torch.Size([1024])\n",
      "643 transformer_model.deberta.encoder.layer.40.attention.self.q_bias torch.Size([1024])\n",
      "644 transformer_model.deberta.encoder.layer.40.attention.self.v_bias torch.Size([1024])\n",
      "645 transformer_model.deberta.encoder.layer.40.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "646 transformer_model.deberta.encoder.layer.40.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "647 transformer_model.deberta.encoder.layer.40.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "648 transformer_model.deberta.encoder.layer.40.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "649 transformer_model.deberta.encoder.layer.40.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "650 transformer_model.deberta.encoder.layer.40.attention.output.dense.bias torch.Size([1024])\n",
      "651 transformer_model.deberta.encoder.layer.40.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "652 transformer_model.deberta.encoder.layer.40.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "653 transformer_model.deberta.encoder.layer.40.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "654 transformer_model.deberta.encoder.layer.40.intermediate.dense.bias torch.Size([4096])\n",
      "655 transformer_model.deberta.encoder.layer.40.output.dense.weight torch.Size([1024, 4096])\n",
      "656 transformer_model.deberta.encoder.layer.40.output.dense.bias torch.Size([1024])\n",
      "657 transformer_model.deberta.encoder.layer.40.output.LayerNorm.weight torch.Size([1024])\n",
      "658 transformer_model.deberta.encoder.layer.40.output.LayerNorm.bias torch.Size([1024])\n",
      "659 transformer_model.deberta.encoder.layer.41.attention.self.q_bias torch.Size([1024])\n",
      "660 transformer_model.deberta.encoder.layer.41.attention.self.v_bias torch.Size([1024])\n",
      "661 transformer_model.deberta.encoder.layer.41.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "662 transformer_model.deberta.encoder.layer.41.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "663 transformer_model.deberta.encoder.layer.41.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "664 transformer_model.deberta.encoder.layer.41.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "665 transformer_model.deberta.encoder.layer.41.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "666 transformer_model.deberta.encoder.layer.41.attention.output.dense.bias torch.Size([1024])\n",
      "667 transformer_model.deberta.encoder.layer.41.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "668 transformer_model.deberta.encoder.layer.41.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "669 transformer_model.deberta.encoder.layer.41.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "670 transformer_model.deberta.encoder.layer.41.intermediate.dense.bias torch.Size([4096])\n",
      "671 transformer_model.deberta.encoder.layer.41.output.dense.weight torch.Size([1024, 4096])\n",
      "672 transformer_model.deberta.encoder.layer.41.output.dense.bias torch.Size([1024])\n",
      "673 transformer_model.deberta.encoder.layer.41.output.LayerNorm.weight torch.Size([1024])\n",
      "674 transformer_model.deberta.encoder.layer.41.output.LayerNorm.bias torch.Size([1024])\n",
      "675 transformer_model.deberta.encoder.layer.42.attention.self.q_bias torch.Size([1024])\n",
      "676 transformer_model.deberta.encoder.layer.42.attention.self.v_bias torch.Size([1024])\n",
      "677 transformer_model.deberta.encoder.layer.42.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "678 transformer_model.deberta.encoder.layer.42.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "679 transformer_model.deberta.encoder.layer.42.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "680 transformer_model.deberta.encoder.layer.42.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "681 transformer_model.deberta.encoder.layer.42.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "682 transformer_model.deberta.encoder.layer.42.attention.output.dense.bias torch.Size([1024])\n",
      "683 transformer_model.deberta.encoder.layer.42.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "684 transformer_model.deberta.encoder.layer.42.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "685 transformer_model.deberta.encoder.layer.42.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "686 transformer_model.deberta.encoder.layer.42.intermediate.dense.bias torch.Size([4096])\n",
      "687 transformer_model.deberta.encoder.layer.42.output.dense.weight torch.Size([1024, 4096])\n",
      "688 transformer_model.deberta.encoder.layer.42.output.dense.bias torch.Size([1024])\n",
      "689 transformer_model.deberta.encoder.layer.42.output.LayerNorm.weight torch.Size([1024])\n",
      "690 transformer_model.deberta.encoder.layer.42.output.LayerNorm.bias torch.Size([1024])\n",
      "691 transformer_model.deberta.encoder.layer.43.attention.self.q_bias torch.Size([1024])\n",
      "692 transformer_model.deberta.encoder.layer.43.attention.self.v_bias torch.Size([1024])\n",
      "693 transformer_model.deberta.encoder.layer.43.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "694 transformer_model.deberta.encoder.layer.43.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "695 transformer_model.deberta.encoder.layer.43.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "696 transformer_model.deberta.encoder.layer.43.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "697 transformer_model.deberta.encoder.layer.43.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "698 transformer_model.deberta.encoder.layer.43.attention.output.dense.bias torch.Size([1024])\n",
      "699 transformer_model.deberta.encoder.layer.43.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "700 transformer_model.deberta.encoder.layer.43.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "701 transformer_model.deberta.encoder.layer.43.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "702 transformer_model.deberta.encoder.layer.43.intermediate.dense.bias torch.Size([4096])\n",
      "703 transformer_model.deberta.encoder.layer.43.output.dense.weight torch.Size([1024, 4096])\n",
      "704 transformer_model.deberta.encoder.layer.43.output.dense.bias torch.Size([1024])\n",
      "705 transformer_model.deberta.encoder.layer.43.output.LayerNorm.weight torch.Size([1024])\n",
      "706 transformer_model.deberta.encoder.layer.43.output.LayerNorm.bias torch.Size([1024])\n",
      "707 transformer_model.deberta.encoder.layer.44.attention.self.q_bias torch.Size([1024])\n",
      "708 transformer_model.deberta.encoder.layer.44.attention.self.v_bias torch.Size([1024])\n",
      "709 transformer_model.deberta.encoder.layer.44.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "710 transformer_model.deberta.encoder.layer.44.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "711 transformer_model.deberta.encoder.layer.44.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "712 transformer_model.deberta.encoder.layer.44.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "713 transformer_model.deberta.encoder.layer.44.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "714 transformer_model.deberta.encoder.layer.44.attention.output.dense.bias torch.Size([1024])\n",
      "715 transformer_model.deberta.encoder.layer.44.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "716 transformer_model.deberta.encoder.layer.44.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "717 transformer_model.deberta.encoder.layer.44.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "718 transformer_model.deberta.encoder.layer.44.intermediate.dense.bias torch.Size([4096])\n",
      "719 transformer_model.deberta.encoder.layer.44.output.dense.weight torch.Size([1024, 4096])\n",
      "720 transformer_model.deberta.encoder.layer.44.output.dense.bias torch.Size([1024])\n",
      "721 transformer_model.deberta.encoder.layer.44.output.LayerNorm.weight torch.Size([1024])\n",
      "722 transformer_model.deberta.encoder.layer.44.output.LayerNorm.bias torch.Size([1024])\n",
      "723 transformer_model.deberta.encoder.layer.45.attention.self.q_bias torch.Size([1024])\n",
      "724 transformer_model.deberta.encoder.layer.45.attention.self.v_bias torch.Size([1024])\n",
      "725 transformer_model.deberta.encoder.layer.45.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "726 transformer_model.deberta.encoder.layer.45.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "727 transformer_model.deberta.encoder.layer.45.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "728 transformer_model.deberta.encoder.layer.45.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "729 transformer_model.deberta.encoder.layer.45.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "730 transformer_model.deberta.encoder.layer.45.attention.output.dense.bias torch.Size([1024])\n",
      "731 transformer_model.deberta.encoder.layer.45.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "732 transformer_model.deberta.encoder.layer.45.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "733 transformer_model.deberta.encoder.layer.45.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "734 transformer_model.deberta.encoder.layer.45.intermediate.dense.bias torch.Size([4096])\n",
      "735 transformer_model.deberta.encoder.layer.45.output.dense.weight torch.Size([1024, 4096])\n",
      "736 transformer_model.deberta.encoder.layer.45.output.dense.bias torch.Size([1024])\n",
      "737 transformer_model.deberta.encoder.layer.45.output.LayerNorm.weight torch.Size([1024])\n",
      "738 transformer_model.deberta.encoder.layer.45.output.LayerNorm.bias torch.Size([1024])\n",
      "739 transformer_model.deberta.encoder.layer.46.attention.self.q_bias torch.Size([1024])\n",
      "740 transformer_model.deberta.encoder.layer.46.attention.self.v_bias torch.Size([1024])\n",
      "741 transformer_model.deberta.encoder.layer.46.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "742 transformer_model.deberta.encoder.layer.46.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "743 transformer_model.deberta.encoder.layer.46.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "744 transformer_model.deberta.encoder.layer.46.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "745 transformer_model.deberta.encoder.layer.46.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "746 transformer_model.deberta.encoder.layer.46.attention.output.dense.bias torch.Size([1024])\n",
      "747 transformer_model.deberta.encoder.layer.46.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "748 transformer_model.deberta.encoder.layer.46.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "749 transformer_model.deberta.encoder.layer.46.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "750 transformer_model.deberta.encoder.layer.46.intermediate.dense.bias torch.Size([4096])\n",
      "751 transformer_model.deberta.encoder.layer.46.output.dense.weight torch.Size([1024, 4096])\n",
      "752 transformer_model.deberta.encoder.layer.46.output.dense.bias torch.Size([1024])\n",
      "753 transformer_model.deberta.encoder.layer.46.output.LayerNorm.weight torch.Size([1024])\n",
      "754 transformer_model.deberta.encoder.layer.46.output.LayerNorm.bias torch.Size([1024])\n",
      "755 transformer_model.deberta.encoder.layer.47.attention.self.q_bias torch.Size([1024])\n",
      "756 transformer_model.deberta.encoder.layer.47.attention.self.v_bias torch.Size([1024])\n",
      "757 transformer_model.deberta.encoder.layer.47.attention.self.in_proj.weight torch.Size([3072, 1024])\n",
      "758 transformer_model.deberta.encoder.layer.47.attention.self.pos_proj.weight torch.Size([1024, 1024])\n",
      "759 transformer_model.deberta.encoder.layer.47.attention.self.pos_q_proj.weight torch.Size([1024, 1024])\n",
      "760 transformer_model.deberta.encoder.layer.47.attention.self.pos_q_proj.bias torch.Size([1024])\n",
      "761 transformer_model.deberta.encoder.layer.47.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "762 transformer_model.deberta.encoder.layer.47.attention.output.dense.bias torch.Size([1024])\n",
      "763 transformer_model.deberta.encoder.layer.47.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "764 transformer_model.deberta.encoder.layer.47.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "765 transformer_model.deberta.encoder.layer.47.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "766 transformer_model.deberta.encoder.layer.47.intermediate.dense.bias torch.Size([4096])\n",
      "767 transformer_model.deberta.encoder.layer.47.output.dense.weight torch.Size([1024, 4096])\n",
      "768 transformer_model.deberta.encoder.layer.47.output.dense.bias torch.Size([1024])\n",
      "769 transformer_model.deberta.encoder.layer.47.output.LayerNorm.weight torch.Size([1024])\n",
      "770 transformer_model.deberta.encoder.layer.47.output.LayerNorm.bias torch.Size([1024])\n",
      "771 transformer_model.deberta.encoder.rel_embeddings.weight torch.Size([1024, 1024])\n",
      "772 transformer_model.pooler.dense.weight torch.Size([1024, 1024])\n",
      "773 transformer_model.pooler.dense.bias torch.Size([1024])\n",
      "774 transformer_model.classifier.weight torch.Size([2, 1024])\n",
      "775 transformer_model.classifier.bias torch.Size([2])\n",
      "776 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "777 attention.hidden_layer.bias torch.Size([512])\n",
      "778 attention.final_layer.weight torch.Size([1, 512])\n",
      "779 attention.final_layer.bias torch.Size([1])\n",
      "780 regressor.weight torch.Size([1, 1024])\n",
      "781 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_input_ids = torch.randint(0, 1000, [2, 248])\n",
    "# sample_attention_mask = torch.randint(0, 1000, [2, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca35f5f4-51d1-4000-ad76-ed912daa8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_records = [sample_ds[i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "709e45b4-ac40-4d67-8fd3-45c0f40d8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'target'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_records[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3389bd94-785b-47b5-bc32-1e4e58dd9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.stack([r['input_ids'] for r in sample_records])\n",
    "sample_attention_mask = torch.stack([r['attention_mask'] for r in sample_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04781a7b-218a-41cc-b81f-d2d248e2c7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 248]), torch.Size([2, 248]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids.shape, sample_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66a3b2b4-920e-4dff-bf9f-210d12b9c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1779,     5,   664,    82,  1835,     7,     5,  1011,  4294,\n",
       "             6,    24,  2633,    10, 27265,  1714,  2772,     4,  2978,     9,\n",
       "            41,  6291,  1310,     6,    24,    21,    10,  2608,  5252,     4,\n",
       "         50118,   133,  1929,    21,  2913,    19,  1958,    12,  9830, 20790,\n",
       "             6,    45,  4976,    15, 17359,     6,    53, 11122, 18331,    81,\n",
       "         24271,     8,  9910,  6368,     6,   101,    10,   588,  1958,   882,\n",
       "             4,    20,  3617, 38325,     8,   655,   571, 18656,    14,    56,\n",
       "         14633,     5,   929,     6,    58, 39143,    19, 15039,     8, 22246,\n",
       "         11538,    19, 13145, 21811,     9, 13178,     6,   101,  1958,     4,\n",
       "          1578, 11720,  8402,    56,    57, 14998, 38073,    15,   106,     6,\n",
       "             8, 19053,   154, 16155, 41591, 20846, 10601,    31,     5,  9836,\n",
       "             4, 50118,  3750,   349,   253,     9,     5,   929,     6,    15,\n",
       "             5,  2204,     6, 10601,    10,  2721,  4649,    12, 33986, 28862,\n",
       "             4, 50118,  4528,   910, 16148,    58,    13, 10761,     6,    65,\n",
       "            13,     5,  1972,     8,    65,    13,     5,  2786,     4,   178,\n",
       "            42,    21,     5,   177,     4, 50118,   133,  1972,    58,  4366,\n",
       "            23,    65,   253,     9,     5,   929,     8,     5,  2786,    23,\n",
       "             5,    97,     6,     8,    65,   253,    21,   373,     5,   369,\n",
       "         24710,     6,     8,     5,    97,     5,   391, 24710,     4,  4028,\n",
       "           869,    21,   576,    10,   650,  3794,    61,    51,    58,     7,\n",
       "          2195,    15,  3970,     5, 24710,     4, 50118,   713,    74,    33,\n",
       "            57,    41,  1365,   948,     6,    53,   349, 39496,    21, 23964,\n",
       "             7,  3568,  1958,  1193,  8013,     4,     2,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    1,  3684,   149,  3630,    86,     6,  3801,     4, 13066,   241,\n",
       "            21,  5568,  8454,     6,    69,  2473, 18403,    15,   211, 10147,\n",
       "            19,    10,   885,   661,  2650,     6,  9684,  8151,     4,   264,\n",
       "           770,     7,   492,     5,   920,     5, 10483,    79, 19656,  5202,\n",
       "             6,    53,    79,    56,   543,   173,     7,   836,  2864,     7,\n",
       "             5,   477,     9, 23968,    69,   308, 16006,     4, 50118,  3750,\n",
       "            94,     6,   959,     6,    77,     5,  5820,    21,   823,    81,\n",
       "             6,    79, 20185,    23,    69,   410,  1354,     6,     8,    26,\n",
       "             6,    22,  3684,   235,     6,   211, 10147,     6,    47,   189,\n",
       "           213,    72, 50118,   113,  7516,     6,   985,  2901,   211, 10147,\n",
       "         16670,     6, 13203,    19,  7207, 13213,     4,    22, 30327,   116,\n",
       "         50118,  7516,     6,    38,   524,    98,  7785,   328,  3945,    47,\n",
       "           686,    47,   214,  2882,  1917, 50118,   113,   100,   348, 23130,\n",
       "          2185,     7,    28,  2882,     6,   136,   127,    40,    60,  1835,\n",
       "          3801,     4, 13066,   241,     6, 29363,  3435,     4,    22,   100,\n",
       "         23721,    38,    95,  4157,     7,    33,    47,   213,     6,    53,\n",
       "            38,    64,    75,  4649,     7, 35858,    47,     9,     5, 10483,\n",
       "          1805,     4,   178,     6,    25,    47,   224,     6,    24,    74,\n",
       "            67,   489, 18449,  2553,    23,   184,     6,     8,    98,     6,\n",
       "         11807,     6,    38,   206,    38,  5658,    33,     7,   492,    11,\n",
       "            72, 50118,   113,  7516,     6,    47, 20285,   985,   328,   370,\n",
       "         12230,  6429,   328,  1336,   205,    47,    32,  2901,   178,   211,\n",
       "         10147,  8294,   198,     5,  2103,     8,   851,    69,   985,    10,\n",
       "         16531,    14,   823, 15544, 23283,    69,     4,     2,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25dd4b70-f9d7-4c3a-81d6-da03dd8cf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_out = sample_model.transformer_model(sample_input_ids, attention_mask=sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a770a11a-485d-49b5-ae8a-9d5dccc90a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "841fcfeb-6e75-40e5-8f82-265ed8da72d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, torch.Size([2, 248, 1024]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(internal_out.hidden_states), internal_out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res = sample_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea66f03e-eac6-478c-ab27-042d97ec1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1024]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res[0].shape, sample_res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.6797, -28.8163,  18.6234,  ..., -11.7443,   8.2338, -43.5786],\n",
       "        [  2.8545,  -1.2901, -12.1370,  ..., -29.8070, -36.4419, -13.0323],\n",
       "        [-10.6063,   1.3496,  -1.7358,  ..., -18.7509, -19.3895, -39.3124],\n",
       "        ...,\n",
       "        [-40.0713,  37.8588, -14.1708,  ...,  28.2023, -26.2297,   9.8448],\n",
       "        [-61.6582, -41.9132, -11.8418,  ...,   8.0438,  -9.5543,  -6.3620],\n",
       "        [-15.7379,  19.2065,   6.3814,  ..., -17.2292,  22.6391,  25.3546]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    attention_param_start = 776\n",
    "    regressor_param_start = 780\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 275\n",
    "    layer_middle_threshold = 571\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    pred, _ = self.model(input_ids, attention_mask)\n",
    "                    mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "                self.scaler.scale(mse).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "#                 mse.backward()\n",
    "#                 self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f3b704f-b4e5-4b33-a33b-159ba8b5685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: { 'base_lr': 4.3596909535440914e-05, 'last_lr': 0.0004188473213340135, 'epochs': 4 } Best value: 0.4722290635108948\n",
    "# Fold 1: {'base_lr': 3.093409522252196e-05, 'last_lr': 0.0004074485086437216, 'epochs': 4} Best is trial 9 with value: 0.4512692391872406\n",
    "# Fold 2: {'base_lr': 5.9004819673113075e-05, 'last_lr': 0.0003701804156340247, 'epochs': 5}   Best value:  0.46230143308639526\n",
    "# Fold 3: {'base_lr': 3.091841397163233e-05, 'last_lr': 0.00010409734625896974, 'epochs': 4}. Best value:  0.474480539560318\n",
    "# Fold 4: {'base_lr': 3.2314567372708084e-05, 'last_lr': 8.327155005618419e-05, 'epochs': 4}. Best is trial 0 with value: 0.45970267057418823\n",
    "# Fold 5: {'base_lr': 3.5379120180791935e-05, 'last_lr': 0.00021137535166837663, 'epochs': 5}. Best is trial 0 with value: 0.46050626039505005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 3e-5, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    epochs = trial.suggest_int('epochs', 3, 5)\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr} epochs {epochs}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler() # fp16\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, \n",
    "                      scheduler = scheduler, num_epochs = epochs)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48210ee3-d9ea-4bab-b852-627f6f75ce0c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 12:32:46,237]\u001b[0m A new study created in memory with name: no-name-08343f36-e798-4667-9c07-e6b584b44ca2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 3.52061547111578e-05 last_lr 0.002526936749634316 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5fe0e85a0040cdaf700ced7e06e40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8216 New best_val_rmse: 0.8216\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6873 New best_val_rmse: 0.6873\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6986 Still best_val_rmse: 0.6873 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5683 New best_val_rmse: 0.5683\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8753 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6333 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6638 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.643 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6932 Still best_val_rmse: 0.5683 (from epoch 0)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5392 New best_val_rmse: 0.5392\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5299 New best_val_rmse: 0.5299\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.536 Still best_val_rmse: 0.5299 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5084 New best_val_rmse: 0.5084\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5516 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5181 Still best_val_rmse: 0.5084 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5142 Still best_val_rmse: 0.4993 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5101 Still best_val_rmse: 0.4993 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.523 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4974 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5204 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4942 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4889 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.505 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4948 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4998 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4863 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4854 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4846 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4873 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4912 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4912 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "8 steps took 6.89 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4898 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4891 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4887 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4887 Still best_val_rmse: 0.4819 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 12:45:55,623]\u001b[0m Trial 0 finished with value: 0.48194995522499084 and parameters: {'base_lr': 3.52061547111578e-05, 'last_lr': 0.002526936749634316, 'epochs': 3}. Best is trial 0 with value: 0.48194995522499084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00025139675070443403 last_lr 8.48669845443173e-05 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45121080ad724eeba450931db4a38ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9014 New best_val_rmse: 0.9014\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.899 New best_val_rmse: 0.899\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9366 Still best_val_rmse: 0.899 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7911 New best_val_rmse: 0.7911\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8751 Still best_val_rmse: 0.7911 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.039 Still best_val_rmse: 0.7911 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.051 Still best_val_rmse: 0.7911 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.149 Still best_val_rmse: 0.7911 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.023 Still best_val_rmse: 0.7911 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 12:49:50,544]\u001b[0m Trial 1 finished with value: 0.7910526394844055 and parameters: {'base_lr': 0.00025139675070443403, 'last_lr': 8.48669845443173e-05, 'epochs': 5}. Best is trial 0 with value: 0.48194995522499084.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.3596909535440914e-05 last_lr 0.0004188473213340135 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d1d4c2bd294e379a627b1628840da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8007 New best_val_rmse: 0.8007\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7216 New best_val_rmse: 0.7216\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8994 Still best_val_rmse: 0.7216 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8762 Still best_val_rmse: 0.7216 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6523 New best_val_rmse: 0.6523\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.782 Still best_val_rmse: 0.6523 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6558 Still best_val_rmse: 0.6523 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5925 New best_val_rmse: 0.5925\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6099 Still best_val_rmse: 0.5925 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5123 New best_val_rmse: 0.5123\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5394 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5501 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5133 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5569 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.536 Still best_val_rmse: 0.5123 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5097 New best_val_rmse: 0.5097\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4897 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4876 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 3.94 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4876 Still best_val_rmse: 0.4857 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.478 New best_val_rmse: 0.478\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.48 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4909 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4844 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4896 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4803 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4823 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4786 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4808 Still best_val_rmse: 0.478 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4759 New best_val_rmse: 0.4759\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.474 New best_val_rmse: 0.474\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4728 New best_val_rmse: 0.4728\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4727 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4754 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4842 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4869 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.476 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4746 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4737 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4739 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.476 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4778 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4808 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4811 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4803 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4792 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4773 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4772 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4806 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4852 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4793 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4775 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4763 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4757 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.475 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4746 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.474 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4735 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4734 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4733 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4736 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4739 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4741 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4743 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4745 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4747 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4748 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4748 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 2.18 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.475 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.475 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4751 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4753 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4755 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4755 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4756 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4756 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4754 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4751 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4746 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4741 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.84 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4736 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.473 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4726 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4726 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4726 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4728 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4733 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4735 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4734 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4736 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4748 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.476 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.479 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.481 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4793 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4744 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4749 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4746 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4742 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4743 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4766 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4868 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4883 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4819 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4804 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4775 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4882 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4892 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4798 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4794 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4791 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.482 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4891 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4808 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.5304 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4765 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4804 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4832 Still best_val_rmse: 0.4722 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:20:56,919]\u001b[0m Trial 2 finished with value: 0.4722290635108948 and parameters: {'base_lr': 4.3596909535440914e-05, 'last_lr': 0.0004188473213340135, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 8.727465061799723e-05 last_lr 9.710102125226154e-05 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15835ac65ca74f0a99b9c027c1a0242d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7606 New best_val_rmse: 0.7606\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7279 New best_val_rmse: 0.7279\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8377 Still best_val_rmse: 0.7279 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7301 Still best_val_rmse: 0.7279 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6221 New best_val_rmse: 0.6221\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5944 New best_val_rmse: 0.5944\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6094 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7479 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8172 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.602 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.472 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.048 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.081 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.025 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.037 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.023 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.02 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.022 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.022 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.026 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.017 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.016 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.017 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.016 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.016 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.016 Still best_val_rmse: 0.5944 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.016 Still best_val_rmse: 0.5944 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:31:59,009]\u001b[0m Trial 3 finished with value: 0.5944061279296875 and parameters: {'base_lr': 8.727465061799723e-05, 'last_lr': 9.710102125226154e-05, 'epochs': 3}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00021673763882290424 last_lr 0.0009094132155860931 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645074d8d5044690bd9a494583a28f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9168 New best_val_rmse: 0.9168\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.16 Still best_val_rmse: 0.9168 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.025 Still best_val_rmse: 0.9168 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.058 Still best_val_rmse: 0.9168 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.03 Still best_val_rmse: 0.9168 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.028 Still best_val_rmse: 0.9168 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:34:38,757]\u001b[0m Trial 4 finished with value: 0.9168067574501038 and parameters: {'base_lr': 0.00021673763882290424, 'last_lr': 0.0009094132155860931, 'epochs': 5}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00017449894035314006 last_lr 9.380576700424332e-05 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30065c92e50c49b087194aacbd754658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8548 New best_val_rmse: 0.8548\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8815 Still best_val_rmse: 0.8548 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.195 Still best_val_rmse: 0.8548 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.063 Still best_val_rmse: 0.8548 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.048 Still best_val_rmse: 0.8548 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.044 Still best_val_rmse: 0.8548 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:37:17,800]\u001b[0m Trial 5 finished with value: 0.8548403382301331 and parameters: {'base_lr': 0.00017449894035314006, 'last_lr': 9.380576700424332e-05, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.000149709311373108 last_lr 0.0001765122619670107 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1bc278c3bd49a894433c5960a1fea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.802 New best_val_rmse: 0.802\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.316 Still best_val_rmse: 0.802 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.07 Still best_val_rmse: 0.802 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.046 Still best_val_rmse: 0.802 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.026 Still best_val_rmse: 0.802 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.037 Still best_val_rmse: 0.802 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:39:59,710]\u001b[0m Trial 6 finished with value: 0.8019664287567139 and parameters: {'base_lr': 0.000149709311373108, 'last_lr': 0.0001765122619670107, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00016319764240705915 last_lr 0.0030962193948501603 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e6dd941d6a469a9d30f9961f71f158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.022 New best_val_rmse: 1.022\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9537 New best_val_rmse: 0.9537\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9725 Still best_val_rmse: 0.9537 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6798 New best_val_rmse: 0.6798\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9345 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7188 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7979 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.029 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7558 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:43:54,721]\u001b[0m Trial 7 finished with value: 0.6798244118690491 and parameters: {'base_lr': 0.00016319764240705915, 'last_lr': 0.0030962193948501603, 'epochs': 5}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.00037494663476135657 last_lr 0.00015163485961944566 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04df9803e0c0462da77e088b48076b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.014 New best_val_rmse: 1.014\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.024 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.101 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.018 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.028 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.019 Still best_val_rmse: 1.014 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 13:46:34,288]\u001b[0m Trial 8 finished with value: 1.0135622024536133 and parameters: {'base_lr': 0.00037494663476135657, 'last_lr': 0.00015163485961944566, 'epochs': 5}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.9054858117051126e-05 last_lr 0.0004702842365807719 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921d585422ab461a835982656e629913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8983 New best_val_rmse: 0.8983\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6421 New best_val_rmse: 0.6421\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7121 Still best_val_rmse: 0.6421 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6883 Still best_val_rmse: 0.6421 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6127 New best_val_rmse: 0.6127\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.55 New best_val_rmse: 0.55\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5921 Still best_val_rmse: 0.55 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6059 Still best_val_rmse: 0.55 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6247 Still best_val_rmse: 0.55 (from epoch 0)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5201 New best_val_rmse: 0.5201\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5551 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5623 Still best_val_rmse: 0.5201 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5187 New best_val_rmse: 0.5187\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.592 Still best_val_rmse: 0.5187 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.559 Still best_val_rmse: 0.5187 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5102 New best_val_rmse: 0.5102\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4979 New best_val_rmse: 0.4979\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4994 Still best_val_rmse: 0.4979 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5088 Still best_val_rmse: 0.4979 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5033 Still best_val_rmse: 0.4979 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5114 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4951 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4929 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "8 steps took 6.74 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4892 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.49 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4837 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4838 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4895 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4872 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.484 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.485 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4844 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4859 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4871 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4883 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4877 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4876 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4878 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4878 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4877 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4876 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4875 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4875 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.86 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4874 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4874 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4874 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4871 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4868 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4865 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4865 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4862 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.52 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4861 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4859 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4846 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4834 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4834 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4835 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.484 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4841 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4849 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4864 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4852 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4862 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4856 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4855 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4874 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4863 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4868 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4956 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4983 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5175 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4877 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.5101 Still best_val_rmse: 0.48 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 14:07:52,519]\u001b[0m Trial 9 finished with value: 0.48002734780311584 and parameters: {'base_lr': 4.9054858117051126e-05, 'last_lr': 0.0004702842365807719, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 7.30578195892316e-05 last_lr 0.0005070855728570029 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967fb49a35e94e14bf7c5597c76c2862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7423 New best_val_rmse: 0.7423\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6939 New best_val_rmse: 0.6939\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.65 New best_val_rmse: 0.65\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7631 Still best_val_rmse: 0.65 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.725 Still best_val_rmse: 0.65 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6271 New best_val_rmse: 0.6271\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6442 Still best_val_rmse: 0.6271 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5929 New best_val_rmse: 0.5929\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.704 Still best_val_rmse: 0.5929 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5705 New best_val_rmse: 0.5705\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5664 New best_val_rmse: 0.5664\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5385 New best_val_rmse: 0.5385\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5096 New best_val_rmse: 0.5096\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5677 Still best_val_rmse: 0.5096 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5519 Still best_val_rmse: 0.5096 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5146 Still best_val_rmse: 0.5096 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5238 Still best_val_rmse: 0.5096 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5106 Still best_val_rmse: 0.5096 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4871 New best_val_rmse: 0.4871\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4882 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5419 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4932 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4967 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4957 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4945 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4984 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4986 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4997 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4998 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4914 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4871 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4868 New best_val_rmse: 0.4868\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4874 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4903 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4934 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4916 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4904 Still best_val_rmse: 0.4868 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 14:20:39,768]\u001b[0m Trial 10 finished with value: 0.48680758476257324 and parameters: {'base_lr': 7.30578195892316e-05, 'last_lr': 0.0005070855728570029, 'epochs': 3}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.149222831637006e-05 last_lr 0.0005758972050563623 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c281ad7597465e981c80782663e0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9105 New best_val_rmse: 0.9105\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.632 New best_val_rmse: 0.632\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8844 Still best_val_rmse: 0.632 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7186 Still best_val_rmse: 0.632 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.713 Still best_val_rmse: 0.632 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7416 Still best_val_rmse: 0.632 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6893 Still best_val_rmse: 0.632 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5858 New best_val_rmse: 0.5858\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6673 Still best_val_rmse: 0.5858 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.521 New best_val_rmse: 0.521\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5543 Still best_val_rmse: 0.521 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.583 Still best_val_rmse: 0.521 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5145 New best_val_rmse: 0.5145\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5858 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5453 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.501 New best_val_rmse: 0.501\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5047 Still best_val_rmse: 0.501 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4904 New best_val_rmse: 0.4904\n",
      "\n",
      "8 steps took 7.23 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4934 Still best_val_rmse: 0.4904 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4903 New best_val_rmse: 0.4903\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5066 Still best_val_rmse: 0.4903 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4883 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4878 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4887 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4926 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4875 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.485 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.5004 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4864 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4862 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4947 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4816 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4835 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4847 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4855 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4858 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4856 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4852 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.485 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.485 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.86 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.485 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.485 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4849 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4845 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4838 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.483 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4823 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4817 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.53 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4818 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.483 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4832 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4823 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4824 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4816 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.482 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4833 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4855 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4839 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4827 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4851 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4903 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4831 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4849 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.5059 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5318 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4836 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4943 Still best_val_rmse: 0.481 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.483 Still best_val_rmse: 0.481 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 14:41:46,823]\u001b[0m Trial 11 finished with value: 0.48100438714027405 and parameters: {'base_lr': 3.149222831637006e-05, 'last_lr': 0.0005758972050563623, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 5.378964540224331e-05 last_lr 0.000321518446584975 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb7a89c6920408eb2b043910fecf633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8802 New best_val_rmse: 0.8802\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.613 New best_val_rmse: 0.613\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6414 Still best_val_rmse: 0.613 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6029 New best_val_rmse: 0.6029\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6926 Still best_val_rmse: 0.6029 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6063 Still best_val_rmse: 0.6029 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6327 Still best_val_rmse: 0.6029 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6559 Still best_val_rmse: 0.6029 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.599 New best_val_rmse: 0.599\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5166 New best_val_rmse: 0.5166\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5766 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5314 Still best_val_rmse: 0.5166 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5007 New best_val_rmse: 0.5007\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5162 Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5074 Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5019 Still best_val_rmse: 0.5007 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4956 New best_val_rmse: 0.4956\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4968 Still best_val_rmse: 0.4956 (from epoch 1)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5069 Still best_val_rmse: 0.4956 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4822 New best_val_rmse: 0.4822\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4851 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4976 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4845 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4876 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4971 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4931 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.491 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4936 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4826 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4826 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4866 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.484 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4823 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4827 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4829 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4868 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4879 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4866 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4852 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4849 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4852 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4855 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4855 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.87 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4855 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4856 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4858 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4858 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4854 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.52 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4841 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4825 Still best_val_rmse: 0.4822 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4821 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4824 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4849 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4836 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4848 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4894 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.485 Still best_val_rmse: 0.482 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4837 Still best_val_rmse: 0.4818 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4811 New best_val_rmse: 0.4811\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4818 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4942 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4866 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.491 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4843 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4909 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4885 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4828 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4856 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4922 Still best_val_rmse: 0.4811 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4963 Still best_val_rmse: 0.4811 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 15:04:05,338]\u001b[0m Trial 12 finished with value: 0.4811325669288635 and parameters: {'base_lr': 5.378964540224331e-05, 'last_lr': 0.000321518446584975, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 5.0197324888794674e-05 last_lr 0.001040925108107359 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506ee0fe79314517b70c6b2a2e1530d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9508 New best_val_rmse: 0.9508\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6423 New best_val_rmse: 0.6423\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8509 Still best_val_rmse: 0.6423 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.725 Still best_val_rmse: 0.6423 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.821 Still best_val_rmse: 0.6423 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5926 New best_val_rmse: 0.5926\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6038 Still best_val_rmse: 0.5926 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.58 New best_val_rmse: 0.58\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.648 Still best_val_rmse: 0.58 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5498 New best_val_rmse: 0.5498\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5666 Still best_val_rmse: 0.5498 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5228 New best_val_rmse: 0.5228\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5149 New best_val_rmse: 0.5149\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5786 Still best_val_rmse: 0.5149 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5218 Still best_val_rmse: 0.5149 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4854 New best_val_rmse: 0.4854\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4875 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5298 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5059 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.489 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.5026 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4892 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4868 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.501 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4855 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4856 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4948 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4898 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4898 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4836 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4814 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4807 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4844 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4815 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4791 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4797 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4804 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4802 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4829 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4871 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4897 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4877 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4867 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4867 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4867 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4864 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4863 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4861 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.486 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.89 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.486 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.486 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4859 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4857 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4854 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4851 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4851 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4848 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.52 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4842 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4835 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4828 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4826 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4835 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4825 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4775 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4778 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4779 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.479 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4823 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4913 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.481 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4831 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4865 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4933 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4798 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4797 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4816 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.5012 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4888 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5344 Still best_val_rmse: 0.4764 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.5004 Still best_val_rmse: 0.4764 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 15:26:32,852]\u001b[0m Trial 13 finished with value: 0.47635605931282043 and parameters: {'base_lr': 5.0197324888794674e-05, 'last_lr': 0.001040925108107359, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.4574105236474114e-05 last_lr 0.0012177898793993265 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74128de877194c0e85da2864b294b692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.848 New best_val_rmse: 0.848\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6961 New best_val_rmse: 0.6961\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6787 New best_val_rmse: 0.6787\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6374 New best_val_rmse: 0.6374\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8057 Still best_val_rmse: 0.6374 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7773 Still best_val_rmse: 0.6374 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6259 New best_val_rmse: 0.6259\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6152 New best_val_rmse: 0.6152\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6125 New best_val_rmse: 0.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 15:30:27,918]\u001b[0m Trial 14 finished with value: 0.6124522686004639 and parameters: {'base_lr': 4.4574105236474114e-05, 'last_lr': 0.0012177898793993265, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 9.82098014680715e-05 last_lr 0.0015505061907821445 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642a5b5c9f5c43a9bab0047c755c55d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7732 New best_val_rmse: 0.7732\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7319 New best_val_rmse: 0.7319\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7905 Still best_val_rmse: 0.7319 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8004 Still best_val_rmse: 0.7319 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.739 Still best_val_rmse: 0.7319 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6267 New best_val_rmse: 0.6267\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7103 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6684 Still best_val_rmse: 0.6267 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7785 Still best_val_rmse: 0.6267 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 15:34:22,654]\u001b[0m Trial 15 finished with value: 0.6266731023788452 and parameters: {'base_lr': 9.82098014680715e-05, 'last_lr': 0.0015505061907821445, 'epochs': 3}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.015168087829758e-05 last_lr 0.00024764700595687967 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d58fcacb6844f51b8135bab60a5c266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9311 New best_val_rmse: 0.9311\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6291 New best_val_rmse: 0.6291\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8726 Still best_val_rmse: 0.6291 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8267 Still best_val_rmse: 0.6291 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6974 Still best_val_rmse: 0.6291 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7037 Still best_val_rmse: 0.6291 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6874 Still best_val_rmse: 0.6291 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.564 New best_val_rmse: 0.564\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6877 Still best_val_rmse: 0.564 (from epoch 0)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5265 New best_val_rmse: 0.5265\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5476 Still best_val_rmse: 0.5265 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5618 Still best_val_rmse: 0.5265 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5105 New best_val_rmse: 0.5105\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5742 Still best_val_rmse: 0.5105 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5429 Still best_val_rmse: 0.5105 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4952 New best_val_rmse: 0.4952\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4972 Still best_val_rmse: 0.4952 (from epoch 1)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.523 Still best_val_rmse: 0.4952 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4931 New best_val_rmse: 0.4931\n",
      "\n",
      "8 steps took 7.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4895 Still best_val_rmse: 0.4888 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5003 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4841 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4815 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4823 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4926 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.491 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4905 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4892 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4879 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4882 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4896 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4872 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4872 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4878 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4874 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4883 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4897 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4916 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4914 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4915 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4909 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4908 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 7.2 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4907 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4904 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4899 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4899 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4895 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4891 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4879 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4866 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.51 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4866 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.488 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4912 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4871 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.486 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4862 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4856 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4859 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4876 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4842 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4835 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4896 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4896 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4873 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4865 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4884 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4978 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.499 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.5062 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4884 Still best_val_rmse: 0.4815 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 15:55:29,298]\u001b[0m Trial 16 finished with value: 0.4814762771129608 and parameters: {'base_lr': 3.015168087829758e-05, 'last_lr': 0.00024764700595687967, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 6.535214839410794e-05 last_lr 0.0008696239949969886 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae0367818104c0a81999096f52e7254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7778 New best_val_rmse: 0.7778\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6669 New best_val_rmse: 0.6669\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8377 Still best_val_rmse: 0.6669 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6336 New best_val_rmse: 0.6336\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9348 Still best_val_rmse: 0.6336 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6796 Still best_val_rmse: 0.6336 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5773 New best_val_rmse: 0.5773\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.9824 Still best_val_rmse: 0.5773 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6515 Still best_val_rmse: 0.5773 (from epoch 0)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5944 Still best_val_rmse: 0.5773 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5196 New best_val_rmse: 0.5196\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5734 Still best_val_rmse: 0.5196 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.6 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5151 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5655 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5161 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5094 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4981 Still best_val_rmse: 0.4947 (from epoch 1)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4851 New best_val_rmse: 0.4851\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4877 Still best_val_rmse: 0.4851 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5449 Still best_val_rmse: 0.4851 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4811 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4903 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.482 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4845 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4804 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4826 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4871 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4872 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4821 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4835 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4815 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4818 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4842 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4838 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4804 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4775 New best_val_rmse: 0.4775\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4774 New best_val_rmse: 0.4774\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4774 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.478 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4792 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4804 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4815 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4817 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4819 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4815 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4811 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4809 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4809 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4809 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.9 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4809 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4808 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4806 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4802 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4798 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4795 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4794 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4793 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4798 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4797 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4799 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4799 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4797 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4796 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.84 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4797 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4805 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4817 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4829 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4818 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4807 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4816 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.484 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4821 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4777 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.478 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4792 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4799 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4834 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4804 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4806 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.5144 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4848 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4886 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4845 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4853 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4856 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.49 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.5495 Still best_val_rmse: 0.4773 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 16:20:19,087]\u001b[0m Trial 17 finished with value: 0.477342814207077 and parameters: {'base_lr': 6.535214839410794e-05, 'last_lr': 0.0008696239949969886, 'epochs': 4}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.840605134767292e-05 last_lr 0.0018472946836358812 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa3b3ac83714d6d886a341cce75c0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8192 New best_val_rmse: 0.8192\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7646 New best_val_rmse: 0.7646\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8818 Still best_val_rmse: 0.7646 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7447 New best_val_rmse: 0.7447\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5804 New best_val_rmse: 0.5804\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6274 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6393 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5887 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6952 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 13.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5911 Still best_val_rmse: 0.5804 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5523 New best_val_rmse: 0.5523\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5881 Still best_val_rmse: 0.5523 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5069 New best_val_rmse: 0.5069\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.574 Still best_val_rmse: 0.5069 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5673 Still best_val_rmse: 0.5069 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5379 Still best_val_rmse: 0.5069 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5042 New best_val_rmse: 0.5042\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5272 Still best_val_rmse: 0.5042 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4905 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4908 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.498 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4884 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4953 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4935 Still best_val_rmse: 0.4879 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4868 New best_val_rmse: 0.4868\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4886 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4888 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4984 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4907 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4905 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4979 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4894 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4843 New best_val_rmse: 0.4843\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4839 New best_val_rmse: 0.4839\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4853 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4874 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4884 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4893 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4895 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4889 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4884 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4882 Still best_val_rmse: 0.4839 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4882 Still best_val_rmse: 0.4839 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 16:34:17,094]\u001b[0m Trial 18 finished with value: 0.48388057947158813 and parameters: {'base_lr': 3.840605134767292e-05, 'last_lr': 0.0018472946836358812, 'epochs': 3}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 9.900413506375257e-05 last_lr 0.004894051644271908 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647aae862d8a43a580e6f97ae6eae86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9259 New best_val_rmse: 0.9259\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7994 New best_val_rmse: 0.7994\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9247 Still best_val_rmse: 0.7994 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.936 Still best_val_rmse: 0.7994 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.194 Still best_val_rmse: 0.7994 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.066 Still best_val_rmse: 0.7994 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.04 Still best_val_rmse: 0.7994 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.16 Still best_val_rmse: 0.7994 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 16:37:43,741]\u001b[0m Trial 19 finished with value: 0.7993664145469666 and parameters: {'base_lr': 9.900413506375257e-05, 'last_lr': 0.004894051644271908, 'epochs': 5}. Best is trial 2 with value: 0.4722290635108948.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 16:37:43,743]\u001b[0m A new study created in memory with name: no-name-051ff919-33eb-4390-a237-cd5bfc133971\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value:  0.4722290635108948\n",
      " Best params: \n",
      "    base_lr: 4.3596909535440914e-05\n",
      "    last_lr: 0.0004188473213340135\n",
      "    epochs: 4\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00014093376329743534 last_lr 0.0005197005435285995 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac9f5091c494e0a8dfe452f99d2e7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8935 New best_val_rmse: 0.8935\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8103 New best_val_rmse: 0.8103\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.172 Still best_val_rmse: 0.8103 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.013 Still best_val_rmse: 0.8103 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.012 Still best_val_rmse: 0.8103 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.017 Still best_val_rmse: 0.8103 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.044 Still best_val_rmse: 0.8103 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.017 Still best_val_rmse: 0.8103 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.038 Still best_val_rmse: 0.8103 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 16:41:32,936]\u001b[0m Trial 0 finished with value: 0.8102750778198242 and parameters: {'base_lr': 0.00014093376329743534, 'last_lr': 0.0005197005435285995, 'epochs': 3}. Best is trial 0 with value: 0.8102750778198242.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 7.276983099283288e-05 last_lr 0.0009825677410960766 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf000474e9f432a8feac5a457407621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.059 New best_val_rmse: 1.059\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8111 New best_val_rmse: 0.8111\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7844 New best_val_rmse: 0.7844\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.975 Still best_val_rmse: 0.7844 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9136 Still best_val_rmse: 0.7844 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.19 Still best_val_rmse: 0.7844 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6964 New best_val_rmse: 0.6964\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6625 New best_val_rmse: 0.6625\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6158 New best_val_rmse: 0.6158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 16:45:28,760]\u001b[0m Trial 1 finished with value: 0.6158090829849243 and parameters: {'base_lr': 7.276983099283288e-05, 'last_lr': 0.0009825677410960766, 'epochs': 5}. Best is trial 1 with value: 0.6158090829849243.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 8.209345092870527e-05 last_lr 0.0005140007857320931 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37f117f4ad6422884e41edfca4fbcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8619 New best_val_rmse: 0.8619\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7083 New best_val_rmse: 0.7083\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9729 Still best_val_rmse: 0.7083 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.663 New best_val_rmse: 0.663\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8279 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6654 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6451 New best_val_rmse: 0.6451\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.531 New best_val_rmse: 0.531\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6947 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6084 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5799 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.7639 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.114 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.03 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.093 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.016 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.008 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.009 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.006 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.005 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.027 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.015 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.009 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.005 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.003 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.004 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.004 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 1.003 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 1.004 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 1.003 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 1.004 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 1.011 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 1.005 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 1.003 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 1.009 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 1.009 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 1.01 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 1.039 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 1.064 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 1.123 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 1.053 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 1.005 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 1.058 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 1.003 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 1.137 Still best_val_rmse: 0.531 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 1.077 Still best_val_rmse: 0.531 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 17:03:46,483]\u001b[0m Trial 2 finished with value: 0.5309603214263916 and parameters: {'base_lr': 8.209345092870527e-05, 'last_lr': 0.0005140007857320931, 'epochs': 5}. Best is trial 2 with value: 0.5309603214263916.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00010042540410701344 last_lr 0.00013194069340692225 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c45fba9c98416ba3260736ddf98bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8349 New best_val_rmse: 0.8349\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8191 New best_val_rmse: 0.8191\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8557 Still best_val_rmse: 0.8191 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.663 New best_val_rmse: 0.663\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7191 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.554 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.056 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.009 Still best_val_rmse: 0.663 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.009 Still best_val_rmse: 0.663 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 17:07:40,851]\u001b[0m Trial 3 finished with value: 0.6630330681800842 and parameters: {'base_lr': 0.00010042540410701344, 'last_lr': 0.00013194069340692225, 'epochs': 4}. Best is trial 2 with value: 0.5309603214263916.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 3.6766910797869016e-05 last_lr 0.0001633214365553677 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1021b03be74947b40480194f1beb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8759 New best_val_rmse: 0.8759\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7709 New best_val_rmse: 0.7709\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.707 New best_val_rmse: 0.707\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6319 New best_val_rmse: 0.6319\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6388 Still best_val_rmse: 0.6319 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.711 Still best_val_rmse: 0.6319 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5259 New best_val_rmse: 0.5259\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5414 Still best_val_rmse: 0.5259 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.571 Still best_val_rmse: 0.5259 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5453 Still best_val_rmse: 0.5259 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5001 New best_val_rmse: 0.5001\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5069 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5049 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5286 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6375 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5381 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5077 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 1 batch_num: 143 val_rmse: 0.471 Still best_val_rmse: 0.4698 (from epoch 1)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 145 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4763 Still best_val_rmse: 0.4696 (from epoch 1)\n",
      "\n",
      "2 steps took 2.21 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5007 Still best_val_rmse: 0.4696 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4755 Still best_val_rmse: 0.4696 (from epoch 1)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4843 Still best_val_rmse: 0.4696 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4727 Still best_val_rmse: 0.4696 (from epoch 1)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.469 New best_val_rmse: 0.469\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4751 Still best_val_rmse: 0.469 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4706 Still best_val_rmse: 0.469 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4687 New best_val_rmse: 0.4687\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4762 Still best_val_rmse: 0.4687 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4845 Still best_val_rmse: 0.4687 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4703 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4644 New best_val_rmse: 0.4644\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4654 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4698 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4761 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4768 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4688 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4659 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4665 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4703 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4738 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4717 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.469 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4715 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4796 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4871 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4778 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4697 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4676 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.466 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4653 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.465 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4646 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4644 New best_val_rmse: 0.4644\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4645 Still best_val_rmse: 0.4644 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4644 New best_val_rmse: 0.4644\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4642 New best_val_rmse: 0.4642\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4642 New best_val_rmse: 0.4642\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4643 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4646 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.465 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4653 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4653 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4659 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4671 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4672 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4673 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4665 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4654 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4645 Still best_val_rmse: 0.4642 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4639 New best_val_rmse: 0.4639\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4638 New best_val_rmse: 0.4638\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4638 Still best_val_rmse: 0.4638 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4641 Still best_val_rmse: 0.4638 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4641 Still best_val_rmse: 0.4638 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4638 Still best_val_rmse: 0.4638 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4636 New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4635 New best_val_rmse: 0.4635\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4635 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4635 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4636 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4638 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4641 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4646 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4651 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4657 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4664 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4666 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4667 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4667 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4665 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4666 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4665 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4664 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4665 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4667 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4668 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4669 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4667 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4666 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4663 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4661 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4657 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4655 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4651 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4648 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4646 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4644 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4642 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.464 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4638 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4637 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4636 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4635 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4635 Still best_val_rmse: 0.4635 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4634 New best_val_rmse: 0.4634\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.545 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 1.73 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4633 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4636 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4636 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4637 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4637 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4638 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4639 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4641 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4642 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4643 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4642 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4643 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4644 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4646 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4646 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4646 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4647 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4646 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4647 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4647 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4648 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4651 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4652 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4654 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4655 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.466 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4661 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4661 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4658 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4655 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 1.01 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4648 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4642 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4638 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4636 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4636 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4639 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4641 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4644 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4648 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4655 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4657 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4657 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4663 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4668 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4662 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4656 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.465 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4645 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4645 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4645 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4651 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4664 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4683 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4702 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4788 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4808 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4655 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4641 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.464 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.464 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4634 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4635 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.463 New best_val_rmse: 0.463\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4628 New best_val_rmse: 0.4628\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4635 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4656 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4701 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4715 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4636 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4662 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4727 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4745 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4661 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4692 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4783 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4976 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.464 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4639 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4643 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4649 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4657 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4666 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4676 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4712 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4741 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4701 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4681 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4677 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4675 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4664 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4655 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4659 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4679 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4676 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4661 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4699 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4772 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4707 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4887 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4909 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "8 steps took 7.28 seconds\n",
      "Epoch: 4 batch_num: 5 val_rmse: 0.4802 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 9 val_rmse: 0.4736 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 11 val_rmse: 0.4659 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 12 val_rmse: 0.4647 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 4 batch_num: 13 val_rmse: 0.4657 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 14 val_rmse: 0.4669 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 4 batch_num: 15 val_rmse: 0.4667 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.467 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 4 batch_num: 17 val_rmse: 0.4675 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 4 batch_num: 18 val_rmse: 0.4761 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.4717 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 22 val_rmse: 0.4678 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 23 val_rmse: 0.4697 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4761 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 26 val_rmse: 0.4691 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 27 val_rmse: 0.4735 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 29 val_rmse: 0.4799 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 31 val_rmse: 0.476 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 33 val_rmse: 0.4792 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 35 val_rmse: 0.4913 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 43 val_rmse: 0.5265 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 59 val_rmse: 0.5003 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 75 val_rmse: 0.4943 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 83 val_rmse: 0.4884 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 87 val_rmse: 0.4846 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 91 val_rmse: 0.5025 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 107 val_rmse: 0.4858 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 111 val_rmse: 0.4805 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 115 val_rmse: 0.4861 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 119 val_rmse: 0.488 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 123 val_rmse: 0.4845 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 127 val_rmse: 0.5286 Still best_val_rmse: 0.4628 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 143 val_rmse: 0.5194 Still best_val_rmse: 0.4628 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 18:06:36,493]\u001b[0m Trial 4 finished with value: 0.46281054615974426 and parameters: {'base_lr': 3.6766910797869016e-05, 'last_lr': 0.0001633214365553677, 'epochs': 5}. Best is trial 4 with value: 0.46281054615974426.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 7.946695511107508e-05 last_lr 0.0012029943571250225 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a606163322445899d4c33bd551b261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8637 New best_val_rmse: 0.8637\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7937 New best_val_rmse: 0.7937\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8605 Still best_val_rmse: 0.7937 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.019 Still best_val_rmse: 0.7937 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.021 Still best_val_rmse: 0.7937 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.032 Still best_val_rmse: 0.7937 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.011 Still best_val_rmse: 0.7937 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.041 Still best_val_rmse: 0.7937 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.005 Still best_val_rmse: 0.7937 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 18:10:30,209]\u001b[0m Trial 5 finished with value: 0.793735146522522 and parameters: {'base_lr': 7.946695511107508e-05, 'last_lr': 0.0012029943571250225, 'epochs': 5}. Best is trial 4 with value: 0.46281054615974426.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 6.276471903172248e-05 last_lr 0.0037184123683035365 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9109a19064a0433aa9fae50c1d287bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9806 New best_val_rmse: 0.9806\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6705 New best_val_rmse: 0.6705\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9365 Still best_val_rmse: 0.6705 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6213 New best_val_rmse: 0.6213\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.019 Still best_val_rmse: 0.6213 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6116 New best_val_rmse: 0.6116\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7607 Still best_val_rmse: 0.6116 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5737 New best_val_rmse: 0.5737\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5333 New best_val_rmse: 0.5333\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5359 Still best_val_rmse: 0.5333 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5446 Still best_val_rmse: 0.5333 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5379 Still best_val_rmse: 0.5333 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5259 New best_val_rmse: 0.5259\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5602 Still best_val_rmse: 0.5259 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6208 Still best_val_rmse: 0.5259 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.508 New best_val_rmse: 0.508\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5195 Still best_val_rmse: 0.508 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4742 New best_val_rmse: 0.4742\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4751 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4767 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4803 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "4 steps took 3.97 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.5276 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4823 Still best_val_rmse: 0.4742 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4662 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4664 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4672 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4676 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4683 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.469 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4705 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4686 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4692 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4695 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.469 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.469 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.47 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4733 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4866 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4723 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4666 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4661 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4672 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4664 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4619 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4636 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4669 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4702 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4731 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4675 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4639 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4637 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4666 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4698 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4709 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4663 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4646 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.464 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.465 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4686 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4705 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4739 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4697 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4682 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4664 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4648 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4627 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4621 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4618 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4618 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4623 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4631 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4642 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4655 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4678 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.469 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4711 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4723 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4686 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4668 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4662 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4661 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4659 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4658 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4655 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4651 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4644 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4639 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4635 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.463 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4629 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4625 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4623 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4621 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4619 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4618 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4619 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4618 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4618 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4617 Still best_val_rmse: 0.4616 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4615 New best_val_rmse: 0.4615\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4612 New best_val_rmse: 0.4612\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.461 New best_val_rmse: 0.461\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4609 New best_val_rmse: 0.4609\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4607 New best_val_rmse: 0.4607\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4606 New best_val_rmse: 0.4606\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4605 New best_val_rmse: 0.4605\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4603 New best_val_rmse: 0.4603\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4602 New best_val_rmse: 0.4602\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4601 New best_val_rmse: 0.4601\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.46 New best_val_rmse: 0.46\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.46 New best_val_rmse: 0.46\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.46 New best_val_rmse: 0.46\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n",
      "\n",
      "1 steps took 0.547 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4599 Still best_val_rmse: 0.4599 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 18:39:51,799]\u001b[0m Trial 6 finished with value: 0.45986250042915344 and parameters: {'base_lr': 6.276471903172248e-05, 'last_lr': 0.0037184123683035365, 'epochs': 3}. Best is trial 6 with value: 0.45986250042915344.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00047215451187118294 last_lr 0.0036797076393644897 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4645732c089147e694bd4f06531579a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.3 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.32 New best_val_rmse: 1.32\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.073 Still best_val_rmse: 1.045 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.133 Still best_val_rmse: 1.045 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.106 Still best_val_rmse: 1.045 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.027 New best_val_rmse: 1.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 18:42:32,448]\u001b[0m Trial 7 finished with value: 1.0271624326705933 and parameters: {'base_lr': 0.00047215451187118294, 'last_lr': 0.0036797076393644897, 'epochs': 5}. Best is trial 6 with value: 0.45986250042915344.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00020424922669723563 last_lr 0.00011726241769744605 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a824dca3fc645aeb06fe0f28e1b4012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8868 New best_val_rmse: 0.8868\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8337 New best_val_rmse: 0.8337\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.057 Still best_val_rmse: 0.8337 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.052 Still best_val_rmse: 0.8337 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.004 Still best_val_rmse: 0.8337 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.005 Still best_val_rmse: 0.8337 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.002 Still best_val_rmse: 0.8337 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.016 Still best_val_rmse: 0.8337 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.0 Still best_val_rmse: 0.8337 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 18:46:25,170]\u001b[0m Trial 8 finished with value: 0.8337472081184387 and parameters: {'base_lr': 0.00020424922669723563, 'last_lr': 0.00011726241769744605, 'epochs': 3}. Best is trial 6 with value: 0.45986250042915344.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 3.093409522252196e-05 last_lr 0.0004074485086437216 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171c1beb3e9d4bd1abd0d78429289a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8201 New best_val_rmse: 0.8201\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7671 New best_val_rmse: 0.7671\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6906 New best_val_rmse: 0.6906\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6273 New best_val_rmse: 0.6273\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5923 New best_val_rmse: 0.5923\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5623 New best_val_rmse: 0.5623\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.56 New best_val_rmse: 0.56\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6405 Still best_val_rmse: 0.56 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.562 Still best_val_rmse: 0.56 (from epoch 0)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5441 New best_val_rmse: 0.5441\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4906 New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.4983 Still best_val_rmse: 0.4906 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4996 Still best_val_rmse: 0.4906 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 56 val_rmse: 0.4938 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.475 New best_val_rmse: 0.475\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 66 val_rmse: 0.4985 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 74 val_rmse: 0.4922 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 82 val_rmse: 0.5749 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 98 val_rmse: 0.5539 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 114 val_rmse: 0.4757 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4805 Still best_val_rmse: 0.475 (from epoch 1)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.471 New best_val_rmse: 0.471\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 1 batch_num: 122 val_rmse: 0.4922 Still best_val_rmse: 0.471 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 130 val_rmse: 0.4831 Still best_val_rmse: 0.471 (from epoch 1)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.5263 Still best_val_rmse: 0.471 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4908 Still best_val_rmse: 0.471 (from epoch 1)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4846 Still best_val_rmse: 0.471 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4687 New best_val_rmse: 0.4687\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.4642 New best_val_rmse: 0.4642\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4611 New best_val_rmse: 0.4611\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4599 New best_val_rmse: 0.4599\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4592 New best_val_rmse: 0.4592\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4586 New best_val_rmse: 0.4586\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.459 Still best_val_rmse: 0.4586 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4547 New best_val_rmse: 0.4547\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4532 New best_val_rmse: 0.4532\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4534 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4536 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4549 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.454 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4523 New best_val_rmse: 0.4523\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4532 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.463 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4731 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4721 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4687 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4702 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.461 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4555 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4526 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4534 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4554 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4549 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4533 Still best_val_rmse: 0.4523 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.452 New best_val_rmse: 0.452\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4519 New best_val_rmse: 0.4519\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4525 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4519 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4523 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4531 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.455 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4549 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4542 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4533 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4523 Still best_val_rmse: 0.4519 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4513 New best_val_rmse: 0.4513\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4513 New best_val_rmse: 0.4513\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4513 New best_val_rmse: 0.4513\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4519 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4533 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4543 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4545 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4537 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4536 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.453 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4529 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4527 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.453 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4532 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4534 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4539 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4545 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4553 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4557 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4568 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4576 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4587 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4592 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4599 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4606 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4619 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4625 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4614 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4604 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4586 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4567 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4554 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4545 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4542 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.454 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4537 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4534 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4533 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4532 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4531 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.453 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4529 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4529 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4527 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4527 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4526 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4522 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4521 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.452 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4519 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4518 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4518 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4517 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4517 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4515 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4517 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4518 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4518 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4519 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.452 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4521 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4521 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4522 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4522 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.544 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 1.74 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4525 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4525 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4525 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4526 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4526 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4527 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.453 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4531 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4532 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4533 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4534 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4536 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4536 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4534 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4532 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4529 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4527 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4526 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4525 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4525 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4524 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4525 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4526 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4527 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4529 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4531 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 1.0 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4533 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4534 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4535 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4536 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4539 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4546 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4551 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4551 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4551 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.455 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4552 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4552 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4551 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4548 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4547 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4548 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4549 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4549 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4549 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4549 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4543 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4532 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4526 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4523 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4521 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4521 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4519 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4516 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4515 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4514 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.452 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.453 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4546 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4559 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4574 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.46 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4637 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4681 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4698 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4654 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.458 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4529 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4531 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4557 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4592 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4599 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4582 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4556 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4543 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4551 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4562 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4548 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4542 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4539 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.454 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4545 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.455 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4563 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4574 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4588 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4588 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4581 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4576 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4595 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4591 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4601 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4624 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4663 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4725 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4665 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4577 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4585 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4603 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4596 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4568 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4585 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.456 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4548 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.456 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4595 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.457 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4553 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4561 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4655 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4802 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4603 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4578 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4555 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4541 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4528 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.458 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4661 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4702 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4589 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4556 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4562 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4632 Still best_val_rmse: 0.4513 (from epoch 2)\n",
      "\n",
      "1 steps took 0.546 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4701 Still best_val_rmse: 0.4513 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 19:46:03,667]\u001b[0m Trial 9 finished with value: 0.4512692391872406 and parameters: {'base_lr': 3.093409522252196e-05, 'last_lr': 0.0004074485086437216, 'epochs': 4}. Best is trial 9 with value: 0.4512692391872406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 3.1366132843316186e-05 last_lr 0.0002947624357351441 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af7165e969340c68ffc5c76c23c308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8231 New best_val_rmse: 0.8231\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7554 New best_val_rmse: 0.7554\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.687 New best_val_rmse: 0.687\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6122 New best_val_rmse: 0.6122\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7245 Still best_val_rmse: 0.6122 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6313 Still best_val_rmse: 0.6122 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6666 Still best_val_rmse: 0.6122 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6489 Still best_val_rmse: 0.6122 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6155 Still best_val_rmse: 0.6122 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 19:50:00,599]\u001b[0m Trial 10 finished with value: 0.6121996641159058 and parameters: {'base_lr': 3.1366132843316186e-05, 'last_lr': 0.0002947624357351441, 'epochs': 4}. Best is trial 9 with value: 0.4512692391872406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 4.502645042406232e-05 last_lr 0.004770565920096722 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dabdd928c86438db4b68eebaf1c9dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8616 New best_val_rmse: 0.8616\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6293 New best_val_rmse: 0.6293\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.028 Still best_val_rmse: 0.6293 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6163 New best_val_rmse: 0.6163\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6817 Still best_val_rmse: 0.6163 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8007 Still best_val_rmse: 0.6163 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5644 New best_val_rmse: 0.5644\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.576 Still best_val_rmse: 0.5644 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5404 New best_val_rmse: 0.5404\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5255 New best_val_rmse: 0.5255\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5507 Still best_val_rmse: 0.5255 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5267 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5593 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5638 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5163 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.4944 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4946 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5487 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5305 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4755 New best_val_rmse: 0.4755\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4799 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4948 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4812 Still best_val_rmse: 0.4752 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4742 New best_val_rmse: 0.4742\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.471 New best_val_rmse: 0.471\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4737 Still best_val_rmse: 0.471 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4819 Still best_val_rmse: 0.471 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4807 Still best_val_rmse: 0.471 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.473 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4707 New best_val_rmse: 0.4707\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.475 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4843 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.484 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4753 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4733 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4722 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4728 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4726 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4754 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4768 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4767 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4762 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4734 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4718 Still best_val_rmse: 0.4707 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4702 New best_val_rmse: 0.4702\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4696 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4698 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4703 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4709 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4721 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4714 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4705 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4696 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4687 New best_val_rmse: 0.4687\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4684 New best_val_rmse: 0.4684\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4683 New best_val_rmse: 0.4683\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4681 New best_val_rmse: 0.4681\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4681 New best_val_rmse: 0.4681\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4681 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4681 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4681 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4681 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4681 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4681 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.468 New best_val_rmse: 0.468\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.468 New best_val_rmse: 0.468\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4679 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4679 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.546 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 20:13:24,816]\u001b[0m Trial 11 finished with value: 0.46788129210472107 and parameters: {'base_lr': 4.502645042406232e-05, 'last_lr': 0.004770565920096722, 'epochs': 3}. Best is trial 9 with value: 0.4512692391872406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 4.5499746096638965e-05 last_lr 0.00216149961050388 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888bc5e51d754dffbfa2417c1ec957ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9461 New best_val_rmse: 0.9461\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6324 New best_val_rmse: 0.6324\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9024 Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6691 Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6765 Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7851 Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5979 New best_val_rmse: 0.5979\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7165 Still best_val_rmse: 0.5979 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5316 New best_val_rmse: 0.5316\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5075 New best_val_rmse: 0.5075\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4948 New best_val_rmse: 0.4948\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5816 Still best_val_rmse: 0.4948 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.4916 New best_val_rmse: 0.4916\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.504 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6021 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6428 Still best_val_rmse: 0.4916 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.4969 Still best_val_rmse: 0.4888 (from epoch 1)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 1 batch_num: 120 val_rmse: 0.5012 Still best_val_rmse: 0.4888 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5326 Still best_val_rmse: 0.4888 (from epoch 1)\n",
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.493 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.5048 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4827 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4788 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4738 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4728 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4832 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4822 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4831 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4741 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4715 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4795 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4894 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4864 Still best_val_rmse: 0.4708 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4684 New best_val_rmse: 0.4684\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4681 New best_val_rmse: 0.4681\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4677 New best_val_rmse: 0.4677\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4675 New best_val_rmse: 0.4675\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4675 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4677 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4682 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4693 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4705 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4761 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.479 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.481 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4752 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4741 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4732 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4728 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4742 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4749 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4748 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4751 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4732 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4708 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4693 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4688 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4681 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4676 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.467 New best_val_rmse: 0.467\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4668 New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4666 New best_val_rmse: 0.4666\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.545 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 1.72 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4656 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4657 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4658 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4658 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4659 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4659 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.466 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4661 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4661 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4662 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4663 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4665 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4667 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.467 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4673 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4676 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.468 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4683 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4685 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4685 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4687 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4686 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4688 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.469 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4689 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4688 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4685 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4684 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4683 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4678 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4673 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.467 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4668 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4665 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4663 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4662 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4658 Still best_val_rmse: 0.4656 (from epoch 2)\n",
      "\n",
      "1 steps took 1.0 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4652 New best_val_rmse: 0.4652\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4651 New best_val_rmse: 0.4651\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.465 New best_val_rmse: 0.465\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4649 New best_val_rmse: 0.4649\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4649 New best_val_rmse: 0.4649\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4651 Still best_val_rmse: 0.4649 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.465 Still best_val_rmse: 0.4649 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4648 New best_val_rmse: 0.4648\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4647 New best_val_rmse: 0.4647\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4646 New best_val_rmse: 0.4646\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4647 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4647 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4652 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4666 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4686 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4704 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4708 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4718 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.47 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4672 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4658 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4654 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4655 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4665 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4669 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4681 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4687 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4683 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4674 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4659 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4656 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4687 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4766 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4972 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4663 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4675 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4697 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4694 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4711 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4712 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4675 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4664 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4668 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4667 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4661 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4664 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4766 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4935 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4694 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4701 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4771 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4864 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4725 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4707 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4883 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4817 Still best_val_rmse: 0.4646 (from epoch 3)\n",
      "\n",
      "4 steps took 3.07 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4916 Still best_val_rmse: 0.4646 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-24 20:58:04,196]\u001b[0m Trial 12 finished with value: 0.4646318554878235 and parameters: {'base_lr': 4.5499746096638965e-05, 'last_lr': 0.00216149961050388, 'epochs': 4}. Best is trial 9 with value: 0.4512692391872406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 5.782684270655918e-05 last_lr 0.00028840065254797455 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb4dff0b25f434f87cfd79d88267af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 07:50:04,801]\u001b[0m A new study created in memory with name: no-name-7b2b036e-d79a-4b20-b589-16cd1ccaf8d4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 2\n",
      "##### Using base_lr 7.033182405116535e-05 last_lr 0.00428360023846075 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a367348c19864f5493f393611be3092d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7287 New best_val_rmse: 0.7287\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9069 Still best_val_rmse: 0.7287 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7714 Still best_val_rmse: 0.7287 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7524 Still best_val_rmse: 0.7287 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8319 Still best_val_rmse: 0.7287 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8324 Still best_val_rmse: 0.7287 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5864 New best_val_rmse: 0.5864\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5985 Still best_val_rmse: 0.5864 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6026 Still best_val_rmse: 0.5864 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5943 Still best_val_rmse: 0.5864 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5771 New best_val_rmse: 0.5771\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5906 Still best_val_rmse: 0.5771 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5302 New best_val_rmse: 0.5302\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5334 Still best_val_rmse: 0.5302 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5217 New best_val_rmse: 0.5217\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5045 New best_val_rmse: 0.5045\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4953 New best_val_rmse: 0.4953\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5195 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5009 Still best_val_rmse: 0.4953 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4807 New best_val_rmse: 0.4807\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4833 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4848 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4969 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.5102 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.485 Still best_val_rmse: 0.4795 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4914 Still best_val_rmse: 0.4794 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4774 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.48 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4755 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4732 New best_val_rmse: 0.4732\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4731 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4743 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4751 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4747 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4743 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.474 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4743 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4742 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4739 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4734 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4731 Still best_val_rmse: 0.4726 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4725 New best_val_rmse: 0.4725\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4723 New best_val_rmse: 0.4723\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4723 New best_val_rmse: 0.4723\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.472 New best_val_rmse: 0.472\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4719 New best_val_rmse: 0.4719\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4717 New best_val_rmse: 0.4717\n",
      "\n",
      "2 steps took 1.83 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4717 New best_val_rmse: 0.4717\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4716 New best_val_rmse: 0.4716\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 2.49 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4713 New best_val_rmse: 0.4713\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4712 New best_val_rmse: 0.4712\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.471 New best_val_rmse: 0.471\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4707 New best_val_rmse: 0.4707\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4708 Still best_val_rmse: 0.4707 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.471 Still best_val_rmse: 0.4707 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4709 Still best_val_rmse: 0.4707 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4709 Still best_val_rmse: 0.4707 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4706 New best_val_rmse: 0.4706\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4702 New best_val_rmse: 0.4702\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4702 New best_val_rmse: 0.4702\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4705 Still best_val_rmse: 0.4702 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4716 Still best_val_rmse: 0.4702 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4724 Still best_val_rmse: 0.4702 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4714 Still best_val_rmse: 0.4702 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4703 Still best_val_rmse: 0.4702 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4694 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4692 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4692 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4694 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4694 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4696 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4701 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4723 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4716 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4703 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4717 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4744 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.474 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4749 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.478 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4808 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4757 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4745 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4705 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4757 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4755 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4731 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4731 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4767 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4763 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4757 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4791 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4882 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4965 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4783 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4935 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.537 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 4 batch_num: 6 val_rmse: 0.4912 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 4 batch_num: 14 val_rmse: 0.48 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4878 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.521 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 36 val_rmse: 0.4904 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 44 val_rmse: 0.4989 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 52 val_rmse: 0.5142 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 68 val_rmse: 0.4904 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 4 batch_num: 76 val_rmse: 0.4829 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.5358 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 96 val_rmse: 0.5606 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 112 val_rmse: 0.588 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 4 batch_num: 128 val_rmse: 0.5911 Still best_val_rmse: 0.4692 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 144 val_rmse: 0.5567 Still best_val_rmse: 0.4692 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 08:25:59,835]\u001b[0m Trial 0 finished with value: 0.46916714310646057 and parameters: {'base_lr': 7.033182405116535e-05, 'last_lr': 0.00428360023846075, 'epochs': 5}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.0002794442210437961 last_lr 0.0013646811725046258 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027b3e05aa884dc3a0f283ca8d864cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.114 New best_val_rmse: 1.114\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.088 New best_val_rmse: 1.088\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.087 New best_val_rmse: 1.087\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.048 New best_val_rmse: 1.048\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.141 Still best_val_rmse: 1.048 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.065 Still best_val_rmse: 1.048 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 08:28:41,515]\u001b[0m Trial 1 finished with value: 1.0484046936035156 and parameters: {'base_lr': 0.0002794442210437961, 'last_lr': 0.0013646811725046258, 'epochs': 5}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 9.234931180472906e-05 last_lr 0.002047504740885993 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f58a7f9c7d41969f631d4cb3f1e325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7477 New best_val_rmse: 0.7477\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8921 Still best_val_rmse: 0.7477 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7388 New best_val_rmse: 0.7388\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.707 New best_val_rmse: 0.707\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7974 Still best_val_rmse: 0.707 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8005 Still best_val_rmse: 0.707 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6308 New best_val_rmse: 0.6308\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6354 Still best_val_rmse: 0.6308 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6137 New best_val_rmse: 0.6137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 08:32:38,881]\u001b[0m Trial 2 finished with value: 0.6136829257011414 and parameters: {'base_lr': 9.234931180472906e-05, 'last_lr': 0.002047504740885993, 'epochs': 4}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 3.20792930444734e-05 last_lr 0.004561950554091286 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebefbacdf3a496b80251a3c06430f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7631 New best_val_rmse: 0.7631\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9567 Still best_val_rmse: 0.7631 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.652 New best_val_rmse: 0.652\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6025 New best_val_rmse: 0.6025\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8982 Still best_val_rmse: 0.6025 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6698 Still best_val_rmse: 0.6025 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6361 Still best_val_rmse: 0.6025 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5611 New best_val_rmse: 0.5611\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5634 Still best_val_rmse: 0.5611 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5944 Still best_val_rmse: 0.5611 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6136 Still best_val_rmse: 0.5611 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5518 New best_val_rmse: 0.5518\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.55 New best_val_rmse: 0.55\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5374 New best_val_rmse: 0.5374\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.548 Still best_val_rmse: 0.5374 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5226 New best_val_rmse: 0.5226\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5221 New best_val_rmse: 0.5221\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5174 New best_val_rmse: 0.5174\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5019 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.483 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.498 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5112 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4862 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4884 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4876 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.486 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4899 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.484 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4846 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4819 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4825 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4804 New best_val_rmse: 0.4804\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4793 New best_val_rmse: 0.4793\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 2.46 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.479 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.479 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.479 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.479 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4789 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4789 Still best_val_rmse: 0.4788 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4789 Still best_val_rmse: 0.4788 (from epoch 3)\n",
      "\n",
      "2 steps took 1.84 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4789 Still best_val_rmse: 0.4788 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4788 Still best_val_rmse: 0.4788 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4792 Still best_val_rmse: 0.4786 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4809 Still best_val_rmse: 0.4786 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4833 Still best_val_rmse: 0.4786 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4812 Still best_val_rmse: 0.4786 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4783 New best_val_rmse: 0.4783\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.478 New best_val_rmse: 0.478\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4788 Still best_val_rmse: 0.478 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4781 Still best_val_rmse: 0.478 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4772 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.477 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4769 New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.477 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4781 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4791 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4797 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4794 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4803 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4835 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4836 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4775 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4778 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4796 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4819 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4833 Still best_val_rmse: 0.4769 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4836 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 3.38 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4775 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4936 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "8 steps took 6.75 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4844 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.491 Still best_val_rmse: 0.4764 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.5076 Still best_val_rmse: 0.4764 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:00:36,851]\u001b[0m Trial 3 finished with value: 0.476400226354599 and parameters: {'base_lr': 3.20792930444734e-05, 'last_lr': 0.004561950554091286, 'epochs': 4}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00026871561067013386 last_lr 0.0010726127570971598 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8ba8cbd8214771886f2a00e7673c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.117 New best_val_rmse: 1.117\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.028 New best_val_rmse: 1.028\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.157 Still best_val_rmse: 1.028 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.141 Still best_val_rmse: 1.028 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.117 Still best_val_rmse: 1.028 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.071 Still best_val_rmse: 1.028 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:03:21,115]\u001b[0m Trial 4 finished with value: 1.0277255773544312 and parameters: {'base_lr': 0.00026871561067013386, 'last_lr': 0.0010726127570971598, 'epochs': 4}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00010220703311425338 last_lr 0.002149079750874539 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de45441903724256b0be34ce5b4a3ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8047 New best_val_rmse: 0.8047\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.021 Still best_val_rmse: 0.8047 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7258 New best_val_rmse: 0.7258\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7058 New best_val_rmse: 0.7058\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6779 New best_val_rmse: 0.6779\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7703 Still best_val_rmse: 0.6779 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6016 New best_val_rmse: 0.6016\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6286 Still best_val_rmse: 0.6016 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.156 Still best_val_rmse: 0.6016 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:07:18,581]\u001b[0m Trial 5 finished with value: 0.6016260385513306 and parameters: {'base_lr': 0.00010220703311425338, 'last_lr': 0.002149079750874539, 'epochs': 5}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00014586429163420997 last_lr 0.0021265123779889887 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ff8985a36045118d1042ed574df116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.87 New best_val_rmse: 0.87\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9596 Still best_val_rmse: 0.87 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8502 New best_val_rmse: 0.8502\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9637 Still best_val_rmse: 0.8502 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.059 Still best_val_rmse: 0.8502 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.069 Still best_val_rmse: 0.8502 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.105 Still best_val_rmse: 0.8502 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.124 Still best_val_rmse: 0.8502 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.066 Still best_val_rmse: 0.8502 (from epoch 0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:11:11,756]\u001b[0m Trial 6 finished with value: 0.8501816987991333 and parameters: {'base_lr': 0.00014586429163420997, 'last_lr': 0.0021265123779889887, 'epochs': 3}. Best is trial 0 with value: 0.46916714310646057.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 2\n",
      "##### Using base_lr 8.355822005934142e-05 last_lr 0.0002848717475584195 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc498f6537a4c08a07be69e6553c9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7324 New best_val_rmse: 0.7324\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.848 Still best_val_rmse: 0.7324 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7172 New best_val_rmse: 0.7172\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6038 New best_val_rmse: 0.6038\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6453 Still best_val_rmse: 0.6038 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7758 Still best_val_rmse: 0.6038 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6982 Still best_val_rmse: 0.6038 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5606 New best_val_rmse: 0.5606\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5481 New best_val_rmse: 0.5481\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6061 Still best_val_rmse: 0.5481 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5498 Still best_val_rmse: 0.5481 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.533 New best_val_rmse: 0.533\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.528 New best_val_rmse: 0.528\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5677 Still best_val_rmse: 0.528 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5247 New best_val_rmse: 0.5247\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5068 New best_val_rmse: 0.5068\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5036 New best_val_rmse: 0.5036\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5107 Still best_val_rmse: 0.5036 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.5054 Still best_val_rmse: 0.4767 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.476 Still best_val_rmse: 0.4738 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4897 Still best_val_rmse: 0.4738 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4714 New best_val_rmse: 0.4714\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4754 Still best_val_rmse: 0.4714 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4817 Still best_val_rmse: 0.4714 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4721 Still best_val_rmse: 0.4714 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4817 Still best_val_rmse: 0.4714 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4726 Still best_val_rmse: 0.4714 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4712 New best_val_rmse: 0.4712\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4722 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4713 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4852 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.479 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.473 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4731 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4791 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.48 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4698 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.47 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.47 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4706 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.471 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.47 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.469 New best_val_rmse: 0.469\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4686 New best_val_rmse: 0.4686\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4678 New best_val_rmse: 0.4678\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4676 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4676 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4677 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4678 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4681 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4682 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4682 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4679 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4677 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4674 New best_val_rmse: 0.4674\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4662 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4662 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4663 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4663 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4662 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4658 Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.544 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:36:55,762]\u001b[0m Trial 7 finished with value: 0.4657308757305145 and parameters: {'base_lr': 8.355822005934142e-05, 'last_lr': 0.0002848717475584195, 'epochs': 3}. Best is trial 7 with value: 0.4657308757305145.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00017595546495067366 last_lr 0.0008292184813936002 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3587d60a61ae4f2bb0a0f0c0d4a51a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7622 New best_val_rmse: 0.7622\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.009 Still best_val_rmse: 0.7622 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.816 Still best_val_rmse: 0.7622 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.342 Still best_val_rmse: 0.7622 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.081 Still best_val_rmse: 0.7622 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.1 Still best_val_rmse: 0.7622 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:39:39,979]\u001b[0m Trial 8 finished with value: 0.7621898651123047 and parameters: {'base_lr': 0.00017595546495067366, 'last_lr': 0.0008292184813936002, 'epochs': 3}. Best is trial 7 with value: 0.4657308757305145.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.0001637178761186698 last_lr 0.00019455912682363454 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db556b034a4497ead099c3a169c6e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7968 New best_val_rmse: 0.7968\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9739 Still best_val_rmse: 0.7968 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8892 Still best_val_rmse: 0.7968 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8723 Still best_val_rmse: 0.7968 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7268 New best_val_rmse: 0.7268\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7304 Still best_val_rmse: 0.7268 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6902 New best_val_rmse: 0.6902\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6111 New best_val_rmse: 0.6111\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8724 Still best_val_rmse: 0.6111 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 09:43:37,274]\u001b[0m Trial 9 finished with value: 0.6110950708389282 and parameters: {'base_lr': 0.0001637178761186698, 'last_lr': 0.00019455912682363454, 'epochs': 4}. Best is trial 7 with value: 0.4657308757305145.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 4.320549847358141e-05 last_lr 0.00019201376406272403 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538822b3d6364e318cca188a59d3d725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7237 New best_val_rmse: 0.7237\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8278 Still best_val_rmse: 0.7237 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8375 Still best_val_rmse: 0.7237 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5945 New best_val_rmse: 0.5945\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6253 Still best_val_rmse: 0.5945 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6751 Still best_val_rmse: 0.5945 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7342 Still best_val_rmse: 0.5945 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5694 New best_val_rmse: 0.5694\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5608 New best_val_rmse: 0.5608\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5808 Still best_val_rmse: 0.5608 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5947 Still best_val_rmse: 0.5608 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5743 Still best_val_rmse: 0.5608 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5224 New best_val_rmse: 0.5224\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5249 Still best_val_rmse: 0.5224 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5312 Still best_val_rmse: 0.5224 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5145 New best_val_rmse: 0.5145\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5244 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.51 New best_val_rmse: 0.51\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4963 Still best_val_rmse: 0.488 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4908 Still best_val_rmse: 0.488 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4956 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.5003 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4818 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4776 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4813 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4766 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4757 New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4817 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4823 Still best_val_rmse: 0.4757 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4744 New best_val_rmse: 0.4744\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4746 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4777 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4797 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4797 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4773 Still best_val_rmse: 0.4744 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4743 New best_val_rmse: 0.4743\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4729 New best_val_rmse: 0.4729\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4732 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4738 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4744 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4734 Still best_val_rmse: 0.4729 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.471 New best_val_rmse: 0.471\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4708 New best_val_rmse: 0.4708\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4706 New best_val_rmse: 0.4706\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4706 New best_val_rmse: 0.4706\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4701 Still best_val_rmse: 0.47 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4693 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4693 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4695 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4694 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4694 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4694 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4693 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4693 Still best_val_rmse: 0.4693 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.566 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4693 Still best_val_rmse: 0.4692 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 10:04:00,560]\u001b[0m Trial 10 finished with value: 0.4692443907260895 and parameters: {'base_lr': 4.320549847358141e-05, 'last_lr': 0.00019201376406272403, 'epochs': 3}. Best is trial 7 with value: 0.4657308757305145.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 5.519639831224118e-05 last_lr 0.00044124301485893967 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e513a15db4f04fed9e27a8a6b637ca08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7494 New best_val_rmse: 0.7494\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.751 Still best_val_rmse: 0.7494 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.5992 New best_val_rmse: 0.5992\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.594 New best_val_rmse: 0.594\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7603 Still best_val_rmse: 0.594 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7473 Still best_val_rmse: 0.594 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6773 Still best_val_rmse: 0.594 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5918 New best_val_rmse: 0.5918\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5774 New best_val_rmse: 0.5774\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6042 Still best_val_rmse: 0.5774 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5517 New best_val_rmse: 0.5517\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5832 Still best_val_rmse: 0.5517 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5747 Still best_val_rmse: 0.5517 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5154 New best_val_rmse: 0.5154\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5183 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.556 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5002 New best_val_rmse: 0.5002\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5198 Still best_val_rmse: 0.5002 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4825 New best_val_rmse: 0.4825\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5227 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4837 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5175 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5062 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4745 New best_val_rmse: 0.4745\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4758 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4773 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4752 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4729 New best_val_rmse: 0.4729\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4713 New best_val_rmse: 0.4713\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4727 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4747 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4758 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.475 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4736 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4722 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4717 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4722 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4728 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4722 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4714 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4704 New best_val_rmse: 0.4704\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4705 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4707 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4707 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4704 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4705 Still best_val_rmse: 0.4704 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4704 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4704 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4702 New best_val_rmse: 0.4702\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.47 New best_val_rmse: 0.47\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4699 New best_val_rmse: 0.4699\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4699 New best_val_rmse: 0.4699\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4698 New best_val_rmse: 0.4698\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.546 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 1.94 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4696 Still best_val_rmse: 0.4696 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4695 New best_val_rmse: 0.4695\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4691 New best_val_rmse: 0.4691\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4691 New best_val_rmse: 0.4691\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4691 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4691 New best_val_rmse: 0.4691\n",
      "\n",
      "1 steps took 1.01 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4691 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4691 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4691 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4692 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4692 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4692 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4692 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4692 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4693 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4694 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4695 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4696 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4698 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.47 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4702 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4705 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4707 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4709 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4714 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4721 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4731 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4726 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4702 Still best_val_rmse: 0.4691 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.467 New best_val_rmse: 0.467\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4667 New best_val_rmse: 0.4667\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4672 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4683 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4689 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4699 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4704 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4683 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4668 Still best_val_rmse: 0.4667 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4663 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4675 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4705 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4762 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.474 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4705 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4733 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4748 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.471 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4705 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4708 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4694 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4679 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4671 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4674 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4677 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4681 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4688 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.47 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4728 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4799 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4766 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4811 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4726 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4766 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4728 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4763 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4761 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4918 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4774 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.475 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4788 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.492 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "8 steps took 6.74 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4817 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 4.21 seconds\n",
      "Epoch: 4 batch_num: 1 val_rmse: 0.4786 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 3 val_rmse: 0.4785 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 5 val_rmse: 0.4794 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 7 val_rmse: 0.4785 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 9 val_rmse: 0.4774 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 11 val_rmse: 0.4743 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 4 batch_num: 13 val_rmse: 0.4782 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 15 val_rmse: 0.4763 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 17 val_rmse: 0.4764 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 19 val_rmse: 0.4768 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 21 val_rmse: 0.4813 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 25 val_rmse: 0.4752 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 27 val_rmse: 0.483 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 4 batch_num: 31 val_rmse: 0.4875 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 35 val_rmse: 0.4877 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 39 val_rmse: 0.4949 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 47 val_rmse: 0.5128 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 4 batch_num: 63 val_rmse: 0.4895 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 4 batch_num: 67 val_rmse: 0.518 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 83 val_rmse: 0.4969 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 91 val_rmse: 0.4988 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 99 val_rmse: 0.4894 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 103 val_rmse: 0.5319 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 119 val_rmse: 0.5776 Still best_val_rmse: 0.466 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 135 val_rmse: 0.5933 Still best_val_rmse: 0.466 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 10:49:31,696]\u001b[0m Trial 11 finished with value: 0.4659542441368103 and parameters: {'base_lr': 5.519639831224118e-05, 'last_lr': 0.00044124301485893967, 'epochs': 5}. Best is trial 7 with value: 0.4657308757305145.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 5.9004819673113075e-05 last_lr 0.0003701804156340247 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3099c37afca845a9ab151e8588cb02f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7255 New best_val_rmse: 0.7255\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7519 Still best_val_rmse: 0.7255 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6375 New best_val_rmse: 0.6375\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6152 New best_val_rmse: 0.6152\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7851 Still best_val_rmse: 0.6152 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6317 Still best_val_rmse: 0.6152 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7351 Still best_val_rmse: 0.6152 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6361 Still best_val_rmse: 0.6152 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5968 New best_val_rmse: 0.5968\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5683 New best_val_rmse: 0.5683\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5401 New best_val_rmse: 0.5401\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.572 Still best_val_rmse: 0.5401 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5239 New best_val_rmse: 0.5239\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5252 Still best_val_rmse: 0.5239 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5667 Still best_val_rmse: 0.5239 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5147 New best_val_rmse: 0.5147\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.39 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.5215 Still best_val_rmse: 0.4866 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4915 Still best_val_rmse: 0.4866 (from epoch 1)\n",
      "\n",
      "8 steps took 7.79 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.5114 Still best_val_rmse: 0.4866 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4958 Still best_val_rmse: 0.4866 (from epoch 1)\n",
      "\n",
      "8 steps took 6.75 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4798 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4788 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.7 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.472 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4887 Still best_val_rmse: 0.4697 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.47 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4821 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4815 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4723 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.477 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4732 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.47 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4739 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4883 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.477 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4762 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4786 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4776 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4752 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4738 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4733 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4727 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4722 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4711 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4701 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4702 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4704 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4703 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4703 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4704 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4703 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4694 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.469 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4688 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4688 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4686 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4685 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4682 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.468 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4679 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4677 Still best_val_rmse: 0.4676 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4671 New best_val_rmse: 0.4671\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4667 New best_val_rmse: 0.4667\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4659 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.546 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 1.99 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4659 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4658 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4657 Still best_val_rmse: 0.4657 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4654 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4654 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4654 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4654 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4655 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4656 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4659 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4663 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4669 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.997 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4676 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4679 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4681 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4683 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4677 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4672 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4669 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4664 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4664 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4671 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4683 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4695 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.47 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4701 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4696 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4691 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.469 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4692 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4693 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4693 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4695 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4696 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4693 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4685 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4677 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.467 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4669 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4674 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4677 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4678 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4671 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4651 New best_val_rmse: 0.4651\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4636 New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4637 Still best_val_rmse: 0.4636 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4646 Still best_val_rmse: 0.4636 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4655 Still best_val_rmse: 0.4636 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4647 Still best_val_rmse: 0.4636 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4623 New best_val_rmse: 0.4623\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4637 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4675 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4669 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4664 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4645 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4633 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4637 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4675 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.471 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4719 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4686 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4693 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4704 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4706 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4715 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4732 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.471 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4675 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4658 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4673 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4683 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4642 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4629 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4656 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.464 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4624 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4634 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4642 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4655 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4733 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.47 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4724 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4725 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.477 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4724 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4707 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4709 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4823 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4937 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "8 steps took 6.41 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.52 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 14.6 seconds\n",
      "Epoch: 4 batch_num: 15 val_rmse: 0.4915 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 23 val_rmse: 0.5078 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 4 batch_num: 39 val_rmse: 0.4959 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 47 val_rmse: 0.4748 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 49 val_rmse: 0.4867 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 53 val_rmse: 0.4742 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 55 val_rmse: 0.4955 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 63 val_rmse: 0.5333 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 4 batch_num: 79 val_rmse: 0.5246 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 4 batch_num: 95 val_rmse: 0.5227 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 111 val_rmse: 1.685 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 4 batch_num: 127 val_rmse: 1.115 Still best_val_rmse: 0.4623 (from epoch 3)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 4 batch_num: 143 val_rmse: 1.086 Still best_val_rmse: 0.4623 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 11:41:36,111]\u001b[0m Trial 12 finished with value: 0.46230143308639526 and parameters: {'base_lr': 5.9004819673113075e-05, 'last_lr': 0.0003701804156340247, 'epochs': 5}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 3.7377002659052826e-05 last_lr 0.0003793496701689377 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f45a4282b5431ead558eec0fb5dd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8384 New best_val_rmse: 0.8384\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6816 New best_val_rmse: 0.6816\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6372 New best_val_rmse: 0.6372\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5714 New best_val_rmse: 0.5714\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7869 Still best_val_rmse: 0.5714 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7194 Still best_val_rmse: 0.5714 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6297 Still best_val_rmse: 0.5714 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7142 Still best_val_rmse: 0.5714 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5514 New best_val_rmse: 0.5514\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6044 Still best_val_rmse: 0.5514 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6199 Still best_val_rmse: 0.5514 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.54 New best_val_rmse: 0.54\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6053 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6399 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6305 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5262 New best_val_rmse: 0.5262\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5026 New best_val_rmse: 0.5026\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5163 Still best_val_rmse: 0.5026 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5069 Still best_val_rmse: 0.5026 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4963 New best_val_rmse: 0.4963\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4904 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4961 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5038 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4903 Still best_val_rmse: 0.4882 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4843 New best_val_rmse: 0.4843\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4853 Still best_val_rmse: 0.4843 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4868 Still best_val_rmse: 0.4843 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4852 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4871 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4854 Still best_val_rmse: 0.484 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4823 New best_val_rmse: 0.4823\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4823 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4822 New best_val_rmse: 0.4822\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4816 Still best_val_rmse: 0.4816 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 11:55:41,032]\u001b[0m Trial 13 finished with value: 0.4815579950809479 and parameters: {'base_lr': 3.7377002659052826e-05, 'last_lr': 0.0003793496701689377, 'epochs': 3}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 6.909690719029559e-05 last_lr 0.0001009920153959834 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c492e02aad4139873e092e242f7d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.6884 New best_val_rmse: 0.6884\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7294 Still best_val_rmse: 0.6884 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8486 Still best_val_rmse: 0.6884 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9075 Still best_val_rmse: 0.6884 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.94 Still best_val_rmse: 0.6884 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7422 Still best_val_rmse: 0.6884 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6321 New best_val_rmse: 0.6321\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7611 Still best_val_rmse: 0.6321 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6832 Still best_val_rmse: 0.6321 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 11:59:38,911]\u001b[0m Trial 14 finished with value: 0.6320924162864685 and parameters: {'base_lr': 6.909690719029559e-05, 'last_lr': 0.0001009920153959834, 'epochs': 4}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 6.00765282551364e-05 last_lr 0.00027678381128333566 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9411f9d34ac4b9f8eb06f1bc6b5e7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.726 New best_val_rmse: 0.726\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6978 New best_val_rmse: 0.6978\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.602 New best_val_rmse: 0.602\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5831 New best_val_rmse: 0.5831\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7001 Still best_val_rmse: 0.5831 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.9152 Still best_val_rmse: 0.5831 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6844 Still best_val_rmse: 0.5831 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6173 Still best_val_rmse: 0.5831 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5435 New best_val_rmse: 0.5435\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.577 Still best_val_rmse: 0.5435 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5435 Still best_val_rmse: 0.5435 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5314 New best_val_rmse: 0.5314\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.544 Still best_val_rmse: 0.5314 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5157 New best_val_rmse: 0.5157\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5484 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5372 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5269 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.527 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 14.6 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4905 New best_val_rmse: 0.4905\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5021 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4793 New best_val_rmse: 0.4793\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4984 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "8 steps took 6.73 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4975 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4814 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4823 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4861 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4832 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4873 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4847 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4798 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4795 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4811 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4819 Still best_val_rmse: 0.4793 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4784 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.479 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4784 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4777 New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4762 New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4763 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4763 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4762 New best_val_rmse: 0.4762\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4762 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4762 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4763 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4764 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4763 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4763 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4762 Still best_val_rmse: 0.4762 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4761 New best_val_rmse: 0.4761\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.476 New best_val_rmse: 0.476\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.476 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.476 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.476 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.476 Still best_val_rmse: 0.476 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.476 Still best_val_rmse: 0.476 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:17:19,018]\u001b[0m Trial 15 finished with value: 0.47596481442451477 and parameters: {'base_lr': 6.00765282551364e-05, 'last_lr': 0.00027678381128333566, 'epochs': 3}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.0004773588679623398 last_lr 0.00010049280280583319 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a11e63e9214300abd26dd308afa52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9982 New best_val_rmse: 0.9982\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.177 Still best_val_rmse: 0.9982 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.052 Still best_val_rmse: 0.9982 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.041 Still best_val_rmse: 0.9982 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.059 Still best_val_rmse: 0.9982 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.055 Still best_val_rmse: 0.9982 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:20:05,535]\u001b[0m Trial 16 finished with value: 0.9981786608695984 and parameters: {'base_lr': 0.0004773588679623398, 'last_lr': 0.00010049280280583319, 'epochs': 5}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 9.475640030300052e-05 last_lr 0.000560355390869576 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9f4210e88043c0a3db7fb7974109a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7632 New best_val_rmse: 0.7632\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9093 Still best_val_rmse: 0.7632 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6859 New best_val_rmse: 0.6859\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.837 Still best_val_rmse: 0.6859 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7826 Still best_val_rmse: 0.6859 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7518 Still best_val_rmse: 0.6859 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6551 New best_val_rmse: 0.6551\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5864 New best_val_rmse: 0.5864\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.568 New best_val_rmse: 0.568\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6128 Still best_val_rmse: 0.568 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.547 New best_val_rmse: 0.547\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5488 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.05 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.066 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.081 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.071 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.059 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.06 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.064 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.059 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.057 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.057 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 14.1 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 1.058 Still best_val_rmse: 0.547 (from epoch 1)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 1.057 Still best_val_rmse: 0.547 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:34:58,783]\u001b[0m Trial 17 finished with value: 0.5470067858695984 and parameters: {'base_lr': 9.475640030300052e-05, 'last_lr': 0.000560355390869576, 'epochs': 4}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 5.347633403724961e-05 last_lr 0.0001662465454143939 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b85d034a314bf1ae8290ee70f5d5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7516 New best_val_rmse: 0.7516\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7395 New best_val_rmse: 0.7395\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6031 New best_val_rmse: 0.6031\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6066 Still best_val_rmse: 0.6031 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7105 Still best_val_rmse: 0.6031 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7058 Still best_val_rmse: 0.6031 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6903 Still best_val_rmse: 0.6031 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6002 New best_val_rmse: 0.6002\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5424 New best_val_rmse: 0.5424\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5968 Still best_val_rmse: 0.5424 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5375 New best_val_rmse: 0.5375\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5221 New best_val_rmse: 0.5221\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5155 New best_val_rmse: 0.5155\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5298 Still best_val_rmse: 0.5155 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5306 Still best_val_rmse: 0.5155 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5386 Still best_val_rmse: 0.5155 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5121 New best_val_rmse: 0.5121\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.507 New best_val_rmse: 0.507\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5124 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5237 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5012 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4776 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4783 Still best_val_rmse: 0.4773 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4773 New best_val_rmse: 0.4773\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4764 New best_val_rmse: 0.4764\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4758 New best_val_rmse: 0.4758\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4752 New best_val_rmse: 0.4752\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4764 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4779 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.478 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4769 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4756 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4747 New best_val_rmse: 0.4747\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4737 New best_val_rmse: 0.4737\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4731 New best_val_rmse: 0.4731\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4728 New best_val_rmse: 0.4728\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.472 New best_val_rmse: 0.472\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4718 New best_val_rmse: 0.4718\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4722 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4725 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4727 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4724 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4724 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4722 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4722 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4722 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.472 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4719 Still best_val_rmse: 0.4718 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4718 New best_val_rmse: 0.4718\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4717 New best_val_rmse: 0.4717\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4716 New best_val_rmse: 0.4716\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4715 Still best_val_rmse: 0.4715 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:52:47,439]\u001b[0m Trial 18 finished with value: 0.4714738428592682 and parameters: {'base_lr': 5.347633403724961e-05, 'last_lr': 0.0001662465454143939, 'epochs': 3}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00011994588027894586 last_lr 0.0003033274865484539 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c3751edf3d4a188e180c82dc672cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.099 New best_val_rmse: 1.099\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.065 New best_val_rmse: 1.065\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.117 Still best_val_rmse: 1.065 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.085 Still best_val_rmse: 1.065 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.065 New best_val_rmse: 1.065\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.059 New best_val_rmse: 1.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:55:31,004]\u001b[0m Trial 19 finished with value: 1.0589079856872559 and parameters: {'base_lr': 0.00011994588027894586, 'last_lr': 0.0003033274865484539, 'epochs': 5}. Best is trial 12 with value: 0.46230143308639526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:55:31,007]\u001b[0m A new study created in memory with name: no-name-5808cef8-4ab5-47a0-84fd-8a85460a4897\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value:  0.46230143308639526\n",
      " Best params: \n",
      "    base_lr: 5.9004819673113075e-05\n",
      "    last_lr: 0.0003701804156340247\n",
      "    epochs: 5\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00017878362193620318 last_lr 0.0037204985677183515 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deb5d5608d040f2bc4fe40d3b30b2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.6768 New best_val_rmse: 0.6768\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6974 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7663 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.145 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.049 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.047 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.048 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.044 Still best_val_rmse: 0.6768 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.041 Still best_val_rmse: 0.6768 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 12:59:28,888]\u001b[0m Trial 0 finished with value: 0.6768454313278198 and parameters: {'base_lr': 0.00017878362193620318, 'last_lr': 0.0037204985677183515, 'epochs': 5}. Best is trial 0 with value: 0.6768454313278198.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.357124481434351e-05 last_lr 0.0006135683687551055 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e116d9d7a2864803b7d7f09e7aafe9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.072 New best_val_rmse: 1.072\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6998 New best_val_rmse: 0.6998\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8744 Still best_val_rmse: 0.6998 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6426 New best_val_rmse: 0.6426\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6572 Still best_val_rmse: 0.6426 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6525 Still best_val_rmse: 0.6426 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6648 Still best_val_rmse: 0.6426 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6452 Still best_val_rmse: 0.6426 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.788 Still best_val_rmse: 0.6426 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:03:27,265]\u001b[0m Trial 1 finished with value: 0.642588198184967 and parameters: {'base_lr': 6.357124481434351e-05, 'last_lr': 0.0006135683687551055, 'epochs': 4}. Best is trial 1 with value: 0.642588198184967.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0002667598428149489 last_lr 0.0027877296041089014 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2be9d63b84f4a35af264d0ca0b8dc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.2 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.515 New best_val_rmse: 1.515\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.072 New best_val_rmse: 1.072\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.306 Still best_val_rmse: 1.072 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.137 Still best_val_rmse: 1.072 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.107 Still best_val_rmse: 1.072 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.052 New best_val_rmse: 1.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:06:09,387]\u001b[0m Trial 2 finished with value: 1.0518639087677002 and parameters: {'base_lr': 0.0002667598428149489, 'last_lr': 0.0027877296041089014, 'epochs': 3}. Best is trial 1 with value: 0.642588198184967.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 7.94016491187567e-05 last_lr 0.0005979266225285472 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6295ed8e5294bafab9f3ca4431609bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.6652 New best_val_rmse: 0.6652\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7094 Still best_val_rmse: 0.6652 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9922 Still best_val_rmse: 0.6652 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7472 Still best_val_rmse: 0.6652 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6323 New best_val_rmse: 0.6323\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6358 Still best_val_rmse: 0.6323 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8027 Still best_val_rmse: 0.6323 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5899 New best_val_rmse: 0.5899\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5926 Still best_val_rmse: 0.5899 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5736 New best_val_rmse: 0.5736\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6419 Still best_val_rmse: 0.5736 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5385 New best_val_rmse: 0.5385\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5933 Still best_val_rmse: 0.5385 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5588 Still best_val_rmse: 0.5385 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5557 Still best_val_rmse: 0.5385 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5139 New best_val_rmse: 0.5139\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5153 Still best_val_rmse: 0.5139 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5367 Still best_val_rmse: 0.5139 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5019 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4953 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4972 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4903 New best_val_rmse: 0.4903\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4929 Still best_val_rmse: 0.4903 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4907 Still best_val_rmse: 0.4903 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4901 New best_val_rmse: 0.4901\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4903 Still best_val_rmse: 0.4901 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4897 New best_val_rmse: 0.4897\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4901 Still best_val_rmse: 0.4897 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4878 New best_val_rmse: 0.4878\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4883 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4885 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4882 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.487 New best_val_rmse: 0.487\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4868 New best_val_rmse: 0.4868\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4867 New best_val_rmse: 0.4867\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4866 New best_val_rmse: 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:19:46,497]\u001b[0m Trial 3 finished with value: 0.48658525943756104 and parameters: {'base_lr': 7.94016491187567e-05, 'last_lr': 0.0005979266225285472, 'epochs': 3}. Best is trial 3 with value: 0.48658525943756104.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 4.1384868313036215e-05 last_lr 0.004064085394675545 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc4801270fb4261aef61da3eef24ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8202 New best_val_rmse: 0.8202\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7123 New best_val_rmse: 0.7123\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6617 New best_val_rmse: 0.6617\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5952 New best_val_rmse: 0.5952\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5908 New best_val_rmse: 0.5908\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7124 Still best_val_rmse: 0.5908 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6163 Still best_val_rmse: 0.5908 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6916 Still best_val_rmse: 0.5908 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6025 Still best_val_rmse: 0.5908 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.525 New best_val_rmse: 0.525\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5368 Still best_val_rmse: 0.525 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6341 Still best_val_rmse: 0.525 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5381 Still best_val_rmse: 0.525 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5141 New best_val_rmse: 0.5141\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5438 Still best_val_rmse: 0.5141 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4985 New best_val_rmse: 0.4985\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5145 Still best_val_rmse: 0.4985 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4993 Still best_val_rmse: 0.4985 (from epoch 1)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5048 Still best_val_rmse: 0.4985 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4875 New best_val_rmse: 0.4875\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4864 New best_val_rmse: 0.4864\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4886 Still best_val_rmse: 0.4864 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4868 Still best_val_rmse: 0.4864 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.483 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4832 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.483 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.482 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4841 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4877 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4821 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4832 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4834 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4818 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4825 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4816 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4816 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.482 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.482 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4818 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4817 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4816 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 4.21 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4814 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4813 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4813 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4814 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4814 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.52 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4815 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4813 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4811 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4833 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4837 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4835 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4845 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4822 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4831 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4836 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4852 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4937 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4917 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4829 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4865 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4823 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.5104 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4922 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4878 Still best_val_rmse: 0.481 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4886 Still best_val_rmse: 0.481 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:42:26,825]\u001b[0m Trial 4 finished with value: 0.48103463649749756 and parameters: {'base_lr': 4.1384868313036215e-05, 'last_lr': 0.004064085394675545, 'epochs': 4}. Best is trial 4 with value: 0.48103463649749756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0001077856054370807 last_lr 0.001845347640590279 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6913fd146834ad99905767c6da22e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8986 New best_val_rmse: 0.8986\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6913 New best_val_rmse: 0.6913\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9822 Still best_val_rmse: 0.6913 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.342 Still best_val_rmse: 0.6913 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.049 Still best_val_rmse: 0.6913 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.06 Still best_val_rmse: 0.6913 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:45:09,313]\u001b[0m Trial 5 finished with value: 0.6913104057312012 and parameters: {'base_lr': 0.0001077856054370807, 'last_lr': 0.001845347640590279, 'epochs': 3}. Best is trial 4 with value: 0.48103463649749756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0003374026343358957 last_lr 0.0011642525648616515 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c9e60b232a424189d958032909af51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 2.096 New best_val_rmse: 2.096\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.049 New best_val_rmse: 1.049\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.051 Still best_val_rmse: 1.045 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.042 New best_val_rmse: 1.042\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.043 Still best_val_rmse: 1.042 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:47:49,838]\u001b[0m Trial 6 finished with value: 1.0422661304473877 and parameters: {'base_lr': 0.0003374026343358957, 'last_lr': 0.0011642525648616515, 'epochs': 5}. Best is trial 4 with value: 0.48103463649749756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00010592609202916195 last_lr 0.004119408133697415 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00938a1005946019e5cf06efd3252b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9151 New best_val_rmse: 0.9151\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6376 New best_val_rmse: 0.6376\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9541 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.393 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.049 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.042 Still best_val_rmse: 0.6376 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.04 Still best_val_rmse: 0.6376 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:50:55,218]\u001b[0m Trial 7 finished with value: 0.6376178860664368 and parameters: {'base_lr': 0.00010592609202916195, 'last_lr': 0.004119408133697415, 'epochs': 5}. Best is trial 4 with value: 0.48103463649749756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00041321857511704467 last_lr 0.0007541790981300038 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7219e470dbdc446890fe0ae6c673a69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.116 New best_val_rmse: 1.116\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.236 Still best_val_rmse: 1.116 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.143 Still best_val_rmse: 1.116 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.046 New best_val_rmse: 1.046\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.04 New best_val_rmse: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:53:34,976]\u001b[0m Trial 8 finished with value: 1.0396934747695923 and parameters: {'base_lr': 0.00041321857511704467, 'last_lr': 0.0007541790981300038, 'epochs': 3}. Best is trial 4 with value: 0.48103463649749756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0002980810441226992 last_lr 0.0028927902818133653 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69696063ae7a493ca18c0365463e7449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9817 New best_val_rmse: 0.9817\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.061 Still best_val_rmse: 0.9817 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.099 Still best_val_rmse: 0.9817 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.172 Still best_val_rmse: 0.9817 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.069 Still best_val_rmse: 0.9817 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.042 Still best_val_rmse: 0.9817 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 13:56:19,247]\u001b[0m Trial 9 finished with value: 0.9816675186157227 and parameters: {'base_lr': 0.0002980810441226992, 'last_lr': 0.0028927902818133653, 'epochs': 4}. Best is trial 4 with value: 0.48103463649749756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.091841397163233e-05 last_lr 0.00010409734625896974 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f93ed0ebb2e46a18093cd130206313e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8222 New best_val_rmse: 0.8222\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6415 New best_val_rmse: 0.6415\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7187 Still best_val_rmse: 0.6415 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6958 Still best_val_rmse: 0.6415 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6556 Still best_val_rmse: 0.6415 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.58 New best_val_rmse: 0.58\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7503 Still best_val_rmse: 0.58 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6572 Still best_val_rmse: 0.58 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5782 New best_val_rmse: 0.5782\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5185 New best_val_rmse: 0.5185\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5668 Still best_val_rmse: 0.5185 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5163 New best_val_rmse: 0.5163\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5405 Still best_val_rmse: 0.5163 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5406 Still best_val_rmse: 0.5163 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5157 New best_val_rmse: 0.5157\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5274 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5047 New best_val_rmse: 0.5047\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4969 New best_val_rmse: 0.4969\n",
      "\n",
      "8 steps took 7.56 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4969 Still best_val_rmse: 0.4969 (from epoch 1)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.518 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4843 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.483 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4811 New best_val_rmse: 0.4811\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4801 Still best_val_rmse: 0.477 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4748 New best_val_rmse: 0.4748\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4816 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4776 Still best_val_rmse: 0.4748 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4745 New best_val_rmse: 0.4745\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4786 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4753 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.477 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4798 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.482 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4779 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4768 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4769 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4784 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4786 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4779 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4762 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4774 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4779 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4778 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4772 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4768 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4769 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.477 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4771 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4771 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.477 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4769 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4768 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 2.6 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4767 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4763 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4763 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.82 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4778 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4779 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4776 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.477 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4763 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4763 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.476 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4762 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4764 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4786 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4794 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4776 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4766 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4765 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.477 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4781 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4795 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.478 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4806 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4779 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4841 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4791 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4839 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4829 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4928 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4816 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.483 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4895 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4864 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4848 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4859 Still best_val_rmse: 0.4745 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4845 Still best_val_rmse: 0.4745 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 14:27:56,182]\u001b[0m Trial 10 finished with value: 0.474480539560318 and parameters: {'base_lr': 3.091841397163233e-05, 'last_lr': 0.00010409734625896974, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.705019083502437e-05 last_lr 9.73975159832815e-05 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2edcca41104966ad6b11dba20938ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8667 New best_val_rmse: 0.8667\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6758 New best_val_rmse: 0.6758\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8294 Still best_val_rmse: 0.6758 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.729 Still best_val_rmse: 0.6758 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6323 New best_val_rmse: 0.6323\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6451 Still best_val_rmse: 0.6323 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6526 Still best_val_rmse: 0.6323 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6231 New best_val_rmse: 0.6231\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5699 New best_val_rmse: 0.5699\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5239 New best_val_rmse: 0.5239\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5599 Still best_val_rmse: 0.5239 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5071 New best_val_rmse: 0.5071\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5217 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5778 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5187 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5125 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5089 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5061 New best_val_rmse: 0.5061\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5295 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4847 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 3.31 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4842 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4861 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4872 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4808 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4778 New best_val_rmse: 0.4778\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4806 Still best_val_rmse: 0.4778 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4794 Still best_val_rmse: 0.4778 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4803 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4778 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4781 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4809 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4823 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4792 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4786 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4798 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4804 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4789 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4795 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4786 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4784 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.479 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4792 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4792 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4795 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4796 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4794 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4792 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.479 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4789 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4789 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4788 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 2.54 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4786 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4786 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4786 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4785 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4784 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4781 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4781 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4783 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4782 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.83 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.478 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4779 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4784 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4794 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4796 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4789 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4791 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4792 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4791 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4793 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4798 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4803 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4804 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4828 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.48 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4795 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4796 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.48 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4799 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4819 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4787 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4795 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4795 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4789 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4802 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4826 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4827 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4852 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.486 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4912 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.5003 Still best_val_rmse: 0.4776 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 14:57:49,859]\u001b[0m Trial 11 finished with value: 0.47759729623794556 and parameters: {'base_lr': 3.705019083502437e-05, 'last_lr': 9.73975159832815e-05, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.559207163174552e-05 last_lr 8.291720349787048e-05 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fdc4053912475c9be50fad3d3a574b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8572 New best_val_rmse: 0.8572\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6538 New best_val_rmse: 0.6538\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8514 Still best_val_rmse: 0.6538 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6875 Still best_val_rmse: 0.6538 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6421 New best_val_rmse: 0.6421\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5634 New best_val_rmse: 0.5634\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6277 Still best_val_rmse: 0.5634 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6626 Still best_val_rmse: 0.5634 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5721 Still best_val_rmse: 0.5634 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5606 New best_val_rmse: 0.5606\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5852 Still best_val_rmse: 0.5606 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5105 New best_val_rmse: 0.5105\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5279 Still best_val_rmse: 0.5105 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5566 Still best_val_rmse: 0.5105 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5271 Still best_val_rmse: 0.5105 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5191 Still best_val_rmse: 0.5105 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5052 New best_val_rmse: 0.5052\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 7.63 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.508 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4926 New best_val_rmse: 0.4926\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4947 Still best_val_rmse: 0.4926 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4954 Still best_val_rmse: 0.488 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4846 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4889 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4836 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4811 New best_val_rmse: 0.4811\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4833 Still best_val_rmse: 0.4811 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4852 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4834 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4808 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4802 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4807 Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4798 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4801 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4798 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4795 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4797 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4798 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.48 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4801 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4801 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4799 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4798 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4797 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4797 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4797 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 2.48 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4796 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4795 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4795 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4798 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4804 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.481 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.5 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4799 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4794 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4795 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4798 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.48 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4805 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4835 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4819 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4828 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4841 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4841 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4819 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4821 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4814 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4829 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.488 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4874 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4901 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4866 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4855 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.5145 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4926 Still best_val_rmse: 0.4792 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4929 Still best_val_rmse: 0.4792 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 15:24:26,696]\u001b[0m Trial 12 finished with value: 0.4792397618293762 and parameters: {'base_lr': 3.559207163174552e-05, 'last_lr': 8.291720349787048e-05, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 4.7971392667718245e-05 last_lr 9.266094011245145e-05 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fee33817aba4fcf9ef64fd7520ee956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7832 New best_val_rmse: 0.7832\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7271 New best_val_rmse: 0.7271\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8338 Still best_val_rmse: 0.7271 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6891 New best_val_rmse: 0.6891\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6302 New best_val_rmse: 0.6302\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5918 New best_val_rmse: 0.5918\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6933 Still best_val_rmse: 0.5918 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.603 Still best_val_rmse: 0.5918 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7475 Still best_val_rmse: 0.5918 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5633 New best_val_rmse: 0.5633\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5605 New best_val_rmse: 0.5605\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5299 New best_val_rmse: 0.5299\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6355 Still best_val_rmse: 0.5299 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5262 New best_val_rmse: 0.5262\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5272 Still best_val_rmse: 0.5262 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5397 Still best_val_rmse: 0.5262 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5007 New best_val_rmse: 0.5007\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4977 New best_val_rmse: 0.4977\n",
      "\n",
      "8 steps took 7.58 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5058 Still best_val_rmse: 0.4977 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4916 New best_val_rmse: 0.4916\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4908 New best_val_rmse: 0.4908\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.487 New best_val_rmse: 0.487\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4848 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4841 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4784 New best_val_rmse: 0.4784\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4787 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4797 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4813 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4817 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4929 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4829 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4825 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4826 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4833 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4821 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4844 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4831 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4817 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4818 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4817 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4817 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4815 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4814 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4813 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 4.28 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.481 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4811 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4813 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.49 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.481 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4811 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4809 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4808 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4812 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4811 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4826 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4811 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.481 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.481 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4824 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4831 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4869 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4875 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.489 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4877 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.488 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4902 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.493 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4877 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4977 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4892 Still best_val_rmse: 0.4782 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4905 Still best_val_rmse: 0.4782 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 15:47:59,559]\u001b[0m Trial 13 finished with value: 0.4782479703426361 and parameters: {'base_lr': 4.7971392667718245e-05, 'last_lr': 9.266094011245145e-05, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.564026861587217e-05 last_lr 0.00020414462061572284 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b807d93fed4b0abf60b2eaf1a43dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8608 New best_val_rmse: 0.8608\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6558 New best_val_rmse: 0.6558\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7571 Still best_val_rmse: 0.6558 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8096 Still best_val_rmse: 0.6558 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7542 Still best_val_rmse: 0.6558 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6217 New best_val_rmse: 0.6217\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5838 New best_val_rmse: 0.5838\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6476 Still best_val_rmse: 0.5838 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7471 Still best_val_rmse: 0.5838 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5732 New best_val_rmse: 0.5732\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5982 Still best_val_rmse: 0.5732 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5446 New best_val_rmse: 0.5446\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.549 Still best_val_rmse: 0.5446 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5774 Still best_val_rmse: 0.5446 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5345 New best_val_rmse: 0.5345\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5425 Still best_val_rmse: 0.5345 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5297 New best_val_rmse: 0.5297\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5409 Still best_val_rmse: 0.5297 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4949 New best_val_rmse: 0.4949\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4971 Still best_val_rmse: 0.4937 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4923 New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.495 Still best_val_rmse: 0.4923 (from epoch 2)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4909 New best_val_rmse: 0.4909\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.49 New best_val_rmse: 0.49\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4882 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4857 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4897 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.49 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4887 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4857 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4858 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4843 New best_val_rmse: 0.4843\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4837 New best_val_rmse: 0.4837\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4837 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4837 Still best_val_rmse: 0.4837 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 4.26 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4835 Still best_val_rmse: 0.4834 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4835 Still best_val_rmse: 0.4834 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4836 Still best_val_rmse: 0.4834 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4834 New best_val_rmse: 0.4834\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4836 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4841 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4833 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4867 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.49 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4913 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4863 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4903 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.5042 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4914 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4856 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4862 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4866 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4892 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4973 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.5038 Still best_val_rmse: 0.4833 (from epoch 3)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.5694 Still best_val_rmse: 0.4833 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 16:09:05,300]\u001b[0m Trial 14 finished with value: 0.483257919549942 and parameters: {'base_lr': 3.564026861587217e-05, 'last_lr': 0.00020414462061572284, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.132222593692676e-05 last_lr 0.00017727201606161639 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622fdcb8ded3451f9aaf9e2b805d9788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8243 New best_val_rmse: 0.8243\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6403 New best_val_rmse: 0.6403\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7419 Still best_val_rmse: 0.6403 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7232 Still best_val_rmse: 0.6403 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7528 Still best_val_rmse: 0.6403 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5622 New best_val_rmse: 0.5622\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7259 Still best_val_rmse: 0.5622 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6574 Still best_val_rmse: 0.5622 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5485 New best_val_rmse: 0.5485\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5248 New best_val_rmse: 0.5248\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6045 Still best_val_rmse: 0.5248 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5437 Still best_val_rmse: 0.5248 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5669 Still best_val_rmse: 0.5248 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.557 Still best_val_rmse: 0.5248 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5172 New best_val_rmse: 0.5172\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5449 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5225 Still best_val_rmse: 0.5172 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5142 New best_val_rmse: 0.5142\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5462 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4941 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4946 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4903 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4882 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4872 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4864 New best_val_rmse: 0.4864\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4866 Still best_val_rmse: 0.4864 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4861 New best_val_rmse: 0.4861\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4887 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.486 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4849 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4844 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4844 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4843 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4844 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4847 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4848 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4848 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4847 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4846 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4845 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4845 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 4.21 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4845 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4845 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4845 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4844 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4842 Still best_val_rmse: 0.4841 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4841 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4843 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4844 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.485 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4883 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4871 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4858 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.5 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4877 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4864 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4845 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4844 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.485 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4872 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4878 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4877 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4857 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4869 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4879 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4944 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4884 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.5113 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.5107 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.491 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "8 steps took 7.57 seconds\n",
      "Epoch: 4 batch_num: 4 val_rmse: 0.5106 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.4938 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 4 batch_num: 28 val_rmse: 0.4946 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 4 batch_num: 36 val_rmse: 0.4989 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 4 batch_num: 44 val_rmse: 0.5032 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 60 val_rmse: 0.506 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 76 val_rmse: 0.5143 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 92 val_rmse: 0.5088 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 108 val_rmse: 0.5201 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 124 val_rmse: 0.5029 Still best_val_rmse: 0.484 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 140 val_rmse: 0.5844 Still best_val_rmse: 0.484 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 16:35:00,086]\u001b[0m Trial 15 finished with value: 0.48396533727645874 and parameters: {'base_lr': 3.132222593692676e-05, 'last_lr': 0.00017727201606161639, 'epochs': 5}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 5.640809897820872e-05 last_lr 0.00016923843273761527 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35fa6e11a0b486ba4258e9f5f662319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9751 New best_val_rmse: 0.9751\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6698 New best_val_rmse: 0.6698\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6747 Still best_val_rmse: 0.6698 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6881 Still best_val_rmse: 0.6698 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7508 Still best_val_rmse: 0.6698 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5598 New best_val_rmse: 0.5598\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6534 Still best_val_rmse: 0.5598 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6081 Still best_val_rmse: 0.5598 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5748 Still best_val_rmse: 0.5598 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5203 New best_val_rmse: 0.5203\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5978 Still best_val_rmse: 0.5203 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5431 Still best_val_rmse: 0.5203 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6308 Still best_val_rmse: 0.5203 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5233 Still best_val_rmse: 0.5203 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.511 New best_val_rmse: 0.511\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5146 Still best_val_rmse: 0.511 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4975 New best_val_rmse: 0.4975\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4854 New best_val_rmse: 0.4854\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5183 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4981 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.49 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4996 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4898 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4929 Still best_val_rmse: 0.4854 (from epoch 1)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4807 New best_val_rmse: 0.4807\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4827 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4884 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4919 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.482 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4864 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4843 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4965 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4848 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5021 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4838 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.481 Still best_val_rmse: 0.4807 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4806 New best_val_rmse: 0.4806\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4804 New best_val_rmse: 0.4804\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4802 New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 2.58 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4798 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4797 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4797 Still best_val_rmse: 0.4797 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.48 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4803 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "4 steps took 3.5 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4807 Still best_val_rmse: 0.4796 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4798 Still best_val_rmse: 0.4795 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4801 Still best_val_rmse: 0.4795 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4787 Still best_val_rmse: 0.4787 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4809 Still best_val_rmse: 0.4787 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4813 Still best_val_rmse: 0.4787 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4831 Still best_val_rmse: 0.4787 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.478 New best_val_rmse: 0.478\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4823 Still best_val_rmse: 0.478 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.478 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4796 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4899 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4983 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4826 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4868 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4995 Still best_val_rmse: 0.4776 (from epoch 3)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.5065 Still best_val_rmse: 0.4776 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 17:02:23,501]\u001b[0m Trial 16 finished with value: 0.4776155352592468 and parameters: {'base_lr': 5.640809897820872e-05, 'last_lr': 0.00016923843273761527, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 7.52349749835514e-05 last_lr 0.00029315990374111886 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9c1c2dc8e3481c9184a503382060eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.1 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.693 New best_val_rmse: 0.693\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7429 Still best_val_rmse: 0.693 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.065 Still best_val_rmse: 0.693 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6229 New best_val_rmse: 0.6229\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5926 New best_val_rmse: 0.5926\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6248 Still best_val_rmse: 0.5926 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6854 Still best_val_rmse: 0.5926 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5543 New best_val_rmse: 0.5543\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6847 Still best_val_rmse: 0.5543 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5413 New best_val_rmse: 0.5413\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.559 Still best_val_rmse: 0.5413 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5309 New best_val_rmse: 0.5309\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6398 Still best_val_rmse: 0.5309 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5365 Still best_val_rmse: 0.5309 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5251 New best_val_rmse: 0.5251\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5328 Still best_val_rmse: 0.5251 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5055 Still best_val_rmse: 0.492 (from epoch 1)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4934 Still best_val_rmse: 0.492 (from epoch 1)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4995 Still best_val_rmse: 0.492 (from epoch 1)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4904 New best_val_rmse: 0.4904\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4915 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.487 Still best_val_rmse: 0.4865 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4865 Still best_val_rmse: 0.4865 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4838 New best_val_rmse: 0.4838\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4839 Still best_val_rmse: 0.4838 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4825 New best_val_rmse: 0.4825\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4827 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4842 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4856 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.483 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.483 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4843 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4852 Still best_val_rmse: 0.4816 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4801 New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4801 New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4802 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4802 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4803 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4801 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4799 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4798 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 2.62 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4796 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4794 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4795 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4797 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4796 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4802 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4815 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4825 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.49 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4817 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.481 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4813 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4817 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.482 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4823 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4818 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4812 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.486 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4814 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4924 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4819 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4851 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4994 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4893 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4985 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4945 Still best_val_rmse: 0.4794 (from epoch 3)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4998 Still best_val_rmse: 0.4794 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 17:28:22,421]\u001b[0m Trial 17 finished with value: 0.47943636775016785 and parameters: {'base_lr': 7.52349749835514e-05, 'last_lr': 0.00029315990374111886, 'epochs': 4}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.0001681916034814876 last_lr 0.00011708139817754288 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef939a7733bb400dbddd6d87ea745ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8056 New best_val_rmse: 0.8056\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7463 New best_val_rmse: 0.7463\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7373 New best_val_rmse: 0.7373\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7882 Still best_val_rmse: 0.7373 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.156 Still best_val_rmse: 0.7373 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.129 Still best_val_rmse: 0.7373 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.164 Still best_val_rmse: 0.7373 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.069 Still best_val_rmse: 0.7373 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.05 Still best_val_rmse: 0.7373 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 17:32:20,826]\u001b[0m Trial 18 finished with value: 0.7373365163803101 and parameters: {'base_lr': 0.0001681916034814876, 'last_lr': 0.00011708139817754288, 'epochs': 3}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.234597423887574e-05 last_lr 0.00032705834462378423 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29250739cc3549a39fdaa2f2bd1cdbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8416 New best_val_rmse: 0.8416\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6419 New best_val_rmse: 0.6419\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7346 Still best_val_rmse: 0.6419 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6815 Still best_val_rmse: 0.6419 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6198 New best_val_rmse: 0.6198\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5965 New best_val_rmse: 0.5965\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6422 Still best_val_rmse: 0.5965 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5541 New best_val_rmse: 0.5541\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5819 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5756 Still best_val_rmse: 0.5541 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5291 New best_val_rmse: 0.5291\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5101 New best_val_rmse: 0.5101\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5315 Still best_val_rmse: 0.5101 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.563 Still best_val_rmse: 0.5101 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5099 New best_val_rmse: 0.5099\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5162 Still best_val_rmse: 0.5099 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4881 New best_val_rmse: 0.4881\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.5041 Still best_val_rmse: 0.4881 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.49 Still best_val_rmse: 0.4881 (from epoch 1)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5115 Still best_val_rmse: 0.4881 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4907 Still best_val_rmse: 0.4881 (from epoch 1)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4869 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4969 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4832 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4809 New best_val_rmse: 0.4809\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4869 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4824 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4959 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4867 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4836 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4849 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4841 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4841 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4846 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4863 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4867 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4848 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.484 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 4.24 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4837 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4838 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4839 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4842 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4841 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4846 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4846 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4847 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4858 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.49 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4841 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.484 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4835 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4842 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4852 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4885 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4836 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4895 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4913 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4843 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4823 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4884 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4845 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4852 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4847 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4867 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4831 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4931 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.65 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4919 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.505 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 4 batch_num: 12 val_rmse: 0.4896 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4847 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.4901 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 4 batch_num: 28 val_rmse: 0.5084 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 44 val_rmse: 0.4927 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 4 batch_num: 52 val_rmse: 0.484 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 56 val_rmse: 0.4913 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 4 batch_num: 64 val_rmse: 0.5036 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4867 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 84 val_rmse: 0.4863 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 4 batch_num: 88 val_rmse: 0.537 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 104 val_rmse: 0.5091 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 120 val_rmse: 0.5142 Still best_val_rmse: 0.4809 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 136 val_rmse: 0.5761 Still best_val_rmse: 0.4809 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 17:59:52,082]\u001b[0m Trial 19 finished with value: 0.48085349798202515 and parameters: {'base_lr': 3.234597423887574e-05, 'last_lr': 0.00032705834462378423, 'epochs': 5}. Best is trial 10 with value: 0.474480539560318.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 17:59:52,085]\u001b[0m A new study created in memory with name: no-name-b95f8224-9fac-4cc2-b00e-b18f3cf9b133\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.474480539560318\n",
      " Best params: \n",
      "    base_lr: 3.091841397163233e-05\n",
      "    last_lr: 0.00010409734625896974\n",
      "    epochs: 4\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.2314567372708084e-05 last_lr 8.327155005618419e-05 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6769a0e8bd492999cbaa0cc89cad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9454 New best_val_rmse: 0.9454\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7717 New best_val_rmse: 0.7717\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7838 Still best_val_rmse: 0.7717 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6301 New best_val_rmse: 0.6301\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5986 New best_val_rmse: 0.5986\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5716 New best_val_rmse: 0.5716\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5557 New best_val_rmse: 0.5557\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5844 Still best_val_rmse: 0.5557 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6491 Still best_val_rmse: 0.5557 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.4925 New best_val_rmse: 0.4925\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 1 batch_num: 20 val_rmse: 0.5092 Still best_val_rmse: 0.4925 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5165 Still best_val_rmse: 0.4925 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.542 Still best_val_rmse: 0.4925 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4924 Still best_val_rmse: 0.4917 (from epoch 1)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 86 val_rmse: 0.4825 Still best_val_rmse: 0.48 (from epoch 1)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 1 batch_num: 90 val_rmse: 0.5071 Still best_val_rmse: 0.48 (from epoch 1)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 1 batch_num: 106 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4861 Still best_val_rmse: 0.4703 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.5094 Still best_val_rmse: 0.4703 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4829 Still best_val_rmse: 0.4703 (from epoch 1)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4627 New best_val_rmse: 0.4627\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.4882 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.4707 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 139 val_rmse: 0.5031 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "16 steps took 14.1 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.5083 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.482 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4706 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4755 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4953 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "8 steps took 6.66 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4666 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4662 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4659 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4657 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4656 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4651 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4648 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.464 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4635 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4633 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4629 Still best_val_rmse: 0.4627 (from epoch 1)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4627 New best_val_rmse: 0.4627\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4626 New best_val_rmse: 0.4626\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4627 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4636 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4641 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4632 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4627 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4627 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4631 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4634 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4634 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4634 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4633 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.463 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4629 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4627 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4626 New best_val_rmse: 0.4626\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4627 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4627 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4629 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4633 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4637 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4642 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4656 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4668 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4673 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4662 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4655 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4645 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4634 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4629 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4628 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.463 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4637 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4644 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4647 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4646 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4644 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4641 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4636 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4629 Still best_val_rmse: 0.4626 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4625 New best_val_rmse: 0.4625\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4623 New best_val_rmse: 0.4623\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4622 New best_val_rmse: 0.4622\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4622 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4623 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4623 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4623 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4624 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4624 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4625 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4625 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4625 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4627 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4627 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4627 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4628 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4628 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4628 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4626 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4625 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4624 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4623 Still best_val_rmse: 0.4622 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4621 New best_val_rmse: 0.4621\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4621 New best_val_rmse: 0.4621\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.462 New best_val_rmse: 0.462\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4619 New best_val_rmse: 0.4619\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4619 New best_val_rmse: 0.4619\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4618 Still best_val_rmse: 0.4618 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.542 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 2.06 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4617 Still best_val_rmse: 0.4617 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4617 New best_val_rmse: 0.4617\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.861 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4616 New best_val_rmse: 0.4616\n",
      "\n",
      "1 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4615 New best_val_rmse: 0.4615\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4615 New best_val_rmse: 0.4615\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4615 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4615 New best_val_rmse: 0.4615\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4617 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4614 New best_val_rmse: 0.4614\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4613 New best_val_rmse: 0.4613\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4612 New best_val_rmse: 0.4612\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4612 New best_val_rmse: 0.4612\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4612 New best_val_rmse: 0.4612\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4613 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4615 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4616 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4616 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4616 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4617 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4617 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4616 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4613 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 1.03 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4615 Still best_val_rmse: 0.4612 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4612 New best_val_rmse: 0.4612\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4611 New best_val_rmse: 0.4611\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4609 New best_val_rmse: 0.4609\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4607 New best_val_rmse: 0.4607\n",
      "\n",
      "1 steps took 0.823 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4606 New best_val_rmse: 0.4606\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4607 Still best_val_rmse: 0.4606 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4607 Still best_val_rmse: 0.4606 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4608 Still best_val_rmse: 0.4606 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4606 New best_val_rmse: 0.4606\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4603 New best_val_rmse: 0.4603\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4603 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4611 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4625 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4642 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4646 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4635 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4623 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4614 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4604 Still best_val_rmse: 0.4603 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.46 New best_val_rmse: 0.46\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4597 New best_val_rmse: 0.4597\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4597 New best_val_rmse: 0.4597\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4598 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4607 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4614 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4616 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4618 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4616 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4622 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4625 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4619 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4612 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4611 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4611 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4612 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4612 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4611 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4609 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4608 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.461 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4612 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4613 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.4621 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4623 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4631 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4636 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4631 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4629 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4631 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4628 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 106 val_rmse: 0.4635 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4647 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4648 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.462 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4612 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4611 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4614 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4618 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4637 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4704 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4683 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4644 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4643 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4651 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4657 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4651 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4629 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4609 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4599 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4597 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4613 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4658 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4672 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.467 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4602 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4624 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4655 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4698 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4641 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4617 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4632 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.4655 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4641 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4635 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4646 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4675 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4701 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4724 Still best_val_rmse: 0.4597 (from epoch 3)\n",
      "\n",
      "2 steps took 1.37 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.476 Still best_val_rmse: 0.4597 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 18:57:56,662]\u001b[0m Trial 0 finished with value: 0.45970267057418823 and parameters: {'base_lr': 3.2314567372708084e-05, 'last_lr': 8.327155005618419e-05, 'epochs': 4}. Best is trial 0 with value: 0.45970267057418823.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 6.228115263581648e-05 last_lr 0.002568618416899804 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02598857dfb7485a9a2d45310216f5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8833 New best_val_rmse: 0.8833\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7761 New best_val_rmse: 0.7761\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.031 Still best_val_rmse: 0.7761 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.665 New best_val_rmse: 0.665\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7338 Still best_val_rmse: 0.665 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6186 New best_val_rmse: 0.6186\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8649 Still best_val_rmse: 0.6186 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5956 New best_val_rmse: 0.5956\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6117 Still best_val_rmse: 0.5956 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6036 Still best_val_rmse: 0.5956 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5687 New best_val_rmse: 0.5687\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5107 New best_val_rmse: 0.5107\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6369 Still best_val_rmse: 0.5107 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5378 Still best_val_rmse: 0.5107 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5291 Still best_val_rmse: 0.5107 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4999 New best_val_rmse: 0.4999\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5072 Still best_val_rmse: 0.4921 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4831 New best_val_rmse: 0.4831\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4856 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4903 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4803 New best_val_rmse: 0.4803\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4805 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4834 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4854 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4875 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4905 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "8 steps took 6.67 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.482 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4874 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4817 Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4796 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4797 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4804 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.481 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4811 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4823 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4804 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.48 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4798 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4796 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4795 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4795 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4796 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4797 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4797 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4797 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4795 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4794 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4793 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 2.63 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4791 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4791 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4792 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4791 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4791 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4791 Still best_val_rmse: 0.4791 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.479 Still best_val_rmse: 0.479 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.479 Still best_val_rmse: 0.479 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4787 Still best_val_rmse: 0.4786 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4787 Still best_val_rmse: 0.4786 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4785 New best_val_rmse: 0.4785\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 1.85 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4782 Still best_val_rmse: 0.4781 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.478 Still best_val_rmse: 0.4779 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.478 Still best_val_rmse: 0.4779 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4778 New best_val_rmse: 0.4778\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4784 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4793 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4779 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4787 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4812 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4828 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.482 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4847 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4879 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.32 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.492 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4804 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4896 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4861 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4826 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4825 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4824 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4813 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.481 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4835 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4785 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4831 Still best_val_rmse: 0.4778 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 2.56 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.475 Still best_val_rmse: 0.4749 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 4 batch_num: 2 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 4 batch_num: 4 val_rmse: 0.4757 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 4 batch_num: 6 val_rmse: 0.4815 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "4 steps took 3.33 seconds\n",
      "Epoch: 4 batch_num: 10 val_rmse: 0.51 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 4 batch_num: 26 val_rmse: 0.5153 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 42 val_rmse: 0.5031 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 4 batch_num: 58 val_rmse: 0.5083 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 74 val_rmse: 0.5073 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 90 val_rmse: 0.4935 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 4 batch_num: 98 val_rmse: 0.5164 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 114 val_rmse: 0.4964 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 122 val_rmse: 0.5199 Still best_val_rmse: 0.4738 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 138 val_rmse: 0.5832 Still best_val_rmse: 0.4738 (from epoch 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 19:31:43,595]\u001b[0m Trial 1 finished with value: 0.47378069162368774 and parameters: {'base_lr': 6.228115263581648e-05, 'last_lr': 0.002568618416899804, 'epochs': 5}. Best is trial 0 with value: 0.45970267057418823.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00029311765128279367 last_lr 0.0002174214933522278 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67643de04d5d4548b6463ffef50ec98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.318 New best_val_rmse: 1.318\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.147 New best_val_rmse: 1.147\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.073 New best_val_rmse: 1.073\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.113 Still best_val_rmse: 1.073 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.139 Still best_val_rmse: 1.073 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.063 New best_val_rmse: 1.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-25 19:34:25,553]\u001b[0m Trial 2 finished with value: 1.0630989074707031 and parameters: {'base_lr': 0.00029311765128279367, 'last_lr': 0.0002174214933522278, 'epochs': 3}. Best is trial 0 with value: 0.45970267057418823.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.591130660171518e-05 last_lr 0.003713677086075337 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83530458523d49bba9f50d9e5b7aa9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8924 New best_val_rmse: 0.8924\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8478 New best_val_rmse: 0.8478\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6503 New best_val_rmse: 0.6503\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7102 Still best_val_rmse: 0.6503 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6758 Still best_val_rmse: 0.6503 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6754 Still best_val_rmse: 0.6503 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5809 New best_val_rmse: 0.5809\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6768 Still best_val_rmse: 0.5809 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6637 Still best_val_rmse: 0.5809 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5777 New best_val_rmse: 0.5777\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5267 New best_val_rmse: 0.5267\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.499 New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5553 Still best_val_rmse: 0.499 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.4898 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.508 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5138 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5283 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5147 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4976 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4881 Still best_val_rmse: 0.488 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5063 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5151 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4709 New best_val_rmse: 0.4709\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4691 New best_val_rmse: 0.4691\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.469 New best_val_rmse: 0.469\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4689 New best_val_rmse: 0.4689\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4694 Still best_val_rmse: 0.4689 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4713 Still best_val_rmse: 0.4689 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4761 Still best_val_rmse: 0.4689 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4695 Still best_val_rmse: 0.4689 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4643 New best_val_rmse: 0.4643\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4644 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4665 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4685 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4683 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4688 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.47 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4653 Still best_val_rmse: 0.4643 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4636 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4661 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4689 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4701 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4665 Still best_val_rmse: 0.4633 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4632 New best_val_rmse: 0.4632\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4625 New best_val_rmse: 0.4625\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4633 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4646 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4664 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.824 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4674 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4676 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4667 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4659 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4657 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4658 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4654 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4646 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4642 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4644 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4655 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.467 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4684 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4686 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4679 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4669 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4664 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4659 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4655 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4654 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4654 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.823 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4657 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.466 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4665 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4668 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4671 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4673 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4674 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4674 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4671 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.467 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4669 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4668 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4668 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.467 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4672 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4674 Still best_val_rmse: 0.4625 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(2, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d360e24c-6ca3-4486-b2ba-27d94cb53913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 06:42:49,908]\u001b[0m A new study created in memory with name: no-name-ea841a57-63cb-41b1-983d-7f8c65378d80\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 5\n",
      "##### Using base_lr 3.5379120180791935e-05 last_lr 0.00021137535166837663 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca860839b5f451bac485ee53c4be385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7172 New best_val_rmse: 0.7172\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7892 Still best_val_rmse: 0.7172 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6249 New best_val_rmse: 0.6249\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7399 Still best_val_rmse: 0.6249 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6589 Still best_val_rmse: 0.6249 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5633 New best_val_rmse: 0.5633\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5695 Still best_val_rmse: 0.5633 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5865 Still best_val_rmse: 0.5633 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5579 New best_val_rmse: 0.5579\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.545 New best_val_rmse: 0.545\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5249 New best_val_rmse: 0.5249\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5637 Still best_val_rmse: 0.5249 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5177 New best_val_rmse: 0.5177\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5159 New best_val_rmse: 0.5159\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.514 New best_val_rmse: 0.514\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5136 New best_val_rmse: 0.5136\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4902 Still best_val_rmse: 0.4882 (from epoch 1)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4856 Still best_val_rmse: 0.4833 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4956 Still best_val_rmse: 0.4787 (from epoch 1)\n",
      "\n",
      "8 steps took 7.55 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.5207 Still best_val_rmse: 0.4787 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4734 New best_val_rmse: 0.4734\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4731 New best_val_rmse: 0.4731\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.473 New best_val_rmse: 0.473\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4724 New best_val_rmse: 0.4724\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4806 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4766 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4727 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4742 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4733 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.477 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4779 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4748 Still best_val_rmse: 0.4724 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4715 New best_val_rmse: 0.4715\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.472 Still best_val_rmse: 0.4715 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4707 New best_val_rmse: 0.4707\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4705 New best_val_rmse: 0.4705\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4773 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4817 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4818 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4711 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4706 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4723 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4707 Still best_val_rmse: 0.4705 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4699 New best_val_rmse: 0.4699\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.47 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4701 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4719 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.472 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4708 Still best_val_rmse: 0.4699 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4693 New best_val_rmse: 0.4693\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.469 New best_val_rmse: 0.469\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4688 New best_val_rmse: 0.4688\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4686 New best_val_rmse: 0.4686\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4685 New best_val_rmse: 0.4685\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4685 New best_val_rmse: 0.4685\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4686 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.469 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4698 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.471 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4738 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4738 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4731 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4718 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4712 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4703 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4694 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4691 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4689 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4687 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4686 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4686 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4685 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4685 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4685 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4685 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4685 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4686 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4686 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4685 Still best_val_rmse: 0.4685 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4684 New best_val_rmse: 0.4684\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4684 New best_val_rmse: 0.4684\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4683 New best_val_rmse: 0.4683\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4683 Still best_val_rmse: 0.4683 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4683 New best_val_rmse: 0.4683\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 1.0 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.542 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 1.89 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4682 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4683 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4683 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4683 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4683 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4683 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4684 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4684 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4685 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4686 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4687 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4689 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4691 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4693 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4696 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4699 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4702 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4705 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4709 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4713 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4715 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4718 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4715 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.471 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4701 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4689 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.468 New best_val_rmse: 0.468\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4676 New best_val_rmse: 0.4676\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4671 New best_val_rmse: 0.4671\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4671 New best_val_rmse: 0.4671\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4671 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4674 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4674 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4672 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.467 New best_val_rmse: 0.467\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4668 New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4668 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4669 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.467 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.467 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.467 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4672 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4674 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4675 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4674 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4676 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4686 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4706 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4745 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4776 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4736 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4679 Still best_val_rmse: 0.4668 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4656 New best_val_rmse: 0.4656\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4653 New best_val_rmse: 0.4653\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4652 New best_val_rmse: 0.4652\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4651 New best_val_rmse: 0.4651\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4652 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4656 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.467 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4699 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4712 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4722 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4702 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.468 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4669 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 94 val_rmse: 0.4668 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.467 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4684 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4709 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4719 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4704 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4828 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4712 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4701 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4739 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4699 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4688 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4724 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 117 val_rmse: 0.4729 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 119 val_rmse: 0.4718 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.472 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.4746 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 125 val_rmse: 0.4729 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4742 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4747 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4684 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4706 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4693 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4677 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4688 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.469 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.472 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4671 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4671 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4736 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4674 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4657 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4715 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 2.44 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.4761 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 2 val_rmse: 0.4687 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 4 batch_num: 3 val_rmse: 0.4673 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 4 val_rmse: 0.4683 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 4 batch_num: 5 val_rmse: 0.468 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 4 batch_num: 6 val_rmse: 0.4658 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 7 val_rmse: 0.4657 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4669 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 4 batch_num: 9 val_rmse: 0.4658 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 4 batch_num: 10 val_rmse: 0.4645 New best_val_rmse: 0.4645\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 4 batch_num: 11 val_rmse: 0.4661 Still best_val_rmse: 0.4645 (from epoch 4)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 12 val_rmse: 0.4649 Still best_val_rmse: 0.4645 (from epoch 4)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 4 batch_num: 13 val_rmse: 0.4623 New best_val_rmse: 0.4623\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 4 batch_num: 14 val_rmse: 0.4605 New best_val_rmse: 0.4605\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 4 batch_num: 15 val_rmse: 0.4664 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4606 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 4 batch_num: 17 val_rmse: 0.4622 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 18 val_rmse: 0.463 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 4 batch_num: 19 val_rmse: 0.4635 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.4634 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 21 val_rmse: 0.4646 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 4 batch_num: 22 val_rmse: 0.4657 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 4 batch_num: 23 val_rmse: 0.4631 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4704 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 4 batch_num: 26 val_rmse: 0.47 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 27 val_rmse: 0.4649 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 4 batch_num: 28 val_rmse: 0.4837 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4889 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 36 val_rmse: 0.4997 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 4 batch_num: 44 val_rmse: 0.4917 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 52 val_rmse: 0.5421 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 68 val_rmse: 0.4969 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "8 steps took 6.68 seconds\n",
      "Epoch: 4 batch_num: 76 val_rmse: 0.4882 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 80 val_rmse: 0.4879 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 84 val_rmse: 0.5159 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 4 batch_num: 100 val_rmse: 0.5707 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 116 val_rmse: 0.5337 Still best_val_rmse: 0.4605 (from epoch 4)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 132 val_rmse: 0.5798 Still best_val_rmse: 0.4605 (from epoch 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:37:52,054]\u001b[0m Trial 0 finished with value: 0.46050626039505005 and parameters: {'base_lr': 3.5379120180791935e-05, 'last_lr': 0.00021137535166837663, 'epochs': 5}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00026457307680595145 last_lr 0.0006234142279885256 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffad9c68ec834e918463a69932e2fe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.138 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.079 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.062 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.021 New best_val_rmse: 1.021\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.025 Still best_val_rmse: 1.021 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:40:32,768]\u001b[0m Trial 1 finished with value: 1.0209934711456299 and parameters: {'base_lr': 0.00026457307680595145, 'last_lr': 0.0006234142279885256, 'epochs': 5}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0003510699921684945 last_lr 0.0009127027790204334 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966f9d35d01849f5b47360894a5a4b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9981 New best_val_rmse: 0.9981\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.089 Still best_val_rmse: 0.9981 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.041 Still best_val_rmse: 0.9981 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.038 Still best_val_rmse: 0.9981 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.025 Still best_val_rmse: 0.9981 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.019 Still best_val_rmse: 0.9981 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:43:13,621]\u001b[0m Trial 2 finished with value: 0.9981076121330261 and parameters: {'base_lr': 0.0003510699921684945, 'last_lr': 0.0009127027790204334, 'epochs': 3}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.000323161672880498 last_lr 0.0013930997171154112 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92325637d224529bc993167de85d4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.047 New best_val_rmse: 1.047\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.039 New best_val_rmse: 1.039\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.075 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.065 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.027 New best_val_rmse: 1.027\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.019 New best_val_rmse: 1.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:45:54,082]\u001b[0m Trial 3 finished with value: 1.0193175077438354 and parameters: {'base_lr': 0.000323161672880498, 'last_lr': 0.0013930997171154112, 'epochs': 3}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00010896104669695506 last_lr 0.0029946969275472285 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca470f913cf642f78f7cbf9d10f9e035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8292 New best_val_rmse: 0.8292\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7292 New best_val_rmse: 0.7292\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7376 Still best_val_rmse: 0.7292 (from epoch 0)\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 2.552 Still best_val_rmse: 0.7292 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9966 Still best_val_rmse: 0.7292 (from epoch 0)\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.035 Still best_val_rmse: 0.7292 (from epoch 0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:48:35,865]\u001b[0m Trial 4 finished with value: 0.7291932702064514 and parameters: {'base_lr': 0.00010896104669695506, 'last_lr': 0.0029946969275472285, 'epochs': 4}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 5\n",
      "##### Using base_lr 0.0003799082469670823 last_lr 0.00021149001927943728 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7461f83079dc488393d01b8c33679d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.388 New best_val_rmse: 1.388\n",
      "\n",
      "16 steps took 13.3 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.067 Still best_val_rmse: 1.04 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.022 New best_val_rmse: 1.022\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.03 Still best_val_rmse: 1.022 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:51:17,103]\u001b[0m Trial 5 finished with value: 1.0216212272644043 and parameters: {'base_lr': 0.0003799082469670823, 'last_lr': 0.00021149001927943728, 'epochs': 4}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002568209038046303 last_lr 0.004062311874892521 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faacd69527dd40b4817b193173a3eb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.213 New best_val_rmse: 1.213\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.059 New best_val_rmse: 1.059\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.056 New best_val_rmse: 1.056\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.205 Still best_val_rmse: 1.056 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.03 New best_val_rmse: 1.03\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.027 New best_val_rmse: 1.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:54:00,368]\u001b[0m Trial 6 finished with value: 1.0267387628555298 and parameters: {'base_lr': 0.0002568209038046303, 'last_lr': 0.004062311874892521, 'epochs': 4}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00037828558432408273 last_lr 0.004913422413649723 epochs 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9514142a7f9427c9cf422710680bc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9992 New best_val_rmse: 0.9992\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.041 Still best_val_rmse: 0.9992 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.021 Still best_val_rmse: 0.9992 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.093 Still best_val_rmse: 0.9992 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.046 Still best_val_rmse: 0.9992 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.044 Still best_val_rmse: 0.9992 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:56:41,562]\u001b[0m Trial 7 finished with value: 0.9992078542709351 and parameters: {'base_lr': 0.00037828558432408273, 'last_lr': 0.004913422413649723, 'epochs': 4}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.000346757166348739 last_lr 8.658505891799743e-05 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840791600a684df4885fa2419a1c5eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.014 New best_val_rmse: 1.014\n",
      "\n",
      "16 steps took 13.2 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.026 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.044 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.079 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.018 Still best_val_rmse: 1.014 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.022 Still best_val_rmse: 1.014 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 07:59:23,031]\u001b[0m Trial 8 finished with value: 1.013664722442627 and parameters: {'base_lr': 0.000346757166348739, 'last_lr': 8.658505891799743e-05, 'epochs': 5}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 5.9744555136421535e-05 last_lr 0.0023670292030870026 epochs 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7caa06ee4e447999ec7b803b127334d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.121 New best_val_rmse: 1.121\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.781 New best_val_rmse: 0.781\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8025 Still best_val_rmse: 0.781 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6496 New best_val_rmse: 0.6496\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6836 Still best_val_rmse: 0.6496 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5734 New best_val_rmse: 0.5734\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5568 New best_val_rmse: 0.5568\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5409 New best_val_rmse: 0.5409\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5718 Still best_val_rmse: 0.5409 (from epoch 0)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4963 New best_val_rmse: 0.4963\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5 Still best_val_rmse: 0.4963 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5452 Still best_val_rmse: 0.4963 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5344 Still best_val_rmse: 0.4963 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5271 Still best_val_rmse: 0.4963 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5234 Still best_val_rmse: 0.4963 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5057 Still best_val_rmse: 0.4963 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4923 New best_val_rmse: 0.4923\n",
      "\n",
      "8 steps took 6.72 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.481 New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4847 Still best_val_rmse: 0.481 (from epoch 1)\n",
      "\n",
      "4 steps took 4.25 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4846 Still best_val_rmse: 0.481 (from epoch 1)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4824 Still best_val_rmse: 0.481 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4853 Still best_val_rmse: 0.481 (from epoch 1)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4755 New best_val_rmse: 0.4755\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4709 New best_val_rmse: 0.4709\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.474 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4706 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4686 New best_val_rmse: 0.4686\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4677 New best_val_rmse: 0.4677\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.4684 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4721 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4744 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4741 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4712 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4706 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4687 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4723 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4786 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4718 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4692 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4691 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4696 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4696 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4694 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4694 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4699 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4704 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4698 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4699 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4695 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4701 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4764 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4776 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4718 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4687 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4682 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4679 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4679 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4681 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.468 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4684 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4679 Still best_val_rmse: 0.4677 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4668 New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4664 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4666 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4673 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4684 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4696 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4712 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4761 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4782 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4741 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4698 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4684 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4674 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4668 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4665 Still best_val_rmse: 0.4663 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4664 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4667 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4673 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4675 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4676 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4673 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4672 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4674 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4673 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4672 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4673 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4674 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4677 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4681 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4681 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4681 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4681 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4681 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n",
      "\n",
      "1 steps took 0.541 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 08:29:40,740]\u001b[0m Trial 9 finished with value: 0.46622639894485474 and parameters: {'base_lr': 5.9744555136421535e-05, 'last_lr': 0.0023670292030870026, 'epochs': 3}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.1267138552113085e-05 last_lr 0.0002785557414483006 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84418c2c97ae47ba884dd78c21a7201a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7192 New best_val_rmse: 0.7192\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7907 Still best_val_rmse: 0.7192 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6839 New best_val_rmse: 0.6839\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5648 New best_val_rmse: 0.5648\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.552 New best_val_rmse: 0.552\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5114 New best_val_rmse: 0.5114\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5738 Still best_val_rmse: 0.5114 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5592 Still best_val_rmse: 0.5114 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5419 Still best_val_rmse: 0.5114 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5214 Still best_val_rmse: 0.5114 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4976 New best_val_rmse: 0.4976\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5083 Still best_val_rmse: 0.4976 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5288 Still best_val_rmse: 0.4976 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 80 val_rmse: 0.5701 Still best_val_rmse: 0.4852 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.4864 Still best_val_rmse: 0.4852 (from epoch 1)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 104 val_rmse: 0.477 New best_val_rmse: 0.477\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 106 val_rmse: 0.4862 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 110 val_rmse: 0.4927 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 118 val_rmse: 0.5072 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.5004 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "16 steps took 14.2 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4821 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4803 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4837 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4867 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.4862 Still best_val_rmse: 0.477 (from epoch 1)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4721 New best_val_rmse: 0.4721\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4823 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.48 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4926 Still best_val_rmse: 0.4721 (from epoch 2)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4713 New best_val_rmse: 0.4713\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4787 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4889 Still best_val_rmse: 0.4713 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4697 New best_val_rmse: 0.4697\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4708 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4711 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4688 New best_val_rmse: 0.4688\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4709 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4825 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4747 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4698 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4701 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4701 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4699 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4698 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4699 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4708 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4749 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4769 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4749 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4698 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4688 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4683 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4683 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4688 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4707 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4718 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4699 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4687 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4677 New best_val_rmse: 0.4677\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4674 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4676 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4679 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4678 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4676 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4674 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4675 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4677 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4682 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4687 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4691 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4693 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4696 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.47 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4706 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4709 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.471 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.471 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4709 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4709 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4708 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4708 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4707 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4706 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4705 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4705 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4705 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4704 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.39 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 2.91 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4703 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4702 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4702 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.47 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4698 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4697 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4696 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4695 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4694 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4693 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4692 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4692 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4692 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4693 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4694 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4696 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4699 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4701 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4706 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4706 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.88 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4704 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4701 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4698 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4696 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4691 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4686 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4682 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4679 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4676 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4675 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4674 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4674 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4674 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4675 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4675 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4674 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4673 Still best_val_rmse: 0.4673 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4671 New best_val_rmse: 0.4671\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4672 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4673 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4679 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4689 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4702 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4734 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4762 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4729 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4684 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4676 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4675 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4675 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4676 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.468 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4688 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4704 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4691 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4685 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4688 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4709 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4822 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4748 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4696 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4686 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4687 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4688 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4691 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4695 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 92 val_rmse: 0.4696 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.47 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4719 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4751 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4796 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4753 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 103 val_rmse: 0.4702 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.4724 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4732 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4675 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 110 val_rmse: 0.4731 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 112 val_rmse: 0.4988 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4679 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 121 val_rmse: 0.4687 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4708 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4722 Still best_val_rmse: 0.4671 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 127 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4664 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 129 val_rmse: 0.4666 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4673 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 131 val_rmse: 0.4741 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4713 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4726 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4708 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 139 val_rmse: 0.4794 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.4741 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.4763 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4773 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.38 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4757 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 2.88 seconds\n",
      "Epoch: 4 batch_num: 1 val_rmse: 0.4972 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 9 val_rmse: 0.4731 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 11 val_rmse: 0.4744 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 13 val_rmse: 0.4716 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 15 val_rmse: 0.4683 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 4 batch_num: 17 val_rmse: 0.47 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 19 val_rmse: 0.476 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 21 val_rmse: 0.4673 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 4 batch_num: 22 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 4 batch_num: 23 val_rmse: 0.4696 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4669 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 4 batch_num: 25 val_rmse: 0.4694 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 4 batch_num: 26 val_rmse: 0.472 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 28 val_rmse: 0.473 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 30 val_rmse: 0.4732 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 32 val_rmse: 0.4809 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 36 val_rmse: 0.4896 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "4 steps took 3.4 seconds\n",
      "Epoch: 4 batch_num: 40 val_rmse: 0.4735 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 42 val_rmse: 0.4945 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 50 val_rmse: 0.4819 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 54 val_rmse: 0.501 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 70 val_rmse: 0.487 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 74 val_rmse: 0.4942 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 82 val_rmse: 0.5047 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 98 val_rmse: 0.5058 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 114 val_rmse: 0.5489 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 130 val_rmse: 0.587 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 146 val_rmse: 0.5136 Still best_val_rmse: 0.4662 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 09:20:43,372]\u001b[0m Trial 10 finished with value: 0.46617308259010315 and parameters: {'base_lr': 3.1267138552113085e-05, 'last_lr': 0.0002785557414483006, 'epochs': 5}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.229859428898334e-05 last_lr 0.0002711377163548122 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f745042bac4adf94089daefa33f77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 16.4 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7183 New best_val_rmse: 0.7183\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7769 Still best_val_rmse: 0.7183 (from epoch 0)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6664 New best_val_rmse: 0.6664\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.768 Still best_val_rmse: 0.6664 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.721 Still best_val_rmse: 0.6664 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5416 New best_val_rmse: 0.5416\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6379 Still best_val_rmse: 0.5416 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6592 Still best_val_rmse: 0.5416 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5961 Still best_val_rmse: 0.5416 (from epoch 0)\n",
      "\n",
      "16 steps took 14.4 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5539 Still best_val_rmse: 0.5416 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5216 New best_val_rmse: 0.5216\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5145 New best_val_rmse: 0.5145\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5249 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.524 Still best_val_rmse: 0.5145 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5136 New best_val_rmse: 0.5136\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5441 Still best_val_rmse: 0.5136 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4998 New best_val_rmse: 0.4998\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4903 Still best_val_rmse: 0.4882 (from epoch 1)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 4.31 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4892 Still best_val_rmse: 0.4815 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4835 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.48 Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4759 New best_val_rmse: 0.4759\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4816 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4776 Still best_val_rmse: 0.4759 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4749 New best_val_rmse: 0.4749\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.482 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4807 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4811 Still best_val_rmse: 0.4749 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4709 New best_val_rmse: 0.4709\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4717 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.471 Still best_val_rmse: 0.4709 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4705 New best_val_rmse: 0.4705\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4701 New best_val_rmse: 0.4701\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4706 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4705 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4708 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4705 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4705 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4773 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4804 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4702 Still best_val_rmse: 0.4701 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4686 New best_val_rmse: 0.4686\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4684 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4689 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4691 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.469 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4684 Still best_val_rmse: 0.4682 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4674 New best_val_rmse: 0.4674\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.467 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4679 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4686 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4691 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4708 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4735 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4735 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4716 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4701 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4679 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4669 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4665 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.466 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4661 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4663 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4667 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4669 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4669 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4669 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.467 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4669 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.467 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.467 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.467 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4672 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4675 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4676 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4676 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4677 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4677 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4677 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4678 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4678 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4678 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4678 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4678 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4677 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4677 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4677 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4676 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4676 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4675 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4675 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4675 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.845 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4675 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.544 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 2.1 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4674 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4672 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4672 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4672 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.467 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.467 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.4671 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4672 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4672 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4675 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4678 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4681 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4686 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.469 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4694 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 1.01 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4699 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4705 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4712 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4716 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4715 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4703 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4695 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.4692 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4687 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4681 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4673 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4668 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4665 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4662 Still best_val_rmse: 0.4659 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4655 New best_val_rmse: 0.4655\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4655 Still best_val_rmse: 0.4655 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4656 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.466 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4667 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4675 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.4678 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4676 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4674 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.4672 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4668 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.466 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.4658 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4659 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4663 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.467 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4675 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.4685 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4687 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.468 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.4671 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4668 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.468 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4716 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4765 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4712 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.468 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4714 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4719 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4696 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4719 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4798 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4802 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4791 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4835 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.39 seconds\n",
      "Epoch: 3 batch_num: 105 val_rmse: 0.477 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 107 val_rmse: 0.4746 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4728 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 111 val_rmse: 0.4726 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4697 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 114 val_rmse: 0.4699 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4698 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 116 val_rmse: 0.4701 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 118 val_rmse: 0.4716 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 120 val_rmse: 0.4715 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 3 batch_num: 122 val_rmse: 0.4808 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4705 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4704 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4706 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4762 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 134 val_rmse: 0.4956 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "8 steps took 6.69 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4855 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 3 batch_num: 146 val_rmse: 0.4763 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 2.56 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.4855 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 4 val_rmse: 0.4851 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.4866 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 12 val_rmse: 0.4701 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 14 val_rmse: 0.4728 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4745 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 18 val_rmse: 0.4794 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.4832 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4791 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.69 seconds\n",
      "Epoch: 4 batch_num: 26 val_rmse: 0.4726 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 28 val_rmse: 0.4722 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 30 val_rmse: 0.4842 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 34 val_rmse: 0.5015 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 50 val_rmse: 0.4893 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 54 val_rmse: 0.5036 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 70 val_rmse: 0.483 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 74 val_rmse: 0.4911 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "8 steps took 6.75 seconds\n",
      "Epoch: 4 batch_num: 82 val_rmse: 0.4903 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 90 val_rmse: 0.4998 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "8 steps took 6.7 seconds\n",
      "Epoch: 4 batch_num: 98 val_rmse: 0.5145 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 114 val_rmse: 0.4989 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 122 val_rmse: 0.5448 Still best_val_rmse: 0.4654 (from epoch 3)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 4 batch_num: 138 val_rmse: 0.5051 Still best_val_rmse: 0.4654 (from epoch 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-26 10:14:49,715]\u001b[0m Trial 11 finished with value: 0.46542713046073914 and parameters: {'base_lr': 3.229859428898334e-05, 'last_lr': 0.0002711377163548122, 'epochs': 5}. Best is trial 0 with value: 0.46050626039505005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.0377004073224824e-05 last_lr 0.00018524387422817287 epochs 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75fc2976f35472f8df4e75e387e159f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 15.9 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7238 New best_val_rmse: 0.7238\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7792 Still best_val_rmse: 0.7238 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6185 New best_val_rmse: 0.6185\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.5717 New best_val_rmse: 0.5717\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5663 New best_val_rmse: 0.5663\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5253 New best_val_rmse: 0.5253\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5886 Still best_val_rmse: 0.5253 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5498 Still best_val_rmse: 0.5253 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5842 Still best_val_rmse: 0.5253 (from epoch 0)\n",
      "\n",
      "16 steps took 14.3 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5374 Still best_val_rmse: 0.5253 (from epoch 0)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5032 New best_val_rmse: 0.5032\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5132 Still best_val_rmse: 0.5032 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5442 Still best_val_rmse: 0.5032 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5035 Still best_val_rmse: 0.5032 (from epoch 1)\n",
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5028 New best_val_rmse: 0.5028\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5132 Still best_val_rmse: 0.5028 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 134 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4781 Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 1 batch_num: 138 val_rmse: 0.4786 Still best_val_rmse: 0.4753 (from epoch 1)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4805 Still best_val_rmse: 0.4738 (from epoch 1)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4776 Still best_val_rmse: 0.4738 (from epoch 1)\n",
      "\n",
      "2 steps took 2.62 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4807 Still best_val_rmse: 0.4738 (from epoch 1)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4811 Still best_val_rmse: 0.4738 (from epoch 1)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5059 Still best_val_rmse: 0.4738 (from epoch 1)\n",
      "\n",
      "16 steps took 13.4 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4704 New best_val_rmse: 0.4704\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4686 New best_val_rmse: 0.4686\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4723 Still best_val_rmse: 0.4686 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4771 Still best_val_rmse: 0.4686 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4714 Still best_val_rmse: 0.4686 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4685 New best_val_rmse: 0.4685\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4682 New best_val_rmse: 0.4682\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.468 New best_val_rmse: 0.468\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4679 New best_val_rmse: 0.4679\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4679 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4684 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4689 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4699 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.942 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.47 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4686 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4692 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4701 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4726 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "2 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4698 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4691 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4685 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4682 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4682 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4682 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4682 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4684 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4694 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4689 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4684 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4681 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.468 Still best_val_rmse: 0.4679 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4675 New best_val_rmse: 0.4675\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4682 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.47 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4702 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.468 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4681 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4685 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4697 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4708 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4691 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4684 Still best_val_rmse: 0.4675 (from epoch 2)\n",
      "\n",
      "1 steps took 0.852 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4674 New best_val_rmse: 0.4674\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4669 New best_val_rmse: 0.4669\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4666 New best_val_rmse: 0.4666\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4666 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4667 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4669 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.467 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4672 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4679 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4672 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4673 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4677 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4681 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4684 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4684 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4683 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4684 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4686 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4689 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4692 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4694 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4695 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4694 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4693 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4691 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4691 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.469 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4689 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4688 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4686 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4685 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4684 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4683 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4683 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.851 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4682 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4682 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4682 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4681 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4681 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.468 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.468 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4679 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4678 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4678 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4677 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4677 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4676 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.544 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 2.03 seconds\n",
      "Epoch: 3 batch_num: 0 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 1 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.847 seconds\n",
      "Epoch: 3 batch_num: 2 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 3 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 4 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 5 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 6 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 7 val_rmse: 0.4675 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 8 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 9 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 10 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 11 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 12 val_rmse: 0.4674 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 13 val_rmse: 0.4673 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 14 val_rmse: 0.4673 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 15 val_rmse: 0.4673 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 16 val_rmse: 0.4672 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 17 val_rmse: 0.4672 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 18 val_rmse: 0.4672 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 19 val_rmse: 0.4671 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 20 val_rmse: 0.4671 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 21 val_rmse: 0.467 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 22 val_rmse: 0.4669 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 23 val_rmse: 0.4669 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 24 val_rmse: 0.4668 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 25 val_rmse: 0.4667 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 26 val_rmse: 0.4667 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 27 val_rmse: 0.4667 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 28 val_rmse: 0.4667 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 29 val_rmse: 0.4666 Still best_val_rmse: 0.4665 (from epoch 2)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 30 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 1.02 seconds\n",
      "Epoch: 3 batch_num: 31 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 32 val_rmse: 0.4663 New best_val_rmse: 0.4663\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 33 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 34 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 35 val_rmse: 0.4662 New best_val_rmse: 0.4662\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 36 val_rmse: 0.4663 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 37 val_rmse: 0.4665 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 38 val_rmse: 0.4669 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 39 val_rmse: 0.4674 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 40 val_rmse: 0.4679 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.837 seconds\n",
      "Epoch: 3 batch_num: 41 val_rmse: 0.4683 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 42 val_rmse: 0.4686 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 43 val_rmse: 0.469 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.852 seconds\n",
      "Epoch: 3 batch_num: 44 val_rmse: 0.4691 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 45 val_rmse: 0.4693 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 46 val_rmse: 0.4691 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 47 val_rmse: 0.4686 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 48 val_rmse: 0.4682 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 49 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 50 val_rmse: 0.4672 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 51 val_rmse: 0.4669 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 52 val_rmse: 0.4667 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 53 val_rmse: 0.4665 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 54 val_rmse: 0.4665 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 55 val_rmse: 0.4664 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 56 val_rmse: 0.4664 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 57 val_rmse: 0.4666 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.842 seconds\n",
      "Epoch: 3 batch_num: 58 val_rmse: 0.4666 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 59 val_rmse: 0.4666 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.839 seconds\n",
      "Epoch: 3 batch_num: 60 val_rmse: 0.4667 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 61 val_rmse: 0.4668 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 62 val_rmse: 0.467 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 63 val_rmse: 0.4674 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.841 seconds\n",
      "Epoch: 3 batch_num: 64 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.846 seconds\n",
      "Epoch: 3 batch_num: 65 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 66 val_rmse: 0.4685 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 67 val_rmse: 0.4683 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 68 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 69 val_rmse: 0.4674 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 70 val_rmse: 0.4672 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 71 val_rmse: 0.4675 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 72 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 73 val_rmse: 0.468 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 74 val_rmse: 0.4678 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.829 seconds\n",
      "Epoch: 3 batch_num: 75 val_rmse: 0.4666 Still best_val_rmse: 0.4662 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 76 val_rmse: 0.466 New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 77 val_rmse: 0.4658 New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 78 val_rmse: 0.4659 Still best_val_rmse: 0.4658 (from epoch 3)\n",
      "\n",
      "1 steps took 0.827 seconds\n",
      "Epoch: 3 batch_num: 79 val_rmse: 0.4657 New best_val_rmse: 0.4657\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 80 val_rmse: 0.4653 New best_val_rmse: 0.4653\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 3 batch_num: 81 val_rmse: 0.4654 Still best_val_rmse: 0.4653 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 82 val_rmse: 0.4651 New best_val_rmse: 0.4651\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 83 val_rmse: 0.4651 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 84 val_rmse: 0.4655 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 85 val_rmse: 0.4655 Still best_val_rmse: 0.4651 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 86 val_rmse: 0.4648 New best_val_rmse: 0.4648\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 87 val_rmse: 0.4647 New best_val_rmse: 0.4647\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 88 val_rmse: 0.4647 New best_val_rmse: 0.4647\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 89 val_rmse: 0.4653 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.843 seconds\n",
      "Epoch: 3 batch_num: 90 val_rmse: 0.4672 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 91 val_rmse: 0.4722 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 93 val_rmse: 0.4765 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 95 val_rmse: 0.4676 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 96 val_rmse: 0.4652 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.84 seconds\n",
      "Epoch: 3 batch_num: 97 val_rmse: 0.4659 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.83 seconds\n",
      "Epoch: 3 batch_num: 98 val_rmse: 0.467 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 99 val_rmse: 0.4675 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.828 seconds\n",
      "Epoch: 3 batch_num: 100 val_rmse: 0.4665 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 101 val_rmse: 0.4664 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 102 val_rmse: 0.4715 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 104 val_rmse: 0.4882 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 108 val_rmse: 0.4685 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 109 val_rmse: 0.4804 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 3 batch_num: 113 val_rmse: 0.4753 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 115 val_rmse: 0.4965 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 3 batch_num: 123 val_rmse: 0.468 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.844 seconds\n",
      "Epoch: 3 batch_num: 124 val_rmse: 0.4705 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 126 val_rmse: 0.4746 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 128 val_rmse: 0.4754 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 130 val_rmse: 0.4758 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 132 val_rmse: 0.4688 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 3 batch_num: 133 val_rmse: 0.4701 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 3 batch_num: 135 val_rmse: 0.4684 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.836 seconds\n",
      "Epoch: 3 batch_num: 136 val_rmse: 0.4657 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 137 val_rmse: 0.4666 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 138 val_rmse: 0.471 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 3 batch_num: 140 val_rmse: 0.4684 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 3 batch_num: 141 val_rmse: 0.469 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 142 val_rmse: 0.4678 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.835 seconds\n",
      "Epoch: 3 batch_num: 143 val_rmse: 0.469 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.833 seconds\n",
      "Epoch: 3 batch_num: 144 val_rmse: 0.4695 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.834 seconds\n",
      "Epoch: 3 batch_num: 145 val_rmse: 0.4759 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.39 seconds\n",
      "Epoch: 3 batch_num: 147 val_rmse: 0.4694 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 2.05 seconds\n",
      "Epoch: 4 batch_num: 0 val_rmse: 0.473 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 2 val_rmse: 0.4734 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 4 val_rmse: 0.4694 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 4 batch_num: 5 val_rmse: 0.4693 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.832 seconds\n",
      "Epoch: 4 batch_num: 6 val_rmse: 0.4701 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 8 val_rmse: 0.48 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 10 val_rmse: 0.4711 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 12 val_rmse: 0.4694 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.831 seconds\n",
      "Epoch: 4 batch_num: 13 val_rmse: 0.4695 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "1 steps took 0.838 seconds\n",
      "Epoch: 4 batch_num: 14 val_rmse: 0.4701 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 16 val_rmse: 0.4847 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 20 val_rmse: 0.472 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 22 val_rmse: 0.4712 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 24 val_rmse: 0.4846 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.36 seconds\n",
      "Epoch: 4 batch_num: 28 val_rmse: 0.4724 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 30 val_rmse: 0.4814 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 34 val_rmse: 0.4784 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 36 val_rmse: 0.4767 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 38 val_rmse: 0.4819 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 42 val_rmse: 0.4759 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.67 seconds\n",
      "Epoch: 4 batch_num: 44 val_rmse: 0.4822 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.34 seconds\n",
      "Epoch: 4 batch_num: 48 val_rmse: 0.4865 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 52 val_rmse: 0.4779 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 54 val_rmse: 0.4828 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "4 steps took 3.35 seconds\n",
      "Epoch: 4 batch_num: 58 val_rmse: 0.4751 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "2 steps took 1.68 seconds\n",
      "Epoch: 4 batch_num: 60 val_rmse: 0.4999 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n",
      "8 steps took 6.71 seconds\n",
      "Epoch: 4 batch_num: 68 val_rmse: 0.5199 Still best_val_rmse: 0.4647 (from epoch 3)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-e3fedb29309a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Best value: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Best params: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    399\u001b[0m             )\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/optuna/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-8d8870be85d5>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m     trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, \n\u001b[1;32m     36\u001b[0m                       scheduler = scheduler, num_epochs = epochs)\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mrmse_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3489922216aa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
