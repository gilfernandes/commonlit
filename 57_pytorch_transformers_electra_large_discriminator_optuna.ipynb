{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc, warnings, random, time, os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ed820-371a-48da-8412-db0701c05c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(df):\n",
    "    df.drop(df[df['target'] == 0].index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2828  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2829  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2830  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2832  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2828  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2829  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2830  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2831  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2832  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.315372        0.480805  \n",
       "2    -0.580118        0.476676  \n",
       "3    -1.054013        0.450007  \n",
       "4     0.247197        0.510845  \n",
       "...        ...             ...  \n",
       "2828  1.711390        0.646900  \n",
       "2829  0.189476        0.535648  \n",
       "2830  0.255209        0.483866  \n",
       "2831 -0.215279        0.514128  \n",
       "2832  0.300779        0.512379  \n",
       "\n",
       "[2833 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79e005-5651-4414-9725-4567d3a9b300",
   "metadata": {},
   "source": [
    "### Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938c53-d840-4889-b9ab-3170c608137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(): \n",
    "    NUM_FOLDS = 6\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 248\n",
    "    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "    MODEL_PATH = 'google/electra-large-discriminator'\n",
    "    TOKENIZER_PATH = 'google/electra-large-discriminator'\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 1000\n",
    "    NUM_WORKERS = 2\n",
    "    MODEL_FOLDER = MODELS_PATH\n",
    "    model_name = 'electra-large-discriminator'\n",
    "    svm_kernels = ['rbf']\n",
    "    svm_c = 5\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b17b48-922f-4a27-8bb4-e641491d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.MODEL_FOLDER.exists():\n",
    "    os.mkdir(cfg.MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd067b3-c1a6-4c4a-900e-9499ca93b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978289c5-dc58-4be5-93d8-64566dad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bins(train_df, num_bins):\n",
    "    train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "    return num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131b79d6-1ec5-492b-930f-e4c75288bcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bins(train_df, cfg.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ee1b97-cef2-46cc-88d7-3f7ae737c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.125765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441</td>\n",
       "      <td>-2.270279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784</td>\n",
       "      <td>-1.412150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886</td>\n",
       "      <td>-0.548095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>1.070237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean\n",
       "bins                 \n",
       "0       122 -3.125765\n",
       "1       441 -2.270279\n",
       "2       784 -1.412150\n",
       "3       886 -0.548095\n",
       "4       494  0.289716\n",
       "5       106  1.070237"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['bins'])['target'].agg(['count', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41922d13-b7af-4675-ae2d-c384025c86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, inference_only=False):\n",
    "        super().__init__()\n",
    "        self.df, self.inference_only = df, inference_only\n",
    "        self.text = df['excerpt'].tolist()\n",
    "        self.bins = df['bins']\n",
    "        if not inference_only:\n",
    "            self.target = torch.tensor(df['target'].to_numpy(), dtype = torch.float32)\n",
    "        \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',\n",
    "            max_length = cfg.MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2329ea-0c9a-407c-8c82-8f247ad9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = CommonLitDataset(train_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ee04e-2d41-46bc-89e0-c0b9476090cb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ef269a-01da-4555-bdb7-265d93940648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n",
    "        self.final_layer = nn.Linear(hidden_dim, num_targets)\n",
    "        self.out_features = hidden_dim\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.hidden_layer(features))\n",
    "        score = self.final_layer(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f7c88c-5970-4b12-bb86-ee4a5de126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n",
    "        config.update({\n",
    "            \"output_hidden_states\": True,\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })\n",
    "        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_PATH, config=config)\n",
    "        self.attention = AttentionHead(config.hidden_size, 512, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n",
    "        last_layer_hidden_states = hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "        return self.regressor(context_vector), context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa41e86-dc36-43ae-a98f-e97cbc46fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sample_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01d5b219-2e0e-4485-99ef-3d2ffa0f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    if(name.find('layer') > -1):\n",
    "        layer_name = re.sub(r'.+(layer\\.\\d+).+', r'\\1', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4929919-01cf-47e1-9e9c-3f040562b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transformer_model.electra.embeddings.word_embeddings.weight torch.Size([30522, 1024])\n",
      "1 transformer_model.electra.embeddings.position_embeddings.weight torch.Size([512, 1024])\n",
      "2 transformer_model.electra.embeddings.token_type_embeddings.weight torch.Size([2, 1024])\n",
      "3 transformer_model.electra.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "4 transformer_model.electra.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "5 transformer_model.electra.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "6 transformer_model.electra.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "7 transformer_model.electra.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "8 transformer_model.electra.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "9 transformer_model.electra.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "10 transformer_model.electra.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "11 transformer_model.electra.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "12 transformer_model.electra.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "13 transformer_model.electra.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "14 transformer_model.electra.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "15 transformer_model.electra.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "16 transformer_model.electra.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "17 transformer_model.electra.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "18 transformer_model.electra.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "19 transformer_model.electra.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "20 transformer_model.electra.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "21 transformer_model.electra.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "22 transformer_model.electra.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "23 transformer_model.electra.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "24 transformer_model.electra.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "25 transformer_model.electra.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "26 transformer_model.electra.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "27 transformer_model.electra.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "28 transformer_model.electra.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "29 transformer_model.electra.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "30 transformer_model.electra.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "31 transformer_model.electra.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "32 transformer_model.electra.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "33 transformer_model.electra.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "34 transformer_model.electra.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "35 transformer_model.electra.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "36 transformer_model.electra.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "37 transformer_model.electra.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "38 transformer_model.electra.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "39 transformer_model.electra.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "40 transformer_model.electra.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "41 transformer_model.electra.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "42 transformer_model.electra.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "43 transformer_model.electra.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "44 transformer_model.electra.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "45 transformer_model.electra.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "46 transformer_model.electra.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "47 transformer_model.electra.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "48 transformer_model.electra.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "49 transformer_model.electra.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "50 transformer_model.electra.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "51 transformer_model.electra.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "52 transformer_model.electra.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "53 transformer_model.electra.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "54 transformer_model.electra.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "55 transformer_model.electra.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "56 transformer_model.electra.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "57 transformer_model.electra.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "58 transformer_model.electra.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "59 transformer_model.electra.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "60 transformer_model.electra.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "61 transformer_model.electra.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "62 transformer_model.electra.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "63 transformer_model.electra.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "64 transformer_model.electra.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "65 transformer_model.electra.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "66 transformer_model.electra.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "67 transformer_model.electra.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "68 transformer_model.electra.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "69 transformer_model.electra.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "70 transformer_model.electra.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "71 transformer_model.electra.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "72 transformer_model.electra.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "73 transformer_model.electra.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "74 transformer_model.electra.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "75 transformer_model.electra.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "76 transformer_model.electra.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "77 transformer_model.electra.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "78 transformer_model.electra.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "79 transformer_model.electra.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "80 transformer_model.electra.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "81 transformer_model.electra.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "82 transformer_model.electra.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "83 transformer_model.electra.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "84 transformer_model.electra.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "85 transformer_model.electra.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "86 transformer_model.electra.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "87 transformer_model.electra.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "88 transformer_model.electra.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "89 transformer_model.electra.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "90 transformer_model.electra.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "91 transformer_model.electra.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "92 transformer_model.electra.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "93 transformer_model.electra.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "94 transformer_model.electra.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "95 transformer_model.electra.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "96 transformer_model.electra.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "97 transformer_model.electra.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "98 transformer_model.electra.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "99 transformer_model.electra.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "100 transformer_model.electra.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "101 transformer_model.electra.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "102 transformer_model.electra.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "103 transformer_model.electra.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "104 transformer_model.electra.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "105 transformer_model.electra.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "106 transformer_model.electra.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "107 transformer_model.electra.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "108 transformer_model.electra.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "109 transformer_model.electra.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "110 transformer_model.electra.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "111 transformer_model.electra.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "112 transformer_model.electra.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "113 transformer_model.electra.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "114 transformer_model.electra.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "115 transformer_model.electra.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "116 transformer_model.electra.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "117 transformer_model.electra.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "118 transformer_model.electra.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "119 transformer_model.electra.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "120 transformer_model.electra.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "121 transformer_model.electra.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "122 transformer_model.electra.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "123 transformer_model.electra.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "124 transformer_model.electra.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "125 transformer_model.electra.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "126 transformer_model.electra.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "127 transformer_model.electra.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "128 transformer_model.electra.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "129 transformer_model.electra.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "130 transformer_model.electra.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "131 transformer_model.electra.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "132 transformer_model.electra.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "133 transformer_model.electra.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "134 transformer_model.electra.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "135 transformer_model.electra.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "136 transformer_model.electra.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "137 transformer_model.electra.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "138 transformer_model.electra.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "139 transformer_model.electra.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "140 transformer_model.electra.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "141 transformer_model.electra.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "142 transformer_model.electra.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "143 transformer_model.electra.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "144 transformer_model.electra.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "145 transformer_model.electra.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "146 transformer_model.electra.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "147 transformer_model.electra.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "148 transformer_model.electra.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "149 transformer_model.electra.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "150 transformer_model.electra.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "151 transformer_model.electra.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "152 transformer_model.electra.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "153 transformer_model.electra.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "154 transformer_model.electra.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "155 transformer_model.electra.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "156 transformer_model.electra.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "157 transformer_model.electra.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "158 transformer_model.electra.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "159 transformer_model.electra.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "160 transformer_model.electra.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "161 transformer_model.electra.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "162 transformer_model.electra.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "163 transformer_model.electra.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "164 transformer_model.electra.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "165 transformer_model.electra.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "166 transformer_model.electra.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "167 transformer_model.electra.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "168 transformer_model.electra.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "169 transformer_model.electra.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "170 transformer_model.electra.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "171 transformer_model.electra.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "172 transformer_model.electra.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "173 transformer_model.electra.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "174 transformer_model.electra.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "175 transformer_model.electra.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "176 transformer_model.electra.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "177 transformer_model.electra.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "178 transformer_model.electra.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "179 transformer_model.electra.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "180 transformer_model.electra.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "181 transformer_model.electra.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "182 transformer_model.electra.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "183 transformer_model.electra.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "184 transformer_model.electra.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "185 transformer_model.electra.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "186 transformer_model.electra.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "187 transformer_model.electra.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "188 transformer_model.electra.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "189 transformer_model.electra.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "190 transformer_model.electra.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "191 transformer_model.electra.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "192 transformer_model.electra.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "193 transformer_model.electra.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "194 transformer_model.electra.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "195 transformer_model.electra.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "196 transformer_model.electra.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "197 transformer_model.electra.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "198 transformer_model.electra.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "199 transformer_model.electra.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "200 transformer_model.electra.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "201 transformer_model.electra.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "202 transformer_model.electra.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "203 transformer_model.electra.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "204 transformer_model.electra.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "205 transformer_model.electra.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "206 transformer_model.electra.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "207 transformer_model.electra.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "208 transformer_model.electra.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "209 transformer_model.electra.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "210 transformer_model.electra.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "211 transformer_model.electra.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "212 transformer_model.electra.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "213 transformer_model.electra.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "214 transformer_model.electra.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "215 transformer_model.electra.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "216 transformer_model.electra.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "217 transformer_model.electra.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "218 transformer_model.electra.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "219 transformer_model.electra.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "220 transformer_model.electra.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "221 transformer_model.electra.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "222 transformer_model.electra.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "223 transformer_model.electra.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "224 transformer_model.electra.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "225 transformer_model.electra.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "226 transformer_model.electra.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "227 transformer_model.electra.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "228 transformer_model.electra.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "229 transformer_model.electra.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "230 transformer_model.electra.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "231 transformer_model.electra.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "232 transformer_model.electra.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "233 transformer_model.electra.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "234 transformer_model.electra.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "235 transformer_model.electra.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "236 transformer_model.electra.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "237 transformer_model.electra.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "238 transformer_model.electra.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "239 transformer_model.electra.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "240 transformer_model.electra.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "241 transformer_model.electra.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "242 transformer_model.electra.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "243 transformer_model.electra.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "244 transformer_model.electra.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "245 transformer_model.electra.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "246 transformer_model.electra.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "247 transformer_model.electra.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "248 transformer_model.electra.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "249 transformer_model.electra.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "250 transformer_model.electra.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "251 transformer_model.electra.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "252 transformer_model.electra.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "253 transformer_model.electra.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "254 transformer_model.electra.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "255 transformer_model.electra.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "256 transformer_model.electra.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "257 transformer_model.electra.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "258 transformer_model.electra.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "259 transformer_model.electra.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "260 transformer_model.electra.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "261 transformer_model.electra.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "262 transformer_model.electra.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "263 transformer_model.electra.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "264 transformer_model.electra.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "265 transformer_model.electra.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "266 transformer_model.electra.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "267 transformer_model.electra.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "268 transformer_model.electra.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "269 transformer_model.electra.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "270 transformer_model.electra.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "271 transformer_model.electra.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "272 transformer_model.electra.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "273 transformer_model.electra.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "274 transformer_model.electra.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "275 transformer_model.electra.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "276 transformer_model.electra.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "277 transformer_model.electra.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "278 transformer_model.electra.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "279 transformer_model.electra.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "280 transformer_model.electra.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "281 transformer_model.electra.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "282 transformer_model.electra.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "283 transformer_model.electra.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "284 transformer_model.electra.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "285 transformer_model.electra.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "286 transformer_model.electra.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "287 transformer_model.electra.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "288 transformer_model.electra.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "289 transformer_model.electra.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "290 transformer_model.electra.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "291 transformer_model.electra.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "292 transformer_model.electra.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "293 transformer_model.electra.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "294 transformer_model.electra.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "295 transformer_model.electra.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "296 transformer_model.electra.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "297 transformer_model.electra.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "298 transformer_model.electra.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "299 transformer_model.electra.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "300 transformer_model.electra.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "301 transformer_model.electra.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "302 transformer_model.electra.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "303 transformer_model.electra.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "304 transformer_model.electra.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "305 transformer_model.electra.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "306 transformer_model.electra.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "307 transformer_model.electra.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "308 transformer_model.electra.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "309 transformer_model.electra.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "310 transformer_model.electra.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "311 transformer_model.electra.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "312 transformer_model.electra.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "313 transformer_model.electra.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "314 transformer_model.electra.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "315 transformer_model.electra.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "316 transformer_model.electra.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "317 transformer_model.electra.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "318 transformer_model.electra.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "319 transformer_model.electra.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "320 transformer_model.electra.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "321 transformer_model.electra.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "322 transformer_model.electra.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "323 transformer_model.electra.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "324 transformer_model.electra.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "325 transformer_model.electra.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "326 transformer_model.electra.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "327 transformer_model.electra.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "328 transformer_model.electra.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "329 transformer_model.electra.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "330 transformer_model.electra.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "331 transformer_model.electra.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "332 transformer_model.electra.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "333 transformer_model.electra.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "334 transformer_model.electra.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "335 transformer_model.electra.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "336 transformer_model.electra.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "337 transformer_model.electra.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "338 transformer_model.electra.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "339 transformer_model.electra.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "340 transformer_model.electra.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "341 transformer_model.electra.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "342 transformer_model.electra.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "343 transformer_model.electra.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "344 transformer_model.electra.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "345 transformer_model.electra.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "346 transformer_model.electra.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "347 transformer_model.electra.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "348 transformer_model.electra.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "349 transformer_model.electra.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "350 transformer_model.electra.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "351 transformer_model.electra.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "352 transformer_model.electra.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "353 transformer_model.electra.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "354 transformer_model.electra.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "355 transformer_model.electra.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "356 transformer_model.electra.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "357 transformer_model.electra.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "358 transformer_model.electra.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "359 transformer_model.electra.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "360 transformer_model.electra.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "361 transformer_model.electra.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "362 transformer_model.electra.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "363 transformer_model.electra.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "364 transformer_model.electra.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "365 transformer_model.electra.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "366 transformer_model.electra.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "367 transformer_model.electra.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "368 transformer_model.electra.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "369 transformer_model.electra.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "370 transformer_model.electra.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "371 transformer_model.electra.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "372 transformer_model.electra.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "373 transformer_model.electra.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "374 transformer_model.electra.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "375 transformer_model.electra.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "376 transformer_model.electra.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "377 transformer_model.electra.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "378 transformer_model.electra.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "379 transformer_model.electra.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "380 transformer_model.electra.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "381 transformer_model.electra.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "382 transformer_model.electra.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "383 transformer_model.electra.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "384 transformer_model.electra.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "385 transformer_model.electra.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "386 transformer_model.electra.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "387 transformer_model.electra.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "388 transformer_model.electra.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "389 transformer_model.classifier.dense.weight torch.Size([1024, 1024])\n",
      "390 transformer_model.classifier.dense.bias torch.Size([1024])\n",
      "391 transformer_model.classifier.out_proj.weight torch.Size([2, 1024])\n",
      "392 transformer_model.classifier.out_proj.bias torch.Size([2])\n",
      "393 attention.hidden_layer.weight torch.Size([512, 1024])\n",
      "394 attention.hidden_layer.bias torch.Size([512])\n",
      "395 attention.final_layer.weight torch.Size([1, 512])\n",
      "396 attention.final_layer.bias torch.Size([1])\n",
      "397 regressor.weight torch.Size([1, 1024])\n",
      "398 regressor.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(sample_model.named_parameters()):\n",
    "    print(i, name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c04f3dd-285e-4d70-8dd5-37fc2737ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = torch.randint(0, 1000, [8, 248])\n",
    "sample_attention_mask = torch.randint(0, 1000, [8, 248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25dd4b70-f9d7-4c3a-81d6-da03dd8cf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_out = sample_model.transformer_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a770a11a-485d-49b5-ae8a-9d5dccc90a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "841fcfeb-6e75-40e5-8f82-265ed8da72d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2]), 25, torch.Size([8, 248, 1024]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_out.logits.shape, len(internal_out.hidden_states), internal_out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ded8f5-d2ec-465f-88ca-317bf1954026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_res = sample_model(sample_input_ids, sample_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea66f03e-eac6-478c-ab27-042d97ec1855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1]), torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res[0].shape, sample_res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb86b195-8d45-41e2-9042-7007e416d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.4525,  34.9220,  35.5474,  ...,  17.2497,   3.8168,   4.0134],\n",
       "        [ 16.6691,  -1.8793,  23.6438,  ...,   4.7158,  -0.5189, -31.0638],\n",
       "        [-18.9652,  31.9174,  50.0490,  ...,   0.2722,  27.7940, -27.1529],\n",
       "        ...,\n",
       "        [-34.4472, -19.2569,   3.0118,  ...,  -1.4974, -10.1866,  -8.4745],\n",
       "        [  7.0862,  11.9600,   3.8157,  ...,   0.5911, -23.2350,  10.0487],\n",
       "        [ -4.8602,  18.3438,  -5.1319,  ...,   0.5165,  51.6398,   0.4375]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn([8, 496, 768]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bb67f-bc5f-4f90-8236-7f7eb949ec92",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mse(model, data_loader):\n",
    "    model.eval()\n",
    "    mse_sum = 0\n",
    "    mse_loss = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in enumerate(data_loader):\n",
    "            input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            mse_sum += mse_loss(pred.flatten().cpu(), target.cpu())\n",
    "            \n",
    "    return mse_sum / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b035767-df66-428f-a297-6db704dfc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, record in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input_ids, attention_mask = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE)\n",
    "            pred, _ = model(input_ids, attention_mask)\n",
    "            result.extend(pred.flatten().to(\"cpu\").tolist())\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b90cd468-30bf-4362-824b-480820edb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dl = DataLoader(sample_ds, shuffle=False, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0ec5d-7c5f-4a70-b792-7cb822fb35ce",
   "metadata": {},
   "source": [
    "### Optimizer and Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fd22b6b-dd73-41b1-81a4-af5e3261207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2e-05, 0.0001, 5e-05)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-5 / 2.5, 5e-5 / 0.5, 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c43c63-bdf7-4493-9f76-7b96b4c3f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, base_lr=5e-5, last_lr=None):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    regressor_param_start = 397\n",
    "    attention_param_start = 393\n",
    "    roberta_parameters = named_parameters[:attention_param_start]\n",
    "    attention_parameters = named_parameters[attention_param_start:regressor_param_start]\n",
    "    regressor_parameters = named_parameters[regressor_param_start:]\n",
    "    \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    if last_lr is not None:\n",
    "        parameters.append({\"params\": attention_group, \"lr\": last_lr})\n",
    "        parameters.append({\"params\": regressor_group, \"lr\": last_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "        \n",
    "    # Change on different models\n",
    "    layer_low_threshold = 133\n",
    "    layer_middle_threshold = 261\n",
    "    \n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if 'bias' in name else 0.01\n",
    "        \n",
    "        lr = base_lr / 2.5 # 2e-05\n",
    "        if layer_num >= layer_middle_threshold:\n",
    "            lr = base_lr / 0.5 # 1e-4\n",
    "        elif layer_num >= layer_low_threshold:        \n",
    "            lr = base_lr    \n",
    "            \n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dd255e8-4568-4dfa-abd2-a429f9d545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_optimizer = create_optimizer(sample_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4830178b-dff7-4635-a447-b9da1ca1ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler,SequentialSampler,RandomSampler,SubsetRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de8f75-5e7a-45d0-8029-ea6146ea2b48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eval_period(val_rmse):\n",
    "    for rmse, period in cfg.EVAL_SCHEDULE:\n",
    "        if val_rmse >= rmse:\n",
    "            return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2501f5b3-fffb-42c7-8fcb-9f026d32499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, model, model_path):\n",
    "    if not best_val_rmse or val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        if not model_path.parent.exists():\n",
    "            os.makedirs(model_path.parent)\n",
    "        \n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "    else:       \n",
    "        print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
    "              f\"(from epoch {best_epoch})\")\n",
    "    return best_epoch, best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01766a88-69dc-4c6d-8dca-2950bdc7e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, scaler, model, model_path, train_loader, val_loader, optimizer, scheduler=None, num_epochs=cfg.NUM_EPOCHS):\n",
    "        self.scaler, self.model, self.model_path, self.train_loader, self.val_loader, self.optimizer, self.scheduler, self.num_epochs = (\n",
    "            scaler, model, model_path, train_loader, val_loader, optimizer, scheduler, num_epochs\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        mse_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        best_val_rmse = None\n",
    "        best_epoch = 0\n",
    "        step = 0\n",
    "        last_eval_step = 0\n",
    "        eval_period = cfg.EVAL_SCHEDULE[0][1]    \n",
    "\n",
    "        start = time.time()\n",
    "        val_rmse_list = []\n",
    "        \n",
    "        tbar = tqdm(range(self.num_epochs), total=self.num_epochs)\n",
    "        for epoch in tbar:\n",
    "            tbar.set_description(f'Epoch: {epoch}')\n",
    "            val_rmse = None\n",
    "            for batch_num, record in enumerate(self.train_loader):\n",
    "                input_ids, attention_mask, target = record['input_ids'].to(cfg.DEVICE), record['attention_mask'].to(cfg.DEVICE), record['target'].to(cfg.DEVICE)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Casts operations to mixed precision\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                pred, _ = self.model(input_ids, attention_mask)\n",
    "                mse = mse_loss(pred.flatten(), target)\n",
    "                    \n",
    "#                 self.scaler.scale(mse).backward()\n",
    "#                 self.scaler.step(self.optimizer)\n",
    "#                 self.scaler.update()\n",
    "                \n",
    "                mse.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                    \n",
    "                if step >= last_eval_step + eval_period:\n",
    "                    elapsed_seconds = time.time() - start\n",
    "                    num_steps = step - last_eval_step\n",
    "                    print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                    last_eval_step = step\n",
    "                    \n",
    "                    val_rmse = np.sqrt(eval_mse(self.model, self.val_loader))\n",
    "                    print(f\"Epoch: {epoch} batch_num: {batch_num}\", f\"val_rmse: {val_rmse:0.4} \", end='')\n",
    "                    \n",
    "                    eval_period = choose_eval_period(val_rmse)\n",
    "                    best_epoch, best_val_rmse = serialize_best(best_val_rmse, best_epoch, val_rmse, epoch, self.model, self.model_path)\n",
    "                    val_rmse_list.append(val_rmse)\n",
    "                    start = time.time()\n",
    "                # Finish early on condition\n",
    "                if epoch > 0 and best_val_rmse > 0.6 or (len(val_rmse_list) > 5 and np.array(val_rmse_list).mean() > 1.0):\n",
    "                    return best_val_rmse\n",
    "                \n",
    "                step += 1\n",
    "        return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2692dcf2-a5b7-404f-bb07-3feecb6ec40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=cfg.NUM_FOLDS, random_state=cfg.SEED, shuffle=True)\n",
    "splits = list(kfold.split(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6380179-d1bc-4102-b82f-73b7f8f1c5aa",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61471dbf-6953-4f76-a5ed-ca322f0bc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results\n",
    "# Fold 0: {'base_lr': 6.155021772017101e-05, 'last_lr': 0.004642225106260296}. Best is trial 11 with value: 0.4787840247154236\n",
    "# Fold 1: {'base_lr': 8.902912488113375e-05, 'last_lr': 0.00023076670834499074}. Best is trial 9 with value: 0.447449654340744\n",
    "# Fold 2: {'base_lr': 0.00010637579365513648, 'last_lr': 8.276647346442369e-05}. Best is trial 9 with value: 0.47160205245018005\n",
    "# Fold 3: {'base_lr': 3.543430147790451e-05, 'last_lr': 0.00011412924670673832}. Best is trial 6 with value: 0.46940940618515015\n",
    "# Fold 4: {'base_lr': 4.8817697487830015e-05, 'last_lr': 0.00010249423984922014}. Best is trial 1 with value: 0.48146629333496094\n",
    "# Fold 5: {'base_lr': 8.26551556012735e-05, 'last_lr': 0.00012486898084560538}. Best is trial 1 with value: 0.47751927375793457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1561a06c-a904-4056-8079-ba5cb737567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "fold = 0\n",
    "\n",
    "def objective(trial):\n",
    "    base_lr = trial.suggest_float(\"base_lr\", 3e-5, 5e-4, log=True)\n",
    "    last_lr = trial.suggest_float(\"last_lr\", 8e-5, 5e-3, log=True)\n",
    "    \n",
    "    print(f'##### Using fold {fold}')\n",
    "    print(f'##### Using base_lr {base_lr} last_lr {last_lr}')\n",
    "    \n",
    "    model_path = cfg.MODEL_FOLDER/f\"{cfg.model_name.replace('/', '_')}_{fold + 1}/model_{fold + 1}.pth\"\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    tokenizer = ElectraTokenizer.from_pretrained(cfg.TOKENIZER_PATH)\n",
    "    \n",
    "    train_indices, val_indices = splits[fold]\n",
    "    train_dataset = CommonLitDataset(train_df.loc[train_indices], tokenizer)    \n",
    "    val_dataset = CommonLitDataset(train_df.loc[val_indices], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                              drop_last=False, shuffle=True, num_workers=cfg.NUM_WORKERS)    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    \n",
    "    set_random_seed(cfg.SEED + fold)\n",
    "    \n",
    "    model = CommonLitModel().to(cfg.DEVICE)\n",
    "    \n",
    "    optimizer = create_optimizer(model, base_lr=base_lr, last_lr=last_lr)\n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_training_steps=cfg.NUM_EPOCHS * len(train_loader), \n",
    "                                                num_warmup_steps=50)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    trainer = Trainer(scaler, model, model_path, train_loader, val_loader, optimizer, scheduler = scheduler)\n",
    "    rmse_val = trainer.train()\n",
    "    \n",
    "    del trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del scaler\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "efb49c7b-f2b8-4929-bd03-2b74c20361cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 08:57:08,338]\u001b[0m A new study created in memory with name: no-name-41a4e5f8-98a1-4af4-b3e9-1e7baa3c3054\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 3\n",
      "##### Using base_lr 3.9221729204532446e-05 last_lr 0.0001143213470823608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f22674d19473384e53d89c9d1fa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.067 New best_val_rmse: 1.067\n",
      "\n",
      "16 steps took 11.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7445 New best_val_rmse: 0.7445\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9626 Still best_val_rmse: 0.7445 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6666 New best_val_rmse: 0.6666\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6087 New best_val_rmse: 0.6087\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6204 Still best_val_rmse: 0.6087 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5539 New best_val_rmse: 0.5539\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6022 Still best_val_rmse: 0.5539 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5351 New best_val_rmse: 0.5351\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5432 Still best_val_rmse: 0.5351 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5889 Still best_val_rmse: 0.5351 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5585 Still best_val_rmse: 0.5351 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5739 Still best_val_rmse: 0.5351 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6132 Still best_val_rmse: 0.5351 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6851 Still best_val_rmse: 0.5351 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5023 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5286 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5215 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5152 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4955 Still best_val_rmse: 0.493 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4875 New best_val_rmse: 0.4875\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4844 New best_val_rmse: 0.4844\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4966 Still best_val_rmse: 0.4844 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4955 Still best_val_rmse: 0.4844 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4858 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4847 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4851 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4941 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4925 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4839 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4862 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4883 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4881 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4857 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4836 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.483 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4829 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4829 Still best_val_rmse: 0.4829 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:09:25,558]\u001b[0m Trial 0 finished with value: 0.48292896151542664 and parameters: {'base_lr': 3.9221729204532446e-05, 'last_lr': 0.0001143213470823608}. Best is trial 0 with value: 0.48292896151542664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00033391960782772637 last_lr 0.0005147256938677182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7715fff2415c4080aac0e8106846e024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.257 New best_val_rmse: 1.257\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.069 New best_val_rmse: 1.069\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.044 New best_val_rmse: 1.044\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.042 New best_val_rmse: 1.042\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.046 Still best_val_rmse: 1.042 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.06 Still best_val_rmse: 1.042 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:11:53,157]\u001b[0m Trial 1 finished with value: 1.0417264699935913 and parameters: {'base_lr': 0.00033391960782772637, 'last_lr': 0.0005147256938677182}. Best is trial 0 with value: 0.48292896151542664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 7.885487280888193e-05 last_lr 0.00010156645272416291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f84310f3b914c32b825ce541eb2461e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9799 New best_val_rmse: 0.9799\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7057 New best_val_rmse: 0.7057\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.256 Still best_val_rmse: 0.7057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.055 Still best_val_rmse: 0.7057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.041 Still best_val_rmse: 0.7057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.063 Still best_val_rmse: 0.7057 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:14:20,184]\u001b[0m Trial 2 finished with value: 0.7056781649589539 and parameters: {'base_lr': 7.885487280888193e-05, 'last_lr': 0.00010156645272416291}. Best is trial 0 with value: 0.48292896151542664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00030817536655917727 last_lr 0.0004468268179472102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d801adb6f8246848098af04007a9654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.981 New best_val_rmse: 0.981\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7054 New best_val_rmse: 0.7054\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.042 Still best_val_rmse: 0.7054 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.041 Still best_val_rmse: 0.7054 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.047 Still best_val_rmse: 0.7054 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.055 Still best_val_rmse: 0.7054 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.045 Still best_val_rmse: 0.7054 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.07 Still best_val_rmse: 0.7054 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.041 Still best_val_rmse: 0.7054 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:17:50,555]\u001b[0m Trial 3 finished with value: 0.7053548693656921 and parameters: {'base_lr': 0.00030817536655917727, 'last_lr': 0.0004468268179472102}. Best is trial 0 with value: 0.48292896151542664.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.559812562224263e-05 last_lr 0.0007934980185832123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755d2123234f441eb8ebf8b6d53f9ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.038 New best_val_rmse: 1.038\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7863 New best_val_rmse: 0.7863\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9318 Still best_val_rmse: 0.7863 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7288 New best_val_rmse: 0.7288\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9383 Still best_val_rmse: 0.7288 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6899 New best_val_rmse: 0.6899\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.616 New best_val_rmse: 0.616\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6132 New best_val_rmse: 0.6132\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5723 New best_val_rmse: 0.5723\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5633 New best_val_rmse: 0.5633\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5969 Still best_val_rmse: 0.5633 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5524 New best_val_rmse: 0.5524\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5925 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5741 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6744 Still best_val_rmse: 0.5524 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5319 New best_val_rmse: 0.5319\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5257 New best_val_rmse: 0.5257\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5147 New best_val_rmse: 0.5147\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5197 Still best_val_rmse: 0.5147 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4991 New best_val_rmse: 0.4991\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5066 Still best_val_rmse: 0.4991 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4827 New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5053 Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4799 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4812 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4844 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.483 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4833 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4822 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.481 Still best_val_rmse: 0.4798 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4797 New best_val_rmse: 0.4797\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4803 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4806 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4802 Still best_val_rmse: 0.4797 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4787 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4788 Still best_val_rmse: 0.4787 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:31:23,650]\u001b[0m Trial 4 finished with value: 0.4787239134311676 and parameters: {'base_lr': 6.559812562224263e-05, 'last_lr': 0.0007934980185832123}. Best is trial 4 with value: 0.4787239134311676.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00026967781912501837 last_lr 0.0001016613078384455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc3f74294754780b6e4c7e1e471c273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9488 New best_val_rmse: 0.9488\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.948 New best_val_rmse: 0.948\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.081 Still best_val_rmse: 0.948 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.01 Still best_val_rmse: 0.948 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.041 Still best_val_rmse: 0.948 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.098 Still best_val_rmse: 0.948 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:33:52,609]\u001b[0m Trial 5 finished with value: 0.9480197429656982 and parameters: {'base_lr': 0.00026967781912501837, 'last_lr': 0.0001016613078384455}. Best is trial 4 with value: 0.4787239134311676.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.543430147790451e-05 last_lr 0.00011412924670673832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900a36a910ac4d4b9d7a3c4ebac5c389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.082 New best_val_rmse: 1.082\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7597 New best_val_rmse: 0.7597\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7525 New best_val_rmse: 0.7525\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6422 New best_val_rmse: 0.6422\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5748 New best_val_rmse: 0.5748\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7174 Still best_val_rmse: 0.5748 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5421 New best_val_rmse: 0.5421\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5537 Still best_val_rmse: 0.5421 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5463 Still best_val_rmse: 0.5421 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5767 Still best_val_rmse: 0.5421 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5247 New best_val_rmse: 0.5247\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5365 Still best_val_rmse: 0.5247 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5607 Still best_val_rmse: 0.5247 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5207 New best_val_rmse: 0.5207\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5109 New best_val_rmse: 0.5109\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5034 New best_val_rmse: 0.5034\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5707 Still best_val_rmse: 0.5034 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5018 New best_val_rmse: 0.5018\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.499 New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5008 Still best_val_rmse: 0.499 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4794 New best_val_rmse: 0.4794\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4744 New best_val_rmse: 0.4744\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4734 New best_val_rmse: 0.4734\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4727 New best_val_rmse: 0.4727\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4711 New best_val_rmse: 0.4711\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4696 New best_val_rmse: 0.4696\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4694 New best_val_rmse: 0.4694\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4697 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4721 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4776 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4779 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4761 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4759 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4732 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4704 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4714 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4724 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.472 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4722 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.473 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4734 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4742 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4755 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4766 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4764 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4757 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4751 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4751 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4751 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.475 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4746 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4743 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4734 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4727 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4726 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4726 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4725 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4722 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4718 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4715 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4714 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4713 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4712 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.471 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.471 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n",
      "\n",
      "2 steps took 1.2 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4711 Still best_val_rmse: 0.4694 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 09:51:17,488]\u001b[0m Trial 6 finished with value: 0.46940940618515015 and parameters: {'base_lr': 3.543430147790451e-05, 'last_lr': 0.00011412924670673832}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00023136374344373656 last_lr 0.00034607050791877667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee7144e1cf8447b906051fda271dc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9946 New best_val_rmse: 0.9946\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7824 New best_val_rmse: 0.7824\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.126 Still best_val_rmse: 0.7824 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.122 Still best_val_rmse: 0.7824 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6531 New best_val_rmse: 0.6531\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.77 Still best_val_rmse: 0.6531 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.585 New best_val_rmse: 0.585\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6383 Still best_val_rmse: 0.585 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6341 Still best_val_rmse: 0.585 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6188 Still best_val_rmse: 0.585 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6427 Still best_val_rmse: 0.585 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5659 New best_val_rmse: 0.5659\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5647 New best_val_rmse: 0.5647\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5728 Still best_val_rmse: 0.5647 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.605 Still best_val_rmse: 0.5647 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5667 Still best_val_rmse: 0.5647 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.6153 Still best_val_rmse: 0.5647 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5328 New best_val_rmse: 0.5328\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5329 Still best_val_rmse: 0.5328 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5476 Still best_val_rmse: 0.5328 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5104 New best_val_rmse: 0.5104\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5342 Still best_val_rmse: 0.5104 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5087 New best_val_rmse: 0.5087\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5181 Still best_val_rmse: 0.5087 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5095 Still best_val_rmse: 0.5087 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5065 New best_val_rmse: 0.5065\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5062 New best_val_rmse: 0.5062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:01:10,255]\u001b[0m Trial 7 finished with value: 0.5061682462692261 and parameters: {'base_lr': 0.00023136374344373656, 'last_lr': 0.00034607050791877667}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00033138965366677297 last_lr 9.83992068017567e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36ebaf6d5e14fafa9b394b47a1897ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9691 New best_val_rmse: 0.9691\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7084 New best_val_rmse: 0.7084\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.434 Still best_val_rmse: 0.7084 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.047 Still best_val_rmse: 0.7084 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.059 Still best_val_rmse: 0.7084 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.059 Still best_val_rmse: 0.7084 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:03:38,510]\u001b[0m Trial 8 finished with value: 0.7083932757377625 and parameters: {'base_lr': 0.00033138965366677297, 'last_lr': 9.83992068017567e-05}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00043098128450056675 last_lr 0.0007715512174267046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd31590625940329700e27988982cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.039 New best_val_rmse: 1.039\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.07 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.057 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.046 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.057 Still best_val_rmse: 1.039 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.054 Still best_val_rmse: 1.039 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:06:05,552]\u001b[0m Trial 9 finished with value: 1.0392134189605713 and parameters: {'base_lr': 0.00043098128450056675, 'last_lr': 0.0007715512174267046}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.103082061786509e-05 last_lr 0.004775431900384966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d232c876a00d455c9f87bd188e9b345e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.043 New best_val_rmse: 1.043\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8488 New best_val_rmse: 0.8488\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7942 New best_val_rmse: 0.7942\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7057 New best_val_rmse: 0.7057\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5971 New best_val_rmse: 0.5971\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6341 Still best_val_rmse: 0.5971 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.611 Still best_val_rmse: 0.5971 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6206 Still best_val_rmse: 0.5971 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.564 New best_val_rmse: 0.564\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5243 New best_val_rmse: 0.5243\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5157 New best_val_rmse: 0.5157\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5366 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6049 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.558 Still best_val_rmse: 0.5157 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.496 New best_val_rmse: 0.496\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.4936 New best_val_rmse: 0.4936\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.4981 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.545 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5227 Still best_val_rmse: 0.4936 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4906 New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4869 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4957 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4864 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4902 Still best_val_rmse: 0.4852 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4833 New best_val_rmse: 0.4833\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4848 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.5021 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4883 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.493 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.49 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4874 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4867 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4892 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4908 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4899 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4896 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4883 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.487 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4872 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4878 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4879 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4879 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4878 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4878 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4877 Still best_val_rmse: 0.4833 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4876 Still best_val_rmse: 0.4833 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:18:48,626]\u001b[0m Trial 10 finished with value: 0.4833422005176544 and parameters: {'base_lr': 3.103082061786509e-05, 'last_lr': 0.004775431900384966}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.133337931136716e-05 last_lr 0.001524412460715417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61d7a45a0294edab340194dff7e5759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.002 New best_val_rmse: 1.002\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7004 New best_val_rmse: 0.7004\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7624 Still best_val_rmse: 0.7004 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6621 New best_val_rmse: 0.6621\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.084 Still best_val_rmse: 0.6621 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.073 Still best_val_rmse: 0.6621 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.042 Still best_val_rmse: 0.6621 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.116 Still best_val_rmse: 0.6621 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.049 Still best_val_rmse: 0.6621 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:22:22,386]\u001b[0m Trial 11 finished with value: 0.6621189713478088 and parameters: {'base_lr': 6.133337931136716e-05, 'last_lr': 0.001524412460715417}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 5.337276554246315e-05 last_lr 0.0015170864781839132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81bed990f7145eeb257716f0c69d91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.038 New best_val_rmse: 1.038\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.769 New best_val_rmse: 0.769\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.304 Still best_val_rmse: 0.769 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.054 Still best_val_rmse: 0.769 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.041 Still best_val_rmse: 0.769 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.066 Still best_val_rmse: 0.769 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:24:49,802]\u001b[0m Trial 12 finished with value: 0.7689698934555054 and parameters: {'base_lr': 5.337276554246315e-05, 'last_lr': 0.0015170864781839132}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00011596965301373618 last_lr 0.0002290996985630546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387f61313605467a9d83625bd8180d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9369 New best_val_rmse: 0.9369\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8947 New best_val_rmse: 0.8947\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9847 Still best_val_rmse: 0.8947 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.042 Still best_val_rmse: 0.8947 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.044 Still best_val_rmse: 0.8947 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.048 Still best_val_rmse: 0.8947 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.055 Still best_val_rmse: 0.8947 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:27:38,917]\u001b[0m Trial 13 finished with value: 0.8946634531021118 and parameters: {'base_lr': 0.00011596965301373618, 'last_lr': 0.0002290996985630546}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00010246766833906455 last_lr 0.001134522287708525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd90112ab7b24a5cbfb6416fb0c01f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9247 New best_val_rmse: 0.9247\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8135 New best_val_rmse: 0.8135\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9865 Still best_val_rmse: 0.8135 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7652 New best_val_rmse: 0.7652\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7546 New best_val_rmse: 0.7546\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.9987 Still best_val_rmse: 0.7546 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8334 Still best_val_rmse: 0.7546 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7827 Still best_val_rmse: 0.7546 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7239 New best_val_rmse: 0.7239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:31:12,708]\u001b[0m Trial 14 finished with value: 0.7239358425140381 and parameters: {'base_lr': 0.00010246766833906455, 'last_lr': 0.001134522287708525}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 3.0207803172612474e-05 last_lr 0.0032293506317271343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002497f91acd42eda3c6a3c11dabad73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.055 New best_val_rmse: 1.055\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7023 New best_val_rmse: 0.7023\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9026 Still best_val_rmse: 0.7023 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.63 New best_val_rmse: 0.63\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7403 Still best_val_rmse: 0.63 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6205 New best_val_rmse: 0.6205\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5523 New best_val_rmse: 0.5523\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5841 Still best_val_rmse: 0.5523 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5284 New best_val_rmse: 0.5284\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5678 Still best_val_rmse: 0.5284 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5144 New best_val_rmse: 0.5144\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5506 Still best_val_rmse: 0.5144 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5605 Still best_val_rmse: 0.5144 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5344 Still best_val_rmse: 0.5144 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5353 Still best_val_rmse: 0.5144 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.508 New best_val_rmse: 0.508\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5405 Still best_val_rmse: 0.508 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5277 Still best_val_rmse: 0.508 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4893 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4967 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4865 Still best_val_rmse: 0.4835 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4806 New best_val_rmse: 0.4806\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4862 Still best_val_rmse: 0.4806 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4808 Still best_val_rmse: 0.4806 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4839 Still best_val_rmse: 0.4806 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4815 Still best_val_rmse: 0.4806 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4806 Still best_val_rmse: 0.4806 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4964 Still best_val_rmse: 0.4806 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.479 New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4803 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4805 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4815 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4803 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4828 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4819 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.48 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4805 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4805 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.48 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4795 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4793 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4794 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4795 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4796 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4796 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4796 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4798 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4798 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4799 Still best_val_rmse: 0.479 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:45:56,315]\u001b[0m Trial 15 finished with value: 0.47899168729782104 and parameters: {'base_lr': 3.0207803172612474e-05, 'last_lr': 0.0032293506317271343}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 4.4980120599992625e-05 last_lr 0.00019972895712460457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a80138654c545f48bf59bf3a3fbac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.021 New best_val_rmse: 1.021\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8023 New best_val_rmse: 0.8023\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6983 New best_val_rmse: 0.6983\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6202 New best_val_rmse: 0.6202\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6056 New best_val_rmse: 0.6056\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.61 Still best_val_rmse: 0.6056 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5887 New best_val_rmse: 0.5887\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5796 New best_val_rmse: 0.5796\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5547 New best_val_rmse: 0.5547\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.59 Still best_val_rmse: 0.5547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.589 Still best_val_rmse: 0.5547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6081 Still best_val_rmse: 0.5547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5658 Still best_val_rmse: 0.5547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5951 Still best_val_rmse: 0.5547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.7024 Still best_val_rmse: 0.5547 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5004 New best_val_rmse: 0.5004\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5058 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5086 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5067 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4957 New best_val_rmse: 0.4957\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4888 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.495 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4832 New best_val_rmse: 0.4832\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4837 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4831 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4862 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4906 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4874 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4848 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4825 New best_val_rmse: 0.4825\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4832 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4838 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4835 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4825 Still best_val_rmse: 0.4825 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4821 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4822 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4822 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4822 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4823 Still best_val_rmse: 0.4821 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4823 Still best_val_rmse: 0.4821 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 10:58:31,130]\u001b[0m Trial 16 finished with value: 0.48208609223365784 and parameters: {'base_lr': 4.4980120599992625e-05, 'last_lr': 0.00019972895712460457}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 8.032455013131949e-05 last_lr 0.0007624001479884987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8671e0f38944b9a592701b3eda5ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9964 New best_val_rmse: 0.9964\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6798 New best_val_rmse: 0.6798\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9649 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7542 Still best_val_rmse: 0.6798 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6761 New best_val_rmse: 0.6761\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6685 New best_val_rmse: 0.6685\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6293 New best_val_rmse: 0.6293\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6255 New best_val_rmse: 0.6255\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5605 New best_val_rmse: 0.5605\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.557 New best_val_rmse: 0.557\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5859 Still best_val_rmse: 0.557 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5216 New best_val_rmse: 0.5216\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.59 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6064 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.555 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5395 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.6348 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5918 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5293 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5086 New best_val_rmse: 0.5086\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4979 New best_val_rmse: 0.4979\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.506 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4887 New best_val_rmse: 0.4887\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4924 Still best_val_rmse: 0.4887 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4891 Still best_val_rmse: 0.4887 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4886 New best_val_rmse: 0.4886\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4892 Still best_val_rmse: 0.4886 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4907 Still best_val_rmse: 0.4886 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4897 Still best_val_rmse: 0.4886 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4871 New best_val_rmse: 0.4871\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4899 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4905 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4871 New best_val_rmse: 0.4871\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.486 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.486 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4862 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4862 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4862 Still best_val_rmse: 0.486 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 11:10:14,813]\u001b[0m Trial 17 finished with value: 0.4859916865825653 and parameters: {'base_lr': 8.032455013131949e-05, 'last_lr': 0.0007624001479884987}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 0.00019743414980651842 last_lr 0.00023746132486575692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e7f4cd34c4d81b0d5a0292954b4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.213 New best_val_rmse: 1.213\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.075 New best_val_rmse: 1.075\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.049 New best_val_rmse: 1.049\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.042 New best_val_rmse: 1.042\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.047 Still best_val_rmse: 1.042 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.058 Still best_val_rmse: 1.042 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 11:12:43,149]\u001b[0m Trial 18 finished with value: 1.041556715965271 and parameters: {'base_lr': 0.00019743414980651842, 'last_lr': 0.00023746132486575692}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 3\n",
      "##### Using base_lr 6.516633034520869e-05 last_lr 0.0028155571124699737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6714a4e49bd546f0b81394028f0d2330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9489 New best_val_rmse: 0.9489\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.667 New best_val_rmse: 0.667\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8921 Still best_val_rmse: 0.667 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7166 Still best_val_rmse: 0.667 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7091 Still best_val_rmse: 0.667 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6824 Still best_val_rmse: 0.667 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5797 New best_val_rmse: 0.5797\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6028 Still best_val_rmse: 0.5797 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5815 Still best_val_rmse: 0.5797 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5609 New best_val_rmse: 0.5609\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5517 New best_val_rmse: 0.5517\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5108 New best_val_rmse: 0.5108\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5737 Still best_val_rmse: 0.5108 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5535 Still best_val_rmse: 0.5108 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5313 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.6102 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5607 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5309 Still best_val_rmse: 0.5076 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5075 New best_val_rmse: 0.5075\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.474 New best_val_rmse: 0.474\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4753 Still best_val_rmse: 0.474 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4791 Still best_val_rmse: 0.474 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.474 New best_val_rmse: 0.474\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4737 New best_val_rmse: 0.4737\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4926 Still best_val_rmse: 0.4737 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4732 New best_val_rmse: 0.4732\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4712 New best_val_rmse: 0.4712\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4725 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4744 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4734 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4722 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4723 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4738 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4736 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4734 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4743 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4745 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4733 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4723 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4724 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4728 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4742 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4755 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4771 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4787 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4791 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4773 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.475 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4739 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4734 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4733 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4736 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4736 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4733 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4728 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.52 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4723 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.472 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4718 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4717 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4715 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4715 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4715 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4715 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4716 Still best_val_rmse: 0.4712 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 11:28:51,374]\u001b[0m Trial 19 finished with value: 0.4711739718914032 and parameters: {'base_lr': 6.516633034520869e-05, 'last_lr': 0.0028155571124699737}. Best is trial 6 with value: 0.46940940618515015.\u001b[0m\n",
      "\u001b[32m[I 2021-07-18 11:28:51,376]\u001b[0m A new study created in memory with name: no-name-cfc91bb7-1e04-4dda-9ebe-742427d1e474\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.46940940618515015\n",
      " Best params: \n",
      "    base_lr: 3.543430147790451e-05\n",
      "    last_lr: 0.00011412924670673832\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00024128117424837637 last_lr 0.0003515748371973511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9db819564c24d0193b545bb260df16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8137 New best_val_rmse: 0.8137\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.02 Still best_val_rmse: 0.8137 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.052 Still best_val_rmse: 0.8137 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.206 Still best_val_rmse: 0.8137 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.036 Still best_val_rmse: 0.8137 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.049 Still best_val_rmse: 0.8137 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 11:31:18,737]\u001b[0m Trial 0 finished with value: 0.8137476444244385 and parameters: {'base_lr': 0.00024128117424837637, 'last_lr': 0.0003515748371973511}. Best is trial 0 with value: 0.8137476444244385.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.8817697487830015e-05 last_lr 0.00010249423984922014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30c5652c84f43ff9633f9e0c0983f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9823 New best_val_rmse: 0.9823\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8517 New best_val_rmse: 0.8517\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6583 New best_val_rmse: 0.6583\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6434 New best_val_rmse: 0.6434\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5579 New best_val_rmse: 0.5579\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5827 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7169 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6163 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5809 Still best_val_rmse: 0.5579 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5231 New best_val_rmse: 0.5231\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5425 Still best_val_rmse: 0.5231 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5732 Still best_val_rmse: 0.5231 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5694 Still best_val_rmse: 0.5231 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5087 New best_val_rmse: 0.5087\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5481 Still best_val_rmse: 0.5087 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5109 Still best_val_rmse: 0.5087 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5066 New best_val_rmse: 0.5066\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4976 New best_val_rmse: 0.4976\n",
      "\n",
      "8 steps took 6.92 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5041 Still best_val_rmse: 0.4929 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5258 Still best_val_rmse: 0.4929 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4879 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4946 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 6.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4823 New best_val_rmse: 0.4823\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4842 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4825 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4897 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4834 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4854 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4882 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4855 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.482 New best_val_rmse: 0.482\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4826 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4842 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4835 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4825 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4822 Still best_val_rmse: 0.482 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4818 New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4817 New best_val_rmse: 0.4817\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4815 New best_val_rmse: 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 11:44:01,212]\u001b[0m Trial 1 finished with value: 0.48146629333496094 and parameters: {'base_lr': 4.8817697487830015e-05, 'last_lr': 0.00010249423984922014}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.427661737141705e-05 last_lr 0.002791313003700882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab1ae1f8d5447f7a9f69f0f91175024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9917 New best_val_rmse: 0.9917\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7435 New best_val_rmse: 0.7435\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7171 New best_val_rmse: 0.7171\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6122 New best_val_rmse: 0.6122\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6102 New best_val_rmse: 0.6102\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.629 Still best_val_rmse: 0.6102 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6018 New best_val_rmse: 0.6018\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6367 Still best_val_rmse: 0.6018 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.546 New best_val_rmse: 0.546\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5519 Still best_val_rmse: 0.546 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5146 New best_val_rmse: 0.5146\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5279 Still best_val_rmse: 0.5146 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.4973 New best_val_rmse: 0.4973\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.5081 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5108 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5223 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.517 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4984 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4998 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "8 steps took 6.95 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4946 New best_val_rmse: 0.4946\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4953 Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.5046 Still best_val_rmse: 0.4891 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4864 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4921 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.488 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4898 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4874 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4874 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4864 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4886 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4873 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4859 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4835 New best_val_rmse: 0.4835\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4831 New best_val_rmse: 0.4831\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4833 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4839 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.484 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4834 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4832 Still best_val_rmse: 0.4831 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4831 Still best_val_rmse: 0.483 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 11:57:10,595]\u001b[0m Trial 2 finished with value: 0.48303160071372986 and parameters: {'base_lr': 3.427661737141705e-05, 'last_lr': 0.002791313003700882}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.0001580663961310632 last_lr 0.00011707760956862297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38c63b52cc8420fa6335663dae61d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7088 New best_val_rmse: 0.7088\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7558 Still best_val_rmse: 0.7088 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7592 Still best_val_rmse: 0.7088 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6098 New best_val_rmse: 0.6098\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6278 Still best_val_rmse: 0.6098 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6095 New best_val_rmse: 0.6095\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7028 Still best_val_rmse: 0.6095 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6637 Still best_val_rmse: 0.6095 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.571 New best_val_rmse: 0.571\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5628 New best_val_rmse: 0.5628\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5451 New best_val_rmse: 0.5451\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5644 Still best_val_rmse: 0.5451 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5856 Still best_val_rmse: 0.5451 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5271 New best_val_rmse: 0.5271\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6145 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6233 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5293 Still best_val_rmse: 0.5271 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5216 New best_val_rmse: 0.5216\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5126 New best_val_rmse: 0.5126\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.525 Still best_val_rmse: 0.5126 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.493 New best_val_rmse: 0.493\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.501 Still best_val_rmse: 0.493 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4907 New best_val_rmse: 0.4907\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.496 Still best_val_rmse: 0.4907 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4996 Still best_val_rmse: 0.4907 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4918 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4909 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4886 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4885 Still best_val_rmse: 0.4885 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4881 New best_val_rmse: 0.4881\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4878 New best_val_rmse: 0.4878\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4876 New best_val_rmse: 0.4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:08:37,103]\u001b[0m Trial 3 finished with value: 0.48762640357017517 and parameters: {'base_lr': 0.0001580663961310632, 'last_lr': 0.00011707760956862297}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.162557743081626e-05 last_lr 0.0009448409692747924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b1e5ff982b4cf5a4ab95786a69e1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.09 New best_val_rmse: 1.09\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.745 New best_val_rmse: 0.745\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7289 New best_val_rmse: 0.7289\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6695 New best_val_rmse: 0.6695\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6965 Still best_val_rmse: 0.6695 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6069 New best_val_rmse: 0.6069\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5694 New best_val_rmse: 0.5694\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6167 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6069 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5791 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5275 New best_val_rmse: 0.5275\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5204 New best_val_rmse: 0.5204\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5292 Still best_val_rmse: 0.5204 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5092 New best_val_rmse: 0.5092\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5461 Still best_val_rmse: 0.5092 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5239 Still best_val_rmse: 0.5092 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5374 Still best_val_rmse: 0.5092 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4993 New best_val_rmse: 0.4993\n",
      "\n",
      "8 steps took 6.92 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4965 New best_val_rmse: 0.4965\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.508 Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5196 Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4991 Still best_val_rmse: 0.4965 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4962 New best_val_rmse: 0.4962\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4953 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4995 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4966 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4962 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4946 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4949 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4944 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4944 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4945 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4944 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4944 Still best_val_rmse: 0.494 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:19:38,307]\u001b[0m Trial 4 finished with value: 0.49399229884147644 and parameters: {'base_lr': 3.162557743081626e-05, 'last_lr': 0.0009448409692747924}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00021642273631281925 last_lr 0.0010479854857927283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500acffe8e6f49f69af24ce1ad682678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8109 New best_val_rmse: 0.8109\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9566 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.849 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9185 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.147 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.146 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.079 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.061 Still best_val_rmse: 0.8109 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.078 Still best_val_rmse: 0.8109 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:23:07,633]\u001b[0m Trial 5 finished with value: 0.8109349012374878 and parameters: {'base_lr': 0.00021642273631281925, 'last_lr': 0.0010479854857927283}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 9.479351815983019e-05 last_lr 8.95451137019212e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d8e7ac77834eddbf899118e84e1cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9025 New best_val_rmse: 0.9025\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7665 New best_val_rmse: 0.7665\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6783 New best_val_rmse: 0.6783\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.08 Still best_val_rmse: 0.6783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.04 Still best_val_rmse: 0.6783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.039 Still best_val_rmse: 0.6783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.046 Still best_val_rmse: 0.6783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.052 Still best_val_rmse: 0.6783 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.081 Still best_val_rmse: 0.6783 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:26:40,904]\u001b[0m Trial 6 finished with value: 0.6783124804496765 and parameters: {'base_lr': 9.479351815983019e-05, 'last_lr': 8.95451137019212e-05}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.6298085873028507e-05 last_lr 0.0016829203069565596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed20b389ae749599ef3a42e4effec78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.028 New best_val_rmse: 1.028\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7716 New best_val_rmse: 0.7716\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6459 New best_val_rmse: 0.6459\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7821 Still best_val_rmse: 0.6459 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.621 New best_val_rmse: 0.621\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.608 New best_val_rmse: 0.608\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5681 New best_val_rmse: 0.5681\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6129 Still best_val_rmse: 0.5681 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5699 Still best_val_rmse: 0.5681 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5251 New best_val_rmse: 0.5251\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5497 Still best_val_rmse: 0.5251 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5229 New best_val_rmse: 0.5229\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5704 Still best_val_rmse: 0.5229 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5064 New best_val_rmse: 0.5064\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5787 Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.508 Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5138 Still best_val_rmse: 0.5064 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5001 New best_val_rmse: 0.5001\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5019 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.505 Still best_val_rmse: 0.5001 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4898 New best_val_rmse: 0.4898\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4908 Still best_val_rmse: 0.4898 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4917 Still best_val_rmse: 0.4898 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.489 New best_val_rmse: 0.489\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4958 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4882 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4934 Still best_val_rmse: 0.4872 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4862 New best_val_rmse: 0.4862\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4874 Still best_val_rmse: 0.4862 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4871 Still best_val_rmse: 0.4862 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4863 Still best_val_rmse: 0.4862 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4861 New best_val_rmse: 0.4861\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.486 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.486 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.486 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4859 New best_val_rmse: 0.4859\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4859 New best_val_rmse: 0.4859\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4858 New best_val_rmse: 0.4858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:38:48,063]\u001b[0m Trial 7 finished with value: 0.48582446575164795 and parameters: {'base_lr': 3.6298085873028507e-05, 'last_lr': 0.0016829203069565596}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.578451138142933e-05 last_lr 0.0001888383008819838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5555950152413494a1f5562a9bc7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9531 New best_val_rmse: 0.9531\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8252 New best_val_rmse: 0.8252\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.749 New best_val_rmse: 0.749\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6586 New best_val_rmse: 0.6586\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6777 Still best_val_rmse: 0.6586 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6198 New best_val_rmse: 0.6198\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5951 New best_val_rmse: 0.5951\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5704 New best_val_rmse: 0.5704\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6072 Still best_val_rmse: 0.5704 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5484 New best_val_rmse: 0.5484\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5524 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5716 Still best_val_rmse: 0.5484 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5154 New best_val_rmse: 0.5154\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5268 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5314 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5574 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5451 Still best_val_rmse: 0.5154 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5047 New best_val_rmse: 0.5047\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5239 Still best_val_rmse: 0.5047 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5267 Still best_val_rmse: 0.5047 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5002 Still best_val_rmse: 0.4961 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5003 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.491 New best_val_rmse: 0.491\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4913 Still best_val_rmse: 0.491 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4904 New best_val_rmse: 0.4904\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4892 New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4893 Still best_val_rmse: 0.4891 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4893 Still best_val_rmse: 0.4891 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4892 Still best_val_rmse: 0.4891 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4891 Still best_val_rmse: 0.4891 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.489 New best_val_rmse: 0.489\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4889 New best_val_rmse: 0.4889\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4889 New best_val_rmse: 0.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:49:55,668]\u001b[0m Trial 8 finished with value: 0.48891398310661316 and parameters: {'base_lr': 8.578451138142933e-05, 'last_lr': 0.0001888383008819838}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 6.021074245983651e-05 last_lr 0.00020394351353837165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8662902bb040308405d0f4c6ee7cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9425 New best_val_rmse: 0.9425\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7679 New best_val_rmse: 0.7679\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6585 New best_val_rmse: 0.6585\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6535 New best_val_rmse: 0.6535\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.042 Still best_val_rmse: 0.6535 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6365 New best_val_rmse: 0.6365\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5855 New best_val_rmse: 0.5855\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5863 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6678 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 1.118 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.046 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.042 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.082 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.068 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.053 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.038 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.043 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.038 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.038 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.038 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.039 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.038 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.04 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.042 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.04 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.039 Still best_val_rmse: 0.5855 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.039 Still best_val_rmse: 0.5855 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 12:59:44,998]\u001b[0m Trial 9 finished with value: 0.5855399370193481 and parameters: {'base_lr': 6.021074245983651e-05, 'last_lr': 0.00020394351353837165}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00043303891630465564 last_lr 0.00044239048866307607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68f8892238d41aea2a9974504dd4a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8347 New best_val_rmse: 0.8347\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.238 Still best_val_rmse: 0.8347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.038 Still best_val_rmse: 0.8347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.141 Still best_val_rmse: 0.8347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.042 Still best_val_rmse: 0.8347 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.047 Still best_val_rmse: 0.8347 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 13:02:13,503]\u001b[0m Trial 10 finished with value: 0.8346537947654724 and parameters: {'base_lr': 0.00043303891630465564, 'last_lr': 0.00044239048866307607}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.665885742771091e-05 last_lr 0.00481452069395669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2826c581e2d9429f9ab2e2540dd6965e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9515 New best_val_rmse: 0.9515\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8063 New best_val_rmse: 0.8063\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7532 New best_val_rmse: 0.7532\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6454 New best_val_rmse: 0.6454\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5874 New best_val_rmse: 0.5874\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6142 Still best_val_rmse: 0.5874 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5914 Still best_val_rmse: 0.5874 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6266 Still best_val_rmse: 0.5874 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5624 New best_val_rmse: 0.5624\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5464 New best_val_rmse: 0.5464\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5455 New best_val_rmse: 0.5455\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5329 New best_val_rmse: 0.5329\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5249 New best_val_rmse: 0.5249\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5246 New best_val_rmse: 0.5246\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6823 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5907 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.532 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5291 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5407 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5336 Still best_val_rmse: 0.5246 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5117 New best_val_rmse: 0.5117\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5129 Still best_val_rmse: 0.5117 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5129 Still best_val_rmse: 0.5117 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5057 New best_val_rmse: 0.5057\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5064 Still best_val_rmse: 0.5057 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5057 Still best_val_rmse: 0.5057 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5056 New best_val_rmse: 0.5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 13:12:05,194]\u001b[0m Trial 11 finished with value: 0.5056087970733643 and parameters: {'base_lr': 4.665885742771091e-05, 'last_lr': 0.00481452069395669}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.8986698024001875e-05 last_lr 0.003946584134608693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f9ea909edd47d9ac8f77835f578845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9444 New best_val_rmse: 0.9444\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7973 New best_val_rmse: 0.7973\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7911 New best_val_rmse: 0.7911\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6406 New best_val_rmse: 0.6406\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.689 Still best_val_rmse: 0.6406 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6195 New best_val_rmse: 0.6195\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5951 New best_val_rmse: 0.5951\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6273 Still best_val_rmse: 0.5951 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5632 New best_val_rmse: 0.5632\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5582 New best_val_rmse: 0.5582\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 1.257 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 1.196 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 1.057 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 1.055 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 1.047 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 1.043 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 1.054 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 1.046 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 1.038 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 1.041 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 1.039 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 1.039 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 1.053 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 1.042 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 1.039 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 1.038 Still best_val_rmse: 0.5582 (from epoch 1)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 1.038 Still best_val_rmse: 0.5582 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 13:21:54,322]\u001b[0m Trial 12 finished with value: 0.5581570863723755 and parameters: {'base_lr': 5.8986698024001875e-05, 'last_lr': 0.003946584134608693}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 3.1452237402460025e-05 last_lr 0.00217949357063039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30a9f6ee35a404686b843faf3e9790e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.058 New best_val_rmse: 1.058\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7499 New best_val_rmse: 0.7499\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6526 New best_val_rmse: 0.6526\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6007 New best_val_rmse: 0.6007\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5698 New best_val_rmse: 0.5698\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6025 Still best_val_rmse: 0.5698 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5687 New best_val_rmse: 0.5687\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5763 Still best_val_rmse: 0.5687 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5595 New best_val_rmse: 0.5595\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5938 Still best_val_rmse: 0.5595 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5211 New best_val_rmse: 0.5211\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5497 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5282 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5294 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5438 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.537 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5233 Still best_val_rmse: 0.5211 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4988 New best_val_rmse: 0.4988\n",
      "\n",
      "8 steps took 6.93 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.502 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5123 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5044 Still best_val_rmse: 0.4988 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4967 New best_val_rmse: 0.4967\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4969 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5023 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4955 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.495 Still best_val_rmse: 0.4947 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4945 New best_val_rmse: 0.4945\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4937 New best_val_rmse: 0.4937\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4934 New best_val_rmse: 0.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 13:32:39,485]\u001b[0m Trial 13 finished with value: 0.4934297502040863 and parameters: {'base_lr': 3.1452237402460025e-05, 'last_lr': 0.00217949357063039}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 5.55414698832763e-05 last_lr 0.002841040578159797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15af1de283664edb908b323dcf63e7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9315 New best_val_rmse: 0.9315\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7318 New best_val_rmse: 0.7318\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7928 Still best_val_rmse: 0.7318 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.608 New best_val_rmse: 0.608\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5832 New best_val_rmse: 0.5832\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.607 Still best_val_rmse: 0.5832 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6841 Still best_val_rmse: 0.5832 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6207 Still best_val_rmse: 0.5832 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6326 Still best_val_rmse: 0.5832 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5593 New best_val_rmse: 0.5593\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5421 New best_val_rmse: 0.5421\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5531 Still best_val_rmse: 0.5421 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5313 New best_val_rmse: 0.5313\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5318 Still best_val_rmse: 0.5313 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5483 Still best_val_rmse: 0.5313 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5473 Still best_val_rmse: 0.5313 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.509 New best_val_rmse: 0.509\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4935 New best_val_rmse: 0.4935\n",
      "\n",
      "8 steps took 6.9 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.495 Still best_val_rmse: 0.4935 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5053 Still best_val_rmse: 0.4935 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5195 Still best_val_rmse: 0.4935 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4962 Still best_val_rmse: 0.4935 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4944 Still best_val_rmse: 0.4935 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4905 New best_val_rmse: 0.4905\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4974 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5007 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4955 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4941 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4946 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4922 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4919 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4918 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4915 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4915 Still best_val_rmse: 0.4905 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 13:43:31,649]\u001b[0m Trial 14 finished with value: 0.49052903056144714 and parameters: {'base_lr': 5.55414698832763e-05, 'last_lr': 0.002841040578159797}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 4.2581929842872666e-05 last_lr 0.0007445548325755319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a7749714d74890b0adadf5282f9367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.009 New best_val_rmse: 1.009\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8005 New best_val_rmse: 0.8005\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6706 New best_val_rmse: 0.6706\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.648 New best_val_rmse: 0.648\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7257 Still best_val_rmse: 0.648 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.614 New best_val_rmse: 0.614\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5542 New best_val_rmse: 0.5542\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5711 Still best_val_rmse: 0.5542 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5491 New best_val_rmse: 0.5491\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5576 Still best_val_rmse: 0.5491 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5184 New best_val_rmse: 0.5184\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5334 Still best_val_rmse: 0.5184 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5049 New best_val_rmse: 0.5049\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5207 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5924 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5172 Still best_val_rmse: 0.5049 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5025 New best_val_rmse: 0.5025\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4958 New best_val_rmse: 0.4958\n",
      "\n",
      "8 steps took 6.88 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4914 New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4973 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5051 Still best_val_rmse: 0.4914 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4902 New best_val_rmse: 0.4902\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4892 New best_val_rmse: 0.4892\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5003 Still best_val_rmse: 0.4892 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.496 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4898 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4947 Still best_val_rmse: 0.4884 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4868 New best_val_rmse: 0.4868\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4885 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.49 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4888 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4875 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.487 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4869 Still best_val_rmse: 0.4868 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4868 New best_val_rmse: 0.4868\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4867 New best_val_rmse: 0.4867\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4866 New best_val_rmse: 0.4866\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4864 New best_val_rmse: 0.4864\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4864 New best_val_rmse: 0.4864\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4864 New best_val_rmse: 0.4864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 13:55:41,373]\u001b[0m Trial 15 finished with value: 0.4863857626914978 and parameters: {'base_lr': 4.2581929842872666e-05, 'last_lr': 0.0007445548325755319}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.510029147462861e-05 last_lr 0.0013711985991853187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e44c54fc724c418762984d4f22166b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.932 New best_val_rmse: 0.932\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8573 New best_val_rmse: 0.8573\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6992 New best_val_rmse: 0.6992\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7571 Still best_val_rmse: 0.6992 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7162 Still best_val_rmse: 0.6992 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.665 New best_val_rmse: 0.665\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5699 New best_val_rmse: 0.5699\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5832 Still best_val_rmse: 0.5699 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5507 New best_val_rmse: 0.5507\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6239 Still best_val_rmse: 0.5507 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5264 New best_val_rmse: 0.5264\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5165 New best_val_rmse: 0.5165\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5004 New best_val_rmse: 0.5004\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5675 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5285 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5255 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5035 Still best_val_rmse: 0.5004 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4999 New best_val_rmse: 0.4999\n",
      "\n",
      "8 steps took 6.87 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4946 New best_val_rmse: 0.4946\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4932 New best_val_rmse: 0.4932\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4981 Still best_val_rmse: 0.4932 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5134 Still best_val_rmse: 0.4932 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4884 New best_val_rmse: 0.4884\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4843 New best_val_rmse: 0.4843\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4853 Still best_val_rmse: 0.4843 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5021 Still best_val_rmse: 0.4843 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4836 New best_val_rmse: 0.4836\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4908 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4855 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4875 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4837 Still best_val_rmse: 0.4836 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4844 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4861 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4854 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4839 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4837 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4836 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4832 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4829 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4827 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4825 New best_val_rmse: 0.4825\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4824 New best_val_rmse: 0.4824\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4824 New best_val_rmse: 0.4824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 14:08:09,645]\u001b[0m Trial 16 finished with value: 0.48239296674728394 and parameters: {'base_lr': 8.510029147462861e-05, 'last_lr': 0.0013711985991853187}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 8.596779419507262e-05 last_lr 0.0013630158228118356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea1e4ddda9a4283b7700c303205aff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9393 New best_val_rmse: 0.9393\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7672 New best_val_rmse: 0.7672\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7444 New best_val_rmse: 0.7444\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7974 Still best_val_rmse: 0.7444 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6219 New best_val_rmse: 0.6219\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7832 Still best_val_rmse: 0.6219 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6336 Still best_val_rmse: 0.6219 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.8381 Still best_val_rmse: 0.6219 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.064 Still best_val_rmse: 0.6219 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 14:11:44,145]\u001b[0m Trial 17 finished with value: 0.6219467520713806 and parameters: {'base_lr': 8.596779419507262e-05, 'last_lr': 0.0013630158228118356}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 0.00013525124851816367 last_lr 0.00048456432513444334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265dbeb34c2b4cac8d86d7528cf121ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7681 New best_val_rmse: 0.7681\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7868 Still best_val_rmse: 0.7681 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6911 New best_val_rmse: 0.6911\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7043 Still best_val_rmse: 0.6911 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7421 Still best_val_rmse: 0.6911 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.641 New best_val_rmse: 0.641\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5617 New best_val_rmse: 0.5617\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6488 Still best_val_rmse: 0.5617 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5706 Still best_val_rmse: 0.5617 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5615 New best_val_rmse: 0.5615\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5482 New best_val_rmse: 0.5482\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5996 Still best_val_rmse: 0.5482 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5483 Still best_val_rmse: 0.5482 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.55 Still best_val_rmse: 0.5482 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5805 Still best_val_rmse: 0.5482 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5703 Still best_val_rmse: 0.5482 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5222 New best_val_rmse: 0.5222\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5048 New best_val_rmse: 0.5048\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4982 New best_val_rmse: 0.4982\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.506 Still best_val_rmse: 0.4982 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5025 Still best_val_rmse: 0.4982 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4967 New best_val_rmse: 0.4967\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4878 New best_val_rmse: 0.4878\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4884 Still best_val_rmse: 0.4878 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4894 Still best_val_rmse: 0.4878 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4871 New best_val_rmse: 0.4871\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4899 Still best_val_rmse: 0.4871 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4881 Still best_val_rmse: 0.4865 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4878 Still best_val_rmse: 0.4865 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4879 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4895 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4891 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4877 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4874 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4874 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4873 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4871 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4869 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4867 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4865 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4864 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4863 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4863 Still best_val_rmse: 0.486 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 14:24:10,434]\u001b[0m Trial 18 finished with value: 0.48597991466522217 and parameters: {'base_lr': 0.00013525124851816367, 'last_lr': 0.00048456432513444334}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 4\n",
      "##### Using base_lr 7.526393961835154e-05 last_lr 0.0002398473868424543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773a6b0c66e049c5b63ab588b672e3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9476 New best_val_rmse: 0.9476\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7828 New best_val_rmse: 0.7828\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.678 New best_val_rmse: 0.678\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7538 Still best_val_rmse: 0.678 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6012 New best_val_rmse: 0.6012\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.601 New best_val_rmse: 0.601\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6465 Still best_val_rmse: 0.601 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.569 New best_val_rmse: 0.569\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6048 Still best_val_rmse: 0.569 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5536 New best_val_rmse: 0.5536\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5347 New best_val_rmse: 0.5347\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5733 Still best_val_rmse: 0.5347 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5178 New best_val_rmse: 0.5178\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5231 Still best_val_rmse: 0.5178 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5187 Still best_val_rmse: 0.5178 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5376 Still best_val_rmse: 0.5178 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5141 New best_val_rmse: 0.5141\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4976 New best_val_rmse: 0.4976\n",
      "\n",
      "8 steps took 6.93 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4913 New best_val_rmse: 0.4913\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5078 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5304 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4918 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4942 Still best_val_rmse: 0.4913 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4909 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4906 Still best_val_rmse: 0.4858 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4856 New best_val_rmse: 0.4856\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4959 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4865 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4879 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4913 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4885 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4867 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4862 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.486 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4859 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4857 Still best_val_rmse: 0.4856 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4855 New best_val_rmse: 0.4855\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4851 New best_val_rmse: 0.4851\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.485 New best_val_rmse: 0.485\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.485 New best_val_rmse: 0.485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 14:36:12,667]\u001b[0m Trial 19 finished with value: 0.48502135276794434 and parameters: {'base_lr': 7.526393961835154e-05, 'last_lr': 0.0002398473868424543}. Best is trial 1 with value: 0.48146629333496094.\u001b[0m\n",
      "\u001b[32m[I 2021-07-18 14:36:12,670]\u001b[0m A new study created in memory with name: no-name-6b32fe68-d5e1-4363-9e00-d044ec228817\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.48146629333496094\n",
      " Best params: \n",
      "    base_lr: 4.8817697487830015e-05\n",
      "    last_lr: 0.00010249423984922014\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00034156537880768314 last_lr 0.0005061073297723381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fc483dff194d4b9ddb34d18d78bf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.157 New best_val_rmse: 1.157\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.263 Still best_val_rmse: 1.157 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.142 New best_val_rmse: 1.142\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.019 New best_val_rmse: 1.019\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.025 Still best_val_rmse: 1.019 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.022 Still best_val_rmse: 1.019 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 14:38:40,140]\u001b[0m Trial 0 finished with value: 1.018947720527649 and parameters: {'base_lr': 0.00034156537880768314, 'last_lr': 0.0005061073297723381}. Best is trial 0 with value: 1.018947720527649.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 8.26551556012735e-05 last_lr 0.00012486898084560538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fd2feab8f1495b843e4b104334d061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9287 New best_val_rmse: 0.9287\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7149 New best_val_rmse: 0.7149\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7342 Still best_val_rmse: 0.7149 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6573 New best_val_rmse: 0.6573\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7195 Still best_val_rmse: 0.6573 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5843 New best_val_rmse: 0.5843\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6044 Still best_val_rmse: 0.5843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6077 Still best_val_rmse: 0.5843 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5619 New best_val_rmse: 0.5619\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.558 New best_val_rmse: 0.558\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6652 Still best_val_rmse: 0.558 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5636 Still best_val_rmse: 0.558 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5853 Still best_val_rmse: 0.558 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5151 New best_val_rmse: 0.5151\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5077 New best_val_rmse: 0.5077\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5596 Still best_val_rmse: 0.5077 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5023 New best_val_rmse: 0.5023\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5143 Still best_val_rmse: 0.5023 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4898 New best_val_rmse: 0.4898\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.487 New best_val_rmse: 0.487\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4853 Still best_val_rmse: 0.4849 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.483 New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.485 Still best_val_rmse: 0.483 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4863 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4856 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4891 Still best_val_rmse: 0.4812 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4796 New best_val_rmse: 0.4796\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4819 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4948 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4843 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4817 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4835 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4817 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4854 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4826 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4797 Still best_val_rmse: 0.4796 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4781 New best_val_rmse: 0.4781\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4777 New best_val_rmse: 0.4777\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4776 New best_val_rmse: 0.4776\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4776 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4776 Still best_val_rmse: 0.4776 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4775 New best_val_rmse: 0.4775\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4777 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4779 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4781 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.478 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4779 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4781 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4784 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4784 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4784 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4783 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4782 Still best_val_rmse: 0.4775 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 14:54:06,790]\u001b[0m Trial 1 finished with value: 0.47751927375793457 and parameters: {'base_lr': 8.26551556012735e-05, 'last_lr': 0.00012486898084560538}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 5.07848230852257e-05 last_lr 0.0027807105491802877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd81cdd71d84e9981a154f54df13ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9518 New best_val_rmse: 0.9518\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.829 New best_val_rmse: 0.829\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6824 New best_val_rmse: 0.6824\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7403 Still best_val_rmse: 0.6824 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6534 New best_val_rmse: 0.6534\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6048 New best_val_rmse: 0.6048\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5869 New best_val_rmse: 0.5869\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7119 Still best_val_rmse: 0.5869 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5415 New best_val_rmse: 0.5415\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.557 Still best_val_rmse: 0.5415 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.706 Still best_val_rmse: 0.5415 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6032 Still best_val_rmse: 0.5415 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.7461 Still best_val_rmse: 0.5415 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5422 Still best_val_rmse: 0.5415 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5155 New best_val_rmse: 0.5155\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5335 Still best_val_rmse: 0.5155 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5015 New best_val_rmse: 0.5015\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5128 Still best_val_rmse: 0.5015 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5017 Still best_val_rmse: 0.5015 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.497 New best_val_rmse: 0.497\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5004 Still best_val_rmse: 0.497 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4947 New best_val_rmse: 0.4947\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4911 New best_val_rmse: 0.4911\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5038 Still best_val_rmse: 0.4911 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4899 New best_val_rmse: 0.4899\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.49 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4911 Still best_val_rmse: 0.4894 (from epoch 2)\n",
      "\n",
      "8 steps took 6.06 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4884 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4887 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4887 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4885 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4884 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4884 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4884 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4884 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4885 Still best_val_rmse: 0.4883 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:05:41,799]\u001b[0m Trial 2 finished with value: 0.4883090555667877 and parameters: {'base_lr': 5.07848230852257e-05, 'last_lr': 0.0027807105491802877}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.128338765685475e-05 last_lr 0.0009565802288329959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cdec6fae164327af72aa544bf23902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.8 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.045 New best_val_rmse: 1.045\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9107 New best_val_rmse: 0.9107\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.74 New best_val_rmse: 0.74\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7275 New best_val_rmse: 0.7275\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6066 New best_val_rmse: 0.6066\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6424 Still best_val_rmse: 0.6066 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5525 New best_val_rmse: 0.5525\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5862 Still best_val_rmse: 0.5525 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5512 New best_val_rmse: 0.5512\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5363 New best_val_rmse: 0.5363\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6912 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5451 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.7016 Still best_val_rmse: 0.5363 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5191 New best_val_rmse: 0.5191\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5012 New best_val_rmse: 0.5012\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5136 Still best_val_rmse: 0.5012 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.4967 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.4902 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.49 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.92 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4971 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4953 Still best_val_rmse: 0.4877 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4936 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.501 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4899 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.489 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4886 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4865 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4857 Still best_val_rmse: 0.4846 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4838 New best_val_rmse: 0.4838\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4858 Still best_val_rmse: 0.4838 (from epoch 2)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4854 Still best_val_rmse: 0.4838 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4847 Still best_val_rmse: 0.4838 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4806 New best_val_rmse: 0.4806\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4793 New best_val_rmse: 0.4793\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4792 New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4789 New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4787 New best_val_rmse: 0.4787\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4787 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4789 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4792 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4796 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4801 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4802 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4802 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4799 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4796 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4795 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4794 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4794 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4793 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4793 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4793 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4793 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4793 Still best_val_rmse: 0.4786 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:20:25,301]\u001b[0m Trial 3 finished with value: 0.4785512387752533 and parameters: {'base_lr': 3.128338765685475e-05, 'last_lr': 0.0009565802288329959}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00020214649664270073 last_lr 0.0013066014388123754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d15f3359b6430c86f42bec4ee798fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.819 New best_val_rmse: 0.819\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7113 New best_val_rmse: 0.7113\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7612 Still best_val_rmse: 0.7113 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6803 New best_val_rmse: 0.6803\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6944 Still best_val_rmse: 0.6803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.518 Still best_val_rmse: 0.6803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.064 Still best_val_rmse: 0.6803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.283 Still best_val_rmse: 0.6803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.02 Still best_val_rmse: 0.6803 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:23:58,258]\u001b[0m Trial 4 finished with value: 0.680277407169342 and parameters: {'base_lr': 0.00020214649664270073, 'last_lr': 0.0013066014388123754}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00019280756020195115 last_lr 0.00025209261521705074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1ac9a50efc494aa708b4fe69eaded1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8803 New best_val_rmse: 0.8803\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.247 Still best_val_rmse: 0.8803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.086 Still best_val_rmse: 0.8803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.019 Still best_val_rmse: 0.8803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.027 Still best_val_rmse: 0.8803 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.024 Still best_val_rmse: 0.8803 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:26:25,448]\u001b[0m Trial 5 finished with value: 0.8803056478500366 and parameters: {'base_lr': 0.00019280756020195115, 'last_lr': 0.00025209261521705074}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0003283469741795501 last_lr 0.0005682650127158392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3f07252ea34401be1e811f123f8c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.073 New best_val_rmse: 1.073\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.259 Still best_val_rmse: 1.073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.04 New best_val_rmse: 1.04\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.019 New best_val_rmse: 1.019\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.028 Still best_val_rmse: 1.019 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.022 Still best_val_rmse: 1.019 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:28:52,240]\u001b[0m Trial 6 finished with value: 1.0189990997314453 and parameters: {'base_lr': 0.0003283469741795501, 'last_lr': 0.0005682650127158392}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0003619342745902642 last_lr 0.0002793166004383098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da939fe11b24d8fae2404db4bc0493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7668 New best_val_rmse: 0.7668\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9914 Still best_val_rmse: 0.7668 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.201 Still best_val_rmse: 0.7668 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.023 Still best_val_rmse: 0.7668 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.022 Still best_val_rmse: 0.7668 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.021 Still best_val_rmse: 0.7668 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:31:19,567]\u001b[0m Trial 7 finished with value: 0.7668207287788391 and parameters: {'base_lr': 0.0003619342745902642, 'last_lr': 0.0002793166004383098}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00018475470361487584 last_lr 0.0017538885737424285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d65c7b5eec48658beac6124973e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8872 New best_val_rmse: 0.8872\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8868 New best_val_rmse: 0.8868\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.441 Still best_val_rmse: 0.8868 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.023 Still best_val_rmse: 0.8868 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.04 Still best_val_rmse: 0.8868 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.029 Still best_val_rmse: 0.8868 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:33:48,324]\u001b[0m Trial 8 finished with value: 0.8868013024330139 and parameters: {'base_lr': 0.00018475470361487584, 'last_lr': 0.0017538885737424285}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0002658884716312595 last_lr 0.0002686222191806717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15bd599f9f9411dbf935ea2317df901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7271 New best_val_rmse: 0.7271\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.404 Still best_val_rmse: 0.7271 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.08 Still best_val_rmse: 0.7271 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.019 Still best_val_rmse: 0.7271 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.022 Still best_val_rmse: 0.7271 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.021 Still best_val_rmse: 0.7271 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:36:15,470]\u001b[0m Trial 9 finished with value: 0.7271288633346558 and parameters: {'base_lr': 0.0002658884716312595, 'last_lr': 0.0002686222191806717}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 8.211697552701408e-05 last_lr 8.57661845639057e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5078567686d74ad8b79e0bd969995620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9113 New best_val_rmse: 0.9113\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6853 New best_val_rmse: 0.6853\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7027 Still best_val_rmse: 0.6853 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.681 New best_val_rmse: 0.681\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7108 Still best_val_rmse: 0.681 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7688 Still best_val_rmse: 0.681 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6625 New best_val_rmse: 0.6625\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5848 New best_val_rmse: 0.5848\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.67 Still best_val_rmse: 0.5848 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.595 Still best_val_rmse: 0.5848 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5469 New best_val_rmse: 0.5469\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5595 Still best_val_rmse: 0.5469 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.7403 Still best_val_rmse: 0.5469 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6359 Still best_val_rmse: 0.5469 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5297 New best_val_rmse: 0.5297\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.6296 Still best_val_rmse: 0.5297 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5276 New best_val_rmse: 0.5276\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.515 New best_val_rmse: 0.515\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5081 New best_val_rmse: 0.5081\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5022 New best_val_rmse: 0.5022\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4906 New best_val_rmse: 0.4906\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.494 Still best_val_rmse: 0.4906 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5144 Still best_val_rmse: 0.4906 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5024 Still best_val_rmse: 0.4906 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4945 Still best_val_rmse: 0.4906 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4875 New best_val_rmse: 0.4875\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4876 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4876 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4872 New best_val_rmse: 0.4872\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4872 New best_val_rmse: 0.4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:47:33,212]\u001b[0m Trial 10 finished with value: 0.4871692955493927 and parameters: {'base_lr': 8.211697552701408e-05, 'last_lr': 8.57661845639057e-05}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.5490024609633844e-05 last_lr 0.0001255600179035026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddca7f25fc44e20a28f6cc46b946aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.035 New best_val_rmse: 1.035\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6964 New best_val_rmse: 0.6964\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6703 New best_val_rmse: 0.6703\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6156 New best_val_rmse: 0.6156\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.587 New best_val_rmse: 0.587\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6486 Still best_val_rmse: 0.587 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5548 New best_val_rmse: 0.5548\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5761 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.605 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.572 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6518 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6098 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.7231 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6655 Still best_val_rmse: 0.5548 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5252 New best_val_rmse: 0.5252\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5423 Still best_val_rmse: 0.5252 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5031 New best_val_rmse: 0.5031\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5147 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5164 Still best_val_rmse: 0.5031 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5014 New best_val_rmse: 0.5014\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4997 New best_val_rmse: 0.4997\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4936 Still best_val_rmse: 0.4893 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4978 Still best_val_rmse: 0.4893 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4891 New best_val_rmse: 0.4891\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.489 New best_val_rmse: 0.489\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4892 Still best_val_rmse: 0.489 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4908 Still best_val_rmse: 0.489 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4918 Still best_val_rmse: 0.489 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.488 New best_val_rmse: 0.488\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4869 New best_val_rmse: 0.4869\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4864 New best_val_rmse: 0.4864\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4861 New best_val_rmse: 0.4861\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.486 New best_val_rmse: 0.486\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4861 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4861 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4861 Still best_val_rmse: 0.486 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4861 Still best_val_rmse: 0.486 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 15:59:26,474]\u001b[0m Trial 11 finished with value: 0.4860093891620636 and parameters: {'base_lr': 3.5490024609633844e-05, 'last_lr': 0.0001255600179035026}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 3.015459400919106e-05 last_lr 0.0010162442423036439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9c19b3aad24da088dfa97a797253b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.05 New best_val_rmse: 1.05\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9636 New best_val_rmse: 0.9636\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.715 New best_val_rmse: 0.715\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8584 Still best_val_rmse: 0.715 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6193 New best_val_rmse: 0.6193\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6181 New best_val_rmse: 0.6181\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5551 New best_val_rmse: 0.5551\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7061 Still best_val_rmse: 0.5551 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5866 Still best_val_rmse: 0.5551 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5365 New best_val_rmse: 0.5365\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6599 Still best_val_rmse: 0.5365 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5449 Still best_val_rmse: 0.5365 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6487 Still best_val_rmse: 0.5365 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6027 Still best_val_rmse: 0.5365 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5102 New best_val_rmse: 0.5102\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5624 Still best_val_rmse: 0.5102 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5034 New best_val_rmse: 0.5034\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5178 Still best_val_rmse: 0.5034 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5131 Still best_val_rmse: 0.5034 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5043 Still best_val_rmse: 0.5034 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.502 New best_val_rmse: 0.502\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4946 New best_val_rmse: 0.4946\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5008 Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4946 Still best_val_rmse: 0.4946 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4945 New best_val_rmse: 0.4945\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4969 Still best_val_rmse: 0.4945 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4936 New best_val_rmse: 0.4936\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4928 New best_val_rmse: 0.4928\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4922 New best_val_rmse: 0.4922\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4919 New best_val_rmse: 0.4919\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4918 New best_val_rmse: 0.4918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:10:01,279]\u001b[0m Trial 12 finished with value: 0.4918219745159149 and parameters: {'base_lr': 3.015459400919106e-05, 'last_lr': 0.0010162442423036439}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 8.48434209883237e-05 last_lr 0.0037678206336956926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e8ac2539524004893c1c58475a4083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9444 New best_val_rmse: 0.9444\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7445 New best_val_rmse: 0.7445\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.009 Still best_val_rmse: 0.7445 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6438 New best_val_rmse: 0.6438\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6251 New best_val_rmse: 0.6251\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.9223 Still best_val_rmse: 0.6251 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7759 Still best_val_rmse: 0.6251 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.077 Still best_val_rmse: 0.6251 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.058 Still best_val_rmse: 0.6251 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:13:35,166]\u001b[0m Trial 13 finished with value: 0.6251128911972046 and parameters: {'base_lr': 8.48434209883237e-05, 'last_lr': 0.0037678206336956926}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 5.039073444083897e-05 last_lr 0.00013636397981993983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426d89280d342cdae79f87513816e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9854 New best_val_rmse: 0.9854\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6976 New best_val_rmse: 0.6976\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7385 Still best_val_rmse: 0.6976 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7511 Still best_val_rmse: 0.6976 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6033 New best_val_rmse: 0.6033\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6509 Still best_val_rmse: 0.6033 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5784 New best_val_rmse: 0.5784\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5439 New best_val_rmse: 0.5439\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5787 Still best_val_rmse: 0.5439 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5727 Still best_val_rmse: 0.5439 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6859 Still best_val_rmse: 0.5439 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5953 Still best_val_rmse: 0.5439 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.7085 Still best_val_rmse: 0.5439 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6276 Still best_val_rmse: 0.5439 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5256 New best_val_rmse: 0.5256\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5673 Still best_val_rmse: 0.5256 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.506 New best_val_rmse: 0.506\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5205 Still best_val_rmse: 0.506 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5146 Still best_val_rmse: 0.506 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.51 Still best_val_rmse: 0.506 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4899 New best_val_rmse: 0.4899\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4901 Still best_val_rmse: 0.4899 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4877 New best_val_rmse: 0.4877\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4889 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4965 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5007 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4888 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4908 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4908 Still best_val_rmse: 0.4877 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4865 New best_val_rmse: 0.4865\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4852 New best_val_rmse: 0.4852\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4849 New best_val_rmse: 0.4849\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4848 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4848 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4848 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4848 Still best_val_rmse: 0.4847 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:25:17,832]\u001b[0m Trial 14 finished with value: 0.4847445785999298 and parameters: {'base_lr': 5.039073444083897e-05, 'last_lr': 0.00013636397981993983}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00010922317894753754 last_lr 0.0008560933625476431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cb77995cca43a1bd739fd4c5c9133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9468 New best_val_rmse: 0.9468\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8095 New best_val_rmse: 0.8095\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7774 New best_val_rmse: 0.7774\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.628 New best_val_rmse: 0.628\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7425 Still best_val_rmse: 0.628 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6998 Still best_val_rmse: 0.628 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5567 New best_val_rmse: 0.5567\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.717 Still best_val_rmse: 0.5567 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6476 Still best_val_rmse: 0.5567 (from epoch 0)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.567 Still best_val_rmse: 0.5567 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.57 Still best_val_rmse: 0.5567 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5441 New best_val_rmse: 0.5441\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6203 Still best_val_rmse: 0.5441 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5061 New best_val_rmse: 0.5061\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5003 New best_val_rmse: 0.5003\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5229 Still best_val_rmse: 0.5003 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5023 Still best_val_rmse: 0.5003 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5037 Still best_val_rmse: 0.5003 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4944 New best_val_rmse: 0.4944\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4848 New best_val_rmse: 0.4848\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4856 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.487 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4878 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4873 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4869 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4857 Still best_val_rmse: 0.4848 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4816 New best_val_rmse: 0.4816\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4812 New best_val_rmse: 0.4812\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4795 New best_val_rmse: 0.4795\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4786 New best_val_rmse: 0.4786\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.479 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4798 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4816 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.493 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4831 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4865 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4836 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4819 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4806 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4799 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4798 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4789 Still best_val_rmse: 0.4786 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4785 New best_val_rmse: 0.4785\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4782 New best_val_rmse: 0.4782\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4779 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4779 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4782 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4785 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4786 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4787 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4787 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4787 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4789 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4791 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4791 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4792 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4792 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4791 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.479 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.479 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.479 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.479 Still best_val_rmse: 0.4779 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.479 Still best_val_rmse: 0.4779 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:40:50,962]\u001b[0m Trial 15 finished with value: 0.4778694808483124 and parameters: {'base_lr': 0.00010922317894753754, 'last_lr': 0.0008560933625476431}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00011114433176355297 last_lr 0.0004177483097837711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a446c6e96fc4e21a6615ae0181618a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9009 New best_val_rmse: 0.9009\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7354 New best_val_rmse: 0.7354\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8748 Still best_val_rmse: 0.7354 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.023 Still best_val_rmse: 0.7354 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.058 Still best_val_rmse: 0.7354 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.024 Still best_val_rmse: 0.7354 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.02 Still best_val_rmse: 0.7354 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.094 Still best_val_rmse: 0.7354 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.019 Still best_val_rmse: 0.7354 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:44:24,049]\u001b[0m Trial 16 finished with value: 0.7354328036308289 and parameters: {'base_lr': 0.00011114433176355297, 'last_lr': 0.0004177483097837711}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.0001243332575249866 last_lr 0.0007661593158737323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c3a1c969b4877a59c39f139baec7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9477 New best_val_rmse: 0.9477\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.063 Still best_val_rmse: 0.9477 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.048 Still best_val_rmse: 0.9477 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.02 Still best_val_rmse: 0.9477 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.024 Still best_val_rmse: 0.9477 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.021 Still best_val_rmse: 0.9477 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:46:50,923]\u001b[0m Trial 17 finished with value: 0.9476915597915649 and parameters: {'base_lr': 0.0001243332575249866, 'last_lr': 0.0007661593158737323}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 6.389127063538818e-05 last_lr 0.0020876227287264706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9312fe74c40f4f0f9ca00cec9ed3dc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9429 New best_val_rmse: 0.9429\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.74 New best_val_rmse: 0.74\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7617 Still best_val_rmse: 0.74 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7081 New best_val_rmse: 0.7081\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.84 Still best_val_rmse: 0.7081 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6406 New best_val_rmse: 0.6406\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5873 New best_val_rmse: 0.5873\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7791 Still best_val_rmse: 0.5873 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5594 New best_val_rmse: 0.5594\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5865 Still best_val_rmse: 0.5594 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.7357 Still best_val_rmse: 0.5594 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5816 Still best_val_rmse: 0.5594 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6443 Still best_val_rmse: 0.5594 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5759 Still best_val_rmse: 0.5594 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.517 New best_val_rmse: 0.517\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5701 Still best_val_rmse: 0.517 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5003 New best_val_rmse: 0.5003\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4973 New best_val_rmse: 0.4973\n",
      "\n",
      "8 steps took 6.95 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5026 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5069 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.5035 Still best_val_rmse: 0.4973 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4968 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5021 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4947 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4957 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4929 New best_val_rmse: 0.4929\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4917 New best_val_rmse: 0.4917\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4917 New best_val_rmse: 0.4917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 16:57:33,951]\u001b[0m Trial 18 finished with value: 0.49168142676353455 and parameters: {'base_lr': 6.389127063538818e-05, 'last_lr': 0.0020876227287264706}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 5\n",
      "##### Using base_lr 0.00013796165088360738 last_lr 0.0001683495630041909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71521123dd574628b84bcb402ee90a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9098 New best_val_rmse: 0.9098\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.203 Still best_val_rmse: 0.9098 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.04 Still best_val_rmse: 0.9098 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.02 Still best_val_rmse: 0.9098 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.023 Still best_val_rmse: 0.9098 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.023 Still best_val_rmse: 0.9098 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:00:01,626]\u001b[0m Trial 19 finished with value: 0.9098045825958252 and parameters: {'base_lr': 0.00013796165088360738, 'last_lr': 0.0001683495630041909}. Best is trial 1 with value: 0.47751927375793457.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.47751927375793457\n",
      " Best params: \n",
      "    base_lr: 8.26551556012735e-05\n",
      "    last_lr: 0.00012486898084560538\n",
      "CPU times: user 6h 14min 46s, sys: 1h 35min 34s, total: 7h 50min 20s\n",
      "Wall time: 8h 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(3, len(list(splits))):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48210ee3-d9ea-4bab-b852-627f6f75ce0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:00:01,637]\u001b[0m A new study created in memory with name: no-name-af99b6bb-7317-45de-acd2-202575794b30\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.0004639416902680064 last_lr 0.004754676740356026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946a7cebc2d04621b00baa740bd0160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.033 New best_val_rmse: 1.033\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.309 Still best_val_rmse: 1.033 (from epoch 0)\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.083 Still best_val_rmse: 1.033 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.292 Still best_val_rmse: 1.033 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.046 Still best_val_rmse: 1.033 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.034 Still best_val_rmse: 1.033 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:02:28,049]\u001b[0m Trial 0 finished with value: 1.0334450006484985 and parameters: {'base_lr': 0.0004639416902680064, 'last_lr': 0.004754676740356026}. Best is trial 0 with value: 1.0334450006484985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00026780595944539176 last_lr 0.0004427113015418867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9de9c6d24c47a1bd807411b9daf022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.08 New best_val_rmse: 1.08\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.041 New best_val_rmse: 1.041\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.277 Still best_val_rmse: 1.041 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.085 Still best_val_rmse: 1.041 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.042 Still best_val_rmse: 1.041 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.043 Still best_val_rmse: 1.041 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:04:54,858]\u001b[0m Trial 1 finished with value: 1.0410969257354736 and parameters: {'base_lr': 0.00026780595944539176, 'last_lr': 0.0004427113015418867}. Best is trial 0 with value: 1.0334450006484985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00048418508239822506 last_lr 0.0001679007782255595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0a1b9006a64950bd118e05b15ad501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.581 New best_val_rmse: 1.581\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.057 New best_val_rmse: 1.057\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.115 Still best_val_rmse: 1.057 (from epoch 0)\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.136 Still best_val_rmse: 1.057 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.034 New best_val_rmse: 1.034\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.037 Still best_val_rmse: 1.034 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:07:22,838]\u001b[0m Trial 2 finished with value: 1.0341919660568237 and parameters: {'base_lr': 0.00048418508239822506, 'last_lr': 0.0001679007782255595}. Best is trial 0 with value: 1.0334450006484985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00017152055749308505 last_lr 8.979830270843491e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faea0762f7dd48b4a61af473c8b0c7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.009 New best_val_rmse: 1.009\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.041 Still best_val_rmse: 1.009 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.1 Still best_val_rmse: 1.009 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.069 Still best_val_rmse: 1.009 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.037 Still best_val_rmse: 1.009 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.045 Still best_val_rmse: 1.009 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:09:50,035]\u001b[0m Trial 3 finished with value: 1.0085803270339966 and parameters: {'base_lr': 0.00017152055749308505, 'last_lr': 8.979830270843491e-05}. Best is trial 3 with value: 1.0085803270339966.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.684280497803618e-05 last_lr 0.001455401689185026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07d4b82256744c3ac4b94b04dde6022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9898 New best_val_rmse: 0.9898\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.911 New best_val_rmse: 0.911\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6758 New best_val_rmse: 0.6758\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6435 New best_val_rmse: 0.6435\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.751 Still best_val_rmse: 0.6435 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.047 Still best_val_rmse: 0.6435 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7194 Still best_val_rmse: 0.6435 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7206 Still best_val_rmse: 0.6435 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6043 New best_val_rmse: 0.6043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:13:23,402]\u001b[0m Trial 4 finished with value: 0.6043326258659363 and parameters: {'base_lr': 3.684280497803618e-05, 'last_lr': 0.001455401689185026}. Best is trial 4 with value: 0.6043326258659363.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 6.646113844445599e-05 last_lr 0.000990784832291025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbd619039dc4799adc1209dd0e96fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8628 New best_val_rmse: 0.8628\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7718 New best_val_rmse: 0.7718\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.008 Still best_val_rmse: 0.7718 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7548 New best_val_rmse: 0.7548\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6864 New best_val_rmse: 0.6864\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7752 Still best_val_rmse: 0.6864 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6081 New best_val_rmse: 0.6081\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5984 New best_val_rmse: 0.5984\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.7358 Still best_val_rmse: 0.5984 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5707 New best_val_rmse: 0.5707\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5461 New best_val_rmse: 0.5461\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6897 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.6633 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.7093 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5672 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5798 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5483 Still best_val_rmse: 0.5461 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5373 New best_val_rmse: 0.5373\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5329 New best_val_rmse: 0.5329\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5015 New best_val_rmse: 0.5015\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5213 Still best_val_rmse: 0.5015 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4964 New best_val_rmse: 0.4964\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5042 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.499 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4985 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4992 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4985 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "8 steps took 6.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4983 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4982 Still best_val_rmse: 0.4964 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4982 Still best_val_rmse: 0.4964 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:23:49,350]\u001b[0m Trial 5 finished with value: 0.4964250326156616 and parameters: {'base_lr': 6.646113844445599e-05, 'last_lr': 0.000990784832291025}. Best is trial 5 with value: 0.4964250326156616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.0002827982851197117 last_lr 0.00014207261282089427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca032c6be6254fed810f74273332fb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.16 New best_val_rmse: 1.16\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.07 New best_val_rmse: 1.07\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.077 Still best_val_rmse: 1.07 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.089 Still best_val_rmse: 1.07 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.038 New best_val_rmse: 1.038\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.042 Still best_val_rmse: 1.038 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:26:17,653]\u001b[0m Trial 6 finished with value: 1.0377686023712158 and parameters: {'base_lr': 0.0002827982851197117, 'last_lr': 0.00014207261282089427}. Best is trial 5 with value: 0.4964250326156616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 0.00012703529957781474 last_lr 0.00011717300579650607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9e5ad3fb9c4ebea6d117576a4a7094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9501 New best_val_rmse: 0.9501\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.004 Still best_val_rmse: 0.9501 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.06 Still best_val_rmse: 0.9501 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.191 Still best_val_rmse: 0.9501 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.036 Still best_val_rmse: 0.9501 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.041 Still best_val_rmse: 0.9501 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:28:44,534]\u001b[0m Trial 7 finished with value: 0.950054943561554 and parameters: {'base_lr': 0.00012703529957781474, 'last_lr': 0.00011717300579650607}. Best is trial 5 with value: 0.4964250326156616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.3423606764108414e-05 last_lr 0.00015748526576262877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa62af4738c45458944b77859a9a4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.032 New best_val_rmse: 1.032\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8521 New best_val_rmse: 0.8521\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6398 New best_val_rmse: 0.6398\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6169 New best_val_rmse: 0.6169\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6504 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8885 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6575 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6632 Still best_val_rmse: 0.6169 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6192 Still best_val_rmse: 0.6169 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:32:19,590]\u001b[0m Trial 8 finished with value: 0.6168890595436096 and parameters: {'base_lr': 3.3423606764108414e-05, 'last_lr': 0.00015748526576262877}. Best is trial 5 with value: 0.4964250326156616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 9.109929231927102e-05 last_lr 0.0011340424636788516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c32d6d9c144dfcb0bcc436e34b6f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8718 New best_val_rmse: 0.8718\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9891 Still best_val_rmse: 0.8718 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7633 New best_val_rmse: 0.7633\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.013 Still best_val_rmse: 0.7633 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.08 Still best_val_rmse: 0.7633 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.118 Still best_val_rmse: 0.7633 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.108 Still best_val_rmse: 0.7633 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.037 Still best_val_rmse: 0.7633 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.038 Still best_val_rmse: 0.7633 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:35:48,443]\u001b[0m Trial 9 finished with value: 0.763325572013855 and parameters: {'base_lr': 9.109929231927102e-05, 'last_lr': 0.0011340424636788516}. Best is trial 5 with value: 0.4964250326156616.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 6.2968821405157e-05 last_lr 0.003659047748949041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b387d44a28314ba19d8c8e33318f1e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9379 New best_val_rmse: 0.9379\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.025 Still best_val_rmse: 0.9379 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8499 New best_val_rmse: 0.8499\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6561 New best_val_rmse: 0.6561\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8844 Still best_val_rmse: 0.6561 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.795 Still best_val_rmse: 0.6561 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6214 New best_val_rmse: 0.6214\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5936 New best_val_rmse: 0.5936\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8482 Still best_val_rmse: 0.5936 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5645 New best_val_rmse: 0.5645\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.6592 Still best_val_rmse: 0.5645 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5682 Still best_val_rmse: 0.5645 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5192 New best_val_rmse: 0.5192\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5663 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5302 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5319 Still best_val_rmse: 0.5192 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5146 New best_val_rmse: 0.5146\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5208 Still best_val_rmse: 0.5146 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5094 New best_val_rmse: 0.5094\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5006 New best_val_rmse: 0.5006\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4902 New best_val_rmse: 0.4902\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.5043 Still best_val_rmse: 0.4902 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.5033 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4965 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4901 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4892 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4955 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4876 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4858 New best_val_rmse: 0.4858\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4857 New best_val_rmse: 0.4857\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4864 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4866 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4866 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4865 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4865 Still best_val_rmse: 0.4857 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4865 Still best_val_rmse: 0.4857 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 17:47:06,198]\u001b[0m Trial 10 finished with value: 0.4857335090637207 and parameters: {'base_lr': 6.2968821405157e-05, 'last_lr': 0.003659047748949041}. Best is trial 10 with value: 0.4857335090637207.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 6.155021772017101e-05 last_lr 0.004642225106260296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88d092697b3421195bfadfefa77fbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9218 New best_val_rmse: 0.9218\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9639 Still best_val_rmse: 0.9218 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7486 New best_val_rmse: 0.7486\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6446 New best_val_rmse: 0.6446\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6073 New best_val_rmse: 0.6073\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6321 Still best_val_rmse: 0.6073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5637 New best_val_rmse: 0.5637\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5644 Still best_val_rmse: 0.5637 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.655 Still best_val_rmse: 0.5637 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5554 New best_val_rmse: 0.5554\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5581 Still best_val_rmse: 0.5554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5946 Still best_val_rmse: 0.5554 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5407 New best_val_rmse: 0.5407\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5513 Still best_val_rmse: 0.5407 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5351 New best_val_rmse: 0.5351\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5195 New best_val_rmse: 0.5195\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5029 New best_val_rmse: 0.5029\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5181 Still best_val_rmse: 0.5029 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4979 New best_val_rmse: 0.4979\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5099 Still best_val_rmse: 0.4979 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4951 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4947 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.5025 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4907 Still best_val_rmse: 0.4883 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4802 New best_val_rmse: 0.4802\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4807 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4821 Still best_val_rmse: 0.4802 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4798 New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4788 New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4791 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4791 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4789 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4797 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4815 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4846 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4834 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4806 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4798 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4797 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4797 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4798 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.48 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4801 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4804 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4804 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4803 Still best_val_rmse: 0.4788 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:00:06,404]\u001b[0m Trial 11 finished with value: 0.4787840247154236 and parameters: {'base_lr': 6.155021772017101e-05, 'last_lr': 0.004642225106260296}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 5.616824860695961e-05 last_lr 0.004562971767435031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15af402c9f5c4995bdf0384fb05a2f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9442 New best_val_rmse: 0.9442\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.101 Still best_val_rmse: 0.9442 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7399 New best_val_rmse: 0.7399\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6475 New best_val_rmse: 0.6475\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7218 Still best_val_rmse: 0.6475 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6749 Still best_val_rmse: 0.6475 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6236 New best_val_rmse: 0.6236\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5901 New best_val_rmse: 0.5901\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.8255 Still best_val_rmse: 0.5901 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5844 New best_val_rmse: 0.5844\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5384 New best_val_rmse: 0.5384\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6141 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5558 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5698 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6814 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5424 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5559 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.544 Still best_val_rmse: 0.5384 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5366 New best_val_rmse: 0.5366\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5279 New best_val_rmse: 0.5279\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5161 New best_val_rmse: 0.5161\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.531 Still best_val_rmse: 0.5161 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5169 Still best_val_rmse: 0.5161 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5196 Still best_val_rmse: 0.5161 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5164 Still best_val_rmse: 0.5161 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5183 Still best_val_rmse: 0.5161 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.517 Still best_val_rmse: 0.5161 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:09:57,725]\u001b[0m Trial 12 finished with value: 0.5160984992980957 and parameters: {'base_lr': 5.616824860695961e-05, 'last_lr': 0.004562971767435031}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 5.2915478433349325e-05 last_lr 0.00372059827950533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c200e96d0934d7ba98605240dae1534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9706 New best_val_rmse: 0.9706\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9569 New best_val_rmse: 0.9569\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7064 New best_val_rmse: 0.7064\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7697 Still best_val_rmse: 0.7064 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6719 New best_val_rmse: 0.6719\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6637 New best_val_rmse: 0.6637\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6762 Still best_val_rmse: 0.6637 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6094 New best_val_rmse: 0.6094\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.775 Still best_val_rmse: 0.6094 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:13:33,042]\u001b[0m Trial 13 finished with value: 0.6094065308570862 and parameters: {'base_lr': 5.2915478433349325e-05, 'last_lr': 0.00372059827950533}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 8.522296358704523e-05 last_lr 0.0023865771027824435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df5af863c940a28d2324bd4d7390ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9054 New best_val_rmse: 0.9054\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8695 New best_val_rmse: 0.8695\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8397 New best_val_rmse: 0.8397\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7787 New best_val_rmse: 0.7787\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6336 New best_val_rmse: 0.6336\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6978 Still best_val_rmse: 0.6336 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6725 Still best_val_rmse: 0.6336 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7524 Still best_val_rmse: 0.6336 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.796 Still best_val_rmse: 0.6336 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:17:06,738]\u001b[0m Trial 14 finished with value: 0.6335976719856262 and parameters: {'base_lr': 8.522296358704523e-05, 'last_lr': 0.0023865771027824435}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.781960529449164e-05 last_lr 0.0023509276681760484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4195354299848299f5e28d8af1556a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.984 New best_val_rmse: 0.984\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8348 New best_val_rmse: 0.8348\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8025 New best_val_rmse: 0.8025\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9103 Still best_val_rmse: 0.8025 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6157 New best_val_rmse: 0.6157\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.673 Still best_val_rmse: 0.6157 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5732 New best_val_rmse: 0.5732\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.57 New best_val_rmse: 0.57\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6999 Still best_val_rmse: 0.57 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5672 New best_val_rmse: 0.5672\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5534 New best_val_rmse: 0.5534\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5679 Still best_val_rmse: 0.5534 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5749 Still best_val_rmse: 0.5534 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.594 Still best_val_rmse: 0.5534 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6499 Still best_val_rmse: 0.5534 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5234 New best_val_rmse: 0.5234\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5245 Still best_val_rmse: 0.5234 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5565 Still best_val_rmse: 0.5234 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5322 Still best_val_rmse: 0.5234 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5122 New best_val_rmse: 0.5122\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5021 New best_val_rmse: 0.5021\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5091 Still best_val_rmse: 0.5021 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5025 Still best_val_rmse: 0.5021 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4987 New best_val_rmse: 0.4987\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4991 Still best_val_rmse: 0.4987 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4986 New best_val_rmse: 0.4986\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4988 Still best_val_rmse: 0.4986 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4975 New best_val_rmse: 0.4975\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4976 Still best_val_rmse: 0.4975 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4976 Still best_val_rmse: 0.4975 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4977 Still best_val_rmse: 0.4975 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:27:33,519]\u001b[0m Trial 15 finished with value: 0.49745792150497437 and parameters: {'base_lr': 4.781960529449164e-05, 'last_lr': 0.0023509276681760484}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 8.768357562742521e-05 last_lr 0.0004004954918230132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e510c4cbb394541a1482c52a228f3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8625 New best_val_rmse: 0.8625\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.906 Still best_val_rmse: 0.8625 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8937 Still best_val_rmse: 0.8625 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7067 New best_val_rmse: 0.7067\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.654 New best_val_rmse: 0.654\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6079 New best_val_rmse: 0.6079\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5513 New best_val_rmse: 0.5513\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5738 Still best_val_rmse: 0.5513 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.835 Still best_val_rmse: 0.5513 (from epoch 0)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.561 Still best_val_rmse: 0.5513 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5465 New best_val_rmse: 0.5465\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.6207 Still best_val_rmse: 0.5465 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5546 Still best_val_rmse: 0.5465 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6224 Still best_val_rmse: 0.5465 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.6827 Still best_val_rmse: 0.5465 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5272 New best_val_rmse: 0.5272\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.529 Still best_val_rmse: 0.5272 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.551 Still best_val_rmse: 0.5272 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5203 New best_val_rmse: 0.5203\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5285 Still best_val_rmse: 0.5203 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5178 New best_val_rmse: 0.5178\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5285 Still best_val_rmse: 0.5178 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.52 Still best_val_rmse: 0.5178 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5163 New best_val_rmse: 0.5163\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5211 Still best_val_rmse: 0.5163 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5192 Still best_val_rmse: 0.5163 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.5173 Still best_val_rmse: 0.5163 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:37:27,281]\u001b[0m Trial 16 finished with value: 0.5162623524665833 and parameters: {'base_lr': 8.768357562742521e-05, 'last_lr': 0.0004004954918230132}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 3.10563052114089e-05 last_lr 0.002255757409956735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4e19d5a1d848fdbeb640112e9497bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.0 New best_val_rmse: 1.0\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9558 New best_val_rmse: 0.9558\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6664 New best_val_rmse: 0.6664\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6105 New best_val_rmse: 0.6105\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8823 Still best_val_rmse: 0.6105 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.9202 Still best_val_rmse: 0.6105 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6787 Still best_val_rmse: 0.6105 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6385 Still best_val_rmse: 0.6105 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5709 New best_val_rmse: 0.5709\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.557 New best_val_rmse: 0.557\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5675 Still best_val_rmse: 0.557 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5868 Still best_val_rmse: 0.557 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5936 Still best_val_rmse: 0.557 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6874 Still best_val_rmse: 0.557 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5628 Still best_val_rmse: 0.557 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5193 New best_val_rmse: 0.5193\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.524 Still best_val_rmse: 0.5193 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5405 Still best_val_rmse: 0.5193 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5365 Still best_val_rmse: 0.5193 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5176 New best_val_rmse: 0.5176\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.5221 Still best_val_rmse: 0.5176 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5408 Still best_val_rmse: 0.5176 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.5186 Still best_val_rmse: 0.5176 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.5167 New best_val_rmse: 0.5167\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.5139 New best_val_rmse: 0.5139\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.5148 Still best_val_rmse: 0.5139 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.515 Still best_val_rmse: 0.5139 (from epoch 2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:47:19,161]\u001b[0m Trial 17 finished with value: 0.513886034488678 and parameters: {'base_lr': 3.10563052114089e-05, 'last_lr': 0.002255757409956735}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 0\n",
      "##### Using base_lr 0.00014923663057185219 last_lr 0.0036700968666363615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d750620f1a4e97bf56c4ddf1ab1a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8881 New best_val_rmse: 0.8881\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8841 New best_val_rmse: 0.8841\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.182 Still best_val_rmse: 0.8841 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.226 Still best_val_rmse: 0.8841 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.048 Still best_val_rmse: 0.8841 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.034 Still best_val_rmse: 0.8841 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:49:46,322]\u001b[0m Trial 18 finished with value: 0.8841065764427185 and parameters: {'base_lr': 0.00014923663057185219, 'last_lr': 0.0036700968666363615}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 0\n",
      "##### Using base_lr 4.505946550034719e-05 last_lr 0.0006950808662543709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39d2988e8c941ee92756430d767a91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9583 New best_val_rmse: 0.9583\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.059 Still best_val_rmse: 0.9583 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7487 New best_val_rmse: 0.7487\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8165 Still best_val_rmse: 0.7487 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6827 New best_val_rmse: 0.6827\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7945 Still best_val_rmse: 0.6827 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6125 New best_val_rmse: 0.6125\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6123 New best_val_rmse: 0.6123\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.644 Still best_val_rmse: 0.6123 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:53:19,779]\u001b[0m Trial 19 finished with value: 0.612310528755188 and parameters: {'base_lr': 4.505946550034719e-05, 'last_lr': 0.0006950808662543709}. Best is trial 11 with value: 0.4787840247154236.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 18:53:19,782]\u001b[0m A new study created in memory with name: no-name-d87b0b41-1a2b-4eb6-bc8d-98a7259c35b7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best value:  0.4787840247154236\n",
      " Best params: \n",
      "    base_lr: 6.155021772017101e-05\n",
      "    last_lr: 0.004642225106260296\n",
      "##### Using fold 1\n",
      "##### Using base_lr 6.958061098448612e-05 last_lr 0.00010150127149249639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8297cf0b5e3d44e9929da094b59e847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7689 New best_val_rmse: 0.7689\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7523 New best_val_rmse: 0.7523\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6449 New best_val_rmse: 0.6449\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6127 New best_val_rmse: 0.6127\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6135 Still best_val_rmse: 0.6127 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6266 Still best_val_rmse: 0.6127 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6024 New best_val_rmse: 0.6024\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5689 New best_val_rmse: 0.5689\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5409 New best_val_rmse: 0.5409\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6043 Still best_val_rmse: 0.5409 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.519 New best_val_rmse: 0.519\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5056 New best_val_rmse: 0.5056\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5177 Still best_val_rmse: 0.5056 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.533 Still best_val_rmse: 0.5056 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5229 Still best_val_rmse: 0.4921 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5174 Still best_val_rmse: 0.4921 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.6051 Still best_val_rmse: 0.4921 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4743 New best_val_rmse: 0.4743\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.5066 Still best_val_rmse: 0.4743 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.48 Still best_val_rmse: 0.4743 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4725 New best_val_rmse: 0.4725\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4746 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.475 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4736 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4778 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4839 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4769 Still best_val_rmse: 0.4725 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4681 New best_val_rmse: 0.4681\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4683 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4698 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4695 Still best_val_rmse: 0.4681 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4672 New best_val_rmse: 0.4672\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4664 New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4666 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4679 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4679 Still best_val_rmse: 0.4664 (from epoch 2)\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4659 New best_val_rmse: 0.4659\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4645 New best_val_rmse: 0.4645\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4649 Still best_val_rmse: 0.4645 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4693 Still best_val_rmse: 0.4645 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4715 Still best_val_rmse: 0.4645 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4728 Still best_val_rmse: 0.4645 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4688 Still best_val_rmse: 0.4645 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4659 Still best_val_rmse: 0.4645 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4641 New best_val_rmse: 0.4641\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4627 New best_val_rmse: 0.4627\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4615 New best_val_rmse: 0.4615\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4616 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4636 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4649 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4629 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4622 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4638 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4703 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4809 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4637 Still best_val_rmse: 0.4615 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4605 New best_val_rmse: 0.4605\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4589 New best_val_rmse: 0.4589\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4585 New best_val_rmse: 0.4585\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4583 New best_val_rmse: 0.4583\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4582 New best_val_rmse: 0.4582\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4582 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4585 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4595 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4597 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.46 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4611 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4624 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4629 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4635 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.464 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4647 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4654 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4659 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4655 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4656 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4657 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4654 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4651 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.464 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4632 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4628 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4626 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.462 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4614 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4612 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4611 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4607 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4599 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4594 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4592 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4592 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4592 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4592 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4592 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4591 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.459 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.459 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4588 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n",
      "\n",
      "1 steps took 0.444 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4589 Still best_val_rmse: 0.4582 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:18:19,336]\u001b[0m Trial 0 finished with value: 0.45818933844566345 and parameters: {'base_lr': 6.958061098448612e-05, 'last_lr': 0.00010150127149249639}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00043321692350401365 last_lr 0.0007649685886782807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877c7de68e504c36ba0b42e325cbe393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.019 New best_val_rmse: 1.019\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.152 Still best_val_rmse: 1.019 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.008 New best_val_rmse: 1.008\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.01 Still best_val_rmse: 1.008 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.007 New best_val_rmse: 1.007\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.012 Still best_val_rmse: 1.007 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:20:48,042]\u001b[0m Trial 1 finished with value: 1.0074111223220825 and parameters: {'base_lr': 0.00043321692350401365, 'last_lr': 0.0007649685886782807}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.00016818585976809472 last_lr 8.40918161514374e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942b5d80e0524a9b8bb031539c65af20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.707 New best_val_rmse: 0.707\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8461 Still best_val_rmse: 0.707 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 2.19 Still best_val_rmse: 0.707 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.003 Still best_val_rmse: 0.707 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9951 Still best_val_rmse: 0.707 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8266 Still best_val_rmse: 0.707 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:23:16,692]\u001b[0m Trial 2 finished with value: 0.7069610953330994 and parameters: {'base_lr': 0.00016818585976809472, 'last_lr': 8.40918161514374e-05}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.0001269955858196054 last_lr 0.0008131643017437049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0278f14602c47ae9a8680043532cdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.763 New best_val_rmse: 0.763\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8391 Still best_val_rmse: 0.763 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9406 Still best_val_rmse: 0.763 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.665 New best_val_rmse: 0.665\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.034 Still best_val_rmse: 0.665 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.123 Still best_val_rmse: 0.665 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.041 Still best_val_rmse: 0.665 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.033 Still best_val_rmse: 0.665 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.023 Still best_val_rmse: 0.665 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:26:49,982]\u001b[0m Trial 3 finished with value: 0.6649777889251709 and parameters: {'base_lr': 0.0001269955858196054, 'last_lr': 0.0008131643017437049}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.0001363275165644892 last_lr 0.0017510925487700946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d39d62578a43e7a6183cc427bb359e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7133 New best_val_rmse: 0.7133\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7225 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9689 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8951 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9028 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.139 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.007 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.012 Still best_val_rmse: 0.7133 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.014 Still best_val_rmse: 0.7133 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:30:24,047]\u001b[0m Trial 4 finished with value: 0.7133458256721497 and parameters: {'base_lr': 0.0001363275165644892, 'last_lr': 0.0017510925487700946}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 5.2677458973125453e-05 last_lr 0.0002728708000650865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755b5c6b55e94cf891c41e377f6ef9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9263 New best_val_rmse: 0.9263\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6716 New best_val_rmse: 0.6716\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6684 New best_val_rmse: 0.6684\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6598 New best_val_rmse: 0.6598\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6181 New best_val_rmse: 0.6181\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5612 New best_val_rmse: 0.5612\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.8101 Still best_val_rmse: 0.5612 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6445 Still best_val_rmse: 0.5612 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5298 New best_val_rmse: 0.5298\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5263 New best_val_rmse: 0.5263\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 36 val_rmse: 0.5139 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 52 val_rmse: 0.5402 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 68 val_rmse: 0.505 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 84 val_rmse: 0.5336 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 100 val_rmse: 0.5152 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 116 val_rmse: 0.5198 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5335 Still best_val_rmse: 0.4994 (from epoch 1)\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4754 New best_val_rmse: 0.4754\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.478 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.5033 Still best_val_rmse: 0.4754 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4753 New best_val_rmse: 0.4753\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4712 New best_val_rmse: 0.4712\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4715 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4728 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4725 Still best_val_rmse: 0.4712 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4703 New best_val_rmse: 0.4703\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4776 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4781 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4755 Still best_val_rmse: 0.4703 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4692 New best_val_rmse: 0.4692\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4694 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4702 Still best_val_rmse: 0.4692 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4673 New best_val_rmse: 0.4673\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.4665 New best_val_rmse: 0.4665\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4661 New best_val_rmse: 0.4661\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4665 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4675 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4696 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4729 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4776 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4725 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4709 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4713 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4709 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4721 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4729 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4725 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4807 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4864 Still best_val_rmse: 0.4661 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4623 New best_val_rmse: 0.4623\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4609 New best_val_rmse: 0.4609\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4604 New best_val_rmse: 0.4604\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4602 New best_val_rmse: 0.4602\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4604 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4606 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4608 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4609 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4609 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4608 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4612 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4619 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4626 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4635 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4646 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4659 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4668 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4675 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4676 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4679 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4682 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4684 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4688 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4684 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4679 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4676 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4671 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4661 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4652 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4647 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4645 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4641 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.463 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4622 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4618 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4617 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4616 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4616 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4616 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4616 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4616 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.752 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4616 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4614 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4614 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4614 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n",
      "\n",
      "1 steps took 0.443 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4615 Still best_val_rmse: 0.4602 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:54:06,451]\u001b[0m Trial 5 finished with value: 0.4601954519748688 and parameters: {'base_lr': 5.2677458973125453e-05, 'last_lr': 0.0002728708000650865}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.000126982597021093 last_lr 0.00043746697815513454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d9ca2e88754b22bd397c32750743b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.729 New best_val_rmse: 0.729\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7682 Still best_val_rmse: 0.729 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7584 Still best_val_rmse: 0.729 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6673 New best_val_rmse: 0.6673\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.215 Still best_val_rmse: 0.6673 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.007 Still best_val_rmse: 0.6673 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.021 Still best_val_rmse: 0.6673 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.008 Still best_val_rmse: 0.6673 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.015 Still best_val_rmse: 0.6673 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 19:57:41,648]\u001b[0m Trial 6 finished with value: 0.667289137840271 and parameters: {'base_lr': 0.000126982597021093, 'last_lr': 0.00043746697815513454}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.0001757311713624694 last_lr 0.0002838836177636023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9801c7bceb994d0395741eae01c8e83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7166 New best_val_rmse: 0.7166\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.159 Still best_val_rmse: 0.7166 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.127 Still best_val_rmse: 0.7166 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.025 Still best_val_rmse: 0.7166 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.011 Still best_val_rmse: 0.7166 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.016 Still best_val_rmse: 0.7166 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 20:00:09,074]\u001b[0m Trial 7 finished with value: 0.7166176438331604 and parameters: {'base_lr': 0.0001757311713624694, 'last_lr': 0.0002838836177636023}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 0.000281983561784283 last_lr 0.00011011481218420704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b98eba62dd4881a8fdfe7328b20a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9047 New best_val_rmse: 0.9047\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.218 Still best_val_rmse: 0.9047 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.014 Still best_val_rmse: 0.9047 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.021 Still best_val_rmse: 0.9047 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.012 Still best_val_rmse: 0.9047 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.015 Still best_val_rmse: 0.9047 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 20:02:35,731]\u001b[0m Trial 8 finished with value: 0.9046867489814758 and parameters: {'base_lr': 0.000281983561784283, 'last_lr': 0.00011011481218420704}. Best is trial 0 with value: 0.45818933844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 8.902912488113375e-05 last_lr 0.00023076670834499074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0ae17af89f46718ca6269b054a2797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.7 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.7079 New best_val_rmse: 0.7079\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6837 New best_val_rmse: 0.6837\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9618 Still best_val_rmse: 0.6837 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6293 New best_val_rmse: 0.6293\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6916 Still best_val_rmse: 0.6293 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5629 New best_val_rmse: 0.5629\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6269 Still best_val_rmse: 0.5629 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5378 New best_val_rmse: 0.5378\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5174 New best_val_rmse: 0.5174\n",
      "\n",
      "16 steps took 13.0 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5937 Still best_val_rmse: 0.5174 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5439 Still best_val_rmse: 0.5174 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5098 New best_val_rmse: 0.5098\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.484 New best_val_rmse: 0.484\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 64 val_rmse: 0.4914 Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 72 val_rmse: 0.504 Still best_val_rmse: 0.484 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 88 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.4822 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 96 val_rmse: 0.5145 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 112 val_rmse: 0.5265 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.496 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 136 val_rmse: 0.486 Still best_val_rmse: 0.4808 (from epoch 1)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.461 New best_val_rmse: 0.461\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 1 batch_num: 141 val_rmse: 0.4594 New best_val_rmse: 0.4594\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4813 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 1 batch_num: 146 val_rmse: 0.4921 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "8 steps took 6.94 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4633 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.741 seconds\n",
      "Epoch: 2 batch_num: 7 val_rmse: 0.4685 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.475 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 10 val_rmse: 0.4781 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4729 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4653 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 15 val_rmse: 0.465 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4639 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 17 val_rmse: 0.4657 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.466 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 19 val_rmse: 0.4605 Still best_val_rmse: 0.4594 (from epoch 1)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4576 New best_val_rmse: 0.4576\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 21 val_rmse: 0.4545 New best_val_rmse: 0.4545\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4536 New best_val_rmse: 0.4536\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 23 val_rmse: 0.4532 New best_val_rmse: 0.4532\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4532 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 25 val_rmse: 0.455 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4585 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4638 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.464 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4603 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4596 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4614 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.463 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4607 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.754 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4554 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4555 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.458 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 37 val_rmse: 0.4621 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4656 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 39 val_rmse: 0.4703 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 41 val_rmse: 0.4665 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4709 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4818 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4632 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4578 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4565 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4544 Still best_val_rmse: 0.4532 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4515 New best_val_rmse: 0.4515\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4497 New best_val_rmse: 0.4497\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4507 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4523 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4528 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4528 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.752 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4528 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4527 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4545 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4568 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4616 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4618 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4611 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4578 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4539 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4507 Still best_val_rmse: 0.4497 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4491 New best_val_rmse: 0.4491\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4489 New best_val_rmse: 0.4489\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4491 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.4498 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4512 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4526 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4548 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4561 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4552 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.454 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4536 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4547 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4566 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4577 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.459 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4582 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4555 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.743 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4531 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4509 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4499 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4491 Still best_val_rmse: 0.4489 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4488 New best_val_rmse: 0.4488\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.449 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.45 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4515 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4535 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4562 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4581 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4586 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4589 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4591 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.758 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4572 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.455 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4529 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.451 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4499 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4491 Still best_val_rmse: 0.4488 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4484 New best_val_rmse: 0.4484\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4481 New best_val_rmse: 0.4481\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4482 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4484 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4484 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4483 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4483 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4481 Still best_val_rmse: 0.4481 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4478 New best_val_rmse: 0.4478\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4477 New best_val_rmse: 0.4477\n",
      "\n",
      "1 steps took 0.755 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4476 New best_val_rmse: 0.4476\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4476 New best_val_rmse: 0.4476\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4475 New best_val_rmse: 0.4475\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4475 New best_val_rmse: 0.4475\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4475 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4475 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4476 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4476 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4476 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4475 Still best_val_rmse: 0.4475 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4475 New best_val_rmse: 0.4475\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4474 New best_val_rmse: 0.4474\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.756 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n",
      "\n",
      "1 steps took 0.446 seconds\n",
      "Epoch: 2 batch_num: 147 val_rmse: 0.4475 Still best_val_rmse: 0.4474 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-18 20:31:17,972]\u001b[0m Trial 9 finished with value: 0.447449654340744 and parameters: {'base_lr': 8.902912488113375e-05, 'last_lr': 0.00023076670834499074}. Best is trial 9 with value: 0.447449654340744.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 1\n",
      "##### Using base_lr 4.067727869504078e-05 last_lr 0.004465318938487712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e9992315424db8936ec5299a95db88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.048 New best_val_rmse: 1.048\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.6598 New best_val_rmse: 0.6598\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6413 New best_val_rmse: 0.6413\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6951 Still best_val_rmse: 0.6413 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.8654 Still best_val_rmse: 0.6413 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.727 Still best_val_rmse: 0.6413 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9201 Still best_val_rmse: 0.6413 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7306 Still best_val_rmse: 0.6413 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5834 New best_val_rmse: 0.5834\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5642 New best_val_rmse: 0.5642\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5275 New best_val_rmse: 0.5275\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5315 Still best_val_rmse: 0.5275 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5377 Still best_val_rmse: 0.5275 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5113 New best_val_rmse: 0.5113\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5083 New best_val_rmse: 0.5083\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5074 New best_val_rmse: 0.5074\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4982 New best_val_rmse: 0.4982\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4921 New best_val_rmse: 0.4921\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.4767 New best_val_rmse: 0.4767\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 1 batch_num: 142 val_rmse: 0.4726 New best_val_rmse: 0.4726\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.5002 Still best_val_rmse: 0.4726 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.5225 Still best_val_rmse: 0.4726 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4801 Still best_val_rmse: 0.4726 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.48 Still best_val_rmse: 0.4726 (from epoch 1)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4839 Still best_val_rmse: 0.4726 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4935 Still best_val_rmse: 0.4726 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4718 New best_val_rmse: 0.4718\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4678 New best_val_rmse: 0.4678\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.4723 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4821 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.471 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 57 val_rmse: 0.4683 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4708 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4743 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.468 Still best_val_rmse: 0.4678 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4654 New best_val_rmse: 0.4654\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4657 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.468 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4711 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4757 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4765 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4704 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4656 Still best_val_rmse: 0.4654 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4642 New best_val_rmse: 0.4642\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.464 New best_val_rmse: 0.464\n",
      "\n",
      "1 steps took 0.742 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4638 New best_val_rmse: 0.4638\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4636 New best_val_rmse: 0.4636\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 79 val_rmse: 0.4633 New best_val_rmse: 0.4633\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4627 New best_val_rmse: 0.4627\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 81 val_rmse: 0.4624 New best_val_rmse: 0.4624\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4623 New best_val_rmse: 0.4623\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 83 val_rmse: 0.4625 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4628 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 85 val_rmse: 0.4629 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4628 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 87 val_rmse: 0.4624 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4623 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 89 val_rmse: 0.4626 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4635 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 91 val_rmse: 0.4649 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4663 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 93 val_rmse: 0.4684 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4695 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 95 val_rmse: 0.4708 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.473 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4738 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4717 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4691 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.468 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4663 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.465 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4638 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4631 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4624 Still best_val_rmse: 0.4623 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4618 New best_val_rmse: 0.4618\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4614 New best_val_rmse: 0.4614\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4612 New best_val_rmse: 0.4612\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.461 New best_val_rmse: 0.461\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.461 New best_val_rmse: 0.461\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.461 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.461 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4611 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4613 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4615 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4617 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4619 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.752 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4621 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4624 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.756 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4626 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4629 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4631 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4631 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4631 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4631 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4631 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.463 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.747 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.463 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.744 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.463 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4629 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4629 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.751 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4629 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.755 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4629 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.745 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4629 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.749 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.757 seconds\n",
      "Epoch: 2 batch_num: 141 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.748 seconds\n",
      "Epoch: 2 batch_num: 143 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.753 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.75 seconds\n",
      "Epoch: 2 batch_num: 145 val_rmse: 0.4628 Still best_val_rmse: 0.461 (from epoch 2)\n",
      "\n",
      "1 steps took 0.746 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d360e24c-6ca3-4486-b2ba-27d94cb53913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:03:49,382]\u001b[0m A new study created in memory with name: no-name-1b0b8ca5-bbbf-4207-94cf-b93393383ddf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Using fold 2\n",
      "##### Using base_lr 0.0003552760540192187 last_lr 0.0038043117994500013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02c54b03e074c5ea30427493bf70662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 14.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.485 New best_val_rmse: 1.485\n",
      "\n",
      "16 steps took 11.8 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.086 New best_val_rmse: 1.086\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.38 Still best_val_rmse: 1.086 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.098 Still best_val_rmse: 1.086 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.076 New best_val_rmse: 1.076\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.101 Still best_val_rmse: 1.076 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:06:17,801]\u001b[0m Trial 0 finished with value: 1.07648766040802 and parameters: {'base_lr': 0.0003552760540192187, 'last_lr': 0.0038043117994500013}. Best is trial 0 with value: 1.07648766040802.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 4.57661676399739e-05 last_lr 0.0004619110960854159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622b46a84dbc4cf2b1b7c8cd860ef61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.163 New best_val_rmse: 1.163\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7022 New best_val_rmse: 0.7022\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.722 Still best_val_rmse: 0.7022 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7581 Still best_val_rmse: 0.7022 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6998 New best_val_rmse: 0.6998\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8563 Still best_val_rmse: 0.6998 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5806 New best_val_rmse: 0.5806\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5663 New best_val_rmse: 0.5663\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6074 Still best_val_rmse: 0.5663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5729 Still best_val_rmse: 0.5663 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5415 New best_val_rmse: 0.5415\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5579 Still best_val_rmse: 0.5415 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5352 New best_val_rmse: 0.5352\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5709 Still best_val_rmse: 0.5352 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5532 Still best_val_rmse: 0.5352 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5115 New best_val_rmse: 0.5115\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5016 New best_val_rmse: 0.5016\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5876 Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5121 Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5013 New best_val_rmse: 0.5013\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4975 New best_val_rmse: 0.4975\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4958 New best_val_rmse: 0.4958\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.5004 Still best_val_rmse: 0.4958 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4951 New best_val_rmse: 0.4951\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4993 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4988 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4968 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4973 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4964 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4963 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4963 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4963 Still best_val_rmse: 0.4951 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4963 Still best_val_rmse: 0.4951 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:16:55,396]\u001b[0m Trial 1 finished with value: 0.49514997005462646 and parameters: {'base_lr': 4.57661676399739e-05, 'last_lr': 0.0004619110960854159}. Best is trial 1 with value: 0.49514997005462646.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00015686819593532155 last_lr 0.00043328911914538524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64bef6c81f440c0a0d1a7091fe7c633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.168 New best_val_rmse: 1.168\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.075 New best_val_rmse: 1.075\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.094 Still best_val_rmse: 1.075 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.076 Still best_val_rmse: 1.075 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.063 New best_val_rmse: 1.063\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.097 Still best_val_rmse: 1.063 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:19:22,081]\u001b[0m Trial 2 finished with value: 1.0626918077468872 and parameters: {'base_lr': 0.00015686819593532155, 'last_lr': 0.00043328911914538524}. Best is trial 1 with value: 0.49514997005462646.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00011472919004535602 last_lr 0.0017329266411097838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf89fb65e4341f2a8baa9e9c92860fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8314 New best_val_rmse: 0.8314\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8183 New best_val_rmse: 0.8183\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9561 Still best_val_rmse: 0.8183 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7514 New best_val_rmse: 0.7514\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.094 Still best_val_rmse: 0.7514 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.685 New best_val_rmse: 0.685\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9212 Still best_val_rmse: 0.685 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6612 New best_val_rmse: 0.6612\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5858 New best_val_rmse: 0.5858\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.612 Still best_val_rmse: 0.5858 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.559 New best_val_rmse: 0.559\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.7623 Still best_val_rmse: 0.559 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5445 New best_val_rmse: 0.5445\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.603 Still best_val_rmse: 0.5445 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.54 New best_val_rmse: 0.54\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5216 New best_val_rmse: 0.5216\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5311 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.585 Still best_val_rmse: 0.5216 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5093 New best_val_rmse: 0.5093\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4989 New best_val_rmse: 0.4989\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.492 New best_val_rmse: 0.492\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4947 Still best_val_rmse: 0.492 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4883 New best_val_rmse: 0.4883\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4883 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4855 New best_val_rmse: 0.4855\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4912 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4891 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4925 Still best_val_rmse: 0.4855 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4863 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4854 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4847 New best_val_rmse: 0.4847\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4897 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4907 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4853 Still best_val_rmse: 0.4847 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4845 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4848 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.485 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.485 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.485 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.485 Still best_val_rmse: 0.4842 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:31:34,364]\u001b[0m Trial 3 finished with value: 0.4841878116130829 and parameters: {'base_lr': 0.00011472919004535602, 'last_lr': 0.0017329266411097838}. Best is trial 3 with value: 0.4841878116130829.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 7.48556563348634e-05 last_lr 0.0004908001885823281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2165fa1b8a0b4543a911f44105b3a817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8619 New best_val_rmse: 0.8619\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.823 New best_val_rmse: 0.823\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9686 Still best_val_rmse: 0.823 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8791 Still best_val_rmse: 0.823 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6282 New best_val_rmse: 0.6282\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6561 Still best_val_rmse: 0.6282 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5937 New best_val_rmse: 0.5937\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6405 Still best_val_rmse: 0.5937 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6364 Still best_val_rmse: 0.5937 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5538 New best_val_rmse: 0.5538\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.523 New best_val_rmse: 0.523\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5402 Still best_val_rmse: 0.523 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5223 New best_val_rmse: 0.5223\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5645 Still best_val_rmse: 0.5223 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5381 Still best_val_rmse: 0.5223 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5073 New best_val_rmse: 0.5073\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 1 batch_num: 128 val_rmse: 0.5091 Still best_val_rmse: 0.4885 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 144 val_rmse: 0.4841 New best_val_rmse: 0.4841\n",
      "\n",
      "4 steps took 3.75 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5608 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5027 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4875 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4871 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4863 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4856 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4857 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4881 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4877 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4862 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4861 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4881 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.488 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4904 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4858 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4866 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4864 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4867 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4865 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4864 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4865 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4864 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4863 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4866 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4868 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.487 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4869 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4869 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4869 Still best_val_rmse: 0.4841 (from epoch 1)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4869 Still best_val_rmse: 0.4841 (from epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:44:27,165]\u001b[0m Trial 4 finished with value: 0.4840955138206482 and parameters: {'base_lr': 7.48556563348634e-05, 'last_lr': 0.0004908001885823281}. Best is trial 4 with value: 0.4840955138206482.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 3.057078843312379e-05 last_lr 0.0009549025910941674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9185c676f5e4bca8cf5642d27692c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.027 New best_val_rmse: 1.027\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8379 New best_val_rmse: 0.8379\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8646 Still best_val_rmse: 0.8379 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8617 Still best_val_rmse: 0.8379 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6574 New best_val_rmse: 0.6574\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.604 New best_val_rmse: 0.604\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5889 New best_val_rmse: 0.5889\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6779 Still best_val_rmse: 0.5889 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.585 New best_val_rmse: 0.585\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5649 New best_val_rmse: 0.5649\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.549 New best_val_rmse: 0.549\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5422 New best_val_rmse: 0.5422\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5289 New best_val_rmse: 0.5289\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5834 Still best_val_rmse: 0.5289 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5627 Still best_val_rmse: 0.5289 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5272 New best_val_rmse: 0.5272\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5071 New best_val_rmse: 0.5071\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.565 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5159 Still best_val_rmse: 0.5071 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5009 New best_val_rmse: 0.5009\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.501 Still best_val_rmse: 0.5009 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4994 New best_val_rmse: 0.4994\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5052 Still best_val_rmse: 0.4994 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.5017 Still best_val_rmse: 0.4994 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.5006 Still best_val_rmse: 0.4994 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.5003 Still best_val_rmse: 0.4994 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4997 Still best_val_rmse: 0.4994 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4997 Still best_val_rmse: 0.4994 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4996 Still best_val_rmse: 0.4994 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:54:33,799]\u001b[0m Trial 5 finished with value: 0.49941134452819824 and parameters: {'base_lr': 3.057078843312379e-05, 'last_lr': 0.0009549025910941674}. Best is trial 4 with value: 0.4840955138206482.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00040362445879098193 last_lr 0.0006691421017614235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19064743632e4ff2811624f9ff3441c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.195 New best_val_rmse: 1.195\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.073 New best_val_rmse: 1.073\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.125 Still best_val_rmse: 1.073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.073 Still best_val_rmse: 1.073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.093 Still best_val_rmse: 1.073 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.069 New best_val_rmse: 1.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 08:57:00,663]\u001b[0m Trial 6 finished with value: 1.0692239999771118 and parameters: {'base_lr': 0.00040362445879098193, 'last_lr': 0.0006691421017614235}. Best is trial 4 with value: 0.4840955138206482.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 6.285357587314095e-05 last_lr 0.0009582771584342349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d9e8e32cbc4536aef76aefae72f865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9278 New best_val_rmse: 0.9278\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8398 New best_val_rmse: 0.8398\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7729 New best_val_rmse: 0.7729\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8208 Still best_val_rmse: 0.7729 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6994 New best_val_rmse: 0.6994\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7497 Still best_val_rmse: 0.6994 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5686 New best_val_rmse: 0.5686\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5668 New best_val_rmse: 0.5668\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5898 Still best_val_rmse: 0.5668 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5908 Still best_val_rmse: 0.5668 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5339 New best_val_rmse: 0.5339\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5347 Still best_val_rmse: 0.5339 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5238 New best_val_rmse: 0.5238\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5602 Still best_val_rmse: 0.5238 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5455 Still best_val_rmse: 0.5238 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.509 New best_val_rmse: 0.509\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.496 Still best_val_rmse: 0.4943 (from epoch 1)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5572 Still best_val_rmse: 0.4943 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5018 Still best_val_rmse: 0.4943 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4939 New best_val_rmse: 0.4939\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4905 New best_val_rmse: 0.4905\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4914 Still best_val_rmse: 0.4905 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4874 New best_val_rmse: 0.4874\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4916 Still best_val_rmse: 0.4874 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4882 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4965 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4911 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4872 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4879 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4864 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4863 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4868 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4869 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4859 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.485 New best_val_rmse: 0.485\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4854 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4858 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4859 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4857 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4855 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4854 Still best_val_rmse: 0.485 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4854 Still best_val_rmse: 0.485 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 09:09:19,818]\u001b[0m Trial 7 finished with value: 0.4850175082683563 and parameters: {'base_lr': 6.285357587314095e-05, 'last_lr': 0.0009582771584342349}. Best is trial 4 with value: 0.4840955138206482.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.0002314602967657199 last_lr 9.851194497387779e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096f9fb495134eb59e849d63a69c7407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.012 New best_val_rmse: 1.012\n",
      "\n",
      "16 steps took 11.9 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.08 Still best_val_rmse: 1.012 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.218 Still best_val_rmse: 1.012 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.071 Still best_val_rmse: 1.012 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.079 Still best_val_rmse: 1.012 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.086 Still best_val_rmse: 1.012 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 09:11:47,707]\u001b[0m Trial 8 finished with value: 1.0120644569396973 and parameters: {'base_lr': 0.0002314602967657199, 'last_lr': 9.851194497387779e-05}. Best is trial 4 with value: 0.4840955138206482.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 5.400989949741741e-05 last_lr 0.00011196462142611125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79a71e74b314b7488befc71fbf33200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.09 New best_val_rmse: 1.09\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9043 New best_val_rmse: 0.9043\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7569 New best_val_rmse: 0.7569\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8641 Still best_val_rmse: 0.7569 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.731 New best_val_rmse: 0.731\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6332 New best_val_rmse: 0.6332\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5742 New best_val_rmse: 0.5742\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.567 New best_val_rmse: 0.567\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5711 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6086 Still best_val_rmse: 0.567 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5425 New best_val_rmse: 0.5425\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5283 New best_val_rmse: 0.5283\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5226 New best_val_rmse: 0.5226\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5741 Still best_val_rmse: 0.5226 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5339 Still best_val_rmse: 0.5226 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5068 New best_val_rmse: 0.5068\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5 New best_val_rmse: 0.5\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5542 Still best_val_rmse: 0.5 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4976 New best_val_rmse: 0.4976\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.5067 Still best_val_rmse: 0.4976 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4873 New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4877 Still best_val_rmse: 0.4873 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4829 New best_val_rmse: 0.4829\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4867 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4842 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4846 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4928 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4841 Still best_val_rmse: 0.4829 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4826 New best_val_rmse: 0.4826\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4857 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4854 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4837 Still best_val_rmse: 0.4826 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4822 New best_val_rmse: 0.4822\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4821 New best_val_rmse: 0.4821\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.482 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.482 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4822 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4822 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4819 Still best_val_rmse: 0.4815 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4815 New best_val_rmse: 0.4815\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 3.03 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4814 New best_val_rmse: 0.4814\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4814 Still best_val_rmse: 0.4814 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 09:24:31,447]\u001b[0m Trial 9 finished with value: 0.4813994765281677 and parameters: {'base_lr': 5.400989949741741e-05, 'last_lr': 0.00011196462142611125}. Best is trial 9 with value: 0.4813994765281677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 3.0349228979439946e-05 last_lr 8.356763721839508e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b32b54d95644161bdb07200ff5a49f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.07 New best_val_rmse: 1.07\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.9153 New best_val_rmse: 0.9153\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7683 New best_val_rmse: 0.7683\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7944 Still best_val_rmse: 0.7683 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6394 New best_val_rmse: 0.6394\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5858 New best_val_rmse: 0.5858\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6185 Still best_val_rmse: 0.5858 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.634 Still best_val_rmse: 0.5858 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5752 New best_val_rmse: 0.5752\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.54 New best_val_rmse: 0.54\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.541 Still best_val_rmse: 0.54 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5286 New best_val_rmse: 0.5286\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.519 New best_val_rmse: 0.519\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5504 Still best_val_rmse: 0.519 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5179 New best_val_rmse: 0.5179\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5076 New best_val_rmse: 0.5076\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5021 New best_val_rmse: 0.5021\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5457 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5143 Still best_val_rmse: 0.5021 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4961 New best_val_rmse: 0.4961\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4943 New best_val_rmse: 0.4943\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4915 New best_val_rmse: 0.4915\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4898 New best_val_rmse: 0.4898\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4901 Still best_val_rmse: 0.4896 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.5003 Still best_val_rmse: 0.4896 (from epoch 2)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4932 Still best_val_rmse: 0.4896 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4924 Still best_val_rmse: 0.4896 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4895 New best_val_rmse: 0.4895\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4893 New best_val_rmse: 0.4893\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4888 New best_val_rmse: 0.4888\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4886 New best_val_rmse: 0.4886\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4882 New best_val_rmse: 0.4882\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4879 New best_val_rmse: 0.4879\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4879 New best_val_rmse: 0.4879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 09:36:17,501]\u001b[0m Trial 10 finished with value: 0.487863153219223 and parameters: {'base_lr': 3.0349228979439946e-05, 'last_lr': 8.356763721839508e-05}. Best is trial 9 with value: 0.4813994765281677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 7.604566984349528e-05 last_lr 0.00013966559365722486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07d02da735045f3abeee56ebce2b729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8743 New best_val_rmse: 0.8743\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7826 New best_val_rmse: 0.7826\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9785 Still best_val_rmse: 0.7826 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7343 New best_val_rmse: 0.7343\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.9431 Still best_val_rmse: 0.7343 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6562 New best_val_rmse: 0.6562\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7393 Still best_val_rmse: 0.6562 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5712 New best_val_rmse: 0.5712\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.576 Still best_val_rmse: 0.5712 (from epoch 0)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.579 Still best_val_rmse: 0.5712 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5346 New best_val_rmse: 0.5346\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5228 New best_val_rmse: 0.5228\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5427 Still best_val_rmse: 0.5228 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.6042 Still best_val_rmse: 0.5228 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5495 Still best_val_rmse: 0.5228 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5098 New best_val_rmse: 0.5098\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4985 New best_val_rmse: 0.4985\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.5183 Still best_val_rmse: 0.4985 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.5855 Still best_val_rmse: 0.4985 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.507 Still best_val_rmse: 0.4985 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4916 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4832 New best_val_rmse: 0.4832\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4823 New best_val_rmse: 0.4823\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4855 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4841 Still best_val_rmse: 0.4823 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4819 New best_val_rmse: 0.4819\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4867 Still best_val_rmse: 0.4819 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4799 New best_val_rmse: 0.4799\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.48 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4811 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4808 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4838 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4818 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.481 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.481 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4805 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4803 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4804 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4803 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4803 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4805 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.0 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4806 Still best_val_rmse: 0.4799 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 09:49:10,119]\u001b[0m Trial 11 finished with value: 0.47985729575157166 and parameters: {'base_lr': 7.604566984349528e-05, 'last_lr': 0.00013966559365722486}. Best is trial 11 with value: 0.47985729575157166.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 8.661283535667235e-05 last_lr 0.00018490772170980246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540c86f8b9804114ad24e82cadecce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8598 New best_val_rmse: 0.8598\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.893 Still best_val_rmse: 0.8598 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9614 Still best_val_rmse: 0.8598 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.9419 Still best_val_rmse: 0.8598 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7173 New best_val_rmse: 0.7173\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.8428 Still best_val_rmse: 0.7173 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.9031 Still best_val_rmse: 0.7173 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.043 Still best_val_rmse: 0.7173 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.031 Still best_val_rmse: 0.7173 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 09:52:43,132]\u001b[0m Trial 12 finished with value: 0.7173497080802917 and parameters: {'base_lr': 8.661283535667235e-05, 'last_lr': 0.00018490772170980246}. Best is trial 11 with value: 0.47985729575157166.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 5.495320159797113e-05 last_lr 0.0001949936611791773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb5fd3b5d37426fb7779943649c8069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.108 New best_val_rmse: 1.108\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.935 New best_val_rmse: 0.935\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8139 New best_val_rmse: 0.8139\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.8465 Still best_val_rmse: 0.8139 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6577 New best_val_rmse: 0.6577\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6899 Still best_val_rmse: 0.6577 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7097 Still best_val_rmse: 0.6577 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5827 New best_val_rmse: 0.5827\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.564 New best_val_rmse: 0.564\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.5867 Still best_val_rmse: 0.564 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.5197 New best_val_rmse: 0.5197\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5306 Still best_val_rmse: 0.5197 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5378 Still best_val_rmse: 0.5197 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5927 Still best_val_rmse: 0.5197 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5476 Still best_val_rmse: 0.5197 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5112 New best_val_rmse: 0.5112\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.4927 New best_val_rmse: 0.4927\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 132 val_rmse: 0.4985 Still best_val_rmse: 0.4927 (from epoch 1)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.5592 Still best_val_rmse: 0.4927 (from epoch 1)\n",
      "\n",
      "16 steps took 12.9 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.5232 Still best_val_rmse: 0.4927 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4957 Still best_val_rmse: 0.4927 (from epoch 1)\n",
      "\n",
      "8 steps took 6.05 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4896 New best_val_rmse: 0.4896\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.4894 New best_val_rmse: 0.4894\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4885 New best_val_rmse: 0.4885\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4876 New best_val_rmse: 0.4876\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4877 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4881 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4924 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4881 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4939 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "8 steps took 6.03 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4885 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4881 Still best_val_rmse: 0.4876 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4853 New best_val_rmse: 0.4853\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4869 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4864 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4861 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4857 Still best_val_rmse: 0.4853 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.485 New best_val_rmse: 0.485\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4846 New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4844 New best_val_rmse: 0.4844\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4842 New best_val_rmse: 0.4842\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4845 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4848 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4847 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4846 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4845 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4844 Still best_val_rmse: 0.4842 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4844 Still best_val_rmse: 0.4842 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:05:27,306]\u001b[0m Trial 13 finished with value: 0.4842141568660736 and parameters: {'base_lr': 5.495320159797113e-05, 'last_lr': 0.0001949936611791773}. Best is trial 11 with value: 0.47985729575157166.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 4.075877307284862e-05 last_lr 0.0001624498006278733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5bca055abb462e9c9ad6467d55e0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.044 New best_val_rmse: 1.044\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8562 New best_val_rmse: 0.8562\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.9492 Still best_val_rmse: 0.8562 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7744 New best_val_rmse: 0.7744\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.857 Still best_val_rmse: 0.7744 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.7259 New best_val_rmse: 0.7259\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6584 New best_val_rmse: 0.6584\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.7345 Still best_val_rmse: 0.6584 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.6501 New best_val_rmse: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:09:00,133]\u001b[0m Trial 14 finished with value: 0.6501085162162781 and parameters: {'base_lr': 4.075877307284862e-05, 'last_lr': 0.0001624498006278733}. Best is trial 11 with value: 0.47985729575157166.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00010637579365513648 last_lr 8.276647346442369e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba98114f6e8a415696f865fb6ac6f371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.6 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.158 New best_val_rmse: 1.158\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7568 New best_val_rmse: 0.7568\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8052 Still best_val_rmse: 0.7568 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7244 New best_val_rmse: 0.7244\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7129 New best_val_rmse: 0.7129\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6232 New best_val_rmse: 0.6232\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6115 New best_val_rmse: 0.6115\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6461 Still best_val_rmse: 0.6115 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 0.5694 New best_val_rmse: 0.5694\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 1 batch_num: 12 val_rmse: 0.6209 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 28 val_rmse: 0.579 Still best_val_rmse: 0.5694 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 44 val_rmse: 0.5409 New best_val_rmse: 0.5409\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 60 val_rmse: 0.5169 New best_val_rmse: 0.5169\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 76 val_rmse: 0.5562 Still best_val_rmse: 0.5169 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 92 val_rmse: 0.5453 Still best_val_rmse: 0.5169 (from epoch 1)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 108 val_rmse: 0.5095 New best_val_rmse: 0.5095\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 124 val_rmse: 0.5044 New best_val_rmse: 0.5044\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 1 batch_num: 140 val_rmse: 0.6063 Still best_val_rmse: 0.5044 (from epoch 1)\n",
      "\n",
      "16 steps took 12.8 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.494 New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4964 Still best_val_rmse: 0.494 (from epoch 2)\n",
      "\n",
      "8 steps took 6.04 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.4808 New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4816 Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 32 val_rmse: 0.4816 Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 36 val_rmse: 0.48 New best_val_rmse: 0.48\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4791 New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.48 Still best_val_rmse: 0.4791 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4779 New best_val_rmse: 0.4779\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4772 New best_val_rmse: 0.4772\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4771 New best_val_rmse: 0.4771\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4778 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4803 Still best_val_rmse: 0.4771 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4766 New best_val_rmse: 0.4766\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4773 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4861 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "4 steps took 3.01 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4851 Still best_val_rmse: 0.4766 (from epoch 2)\n",
      "\n",
      "4 steps took 3.02 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4738 New best_val_rmse: 0.4738\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4736 New best_val_rmse: 0.4736\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4722 New best_val_rmse: 0.4722\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.476 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4782 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4729 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4733 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4751 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4732 Still best_val_rmse: 0.4722 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4716 New best_val_rmse: 0.4716\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4717 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.473 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4738 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4735 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4729 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4726 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4727 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4735 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4737 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4738 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4734 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.49 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4733 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4732 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.473 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4731 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4733 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4736 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4739 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.474 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4743 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4744 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4745 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.51 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4745 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4745 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 142 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 144 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n",
      "\n",
      "2 steps took 1.5 seconds\n",
      "Epoch: 2 batch_num: 146 val_rmse: 0.4746 Still best_val_rmse: 0.4716 (from epoch 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:25:31,053]\u001b[0m Trial 15 finished with value: 0.47160205245018005 and parameters: {'base_lr': 0.00010637579365513648, 'last_lr': 8.276647346442369e-05}. Best is trial 15 with value: 0.47160205245018005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00014957407459464006 last_lr 0.00028294729583032354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb00b90dcc640ec8d1f36f4bab97aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8874 New best_val_rmse: 0.8874\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7423 New best_val_rmse: 0.7423\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.8506 Still best_val_rmse: 0.7423 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.049 Still best_val_rmse: 0.7423 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.128 Still best_val_rmse: 0.7423 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.083 Still best_val_rmse: 0.7423 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.06 Still best_val_rmse: 0.7423 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.059 Still best_val_rmse: 0.7423 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.065 Still best_val_rmse: 0.7423 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:29:02,494]\u001b[0m Trial 16 finished with value: 0.7422624826431274 and parameters: {'base_lr': 0.00014957407459464006, 'last_lr': 0.00028294729583032354}. Best is trial 15 with value: 0.47160205245018005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00010197064730942197 last_lr 0.00012323780507671805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239d98d4f9c14a0eb81a1b8ea5541903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.067 New best_val_rmse: 1.067\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8543 New best_val_rmse: 0.8543\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.097 Still best_val_rmse: 0.8543 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.062 Still best_val_rmse: 0.8543 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.073 Still best_val_rmse: 0.8543 (from epoch 0)\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.069 Still best_val_rmse: 0.8543 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:31:29,153]\u001b[0m Trial 17 finished with value: 0.8542517423629761 and parameters: {'base_lr': 0.00010197064730942197, 'last_lr': 0.00012323780507671805}. Best is trial 15 with value: 0.47160205245018005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 0.00020300382337553154 last_lr 0.0002578690732592039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1973ee8421644afb9161d993a4180252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.018 New best_val_rmse: 1.018\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 1.161 Still best_val_rmse: 1.018 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 1.098 Still best_val_rmse: 1.018 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.08 Still best_val_rmse: 1.018 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.077 Still best_val_rmse: 1.018 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.096 Still best_val_rmse: 1.018 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:33:56,418]\u001b[0m Trial 18 finished with value: 1.0176031589508057 and parameters: {'base_lr': 0.00020300382337553154, 'last_lr': 0.0002578690732592039}. Best is trial 15 with value: 0.47160205245018005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Using fold 2\n",
      "##### Using base_lr 8.121779661903581e-05 last_lr 8.341126960078048e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2a37c377664a1b9a9cc39f68557abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 steps took 13.5 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.8434 New best_val_rmse: 0.8434\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8109 New best_val_rmse: 0.8109\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.7472 New best_val_rmse: 0.7472\n",
      "\n",
      "16 steps took 12.1 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 1.109 Still best_val_rmse: 0.7472 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 1.091 Still best_val_rmse: 0.7472 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 1.094 Still best_val_rmse: 0.7472 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 1.072 Still best_val_rmse: 0.7472 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 1.06 Still best_val_rmse: 0.7472 (from epoch 0)\n",
      "\n",
      "16 steps took 12.0 seconds\n",
      "Epoch: 0 batch_num: 144 val_rmse: 1.062 Still best_val_rmse: 0.7472 (from epoch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-19 10:37:28,281]\u001b[0m Trial 19 finished with value: 0.7471550703048706 and parameters: {'base_lr': 8.121779661903581e-05, 'last_lr': 8.341126960078048e-05}. Best is trial 15 with value: 0.47160205245018005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best value:  0.47160205245018005\n",
      " Best params: \n",
      "    base_lr: 0.00010637579365513648\n",
      "    last_lr: 8.276647346442369e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 3):\n",
    "    fold = i\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    print(\" Best value: \", study.best_trial.value)\n",
    "    print(\" Best params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69a20-e7bd-4426-8394-9fe92ff4ceba",
   "metadata": {},
   "source": [
    "### Verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2f26d-f0bc-4d35-b970-a18b100c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cfbb0-36c6-41e7-b98e-d5ecc379c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_offset = 0\n",
    "cfg.model_limit = 6\n",
    "cfg.n_folds = 5\n",
    "cfg.svm_kernels = ['rbf']\n",
    "cfg.svm_c = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe3330-3d2c-49c5-be98-69a13cf2a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.ceil(np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\n",
    "bins = train_df['bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508c0ef-984f-4af5-a283-88498c1dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inference_models = []\n",
    "for i in range(1, cfg.NUM_FOLDS + 1):\n",
    "    print(f'Model {i}')\n",
    "    inference_model = CommonLitModel()\n",
    "    inference_model = inference_model.cuda()\n",
    "    inference_model.load_state_dict(torch.load(str(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}/model_{i}.pth\")))\n",
    "    inference_model.eval();\n",
    "    inference_models.append(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6b85-3e21-44c5-bbe1-347c12d4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizers = []\n",
    "for i in range(1, cfg.NUM_FOLDS):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODELS_PATH/f\"{cfg.model_name.replace('/', '_')}_{i}\")\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6479666-2703-4691-831c-6a1a493924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(dl, transformer_model):\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for input_features in tqdm(dl, total=len(dl)):\n",
    "            output, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n",
    "#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n",
    "            embedding_out = context_vector.detach().cpu().numpy()\n",
    "            cls_embeddings.extend(embedding_out)\n",
    "    return np.array(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0cd48-b89a-4be9-b3f8-75f79133292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(X, y):\n",
    "    return np.sqrt(mean_squared_error(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dc0cb-b3d7-448c-8166-0716b76860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, test_id, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.test_id = test_id\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return {'input_ids': convert_to_list(encode['input_ids']),\n",
    "                'attention_mask': convert_to_list(encode['attention_mask']),\n",
    "                'id': self.test_id[idx]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fc14c-d0c9-486c-b15c-1aa2d81ad424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(df, tokenizer):\n",
    "    text = df['excerpt'].values\n",
    "    ids = df['id'].values\n",
    "    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n",
    "    return DataLoader(ds, \n",
    "                      batch_size = cfg.BATCH_SIZE,\n",
    "                      shuffle=False,\n",
    "                      num_workers = 1,\n",
    "                      pin_memory=True,\n",
    "                      drop_last=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052da7-31ff-4863-a4bf-ff6bb5829873",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train-orig.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "remove_unnecessary(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f88ec-0471-4d1a-8270-f610141382b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = train_df['target'].mean()\n",
    "train_target_std = train_df['target'].std()\n",
    "train_df['normalized_target'] = (train_df['target'] - train_target_mean) / train_target_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb532e-9f76-406b-ba60-c8991851faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = train_df['normalized_target'].values\n",
    "\n",
    "def calc_mean(scores):\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "final_scores = []\n",
    "final_rmse = []\n",
    "kernel_rmse_score_mean = []\n",
    "final_kernel_predictions_means = []\n",
    "for j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n",
    "    print('Model', j)\n",
    "    test_dl = create_dl(test_df, tokenizer)\n",
    "    train_dl = create_dl(train_df, tokenizer)\n",
    "    transformer_model = inference_model\n",
    "    transformer_model.cuda()\n",
    "    X = get_cls_embeddings(train_dl, transformer_model)\n",
    "    \n",
    "    y = train_target\n",
    "    X_test = get_cls_embeddings(test_dl, transformer_model)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n",
    "    scores = []\n",
    "    rmse_scores = []\n",
    "    kernel_predictions_means = []\n",
    "    for kernel in cfg.svm_kernels:\n",
    "        print('Kernel', kernel)\n",
    "        kernel_scores = []\n",
    "        kernel_rmse_scores = []\n",
    "        kernel_predictions = []\n",
    "        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "\n",
    "            print('Fold', k, train_idx.shape, valid_idx.shape)\n",
    "            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n",
    "\n",
    "            X_train, y_train = X[train_idx], y[train_idx]\n",
    "            X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            prediction = model.predict(X_valid)\n",
    "            kernel_predictions.append(prediction)\n",
    "            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n",
    "            print('rmse_score', kernel_rmse_scores[k])\n",
    "            kernel_scores.append(model.predict(X_test))\n",
    "        kernel_predictions_means.append(np.array([np.mean(kp) for kp in kernel_predictions]).mean())\n",
    "        scores.append(calc_mean(kernel_scores))\n",
    "        kernel_rmse_score = calc_mean(kernel_rmse_scores)\n",
    "        kernel_rmse_score_mean.append(kernel_rmse_score)\n",
    "        rmse_scores.append(kernel_rmse_score)\n",
    "    final_kernel_predictions_means.append(kernel_predictions_means)\n",
    "    final_scores.append(calc_mean(scores))\n",
    "    final_rmse.append(calc_mean(rmse_scores))\n",
    "print('FINAL RMSE score', np.mean(np.array(final_rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ac2d7-605a-4cc7-8bd0-8eec0ec6f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kernel_predictions_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b649aa-784d-4dbf-83e4-252ca3f2bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\n",
    "final_scores_normalized = np.array(final_scores) * train_target_std + train_target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd46e8-1542-4a71-82ca-d6d4838d7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rmse_score_mean_array = np.array(kernel_rmse_score_mean)\n",
    "kernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\n",
    "prop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\n",
    "prop_losses_sum = (1 - prop_losses).sum()\n",
    "weights = (1 - prop_losses) / prop_losses_sum\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9381-2a90-4183-b305-59f6d233017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores, weights=weights):\n",
    "    return np.average(np.array(scores), weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0f5a2-7e63-4799-ad55-1a733b24a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = train_df['target'].mean()\n",
    "final_scores_flat = calc_mean(final_scores_normalized).flatten()\n",
    "final_scores_mean = final_scores_flat.mean()\n",
    "target_mean, np.array(final_scores_normalized).mean()\n",
    "# (-0.9579984513405823, -0.8029817438292849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4669-2c5d-49e6-8dec-f9abb9cd8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614a6c5-af82-4b2c-bf5a-f1180109426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = target_mean - final_scores_mean\n",
    "mean_diff, mean_diff / len(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd92a7-a55f-422a-bfae-7f475bd5f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['target'] = final_scores_flat + mean_diff\n",
    "# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c995ca-ff1f-4b43-a41c-28d6ec11fa97",
   "metadata": {},
   "source": [
    "### Prepare Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5d5b-b293-4d59-b2e7-53131745079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3faa7c-7b90-4fe2-aebf-cf9733a3674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44851d5-e456-4abd-972c-0838dd792714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30572a-121f-4432-a83d-f49c6fa5e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels = [MODELS_PATH/f'{cfg.model_name}_{i + 1}' for i in range(0, cfg.NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf4b3-862c-4676-bc1d-875cd32ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def normalize_name(path_name):\n",
    "    return path_name.replace('', '')\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    print(f'Processing {i}th model')\n",
    "    i = i + 1\n",
    "    best_model_file = f'{best_model}/model_{i}.pth'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/tokenizer_config.json'))\n",
    "        assert tokenizer_json.exists(), f'{tokenizer_json} does not exist'\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/vocab.json'))\n",
    "        assert vocab_txt.exists(), f'{vocab_txt} does not exist'\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        merges = Path(normalize_name(f'{MODELS_PATH/cfg.model_name}_{i}/merges.txt'))\n",
    "        assert merges.exists()\n",
    "        copyfile(merges, tokenizer_path/'merges.txt')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d2659-6041-47d9-ba4c-a8ecade644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7473-d8fd-4ff4-8b51-67028bc5d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {MODELS_PATH}/{cfg.model_name}.yaml {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdcbd8-bed2-4ac9-91a0-93b35b0d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.transformer_model.save_pretrained(save_directory=f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5088d-df35-4b54-8de6-9c8a3bdc5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -h {MODELS_PATH/cfg.model_name}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a2e2d-e325-4b5f-ab68-71b1cc9d3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'lm', 'zip', f'{MODELS_PATH/cfg.model_name}/lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616c042-2877-470a-b227-948606188b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets init -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6984b-07d9-49e6-89b2-6066503bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(MODELS_PATH/cfg.model_name/'dataset-metadata.json')\n",
    "assert dataset_json_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa049c-faa9-45da-af4f-554a2000f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {str(dataset_json_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf108e8-c48c-4134-809b-6c775ef5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = f.read()\n",
    "    dataset_json = dataset_json.replace('INSERT_TITLE_HERE', f'commonlit-{cfg.model_name}-light').replace('INSERT_SLUG_HERE', f'commonlit-{cfg.model_name}-light')\n",
    "    print(dataset_json)\n",
    "with(open(dataset_json_path, 'w')) as f:\n",
    "    f.write(dataset_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c0cb-0d80-43d6-a064-f929ad92b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {MODELS_PATH/cfg.model_name}/best\n",
    "!rm -rf {MODELS_PATH/cfg.model_name}/lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851185dc-f532-4920-bfc0-39f36f0224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets create -p {MODELS_PATH/cfg.model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f40a-df46-4f1d-b247-c627e7cf091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets version -p {MODELS_PATH/cfg.model_name} -m \"Version with merges.txt\" -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(str(MODELS_PATH/f'distilroberta-0/checkpoint-105/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e77de-3b71-408f-8d6c-25bae3e60f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de19b9-2d6b-41c1-a765-5c39551fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859231b7-d595-463e-8ab7-1ac150193306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
