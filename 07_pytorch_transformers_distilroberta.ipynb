{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505e1dbe-f484-4304-8001-f10b5e0321c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef39394-5986-44bb-a6d6-84957a492ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys, time, collections, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Optional, Union, Any, List, Tuple\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data as D\n",
    "from torch.utils.data.dataset import Dataset, IterableDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertModel\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertModel\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "from transformers.trainer import logging\n",
    "from transformers.file_utils import is_torch_tpu_available, is_sagemaker_mp_enabled\n",
    "from transformers.trainer_pt_utils import find_batch_size, nested_concat, nested_numpify, nested_truncate, nested_detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c54d1-55c1-4701-9fde-692cf4450c84",
   "metadata": {},
   "source": [
    "### Folders and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c75e83-4760-4511-bf31-a144abfc01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/commonlit/data/')\n",
    "assert DATA_PATH.exists()\n",
    "MODELS_PATH = Path('/home/commonlit/models/')\n",
    "if not MODELS_PATH.exists():\n",
    "    os.mkdir(MODELS_PATH)\n",
    "assert MODELS_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b25934-3c16-4cb0-ab3b-6d95223432f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonlitreadabilityprize.zip  test.csv        train.csv\n",
      "sample_submission.csv\t       train-orig.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12796f2-c49a-4d32-9f38-0ecdec520539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH/'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH/'test.csv')\n",
    "sample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a075d-6fa8-4cf4-b703-db4f09c9649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2836 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     c12129c31                                                NaN   \n",
       "2     85aa80a4c                                                NaN   \n",
       "3     b69ac6792                                                NaN   \n",
       "4     dd1000b26                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2831  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2832  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2833  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2834  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2835  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  When the young people returned to the ballroom...   \n",
       "2              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "3              NaN  As Roger had predicted, the snow departed as q...   \n",
       "4              NaN  And outside before the palace a great garden w...   \n",
       "...            ...                                                ...   \n",
       "2831  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2832  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2833  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2834  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2835  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  \n",
       "0    -0.340259        0.464009  \n",
       "1    -0.340259        0.464009  \n",
       "2    -0.315372        0.480805  \n",
       "3    -0.580118        0.476676  \n",
       "4    -1.054013        0.450007  \n",
       "...        ...             ...  \n",
       "2831  1.711390        0.646900  \n",
       "2832  0.189476        0.535648  \n",
       "2833  0.255209        0.483866  \n",
       "2834 -0.215279        0.514128  \n",
       "2835  0.300779        0.512379  \n",
       "\n",
       "[2836 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efffba57-a6ab-4210-8391-5f2fccb3fd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And outside before the palace a great garden was walled round, filled full of stately fruit-trees, gray olives and sweet figs, and pomegranates, bananas, pineapples, grapes, pears, and apples, which bore the whole year round. For the rich south-west wind fed them, till pear grew ripe on pear, fig on fig, and grape on grape, all the winter and the spring. And at the farther end gay flower-beds bloomed through all seasons of the year; and two fair fountains of pure water rose, and ran, one through the garden grounds, and one beneath the palace gate, to water all the town and surrounding villages. Such noble gifts the heavens had given to Alcinous the wise.\\nSo they went inside, and saw him sitting, like Zeus, on his throne, with his golden sceptre by him, in garments stiff with platinnum, and in his hand a sculptured goblet, as he pledged the merchant kings; and beside him stood Hera, his wise and lovely wife and queen, and leaned against a pillar as she spun her golden threads.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['excerpt'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab8b20-6c63-4d51-b6fe-39ff141ad03e",
   "metadata": {},
   "source": [
    "### Prepare Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42787f35-115b-4258-925f-6575f3063924",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbc9a60-4c04-447a-9ddc-e5125688f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.floor(np.log2(len(train_df))) + 1)\n",
    "train_df.loc[:, 'bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0769b07-e9ea-42b7-95a6-5becf82dd824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.413097</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.969369</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.526589</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.106393</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.652726</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.201502</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.748738</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.309875</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.130016</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.560407</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.978923</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.399764</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target      \n",
       "          mean count\n",
       "bins                \n",
       "0    -3.413097    43\n",
       "1    -2.969369    79\n",
       "2    -2.526589   172\n",
       "3    -2.106393   269\n",
       "4    -1.652726   366\n",
       "5    -1.201502   418\n",
       "6    -0.748738   481\n",
       "7    -0.309875   406\n",
       "8     0.130016   312\n",
       "9     0.560407   183\n",
       "10    0.978923    83\n",
       "11    1.399764    23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['target', 'bins']].groupby(['bins']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f7c55d-a9c2-4e76-a7ef-42acd56f7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70453a0-e80b-4953-95cb-fd06547c6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (t_, v_) in enumerate(kf.split(X=train_df, y=train_df.bins.values)):\n",
    "    train_df.loc[v_, 'kfold'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b311fb49-dfdc-43e7-a89a-182b731c879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['kfold'] = train_df['kfold'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de386c06-bb90-4034-b290-cd03cb61cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('bins', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e6e9bd-9ae3-4871-a47d-37ed129634fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2835 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     c12129c31                                                NaN   \n",
       "2     85aa80a4c                                                NaN   \n",
       "3     b69ac6792                                                NaN   \n",
       "4     dd1000b26                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2830  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2831  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2832  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2833  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2834  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  When the young people returned to the ballroom...   \n",
       "2              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "3              NaN  As Roger had predicted, the snow departed as q...   \n",
       "4              NaN  And outside before the palace a great garden w...   \n",
       "...            ...                                                ...   \n",
       "2830  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2831  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2832  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2833  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2834  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  kfold  \n",
       "0    -0.340259        0.464009      0  \n",
       "1    -0.340259        0.464009      0  \n",
       "2    -0.315372        0.480805      0  \n",
       "3    -0.580118        0.476676      0  \n",
       "4    -1.054013        0.450007      0  \n",
       "...        ...             ...    ...  \n",
       "2830  1.711390        0.646900     11  \n",
       "2831  0.189476        0.535648     11  \n",
       "2832  0.255209        0.483866     11  \n",
       "2833 -0.215279        0.514128     11  \n",
       "2834  0.300779        0.512379     11  \n",
       "\n",
       "[2835 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3e0ed2-719d-483c-976f-173b19c8070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 3, 9, 6, 1, 0, 2, 11, 10, 7, 5, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_list = list(range(num_bins))\n",
    "random.shuffle(bin_list)\n",
    "bin_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fd3a4-9d7b-42be-b31c-2f6c6c15a3a0",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d57cfc8c-551e-4f67-91f8-5aec7002d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "\n",
    "def rmse_score_2(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18e1c48e-da7e-4a1e-b91e-c9c5a262a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10)\n",
    "b = np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883bd02c-7ab8-4fdc-bb6e-42e6c98a0935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4713419392508998, 0.4713419392508998)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_score(a, b), rmse_score_2(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447f318-60fd-4ea4-b08b-50c2be1dfdb7",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09389d9d-fa8a-4588-802e-5d6cb3d22d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    model_name = 'distilroberta'\n",
    "    batch_size = 128\n",
    "    max_len = 256\n",
    "    save_dir = f'trained/{model_name}'\n",
    "    num_workers = 2\n",
    "    epochs = 30\n",
    "    pretrained_transformers_model = f'distilroberta-base'\n",
    "    split = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91fcbcf0-e16f-4484-bc0a-a72f24d9536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CONFIG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96bf65-b75e-468d-83e2-1d60792baebd",
   "metadata": {},
   "source": [
    "### Prepare train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e601b365-a123-4a1d-bd31-3e6aeeee2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(fold = [1]):\n",
    "    valid_df = train_df[train_df['kfold'].isin(fold)]\n",
    "    valid_text = valid_df['excerpt'].values\n",
    "    valid_target = valid_df['target'].values\n",
    "    training_df = train_df[~train_df['kfold'].isin(fold)]\n",
    "    train_text = training_df['excerpt'].values\n",
    "    train_target = training_df['target'].values\n",
    "    return train_text, train_target, valid_text, valid_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8575c418-6c76-409c-9ce2-f907a92764d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2598, 237)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text, train_target, valid_text, valid_target = create_split([cfg.split])\n",
    "len(train_text), len(valid_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c638-0a6f-4b0b-baab-9422908e0343",
   "metadata": {},
   "source": [
    "### Prepare Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34addbcd-c989-475e-9c61-72b2e3ddf8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained/distilroberta/tokenizer_config.json',\n",
       " 'trained/distilroberta/special_tokens_map.json',\n",
       " 'trained/distilroberta/vocab.json',\n",
       " 'trained/distilroberta/merges.txt',\n",
       " 'trained/distilroberta/added_tokens.json',\n",
       " 'trained/distilroberta/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "# Save the tokenizer so that you can download the files and move it to a Kaggle dataset.\n",
    "tokenizer.save_pretrained(cfg.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10cea73f-5dd2-42f2-bc8d-454190f84655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict = tokenizer(train_df['excerpt'].values[0],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=cfg.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"].squeeze())\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab4041e-0b2d-41d6-94a3-6e3646e3427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d093a87-1edd-4e0b-8545-5e97a91a7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(t):\n",
    "    return t.flatten().long()\n",
    "\n",
    "class CommonLitDataset(nn.Module):\n",
    "    def __init__(self, text, target, tokenizer, max_len=128):\n",
    "        self.excerpt = text\n",
    "        self.target = target\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)\n",
    "        return InputFeatures(input_ids=convert_to_list(encode['input_ids']),\n",
    "                      attention_mask=convert_to_list(encode['attention_mask']),\n",
    "                      label=torch.tensor(self.target[idx]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a038b280-5d95-4a08-b3c6-da445770f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target):\n",
    "    train_ds = CommonLitDataset(train_text, train_target, tokenizer, cfg.max_len)\n",
    "    valid_ds = CommonLitDataset(valid_text, valid_target, tokenizer, cfg.max_len)\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f344167-247e-4675-a901-6f619da8e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl = D.DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "# train_dl = D.DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbdf0e73-0fa0-4ada-83f1-7f9e639345a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode, target = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ae453b-84a5-4a13-a2fc-00e5562a95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode.keys(), target.shape, encode['input_ids'].shape, encode['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddf1a2cf-19d7-42c1-ac39-83a8bb2655bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode['input_ids'][0].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed362b-cd1d-4d1a-b343-6ecddef7e324",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "863d53eb-7874-4bef-8b6a-d542749f7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# You can use a Transformer model of your choice.\n",
    "# transformer_model = DistilBertModel.from_pretrained(cfg.pretrained_transformers_model)\n",
    "transformer_model = AutoModel.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1f92770-7aa6-4ff0-b656-33f6b4d00bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_out = transformer_model(input_ids=encode['input_ids'].squeeze(), attention_mask=encode['attention_mask'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2551c8b-5ac7-4af0-bb2f-854c7fc45559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(transformer_out)['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "814e739b-f7a1-405c-b13c-31e6b93dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean(transformer_out.last_hidden_state, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecf50196-0a69-4db8-bc17-b967f89911f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_layer = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f2dbb7c-e9b8-47af-a124-32c5a5448ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_layer(torch.mean(transformer_out.last_hidden_state, axis=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5021054-9f67-404b-9a03-a34db81e49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class CommonLitModel(PreTrainedModel):\n",
    "    def __init__(self):\n",
    "        super(PreTrainedModel, self).__init__()\n",
    "        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_transformers_model)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "        self.config = self.transformer_model.config\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer_model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), output_hidden_states=False)\n",
    "        x = transformer_out.pooler_output\n",
    "#         x = transformer_out.last_hidden_state[:, 0, :]\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n",
    "        \"\"\"\n",
    "        For models that inherit from :class:`~transformers.PreTrainedModel`, uses that method to compute the number of\n",
    "        floating point operations for every backward + forward pass. If using another model, either implement such a\n",
    "        method in the model or subclass and override this method.\n",
    "        Args:\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "        Returns:\n",
    "            :obj:`int`: The number of floating-point operations.\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68bf7bb6-efa1-438e-8f61-a60f46df79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CommonLitModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d402996c-e965-4047-a39c-68a2c465280c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a04bb94-1364-4709-bc76-878d6d9ced4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = transformer_model.cuda()\n",
    "sample_out = transformer_model(encoded_dict.input_ids.cuda(), encoded_dict.attention_mask.cuda(), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0504d936-bebe-4fbd-90b5-b5b990501033",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfcf10f5-20f7-4f97-9a17-35c63ec4915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7265d3b2-f7cc-48aa-9f2b-66987f10182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.attention_mask.unsqueeze(0).shape, encoded_dict.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b825b57-a244-4a0c-be59-102e9c8f2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_out = transformer_model(encode.input_ids.unsqueeze(0).cuda(), encode.attention_mask.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5acec136-7984-4a70-9881-a58c7948517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d826f6b-79c8-447d-9d78-df79b99e6655",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68a48686-ddb0-4046-ab70-40e16117bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8fa4e6a-6a87-4860-b33a-075c5e693a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a861b0e-1207-454d-81fa-4a684a153d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_args(fold):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(MODELS_PATH/f'{cfg.model_name}-{fold}'),\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_first_step=True,\n",
    "        save_steps=40000,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_total_limit = 3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='mse',\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=5e-5\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53b59def-e897-4808-becc-7489ee29d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = create_training_args(cfg.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb7ea1ed-e72f-43fd-aa2c-b1645480f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    return {'mse': mean_squared_error(logits, labels), 'rmse': rmse_score_2(logits, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "325a9194-570e-4c1b-83c7-eb0fe7723de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "826b6f17-dd24-4be8-8644-4e4e40712f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class CommonLitTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        input_ids = inputs.pop(\"input_ids\")\n",
    "        attention_mask = inputs.pop(\"attention_mask\")\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs\n",
    "        loss = loss_fct(logits.flatten(),\n",
    "                        labels.float().flatten())\n",
    "        zero_cat = torch.zeros([1, 1]).to(outputs.device)\n",
    "        return (loss, torch.cat([zero_cat, outputs])) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34a504b1-6a02-4a36-81a2-d5b9bb4139b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/commonlit/models/bert-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3822354-7ee5-4f5f-93e6-f357a0157811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bins 0: [8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgilf\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">comic-night-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/860pqnt0\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/860pqnt0</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_133105-860pqnt0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='273' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/630 02:48 < 03:42, 1.61 it/s, Epoch 13/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.053000</td>\n",
       "      <td>0.534246</td>\n",
       "      <td>0.534246</td>\n",
       "      <td>0.730921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.427268</td>\n",
       "      <td>0.427268</td>\n",
       "      <td>0.653657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.426721</td>\n",
       "      <td>0.426721</td>\n",
       "      <td>0.653239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.284139</td>\n",
       "      <td>0.284139</td>\n",
       "      <td>0.533047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.372277</td>\n",
       "      <td>0.372277</td>\n",
       "      <td>0.610145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.359213</td>\n",
       "      <td>0.359213</td>\n",
       "      <td>0.599343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.152700</td>\n",
       "      <td>0.343146</td>\n",
       "      <td>0.343146</td>\n",
       "      <td>0.585787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.470889</td>\n",
       "      <td>0.470889</td>\n",
       "      <td>0.686214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.396765</td>\n",
       "      <td>0.396765</td>\n",
       "      <td>0.629893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.532581</td>\n",
       "      <td>0.532582</td>\n",
       "      <td>0.729782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.480309</td>\n",
       "      <td>0.480309</td>\n",
       "      <td>0.693043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.395261</td>\n",
       "      <td>0.395261</td>\n",
       "      <td>0.628698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.376038</td>\n",
       "      <td>0.376038</td>\n",
       "      <td>0.613219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-0/checkpoint-84\n",
      "result {'eval_loss': 0.2841387391090393, 'eval_mse': 0.28413867950439453, 'eval_rmse': 0.5330466032028198, 'eval_runtime': 0.7602, 'eval_samples_per_second': 311.757, 'epoch': 13.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 1: [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:860pqnt0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2081<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_133105-860pqnt0/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_133105-860pqnt0/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0652</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>13.0</td></tr><tr><td>train/global_step</td><td>273</td></tr><tr><td>_runtime</td><td>176</td></tr><tr><td>_timestamp</td><td>1622208841</td></tr><tr><td>_step</td><td>28</td></tr><tr><td>eval/loss</td><td>0.28414</td></tr><tr><td>eval/mse</td><td>0.28414</td></tr><tr><td>eval/rmse</td><td>0.53305</td></tr><tr><td>eval/runtime</td><td>0.7602</td></tr><tr><td>eval/samples_per_second</td><td>311.757</td></tr><tr><td>train/train_runtime</td><td>169.2406</td></tr><tr><td>train/train_samples_per_second</td><td>3.723</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>eval/loss</td><td>█▅▅▁▃▃▃▆▄█▆▄▄▁</td></tr><tr><td>eval/mse</td><td>█▅▅▁▃▃▃▆▄█▆▄▄▁</td></tr><tr><td>eval/rmse</td><td>█▅▅▁▄▃▃▆▄█▇▄▄▁</td></tr><tr><td>eval/runtime</td><td>▆▁▂▆▃█▂▃▃▃▂▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▃█▇▃▆▁▆▆▆▆▆▆▆▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">comic-night-5</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/860pqnt0\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/860pqnt0</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:860pqnt0). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">devoted-oath-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/30dp02sc\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/30dp02sc</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_133410-30dp02sc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/630 03:34 < 03:09, 1.55 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>0.586730</td>\n",
       "      <td>0.586730</td>\n",
       "      <td>0.765983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.339248</td>\n",
       "      <td>0.339248</td>\n",
       "      <td>0.582450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.332102</td>\n",
       "      <td>0.332102</td>\n",
       "      <td>0.576283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.363368</td>\n",
       "      <td>0.363368</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.256300</td>\n",
       "      <td>0.427736</td>\n",
       "      <td>0.427736</td>\n",
       "      <td>0.654016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.420979</td>\n",
       "      <td>0.420979</td>\n",
       "      <td>0.648829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.268144</td>\n",
       "      <td>0.268144</td>\n",
       "      <td>0.517826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.296264</td>\n",
       "      <td>0.296264</td>\n",
       "      <td>0.544302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.694349</td>\n",
       "      <td>0.694349</td>\n",
       "      <td>0.833276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.399650</td>\n",
       "      <td>0.399650</td>\n",
       "      <td>0.632178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.309486</td>\n",
       "      <td>0.309486</td>\n",
       "      <td>0.556315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.499520</td>\n",
       "      <td>0.499520</td>\n",
       "      <td>0.706767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.285855</td>\n",
       "      <td>0.285855</td>\n",
       "      <td>0.534654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.070500</td>\n",
       "      <td>0.417155</td>\n",
       "      <td>0.417155</td>\n",
       "      <td>0.645875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.349062</td>\n",
       "      <td>0.349062</td>\n",
       "      <td>0.590815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.301896</td>\n",
       "      <td>0.301896</td>\n",
       "      <td>0.549450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-1/checkpoint-147\n",
      "result {'eval_loss': 0.2681438624858856, 'eval_mse': 0.268143892288208, 'eval_rmse': 0.5178261399269104, 'eval_runtime': 0.7809, 'eval_samples_per_second': 303.5, 'epoch': 16.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 2: [9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:30dp02sc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2130<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_133410-30dp02sc/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_133410-30dp02sc/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.056</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/epoch</td><td>16.0</td></tr><tr><td>train/global_step</td><td>336</td></tr><tr><td>_runtime</td><td>222</td></tr><tr><td>_timestamp</td><td>1622209077</td></tr><tr><td>_step</td><td>34</td></tr><tr><td>eval/loss</td><td>0.26814</td></tr><tr><td>eval/mse</td><td>0.26814</td></tr><tr><td>eval/rmse</td><td>0.51783</td></tr><tr><td>eval/runtime</td><td>0.7809</td></tr><tr><td>eval/samples_per_second</td><td>303.5</td></tr><tr><td>train/train_runtime</td><td>215.3758</td></tr><tr><td>train/train_samples_per_second</td><td>2.925</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▆▂▂▃▄▄▁▁█▃▂▅▁▃▂▂▁</td></tr><tr><td>eval/mse</td><td>▆▂▂▃▄▄▁▁█▃▂▅▁▃▂▂▁</td></tr><tr><td>eval/rmse</td><td>▇▂▂▃▄▄▁▂█▄▂▅▁▄▃▂▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▁▂▁▂▁▂▂▂▂▂▁█▂▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇█▇█▇█▇▇▇▇▇█▁▇▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">devoted-oath-6</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/30dp02sc\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/30dp02sc</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:30dp02sc). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">radiant-frost-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/25wmx8xq\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/25wmx8xq</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_133807-25wmx8xq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/630 03:05 < 03:33, 1.57 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.048000</td>\n",
       "      <td>0.566812</td>\n",
       "      <td>0.566812</td>\n",
       "      <td>0.752869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.351154</td>\n",
       "      <td>0.351154</td>\n",
       "      <td>0.592583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.293169</td>\n",
       "      <td>0.293169</td>\n",
       "      <td>0.541451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.355234</td>\n",
       "      <td>0.355234</td>\n",
       "      <td>0.596015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.280179</td>\n",
       "      <td>0.280179</td>\n",
       "      <td>0.529319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.391744</td>\n",
       "      <td>0.391744</td>\n",
       "      <td>0.625894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.188900</td>\n",
       "      <td>0.344985</td>\n",
       "      <td>0.344985</td>\n",
       "      <td>0.587354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.370121</td>\n",
       "      <td>0.370121</td>\n",
       "      <td>0.608376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.465760</td>\n",
       "      <td>0.465760</td>\n",
       "      <td>0.682466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.375950</td>\n",
       "      <td>0.375950</td>\n",
       "      <td>0.613148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.285044</td>\n",
       "      <td>0.285044</td>\n",
       "      <td>0.533895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.324747</td>\n",
       "      <td>0.324747</td>\n",
       "      <td>0.569866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.378287</td>\n",
       "      <td>0.378287</td>\n",
       "      <td>0.615050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.419026</td>\n",
       "      <td>0.419026</td>\n",
       "      <td>0.647322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-2/checkpoint-105\n",
      "result {'eval_loss': 0.28017911314964294, 'eval_mse': 0.28017911314964294, 'eval_rmse': 0.5293194651603699, 'eval_runtime': 0.7756, 'eval_samples_per_second': 305.579, 'epoch': 14.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 3: [6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:25wmx8xq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2178<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_133807-25wmx8xq/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_133807-25wmx8xq/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0559</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>14.0</td></tr><tr><td>train/global_step</td><td>294</td></tr><tr><td>_runtime</td><td>193</td></tr><tr><td>_timestamp</td><td>1622209285</td></tr><tr><td>_step</td><td>30</td></tr><tr><td>eval/loss</td><td>0.28018</td></tr><tr><td>eval/mse</td><td>0.28018</td></tr><tr><td>eval/rmse</td><td>0.52932</td></tr><tr><td>eval/runtime</td><td>0.7756</td></tr><tr><td>eval/samples_per_second</td><td>305.579</td></tr><tr><td>train/train_runtime</td><td>186.4968</td></tr><tr><td>train/train_samples_per_second</td><td>3.378</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▃▁▃▁▄▃▃▆▃▁▂▃▄▁</td></tr><tr><td>eval/mse</td><td>█▃▁▃▁▄▃▃▆▃▁▂▃▄▁</td></tr><tr><td>eval/rmse</td><td>█▃▁▃▁▄▃▃▆▄▁▂▄▅▁</td></tr><tr><td>eval/runtime</td><td>▁▂▁▁▂▂█▂█▁▁▂▂▂▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇██▇▇▁▇▁██▇▇▇▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">radiant-frost-7</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/25wmx8xq\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/25wmx8xq</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:25wmx8xq). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">earnest-violet-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/3vl29b09\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/3vl29b09</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_134134-3vl29b09</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/630 02:41 < 04:03, 1.55 it/s, Epoch 12/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.146700</td>\n",
       "      <td>0.398796</td>\n",
       "      <td>0.398796</td>\n",
       "      <td>0.631503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>0.350733</td>\n",
       "      <td>0.350733</td>\n",
       "      <td>0.592227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.306387</td>\n",
       "      <td>0.306387</td>\n",
       "      <td>0.553523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.375304</td>\n",
       "      <td>0.375304</td>\n",
       "      <td>0.612621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.278600</td>\n",
       "      <td>0.513619</td>\n",
       "      <td>0.513619</td>\n",
       "      <td>0.716672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.738291</td>\n",
       "      <td>0.738291</td>\n",
       "      <td>0.859238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.429806</td>\n",
       "      <td>0.429806</td>\n",
       "      <td>0.655596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.143900</td>\n",
       "      <td>0.610627</td>\n",
       "      <td>0.610626</td>\n",
       "      <td>0.781426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.425327</td>\n",
       "      <td>0.425327</td>\n",
       "      <td>0.652171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.367399</td>\n",
       "      <td>0.367399</td>\n",
       "      <td>0.606135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.537773</td>\n",
       "      <td>0.537773</td>\n",
       "      <td>0.733330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.557189</td>\n",
       "      <td>0.557189</td>\n",
       "      <td>0.746451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-3/checkpoint-63\n",
      "result {'eval_loss': 0.306387335062027, 'eval_mse': 0.306387335062027, 'eval_rmse': 0.5535226464271545, 'eval_runtime': 0.7769, 'eval_samples_per_second': 303.764, 'epoch': 12.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 4: [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3vl29b09) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2226<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_134134-3vl29b09/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_134134-3vl29b09/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.09</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>12.0</td></tr><tr><td>train/global_step</td><td>252</td></tr><tr><td>_runtime</td><td>168</td></tr><tr><td>_timestamp</td><td>1622209466</td></tr><tr><td>_step</td><td>26</td></tr><tr><td>eval/loss</td><td>0.30639</td></tr><tr><td>eval/mse</td><td>0.30639</td></tr><tr><td>eval/rmse</td><td>0.55352</td></tr><tr><td>eval/runtime</td><td>0.7769</td></tr><tr><td>eval/samples_per_second</td><td>303.764</td></tr><tr><td>train/train_runtime</td><td>161.9003</td></tr><tr><td>train/train_samples_per_second</td><td>3.891</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▃▃▂▂▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>eval/loss</td><td>▂▂▁▂▄█▃▆▃▂▅▅▁</td></tr><tr><td>eval/mse</td><td>▂▂▁▂▄█▃▆▃▂▅▅▁</td></tr><tr><td>eval/rmse</td><td>▃▂▁▂▅█▃▆▃▂▅▅▁</td></tr><tr><td>eval/runtime</td><td>▁▁▂▂▂█▂▂▂▃▂▂▅</td></tr><tr><td>eval/samples_per_second</td><td>██▇▇▇▁▇▇▇▆▇▇▄</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">earnest-violet-8</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/3vl29b09\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/3vl29b09</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3vl29b09). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">young-dream-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/2gg9etzd\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/2gg9etzd</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_134435-2gg9etzd</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/630 03:33 < 03:08, 1.56 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.768700</td>\n",
       "      <td>0.380358</td>\n",
       "      <td>0.380358</td>\n",
       "      <td>0.616732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.265792</td>\n",
       "      <td>0.265792</td>\n",
       "      <td>0.515550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.416182</td>\n",
       "      <td>0.416181</td>\n",
       "      <td>0.645121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.374827</td>\n",
       "      <td>0.374827</td>\n",
       "      <td>0.612231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.360210</td>\n",
       "      <td>0.360210</td>\n",
       "      <td>0.600175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.261842</td>\n",
       "      <td>0.261842</td>\n",
       "      <td>0.511705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.259220</td>\n",
       "      <td>0.259220</td>\n",
       "      <td>0.509137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.318741</td>\n",
       "      <td>0.318741</td>\n",
       "      <td>0.564572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.425549</td>\n",
       "      <td>0.425549</td>\n",
       "      <td>0.652341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.362433</td>\n",
       "      <td>0.362433</td>\n",
       "      <td>0.602024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>0.600174</td>\n",
       "      <td>0.600174</td>\n",
       "      <td>0.774709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.342413</td>\n",
       "      <td>0.342413</td>\n",
       "      <td>0.585160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.330787</td>\n",
       "      <td>0.330787</td>\n",
       "      <td>0.575141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.399453</td>\n",
       "      <td>0.399453</td>\n",
       "      <td>0.632023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.499258</td>\n",
       "      <td>0.499258</td>\n",
       "      <td>0.706582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.370918</td>\n",
       "      <td>0.370918</td>\n",
       "      <td>0.609031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-4/checkpoint-147\n",
      "result {'eval_loss': 0.259220153093338, 'eval_mse': 0.25922009348869324, 'eval_rmse': 0.5091366171836853, 'eval_runtime': 0.7676, 'eval_samples_per_second': 307.472, 'epoch': 16.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 5: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gg9etzd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2274<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_134435-2gg9etzd/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_134435-2gg9etzd/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0517</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/epoch</td><td>16.0</td></tr><tr><td>train/global_step</td><td>336</td></tr><tr><td>_runtime</td><td>220</td></tr><tr><td>_timestamp</td><td>1622209700</td></tr><tr><td>_step</td><td>34</td></tr><tr><td>eval/loss</td><td>0.25922</td></tr><tr><td>eval/mse</td><td>0.25922</td></tr><tr><td>eval/rmse</td><td>0.50914</td></tr><tr><td>eval/runtime</td><td>0.7676</td></tr><tr><td>eval/samples_per_second</td><td>307.472</td></tr><tr><td>train/train_runtime</td><td>214.5103</td></tr><tr><td>train/train_samples_per_second</td><td>2.937</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▃▁▄▃▃▁▁▂▄▃█▃▂▄▆▃▁</td></tr><tr><td>eval/mse</td><td>▃▁▄▃▃▁▁▂▄▃█▃▂▄▆▃▁</td></tr><tr><td>eval/rmse</td><td>▄▁▅▄▃▁▁▂▅▃█▃▃▄▆▄▁</td></tr><tr><td>eval/runtime</td><td>▄▁▃▃▂▂▂▃▂▆▂▃▃▃▂▄█</td></tr><tr><td>eval/samples_per_second</td><td>▅█▆▆▇▇▇▆▇▃▇▆▆▆▇▅▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">young-dream-9</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/2gg9etzd\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/2gg9etzd</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2gg9etzd). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">vocal-feather-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/3ek3lgux\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/3ek3lgux</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_134829-3ek3lgux</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='483' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [483/630 05:08 < 01:34, 1.56 it/s, Epoch 23/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.077300</td>\n",
       "      <td>0.468156</td>\n",
       "      <td>0.468156</td>\n",
       "      <td>0.684219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451800</td>\n",
       "      <td>0.295924</td>\n",
       "      <td>0.295924</td>\n",
       "      <td>0.543989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.324545</td>\n",
       "      <td>0.324545</td>\n",
       "      <td>0.569688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.273390</td>\n",
       "      <td>0.273390</td>\n",
       "      <td>0.522867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.292563</td>\n",
       "      <td>0.292563</td>\n",
       "      <td>0.540891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.227917</td>\n",
       "      <td>0.227917</td>\n",
       "      <td>0.477407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.346039</td>\n",
       "      <td>0.346039</td>\n",
       "      <td>0.588251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.138400</td>\n",
       "      <td>0.396805</td>\n",
       "      <td>0.396805</td>\n",
       "      <td>0.629924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.324557</td>\n",
       "      <td>0.324557</td>\n",
       "      <td>0.569699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.340690</td>\n",
       "      <td>0.340690</td>\n",
       "      <td>0.583687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.292961</td>\n",
       "      <td>0.292961</td>\n",
       "      <td>0.541259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.254245</td>\n",
       "      <td>0.254245</td>\n",
       "      <td>0.504227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.281728</td>\n",
       "      <td>0.281728</td>\n",
       "      <td>0.530781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.298751</td>\n",
       "      <td>0.298751</td>\n",
       "      <td>0.546581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.281798</td>\n",
       "      <td>0.281798</td>\n",
       "      <td>0.530847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.300440</td>\n",
       "      <td>0.300439</td>\n",
       "      <td>0.548124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>0.566765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.443576</td>\n",
       "      <td>0.443576</td>\n",
       "      <td>0.666015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.270319</td>\n",
       "      <td>0.270319</td>\n",
       "      <td>0.519922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.302974</td>\n",
       "      <td>0.302974</td>\n",
       "      <td>0.550430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.312155</td>\n",
       "      <td>0.312155</td>\n",
       "      <td>0.558708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.283995</td>\n",
       "      <td>0.283995</td>\n",
       "      <td>0.532911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-5/checkpoint-294\n",
      "result {'eval_loss': 0.22544008493423462, 'eval_mse': 0.2254400998353958, 'eval_rmse': 0.4748053252696991, 'eval_runtime': 0.7691, 'eval_samples_per_second': 306.836, 'epoch': 23.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 6: [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ek3lgux) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2322<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_134829-3ek3lgux/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_134829-3ek3lgux/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0289</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>23.0</td></tr><tr><td>train/global_step</td><td>483</td></tr><tr><td>_runtime</td><td>316</td></tr><tr><td>_timestamp</td><td>1622210029</td></tr><tr><td>_step</td><td>48</td></tr><tr><td>eval/loss</td><td>0.22544</td></tr><tr><td>eval/mse</td><td>0.22544</td></tr><tr><td>eval/rmse</td><td>0.47481</td></tr><tr><td>eval/runtime</td><td>0.7691</td></tr><tr><td>eval/samples_per_second</td><td>306.836</td></tr><tr><td>train/train_runtime</td><td>308.8218</td></tr><tr><td>train/train_samples_per_second</td><td>2.04</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▃▄▂▃▁▄▆▄▄▃▂▃▁▃▃▃▄▇▂▃▄▃▁</td></tr><tr><td>eval/mse</td><td>█▃▄▂▃▁▄▆▄▄▃▂▃▁▃▃▃▄▇▂▃▄▃▁</td></tr><tr><td>eval/rmse</td><td>█▃▄▃▃▁▅▆▄▅▃▂▃▁▃▃▃▄▇▃▄▄▃▁</td></tr><tr><td>eval/runtime</td><td>▂▁█▂▂▂▂▂▁▁▃▂▂▁▄▂▂▂▂▂▂▂▁▄</td></tr><tr><td>eval/samples_per_second</td><td>▇█▁▇▇▇▇▇██▆▇▇█▅▇▇▇▇▇▇▇█▅</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">vocal-feather-10</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/3ek3lgux\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/3ek3lgux</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3ek3lgux). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">denim-thunder-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/25xr3zlc\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/25xr3zlc</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_135358-25xr3zlc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/630 03:07 < 03:35, 1.56 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.058100</td>\n",
       "      <td>0.515341</td>\n",
       "      <td>0.515341</td>\n",
       "      <td>0.717872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.439810</td>\n",
       "      <td>0.439810</td>\n",
       "      <td>0.663182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.227652</td>\n",
       "      <td>0.227652</td>\n",
       "      <td>0.477129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.224321</td>\n",
       "      <td>0.224321</td>\n",
       "      <td>0.473625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.209439</td>\n",
       "      <td>0.209439</td>\n",
       "      <td>0.457645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>0.273388</td>\n",
       "      <td>0.273388</td>\n",
       "      <td>0.522866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.241582</td>\n",
       "      <td>0.241582</td>\n",
       "      <td>0.491509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.221720</td>\n",
       "      <td>0.221720</td>\n",
       "      <td>0.470872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.398688</td>\n",
       "      <td>0.398688</td>\n",
       "      <td>0.631418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.246421</td>\n",
       "      <td>0.246421</td>\n",
       "      <td>0.496408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.267987</td>\n",
       "      <td>0.267987</td>\n",
       "      <td>0.517675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.293161</td>\n",
       "      <td>0.293161</td>\n",
       "      <td>0.541444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.426761</td>\n",
       "      <td>0.426761</td>\n",
       "      <td>0.653269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.259444</td>\n",
       "      <td>0.259444</td>\n",
       "      <td>0.509356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-6/checkpoint-105\n",
      "result {'eval_loss': 0.20943906903266907, 'eval_mse': 0.20943905413150787, 'eval_rmse': 0.45764511823654175, 'eval_runtime': 0.7665, 'eval_samples_per_second': 307.876, 'epoch': 14.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 7: [11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:25xr3zlc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2370<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_135358-25xr3zlc/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_135358-25xr3zlc/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0622</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>14.0</td></tr><tr><td>train/global_step</td><td>294</td></tr><tr><td>_runtime</td><td>195</td></tr><tr><td>_timestamp</td><td>1622210237</td></tr><tr><td>_step</td><td>30</td></tr><tr><td>eval/loss</td><td>0.20944</td></tr><tr><td>eval/mse</td><td>0.20944</td></tr><tr><td>eval/rmse</td><td>0.45765</td></tr><tr><td>eval/runtime</td><td>0.7665</td></tr><tr><td>eval/samples_per_second</td><td>307.876</td></tr><tr><td>train/train_runtime</td><td>188.1837</td></tr><tr><td>train/train_samples_per_second</td><td>3.348</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▆▁▁▁▂▂▁▅▂▂▃▆▂▁</td></tr><tr><td>eval/mse</td><td>█▆▁▁▁▂▂▁▅▂▂▃▆▂▁</td></tr><tr><td>eval/rmse</td><td>█▇▂▁▁▃▂▁▆▂▃▃▆▂▁</td></tr><tr><td>eval/runtime</td><td>█▂▁▁▁▂▁▂▁▇▁▇▁▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▇███▇█▇▇▂█▂█▇▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">denim-thunder-11</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/25xr3zlc\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/25xr3zlc</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:25xr3zlc). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">stellar-pine-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/3cvl5s2d\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/3cvl5s2d</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_135726-3cvl5s2d</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [378/630 04:04 < 02:43, 1.54 it/s, Epoch 18/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.175400</td>\n",
       "      <td>0.494142</td>\n",
       "      <td>0.494142</td>\n",
       "      <td>0.702952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>0.305425</td>\n",
       "      <td>0.305425</td>\n",
       "      <td>0.552653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.231956</td>\n",
       "      <td>0.231956</td>\n",
       "      <td>0.481618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.326150</td>\n",
       "      <td>0.326150</td>\n",
       "      <td>0.571096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>0.318785</td>\n",
       "      <td>0.318785</td>\n",
       "      <td>0.564610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.257679</td>\n",
       "      <td>0.257679</td>\n",
       "      <td>0.507621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.332327</td>\n",
       "      <td>0.332327</td>\n",
       "      <td>0.576478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.527658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.224516</td>\n",
       "      <td>0.224516</td>\n",
       "      <td>0.473831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.295446</td>\n",
       "      <td>0.295446</td>\n",
       "      <td>0.543549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.390740</td>\n",
       "      <td>0.390740</td>\n",
       "      <td>0.625092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>0.521243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.375930</td>\n",
       "      <td>0.375930</td>\n",
       "      <td>0.613131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.388773</td>\n",
       "      <td>0.388773</td>\n",
       "      <td>0.623517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.305013</td>\n",
       "      <td>0.305013</td>\n",
       "      <td>0.552279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.291862</td>\n",
       "      <td>0.291862</td>\n",
       "      <td>0.540242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.285789</td>\n",
       "      <td>0.285789</td>\n",
       "      <td>0.534593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.355435</td>\n",
       "      <td>0.355435</td>\n",
       "      <td>0.596183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-7/checkpoint-189\n",
      "result {'eval_loss': 0.22451607882976532, 'eval_mse': 0.22451604902744293, 'eval_rmse': 0.4738312363624573, 'eval_runtime': 0.7662, 'eval_samples_per_second': 308.005, 'epoch': 18.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 8: [10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3cvl5s2d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2418<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_135726-3cvl5s2d/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_135726-3cvl5s2d/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0407</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/epoch</td><td>18.0</td></tr><tr><td>train/global_step</td><td>378</td></tr><tr><td>_runtime</td><td>253</td></tr><tr><td>_timestamp</td><td>1622210503</td></tr><tr><td>_step</td><td>38</td></tr><tr><td>eval/loss</td><td>0.22452</td></tr><tr><td>eval/mse</td><td>0.22452</td></tr><tr><td>eval/rmse</td><td>0.47383</td></tr><tr><td>eval/runtime</td><td>0.7662</td></tr><tr><td>eval/samples_per_second</td><td>308.005</td></tr><tr><td>train/train_runtime</td><td>244.7384</td></tr><tr><td>train/train_samples_per_second</td><td>2.574</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>█▃▁▄▃▂▄▂▁▃▅▂▅▅▃▃▃▄▁</td></tr><tr><td>eval/mse</td><td>█▃▁▄▃▂▄▂▁▃▅▂▅▅▃▃▃▄▁</td></tr><tr><td>eval/rmse</td><td>█▃▁▄▄▂▄▃▁▃▆▂▅▆▃▃▃▅▁</td></tr><tr><td>eval/runtime</td><td>▁▁▇▂▁▂█▆▃▁▃▂▂▂▁▂▂▁▃</td></tr><tr><td>eval/samples_per_second</td><td>██▂▇█▇▁▃▆█▆▇▇▇█▇▇█▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stellar-pine-12</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/3cvl5s2d\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/3cvl5s2d</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3cvl5s2d). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">happy-grass-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/24uwvh3x\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/24uwvh3x</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_140152-24uwvh3x</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/630 03:26 < 03:27, 1.52 it/s, Epoch 15/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.943800</td>\n",
       "      <td>0.401596</td>\n",
       "      <td>0.401596</td>\n",
       "      <td>0.633716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.345126</td>\n",
       "      <td>0.345126</td>\n",
       "      <td>0.587474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.363751</td>\n",
       "      <td>0.363751</td>\n",
       "      <td>0.603118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.329950</td>\n",
       "      <td>0.329950</td>\n",
       "      <td>0.574413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.587759</td>\n",
       "      <td>0.587759</td>\n",
       "      <td>0.766655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.324731</td>\n",
       "      <td>0.324731</td>\n",
       "      <td>0.569852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.509996</td>\n",
       "      <td>0.509996</td>\n",
       "      <td>0.714140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.398790</td>\n",
       "      <td>0.398790</td>\n",
       "      <td>0.631498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.492546</td>\n",
       "      <td>0.492546</td>\n",
       "      <td>0.701816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.351798</td>\n",
       "      <td>0.351798</td>\n",
       "      <td>0.593126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.379279</td>\n",
       "      <td>0.379279</td>\n",
       "      <td>0.615856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.325712</td>\n",
       "      <td>0.325712</td>\n",
       "      <td>0.570712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.519611</td>\n",
       "      <td>0.519611</td>\n",
       "      <td>0.720840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.621932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.470744</td>\n",
       "      <td>0.470744</td>\n",
       "      <td>0.686108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-8/checkpoint-126\n",
      "result {'eval_loss': 0.324730783700943, 'eval_mse': 0.3247307538986206, 'eval_rmse': 0.5698515176773071, 'eval_runtime': 0.7627, 'eval_samples_per_second': 309.42, 'epoch': 15.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 9: [7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:24uwvh3x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2466<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_140152-24uwvh3x/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_140152-24uwvh3x/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.046</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>315</td></tr><tr><td>_runtime</td><td>213</td></tr><tr><td>_timestamp</td><td>1622210730</td></tr><tr><td>_step</td><td>32</td></tr><tr><td>eval/loss</td><td>0.32473</td></tr><tr><td>eval/mse</td><td>0.32473</td></tr><tr><td>eval/rmse</td><td>0.56985</td></tr><tr><td>eval/runtime</td><td>0.7627</td></tr><tr><td>eval/samples_per_second</td><td>309.42</td></tr><tr><td>train/train_runtime</td><td>207.1538</td></tr><tr><td>train/train_samples_per_second</td><td>3.041</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>▃▂▂▁█▁▆▃▅▂▂▁▆▃▅▁</td></tr><tr><td>eval/mse</td><td>▃▂▂▁█▁▆▃▅▂▂▁▆▃▅▁</td></tr><tr><td>eval/rmse</td><td>▃▂▂▁█▁▆▃▆▂▃▁▆▃▅▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▇▂▁▁█▁▆▁▁▂▃</td></tr><tr><td>eval/samples_per_second</td><td>█████▂▇██▁█▃██▇▆</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">happy-grass-13</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/24uwvh3x\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/24uwvh3x</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:24uwvh3x). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dry-gorge-14</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/wp03cwcn\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/wp03cwcn</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_140539-wp03cwcn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/630 06:07 < 00:56, 1.48 it/s, Epoch 26/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.530442</td>\n",
       "      <td>0.530442</td>\n",
       "      <td>0.728315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.523266</td>\n",
       "      <td>0.523266</td>\n",
       "      <td>0.723371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.629280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.306350</td>\n",
       "      <td>0.306350</td>\n",
       "      <td>0.553489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.322544</td>\n",
       "      <td>0.322544</td>\n",
       "      <td>0.567930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.407864</td>\n",
       "      <td>0.407864</td>\n",
       "      <td>0.638642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.336343</td>\n",
       "      <td>0.336343</td>\n",
       "      <td>0.579951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.303326</td>\n",
       "      <td>0.303326</td>\n",
       "      <td>0.550750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.420281</td>\n",
       "      <td>0.420281</td>\n",
       "      <td>0.648291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.340809</td>\n",
       "      <td>0.340808</td>\n",
       "      <td>0.583788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.333349</td>\n",
       "      <td>0.333349</td>\n",
       "      <td>0.577364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.357474</td>\n",
       "      <td>0.357474</td>\n",
       "      <td>0.597891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.391875</td>\n",
       "      <td>0.391875</td>\n",
       "      <td>0.625999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.344561</td>\n",
       "      <td>0.344561</td>\n",
       "      <td>0.586993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.408372</td>\n",
       "      <td>0.408372</td>\n",
       "      <td>0.639040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.305926</td>\n",
       "      <td>0.305926</td>\n",
       "      <td>0.553106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.303278</td>\n",
       "      <td>0.550707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.442590</td>\n",
       "      <td>0.442590</td>\n",
       "      <td>0.665275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.375603</td>\n",
       "      <td>0.375603</td>\n",
       "      <td>0.612865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.351934</td>\n",
       "      <td>0.351934</td>\n",
       "      <td>0.593240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.348543</td>\n",
       "      <td>0.348543</td>\n",
       "      <td>0.590375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.359632</td>\n",
       "      <td>0.359632</td>\n",
       "      <td>0.599693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.344773</td>\n",
       "      <td>0.344773</td>\n",
       "      <td>0.587174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.353659</td>\n",
       "      <td>0.353659</td>\n",
       "      <td>0.594692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.381590</td>\n",
       "      <td>0.381590</td>\n",
       "      <td>0.617730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.360353</td>\n",
       "      <td>0.360353</td>\n",
       "      <td>0.600294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-9/checkpoint-357\n",
      "result {'eval_loss': 0.3032778203487396, 'eval_mse': 0.30327773094177246, 'eval_rmse': 0.5507065653800964, 'eval_runtime': 0.7603, 'eval_samples_per_second': 310.388, 'epoch': 26.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 10: [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wp03cwcn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2514<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_140539-wp03cwcn/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_140539-wp03cwcn/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0245</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/epoch</td><td>26.0</td></tr><tr><td>train/global_step</td><td>546</td></tr><tr><td>_runtime</td><td>376</td></tr><tr><td>_timestamp</td><td>1622211119</td></tr><tr><td>_step</td><td>54</td></tr><tr><td>eval/loss</td><td>0.30328</td></tr><tr><td>eval/mse</td><td>0.30328</td></tr><tr><td>eval/rmse</td><td>0.55071</td></tr><tr><td>eval/runtime</td><td>0.7603</td></tr><tr><td>eval/samples_per_second</td><td>310.388</td></tr><tr><td>train/train_runtime</td><td>368.4727</td></tr><tr><td>train/train_samples_per_second</td><td>1.71</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval/loss</td><td>██▄▁▂▄▂▁▅▂▂▃▄▂▄▁▁▅▃▂▂▃▂▃▃▃▁</td></tr><tr><td>eval/mse</td><td>██▄▁▂▄▂▁▅▂▂▃▄▂▄▁▁▅▃▂▂▃▂▃▃▃▁</td></tr><tr><td>eval/rmse</td><td>██▄▁▂▄▂▁▅▂▂▃▄▂▄▁▁▆▃▃▃▃▂▃▄▃▁</td></tr><tr><td>eval/runtime</td><td>▁▂▁▂▂▁▁▂▁▂█▂▂█▂▁▂▇▅█▂█▂▃▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▇█▇▇██▇▇▇▁▇▇▁▇█▇▂▄▁▇▁▇▆▇▇▇</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">dry-gorge-14</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/wp03cwcn\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/wp03cwcn</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:wp03cwcn). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fine-glitter-15</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/2wr5k57n\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/2wr5k57n</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_141208-2wr5k57n</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='273' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/630 03:05 < 04:04, 1.46 it/s, Epoch 13/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>0.489412</td>\n",
       "      <td>0.489412</td>\n",
       "      <td>0.699580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>0.432640</td>\n",
       "      <td>0.432640</td>\n",
       "      <td>0.657753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.368547</td>\n",
       "      <td>0.368547</td>\n",
       "      <td>0.607081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.282948</td>\n",
       "      <td>0.282948</td>\n",
       "      <td>0.531929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.676679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.369432</td>\n",
       "      <td>0.369432</td>\n",
       "      <td>0.607809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.456361</td>\n",
       "      <td>0.456361</td>\n",
       "      <td>0.675545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.152900</td>\n",
       "      <td>0.318036</td>\n",
       "      <td>0.318036</td>\n",
       "      <td>0.563947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.335528</td>\n",
       "      <td>0.335528</td>\n",
       "      <td>0.579248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.439288</td>\n",
       "      <td>0.439288</td>\n",
       "      <td>0.662788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.362011</td>\n",
       "      <td>0.362011</td>\n",
       "      <td>0.601673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.384284</td>\n",
       "      <td>0.384284</td>\n",
       "      <td>0.619907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.447470</td>\n",
       "      <td>0.447470</td>\n",
       "      <td>0.668932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-10/checkpoint-84\n",
      "result {'eval_loss': 0.28294819593429565, 'eval_mse': 0.2829481363296509, 'eval_rmse': 0.5319287180900574, 'eval_runtime': 0.7647, 'eval_samples_per_second': 308.6, 'epoch': 13.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "train_bins 11: [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2wr5k57n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2562<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_141208-2wr5k57n/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/commonlit/notebooks/wandb/run-20210528_141208-2wr5k57n/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>0.0672</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/epoch</td><td>13.0</td></tr><tr><td>train/global_step</td><td>273</td></tr><tr><td>_runtime</td><td>192</td></tr><tr><td>_timestamp</td><td>1622211325</td></tr><tr><td>_step</td><td>28</td></tr><tr><td>eval/loss</td><td>0.28295</td></tr><tr><td>eval/mse</td><td>0.28295</td></tr><tr><td>eval/rmse</td><td>0.53193</td></tr><tr><td>eval/runtime</td><td>0.7647</td></tr><tr><td>eval/samples_per_second</td><td>308.6</td></tr><tr><td>train/train_runtime</td><td>185.9635</td></tr><tr><td>train/train_samples_per_second</td><td>3.388</td></tr><tr><td>train/total_flos</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train/loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>eval/loss</td><td>█▆▄▁▇▄▇▂▃▆▄▄▇▁</td></tr><tr><td>eval/mse</td><td>█▆▄▁▇▄▇▂▃▆▄▄▇▁</td></tr><tr><td>eval/rmse</td><td>█▆▄▁▇▄▇▂▃▆▄▅▇▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▃█▂▂▂▂▂▂▂▂▄</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▆▁▇▇▇▇▇▇▇▇▅</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fine-glitter-15</strong>: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/2wr5k57n\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/2wr5k57n</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2wr5k57n). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">astral-morning-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/gilf/commonlit_distilroberta/runs/36201cym\" target=\"_blank\">https://wandb.ai/gilf/commonlit_distilroberta/runs/36201cym</a><br/>\n",
       "                Run data is saved locally in <code>/home/commonlit/notebooks/wandb/run-20210528_141534-36201cym</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='399' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [399/630 04:30 < 02:37, 1.47 it/s, Epoch 19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.033600</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>0.771849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.459253</td>\n",
       "      <td>0.459253</td>\n",
       "      <td>0.677682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.364113</td>\n",
       "      <td>0.364113</td>\n",
       "      <td>0.603417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.365634</td>\n",
       "      <td>0.365634</td>\n",
       "      <td>0.604677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.378458</td>\n",
       "      <td>0.378458</td>\n",
       "      <td>0.615189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.370322</td>\n",
       "      <td>0.370322</td>\n",
       "      <td>0.608541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.388141</td>\n",
       "      <td>0.388141</td>\n",
       "      <td>0.623010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.360925</td>\n",
       "      <td>0.360925</td>\n",
       "      <td>0.600771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.502921</td>\n",
       "      <td>0.502921</td>\n",
       "      <td>0.709169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.339760</td>\n",
       "      <td>0.339760</td>\n",
       "      <td>0.582889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.392730</td>\n",
       "      <td>0.392730</td>\n",
       "      <td>0.626682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.451206</td>\n",
       "      <td>0.451206</td>\n",
       "      <td>0.671719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.605889</td>\n",
       "      <td>0.605889</td>\n",
       "      <td>0.778389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.397055</td>\n",
       "      <td>0.397055</td>\n",
       "      <td>0.630123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.382573</td>\n",
       "      <td>0.382573</td>\n",
       "      <td>0.618525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.466629</td>\n",
       "      <td>0.466629</td>\n",
       "      <td>0.683102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.372926</td>\n",
       "      <td>0.372926</td>\n",
       "      <td>0.610677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.486803</td>\n",
       "      <td>0.486803</td>\n",
       "      <td>0.697713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.409669</td>\n",
       "      <td>0.409669</td>\n",
       "      <td>0.640054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args.output_dir /home/commonlit/models/distilroberta-11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint /home/commonlit/models/distilroberta-11/checkpoint-210\n",
      "result {'eval_loss': 0.33976003527641296, 'eval_mse': 0.3397601544857025, 'eval_rmse': 0.5828894972801208, 'eval_runtime': 0.7715, 'eval_samples_per_second': 305.892, 'epoch': 19.0, 'eval_mem_cpu_alloc_delta': 0, 'eval_mem_gpu_alloc_delta': 0, 'eval_mem_cpu_peaked_delta': 0, 'eval_mem_gpu_peaked_delta': 1510867456}\n",
      "CPU times: user 1h 5min 27s, sys: 16min 29s, total: 1h 21min 57s\n",
      "Wall time: 49min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "bin_step = 1\n",
    "bestmodels = []\n",
    "eval_rmses = []\n",
    "for i in range(0, num_bins, bin_step):\n",
    "    train_bins = bin_list[i:i+bin_step]\n",
    "    print('train_bins', f'{i}: {train_bins}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_transformers_model)\n",
    "    train_text, train_target, valid_text, valid_target = create_split([i])\n",
    "    train_ds, valid_ds = create_train_valid_ds(tokenizer, train_text, train_target, valid_text, valid_target)\n",
    "    training_args = create_training_args(i)\n",
    "    model = CommonLitModel()\n",
    "    wandb.init(project=f\"commonlit_{cfg.model_name}\")\n",
    "    trainer = CommonLitTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=9)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    print('training_args.output_dir', training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    result = trainer.evaluate()\n",
    "    bestmodels.append(trainer.state.best_model_checkpoint)\n",
    "    print('best_model_checkpoint', trainer.state.best_model_checkpoint)\n",
    "    print('result', result)\n",
    "    eval_rmses.append(result['eval_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4993d2b0-3c68-4241-b15d-c51e10ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_FOLDER = MODELS_PATH/cfg.model_name/'best'\n",
    "!rm -rf {BEST_MODEL_FOLDER}\n",
    "!mkdir -p {BEST_MODEL_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85f114e7-7e75-43d8-8c4c-6889f6393b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/commonlit/models/distilroberta-11/checkpoint-210/pytorch_model.bin is missing\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for i, best_model in enumerate(bestmodels):\n",
    "    best_model_file = f'{best_model}/pytorch_model.bin'\n",
    "    if Path(best_model_file).exists():\n",
    "        copyfile(best_model_file, f'{BEST_MODEL_FOLDER}/{i}_pytorch_model.bin')\n",
    "        tokenizer_path = Path(BEST_MODEL_FOLDER/f'tokenizer-{i}')\n",
    "        tokenizer_path.mkdir(parents=True, exist_ok=True)\n",
    "        assert tokenizer_path.exists()\n",
    "\n",
    "        tokenizer_json = Path(f'{MODELS_PATH/cfg.model_name}-{i}/tokenizer.json')\n",
    "        assert tokenizer_json.exists()\n",
    "        copyfile(tokenizer_json, tokenizer_path/'tokenizer.json')\n",
    "\n",
    "        vocab_txt = Path(f'{MODELS_PATH/cfg.model_name}-{i}/vocab.json')\n",
    "        assert vocab_txt.exists()\n",
    "        copyfile(vocab_txt, tokenizer_path/'vocab.json')\n",
    "\n",
    "        config_json = Path(f'{MODELS_PATH/cfg.model_name}-{i}/config.json')\n",
    "        assert config_json.exists()\n",
    "        copyfile(config_json, tokenizer_path/'config.json')\n",
    "    else:\n",
    "        print(f'{best_model_file} is missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc422f71-d671-4eca-82f4-0dd059b1200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/commonlit/models/distilroberta/best_models.zip'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(MODELS_PATH/cfg.model_name/'best_models', 'zip', BEST_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5290ec66-8db4-4047-8fea-8d8f609c3d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mean best RSME losses', 0.5237091208497683)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Mean best RSME losses', np.array(eval_rmses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffe0ba-8412-4616-a0a4-78c0b4552f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
